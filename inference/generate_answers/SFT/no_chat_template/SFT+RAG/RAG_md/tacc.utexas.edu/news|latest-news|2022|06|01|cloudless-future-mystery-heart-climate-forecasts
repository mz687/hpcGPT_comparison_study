1. [Home](/)
2. [News](/news/)
3. Latest News

*List*

*Grid*

# A Cloudless Future? The Mystery at the Heart of Climate Forecasts

## New computational approaches help researchers include cloud physics in global models, addressing long-standing questions

* by[Aaron Dubrow](/news/latest-news/author/adubrow/)

* May 31, 2022

* [Feature Story](/news/latest-news/category/feature-story/)

* [Frontera](/news/latest-news/tag/frontera/)

share this:

Shallow clouds formed by fine-scale eddies as observed in nature. Researchers are using advanced computing to add higher-resolution cloud dynamics into global simulations.

We hear a lot about how climate change will change the land, sea, and ice. But how will it affect clouds?

"Low clouds could dry up and shrink like the ice sheets," says [Michael Pritchard](https://sites.ps.uci.edu/pritchard/), professor of Earth System science at UC Irvine. "Or they could thicken and become more reflective."

These two scenarios would result in very different future climates. And that, Pritchard says, is part of the problem.

"If you ask two different climate models what the future will be like when we add a lot more CO2, you get two very different answers. And the key reason for this is the way clouds are included in climate models."

No one denies that clouds and aerosols — bits of soot and dust that nucleate cloud droplets — are an important part of the climate equation. The problem is these phenomena occur on a length- and time-scale that today's models can't come close to reproducing. They are therefore included in models through a variety of approximations.

Analyses of global climate models consistently show that clouds constitute the biggest source of uncertainty and instability.

### Re-Tooling Community Codes

Whereas the most advanced U.S. global climate model are struggling to approach 4 kilometer global resolution, Pritchard estimates that models need a resolution of at least 100 meters to capture the fine-scale turbulent eddies that form shallow cloud systems — 40 times more resolved in every direction. It could take until 2060, according to Moore's law, before the computing power is available to capture this level of detail.

Pritchard is working to fix this glaring gap by breaking the climate modeling problem into two parts: a coarse-grained, lower-resolution (100km) planetary model and many small patches with 100 to 200 meter resolution. The two simulations run independently and then exchange data every 30 minutes to make sure that neither simulation goes off-track nor becomes unrealistic.

His team's reported the results of these efforts in the *[Journal of Advances in Modeling Earth Systems](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021MS002841)* in April 2022. The research is supported by grants from the National Science Foundation (NSF) and the Department of Energy (DOE).

This climate simulation method, called a ‘Multiscale Modeling Framework (MMF),' has been around since 2000 and has long been an option within the [Community Earth System Model (CESM)](https://www.cesm.ucar.edu/) model, developed at the National Center for Atmospheric Research. The idea has lately been enjoying a renaissance at the Department of Energy, where researchers from the [Energy Exascale Earth System Model (E3SM)](https://e3sm.org/) have been pushing it to new computational frontiers as part of the [Exascale Computing Project](https://www.exascaleproject.org/). Pritchard's co-author Walter Hannah from the Lawrence Livermore national laboratory helps lead this effort.

"The model does an end-run around the hardest problem – whole planet modeling," Pritchard explained. "It has thousands of little micromodels that capture things like realistic shallow cloud formation that only emerge in very high resolution."

Michael Pritchard, professor of Earth System science at UC Irvine, studies how the planetary water cycle and climate work, and how it may change in the future, focusing on cloud physics and moist convection processes.

"The Multiscale Modeling Framework approach is also ideal for DOE's upcoming GPU-based exascale computers," said Mark Taylor, Chief Computational Scientist for the DOE's Energy Exascale Earth System Model (E3SM) project and a research scientist at Sandia National Laboratories. "Each GPU has the horsepower to run hundreds of the micromodels while still matching the throughput of the coarse-grained lower-resolution planetary model."

Pritchard's research and new approach is made possible in part by the NSF-funded [Frontera](//tacc.utexas.edu/systems/frontera/) supercomputer at the Texas Advanced Computing Center (TACC). The fastest university supercomputer in the world, Pritchard can run his models on Frontera at a time- and length-scale accessible only on a handful of systems in the U.S. and test their potential for cloud modeling.

"We developed a way for a supercomputer to best split up the work of simulating the cloud physics over different parts of the world that deserve different amounts of resolution… so that it runs much faster," the team wrote.

Simulating the atmosphere in this way provides Pritchard the resolution needed to capture the physical processes and turbulent eddies involved in cloud formation. The researchers showed that the multi-model approach did not produce unwanted side-effects even where patches using different cloud-resolving grid structures met.

"We were happy so see that the differences were small," he said. "This will provide new flexibility to all users of climate models who want to focus high resolution in different places."

Disentangling and reconnecting the various scales of the CESM model was one challenge that Pritchard's team overcame. Another involved re-programming the model so it can take advantage of the ever-increasing number of processors available on modern supercomputing systems.

Pritchard and his team — UCI postdoctoral scholar Liran Peng and University of Washington research scientist Peter Blossey — tackled this by breaking the inner domains of the CESM's embedded cloud models into smaller parts that could be solved in parallel using MPI, or message passing interface — a way of exchanging messages between multiple computers running a parallel program across distributed memory — and orchestrating these calculations to use many more processors.

"Doing so seems to already provide a four time speed up with great efficiency. That means, I can be four times as ambitious for my cloud-resolving models," he said. "I'm really optimistic that this dream of regionalizing and MPI decomposing is leading to a totally different landscape of what's possible."

### Machine Learning Clouds

Pritchard sees another promising approach in machine learning, which his team has been exploring since 2017. "I've been very provoked by how performantly a dumb sheet of neurons can reproduce these partial differential equations," Pritchard said.

Pritchard's research and new approach is made possible in part by the NSF-funded Frontera supercomputer at TACC. The fastest university supercomputer in the world, Pritchard can run his models on Frontera at a time and length-scale accessible only on a handful of systems in the U.S. and test their potential for cloud modeling.

In a [paper](https://arxiv.org/pdf/2112.08440.pdf) submitted last fall, Pritchard, lead author Tom Beucler, of UCI, and others describe a machine learning approach that successfully predicts atmospheric conditions even in climate regimes it was not trained on, where others have struggled to do so.

This ‘climate invariant' model incorporates physical knowledge of climate processes into the machine learning algorithms. Their study — which used Stampede2 at TACC, [Cheyenne](https://arc.ucar.edu/knowledge_base/70549542) at the National Center for Atmospheric Research, and [Expanse](https://www.sdsc.edu/services/hpc/expanse/) at the San Diego Supercomputer Center — showed the machine learning method can maintain high accuracy across a wide range of climates and geographies.

"If machine learning high-resolution cloud physics ever succeeded, it would transform everything about how we do climate simulations," Pritchard said. "I'm interested in seeing how reproducibly and reliably the machine learning approach can succeed in complex settings."

Pritchard is well-positioned to do so. He is on the Executive Committee of the [NSF Center for Learning the Earth with Artificial Intelligence and Physics](https://leap.columbia.edu/), or LEAP — a new Science and Technology Center, funded by NSF in 2021 directed by his long-time collaborator on this topic, Professor Pierre Gentine. LEAP brings together climate and data scientists to narrow the range of uncertainty in climate modeling, providing more precise and actionable climate projections that achieve immediate societal impact.

"All of the research I've done before is what I would call ‘throughput-limited.'" Pritchard said. "My job was to produce 10- to 100-year simulations. That constrained all my grid choices. However, if the goal is to produce short simulations to train machine learning models, that's a different landscape."

Pritchard hopes to soon use the results of his 50 meter embedded models to start building up a large training library. "It's a really nice dataset to do machine learning on."

But will AI mature fast enough? Time is of the essence to figure out the destiny of clouds.

"If those clouds shrink away, like ice sheets will, exposing darker surfaces, that will amplify global warming and all the hazards that come with it. But if they do the opposites of ice sheets and thicken up, which they could, that's less hazardous. Some have estimated this as a multi-trillion dollar issue for society. And this has been in question for a long time," Pritchard said.

Simulation by simulation, federally-funded supercomputers are helping Pritchard and others approach the answer to this critical question.

"I'm torn between genuine gratitude for the U.S. national computing infrastructure, which is so incredible at helping us develop and run climate models," Pritchard said, "and feeling that we need a Manhattan Project level of new federal funding and interagency coordination to actually solve this problem."

---

The research was funded by the National Science Foundation (NSF) Climate and Large-scale Dynamics program under grants AGS-1912134 and AGS-1912130; and under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. The research used computational resources from the Texas Advanced Computing Center and from the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by NSF Grant ACI-1548562.