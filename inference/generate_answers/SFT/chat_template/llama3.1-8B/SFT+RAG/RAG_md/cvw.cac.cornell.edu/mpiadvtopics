This roadmap discusses some of the more advanced functionality available in MPI-3. It will show you how to:

* overlay your data with datatypes to speed message passing
* arrange your MPI processes into virtual groupings and topologies
* use the MPI Tool Information Interface to get and sometimes set control variables and view performance variables

This roadmap does not cover MPI's Parallel I/O features. Although using parallel I/O requires some additional work up front, the pay-off can be well worth it as it offers a single (unified) file for visualization and pre/post-processing. As this is a large topic in itself, we refer the reader to the [Parallel I/O roadmap](/parallelio).

All exercises and examples are verified to work on [Stampede2](https://www.tacc.utexas.edu/systems/stampede2) and [Frontera](https://www.tacc.utexas.edu/systems/frontera).

This is the ***fourth of five*** related roadmaps in the Cornell Virtual Workshop that cover MPI. To see the other roadmaps available, please visit the complete [roadmaps list](/roadmaps).

##### Objectives

After you complete this roadmap, you should be able to:

* Incorporate more advanced MPI routines into individual code
* Define and use general datatypes
* Create logical groupings and layouts among MPI processes
* Inspect MPI-implementation variables related to control and performance

##### Prerequisites

* A basic knowledge of parallel programming and MPI. Information on these prerequisites can be found in other topics ([Parallel Programming Concepts and High-Performance Computing](/parallel/intro/index), [MPI Basics](/mpi/intro/index)).
* Ability to program in a high-level language such as [Fortran](/fintro/intro/index) or [C](/cintro/intro/index).
* The [MPI Collective Communications](/mpicc/intro/index) roadmap logically proceeds this roadmap, but it is not a prerequisite.

##### Requirements

Requirements include:

* A TACC account to login to Stampede2 or Frontera
* A TACC computation [allocation](/environment/allocation/index)