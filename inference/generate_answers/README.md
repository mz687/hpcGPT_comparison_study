# Inference Scripts

This directory contains scripts used for generating answers to question in the evaluation dataset for the purpose of evaluation.

## Directory Structure

Each subfolder corresponds to a specific method or experimental setup. Within each subfolder, you will find:

- `eval_chatbot.py`: The inference script used to evaluate the method.
- `*.slurm`: A SLURM job submission script for running the evaluation on a cluster environment.

|       Approach       |                       Description                       |                                                              Python script                                                             |                                                                      SLURM script                                                                     |
|:--------------------:|:-------------------------------------------------------:|:--------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------:|
|         Base         |            Base model: Llama-3.1-8B-Instruct            |           [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/BASE/eval_chatbot.py)           |                 [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/BASE/run_eval_BASE.slurm)                |
|          ICL         |           Base model with In-Context Learning           |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/ICL/eval_chatbot.py)           |               [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/ICL/run_eval_ICL_vista.slurm)              |
|          RAG         |      Base model with Retrieval Augmented Generation     |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/RAG/eval_chatbot.py)           |               [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/RAG/run_eval_RAG_vista.slurm)              |
|        ICL+RAG       |             Base model with both ICL and RAG            |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/RAG%2BICL/eval_chatbot.py)        |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/RAG%2BICL/run_eval_RAG%2BICL_vista.slurm)        |
|          SFT         |         Supervised fine-tuned Llama-3.1-8B model        |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT/eval_chatbot.py)           |                  [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT/run_eval_SFT.slurm)                 |
|        SFT+ICL       |          Fine-tuned Llama-3.1-8B model with ICL         |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BICL/eval_chatbot.py)        |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BICL/run_eval_SFT%2BICL.slurm)           |
|        SFT+RAG       |          Fine-tuned Llama-3.1-8B model with RAG         |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG/eval_chatbot.py)        |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG/run_eval_SFT%2BRAG.slurm)           |
|      SFT+ICL+RAG     |      Fine-tuned Llama-3.1-8B model with ICL and RAG     |      [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG%2BICL/eval_chatbot.py)     |      [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG%2BICL/run_eval_SFT%2BRAG%2BICL.slurm)     |
|     SFT-Instruct     |    Supervised fine-tuned Llama-3.1-8B-Instruct model    |       [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT-Instruct/eval_chatbot.py)       |             [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT-Instruct/run_eval_SFT.slurm)             |
|   SFT-Instruct+ICL   |     Fine-tuned Llama-3.1-8B-Instruct model with ICL     |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BICL/eval_chatbot.py)        |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BICL/run_eval_SFT%2BICL.slurm)           |
|   SFT-Instruct+RAG   |     Fine-tuned Llama-3.1-8B-Instruct model with RAG     |         [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG/eval_chatbot.py)        |            [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT%2BRAG/run_eval_SFT%2BRAG.slurm)           |
| SFT-Instruct+ICL+RAG | Fine-tuned Llama-3.1-8B-Instruct model with ICL and RAG | [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT-Instruct%2BRAG%2BICL/eval_chatbot.py) | [link](https://github.com/mz687/hpcGPT_comparison_study/blob/main/inference/generate_answers/SFT-Instruct%2BRAG%2BICL/run_eval_SFT%2BRAG%2BICL.slurm) |