ln: failed to create symbolic link '/home1/09308/zhengmk/python3-config': File exists
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
slurmstepd: error: _get_joules_task: can't get info from slurmd
WORLD_SIZE=64
MASTER_ADDR=c613-101.vista.tacc.utexas.edu
MASTER_PORT=12345
c613-101.vista.tacc.utexas.edu c613-102.vista.tacc.utexas.edu c613-111.vista.tacc.utexas.edu c613-112.vista.tacc.utexas.edu c613-121.vista.tacc.utexas.edu c613-122.vista.tacc.utexas.edu c613-131.vista.tacc.utexas.edu c613-132.vista.tacc.utexas.edu c613-141.vista.tacc.utexas.edu c613-142.vista.tacc.utexas.edu c613-151.vista.tacc.utexas.edu c613-152.vista.tacc.utexas.edu c619-001.vista.tacc.utexas.edu c619-002.vista.tacc.utexas.edu c619-011.vista.tacc.utexas.edu c619-012.vista.tacc.utexas.edu c619-021.vista.tacc.utexas.edu c619-022.vista.tacc.utexas.edu c619-031.vista.tacc.utexas.edu c619-032.vista.tacc.utexas.edu c619-041.vista.tacc.utexas.edu c621-052.vista.tacc.utexas.edu c621-061.vista.tacc.utexas.edu c621-062.vista.tacc.utexas.edu c621-071.vista.tacc.utexas.edu c621-072.vista.tacc.utexas.edu c621-081.vista.tacc.utexas.edu c621-082.vista.tacc.utexas.edu c621-091.vista.tacc.utexas.edu c621-092.vista.tacc.utexas.edu c621-101.vista.tacc.utexas.edu c621-102.vista.tacc.utexas.edu c621-111.vista.tacc.utexas.edu c621-112.vista.tacc.utexas.edu c621-121.vista.tacc.utexas.edu c621-122.vista.tacc.utexas.edu c621-131.vista.tacc.utexas.edu c621-132.vista.tacc.utexas.edu c621-141.vista.tacc.utexas.edu c621-142.vista.tacc.utexas.edu c621-151.vista.tacc.utexas.edu c621-152.vista.tacc.utexas.edu c622-001.vista.tacc.utexas.edu c622-002.vista.tacc.utexas.edu c622-011.vista.tacc.utexas.edu c622-012.vista.tacc.utexas.edu c622-021.vista.tacc.utexas.edu c622-022.vista.tacc.utexas.edu c622-031.vista.tacc.utexas.edu c622-032.vista.tacc.utexas.edu c622-041.vista.tacc.utexas.edu c622-042.vista.tacc.utexas.edu c622-051.vista.tacc.utexas.edu c622-052.vista.tacc.utexas.edu c622-061.vista.tacc.utexas.edu c622-062.vista.tacc.utexas.edu c622-071.vista.tacc.utexas.edu c622-072.vista.tacc.utexas.edu c622-081.vista.tacc.utexas.edu c622-082.vista.tacc.utexas.edu c622-091.vista.tacc.utexas.edu c622-092.vista.tacc.utexas.edu c622-101.vista.tacc.utexas.edu c622-102.vista.tacc.utexas.edu
NODE_RANK=0
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
The token `vista` has been saved to /scratch/09308/zhengmk/hf_home/stored_tokens
Your token has been saved to /scratch/09308/zhengmk/hf_home/token
Login successful.
The current active token is: `vista`
[2025-03-01 17:46:33,721] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-01 17:46:41,476] [INFO] [multinode_runner.py:81:get_cmd] Running on the following workers: c613-101.vista.tacc.utexas.edu,c613-102.vista.tacc.utexas.edu,c613-111.vista.tacc.utexas.edu,c613-112.vista.tacc.utexas.edu,c613-121.vista.tacc.utexas.edu,c613-122.vista.tacc.utexas.edu,c613-131.vista.tacc.utexas.edu,c613-132.vista.tacc.utexas.edu,c613-141.vista.tacc.utexas.edu,c613-142.vista.tacc.utexas.edu,c613-151.vista.tacc.utexas.edu,c613-152.vista.tacc.utexas.edu,c619-001.vista.tacc.utexas.edu,c619-002.vista.tacc.utexas.edu,c619-011.vista.tacc.utexas.edu,c619-012.vista.tacc.utexas.edu,c619-021.vista.tacc.utexas.edu,c619-022.vista.tacc.utexas.edu,c619-031.vista.tacc.utexas.edu,c619-032.vista.tacc.utexas.edu,c619-041.vista.tacc.utexas.edu,c621-052.vista.tacc.utexas.edu,c621-061.vista.tacc.utexas.edu,c621-062.vista.tacc.utexas.edu,c621-071.vista.tacc.utexas.edu,c621-072.vista.tacc.utexas.edu,c621-081.vista.tacc.utexas.edu,c621-082.vista.tacc.utexas.edu,c621-091.vista.tacc.utexas.edu,c621-092.vista.tacc.utexas.edu,c621-101.vista.tacc.utexas.edu,c621-102.vista.tacc.utexas.edu,c621-111.vista.tacc.utexas.edu,c621-112.vista.tacc.utexas.edu,c621-121.vista.tacc.utexas.edu,c621-122.vista.tacc.utexas.edu,c621-131.vista.tacc.utexas.edu,c621-132.vista.tacc.utexas.edu,c621-141.vista.tacc.utexas.edu,c621-142.vista.tacc.utexas.edu,c621-151.vista.tacc.utexas.edu,c621-152.vista.tacc.utexas.edu,c622-001.vista.tacc.utexas.edu,c622-002.vista.tacc.utexas.edu,c622-011.vista.tacc.utexas.edu,c622-012.vista.tacc.utexas.edu,c622-021.vista.tacc.utexas.edu,c622-022.vista.tacc.utexas.edu,c622-031.vista.tacc.utexas.edu,c622-032.vista.tacc.utexas.edu,c622-041.vista.tacc.utexas.edu,c622-042.vista.tacc.utexas.edu,c622-051.vista.tacc.utexas.edu,c622-052.vista.tacc.utexas.edu,c622-061.vista.tacc.utexas.edu,c622-062.vista.tacc.utexas.edu,c622-071.vista.tacc.utexas.edu,c622-072.vista.tacc.utexas.edu,c622-081.vista.tacc.utexas.edu,c622-082.vista.tacc.utexas.edu,c622-091.vista.tacc.utexas.edu,c622-092.vista.tacc.utexas.edu,c622-101.vista.tacc.utexas.edu,c622-102.vista.tacc.utexas.edu
[2025-03-01 17:46:41,476] [INFO] [runner.py:607:main] cmd = pdsh -S -f 1024 -w c613-101.vista.tacc.utexas.edu,c613-102.vista.tacc.utexas.edu,c613-111.vista.tacc.utexas.edu,c613-112.vista.tacc.utexas.edu,c613-121.vista.tacc.utexas.edu,c613-122.vista.tacc.utexas.edu,c613-131.vista.tacc.utexas.edu,c613-132.vista.tacc.utexas.edu,c613-141.vista.tacc.utexas.edu,c613-142.vista.tacc.utexas.edu,c613-151.vista.tacc.utexas.edu,c613-152.vista.tacc.utexas.edu,c619-001.vista.tacc.utexas.edu,c619-002.vista.tacc.utexas.edu,c619-011.vista.tacc.utexas.edu,c619-012.vista.tacc.utexas.edu,c619-021.vista.tacc.utexas.edu,c619-022.vista.tacc.utexas.edu,c619-031.vista.tacc.utexas.edu,c619-032.vista.tacc.utexas.edu,c619-041.vista.tacc.utexas.edu,c621-052.vista.tacc.utexas.edu,c621-061.vista.tacc.utexas.edu,c621-062.vista.tacc.utexas.edu,c621-071.vista.tacc.utexas.edu,c621-072.vista.tacc.utexas.edu,c621-081.vista.tacc.utexas.edu,c621-082.vista.tacc.utexas.edu,c621-091.vista.tacc.utexas.edu,c621-092.vista.tacc.utexas.edu,c621-101.vista.tacc.utexas.edu,c621-102.vista.tacc.utexas.edu,c621-111.vista.tacc.utexas.edu,c621-112.vista.tacc.utexas.edu,c621-121.vista.tacc.utexas.edu,c621-122.vista.tacc.utexas.edu,c621-131.vista.tacc.utexas.edu,c621-132.vista.tacc.utexas.edu,c621-141.vista.tacc.utexas.edu,c621-142.vista.tacc.utexas.edu,c621-151.vista.tacc.utexas.edu,c621-152.vista.tacc.utexas.edu,c622-001.vista.tacc.utexas.edu,c622-002.vista.tacc.utexas.edu,c622-011.vista.tacc.utexas.edu,c622-012.vista.tacc.utexas.edu,c622-021.vista.tacc.utexas.edu,c622-022.vista.tacc.utexas.edu,c622-031.vista.tacc.utexas.edu,c622-032.vista.tacc.utexas.edu,c622-041.vista.tacc.utexas.edu,c622-042.vista.tacc.utexas.edu,c622-051.vista.tacc.utexas.edu,c622-052.vista.tacc.utexas.edu,c622-061.vista.tacc.utexas.edu,c622-062.vista.tacc.utexas.edu,c622-071.vista.tacc.utexas.edu,c622-072.vista.tacc.utexas.edu,c622-081.vista.tacc.utexas.edu,c622-082.vista.tacc.utexas.edu,c622-091.vista.tacc.utexas.edu,c622-092.vista.tacc.utexas.edu,c622-101.vista.tacc.utexas.edu,c622-102.vista.tacc.utexas.edu export NCCL_DEBUG="INFO"; export PYTHONPATH="/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/scripts/llama3.1/55_slurm_logs";  cd /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/scripts/llama3.1/55_slurm_logs; /work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJjNjEzLTEwMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTEwMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTExMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTExMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTEyMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTEyMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTEzMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTEzMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTE0MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTE0Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTE1MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjEzLTE1Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAwMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAwMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAxMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAxMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAyMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAyMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAzMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTAzMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjE5LTA0MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA1Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA2MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA2Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA3MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA3Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA4MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA4Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA5MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTA5Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEwMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEwMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTExMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTExMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEyMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEyMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEzMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTEzMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTE0MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTE0Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTE1MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIxLTE1Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAwMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAwMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAxMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAxMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAyMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAyMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAzMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTAzMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA0MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA0Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA1MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA1Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA2MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA2Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA3MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA3Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA4MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA4Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA5MS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTA5Mi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTEwMS52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF0sICJjNjIyLTEwMi52aXN0YS50YWNjLnV0ZXhhcy5lZHUiOiBbMF19 --node_rank=%n --master_addr=c613-101.vista.tacc.utexas.edu --master_port=12345 --enable_each_rank_log=None /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py --data_path local/jsonfile --data_split 5,5,0 --model_name_or_path /scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --max_seq_len 2048 --learning_rate 9.65e-5 --weight_decay 0.1 --num_train_epochs 3 --gradient_accumulation_steps 16 --lr_scheduler_type cosine --num_warmup_steps 100 --seed 1234 --gradient_checkpointing --zero_stage 3 --deepspeed --dtype bf16 --print_loss --output_dir /scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55 --offload --offload_reference_model
c613-101: [2025-03-01 17:46:48,044] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-152: [2025-03-01 17:46:53,178] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-081: [2025-03-01 17:46:53,179] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-122: [2025-03-01 17:46:53,179] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-142: [2025-03-01 17:46:53,184] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-021: [2025-03-01 17:46:53,186] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-141: [2025-03-01 17:46:53,209] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-101: [2025-03-01 17:46:53,217] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-131: [2025-03-01 17:46:53,230] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-112: [2025-03-01 17:46:53,262] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-081: [2025-03-01 17:46:53,295] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-132: [2025-03-01 17:46:53,350] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-111: [2025-03-01 17:46:53,354] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-011: [2025-03-01 17:46:53,356] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-031: [2025-03-01 17:46:53,363] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-051: [2025-03-01 17:46:53,367] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-121: [2025-03-01 17:46:53,375] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-061: [2025-03-01 17:46:53,396] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-071: [2025-03-01 17:46:53,397] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-102: [2025-03-01 17:46:53,442] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-091: [2025-03-01 17:46:53,443] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-101: [2025-03-01 17:46:53,447] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-061: [2025-03-01 17:46:53,450] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-151: [2025-03-01 17:46:53,453] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-071: [2025-03-01 17:46:53,460] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-001: [2025-03-01 17:46:53,461] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-041: [2025-03-01 17:46:53,467] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-091: [2025-03-01 17:46:53,468] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-012: [2025-03-01 17:46:53,474] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-032: [2025-03-01 17:46:53,497] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-002: [2025-03-01 17:46:53,507] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-052: [2025-03-01 17:46:53,511] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-022: [2025-03-01 17:46:53,532] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-072: [2025-03-01 17:46:53,535] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-092: [2025-03-01 17:46:53,536] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-141: [2025-03-01 17:46:53,538] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-102: [2025-03-01 17:46:53,542] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-001: [2025-03-01 17:46:53,549] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-032: [2025-03-01 17:46:53,572] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-052: [2025-03-01 17:46:53,574] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-112: [2025-03-01 17:46:53,574] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-022: [2025-03-01 17:46:53,581] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-031: [2025-03-01 17:46:53,582] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-011: [2025-03-01 17:46:53,583] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-002: [2025-03-01 17:46:53,585] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-142: [2025-03-01 17:46:53,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-041: [2025-03-01 17:46:53,586] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-092: [2025-03-01 17:46:53,587] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-082: [2025-03-01 17:46:53,590] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-062: [2025-03-01 17:46:53,594] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-082: [2025-03-01 17:46:53,595] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-102: [2025-03-01 17:46:53,596] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-042: [2025-03-01 17:46:53,597] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-152: [2025-03-01 17:46:53,597] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-132: [2025-03-01 17:46:53,599] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-072: [2025-03-01 17:46:53,600] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-151: [2025-03-01 17:46:53,601] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-121: [2025-03-01 17:46:53,601] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-122: [2025-03-01 17:46:53,603] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-012: [2025-03-01 17:46:53,604] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-062: [2025-03-01 17:46:53,607] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-131: [2025-03-01 17:46:53,608] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-021: [2025-03-01 17:46:53,619] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-111: [2025-03-01 17:46:53,634] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=INFO
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=0
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:164:main] dist_world_size=64
c613-101: [2025-03-01 17:46:54,178] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-101: [2025-03-01 17:46:54,181] [INFO] [launch.py:256:main] process 387979 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-101: [2025-03-01 17:46:58,319] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-101: [2025-03-01 17:47:07,664] [INFO] [comm.py:652:init_distributed] cdb=None
c613-101: [2025-03-01 17:47:07,664] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
c613-152: [2025-03-01 17:47:09,268] [INFO] [launch.py:139:main] 11 NCCL_DEBUG=INFO
c613-152: [2025-03-01 17:47:09,268] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-152: [2025-03-01 17:47:09,269] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=11
c613-152: [2025-03-01 17:47:09,269] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-152: [2025-03-01 17:47:09,269] [INFO] [launch.py:164:main] dist_world_size=64
c613-152: [2025-03-01 17:47:09,269] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-152: [2025-03-01 17:47:09,272] [INFO] [launch.py:256:main] process 3770490 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-122: [2025-03-01 17:47:09,281] [INFO] [launch.py:139:main] 5 NCCL_DEBUG=INFO
c613-122: [2025-03-01 17:47:09,282] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-122: [2025-03-01 17:47:09,282] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=5
c613-122: [2025-03-01 17:47:09,282] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-122: [2025-03-01 17:47:09,282] [INFO] [launch.py:164:main] dist_world_size=64
c613-122: [2025-03-01 17:47:09,282] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-122: [2025-03-01 17:47:09,285] [INFO] [launch.py:256:main] process 1267677 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:139:main] 24 NCCL_DEBUG=INFO
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=24
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:164:main] dist_world_size=64
c621-071: [2025-03-01 17:47:09,285] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:139:main] 62 NCCL_DEBUG=INFO
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=62
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:164:main] dist_world_size=64
c622-101: [2025-03-01 17:47:09,286] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:139:main] 58 NCCL_DEBUG=INFO
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=58
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:164:main] dist_world_size=64
c622-081: [2025-03-01 17:47:09,287] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-111: [2025-03-01 17:47:09,288] [INFO] [launch.py:139:main] 32 NCCL_DEBUG=INFO
c621-111: [2025-03-01 17:47:09,288] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-111: [2025-03-01 17:47:09,288] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=32
c621-111: [2025-03-01 17:47:09,289] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-111: [2025-03-01 17:47:09,289] [INFO] [launch.py:164:main] dist_world_size=64
c621-111: [2025-03-01 17:47:09,289] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-071: [2025-03-01 17:47:09,289] [INFO] [launch.py:256:main] process 2842568 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-101: [2025-03-01 17:47:09,289] [INFO] [launch.py:256:main] process 1627370 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-081: [2025-03-01 17:47:09,291] [INFO] [launch.py:256:main] process 41435 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-111: [2025-03-01 17:47:09,292] [INFO] [launch.py:256:main] process 3868077 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:139:main] 7 NCCL_DEBUG=INFO
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=7
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:164:main] dist_world_size=64
c613-132: [2025-03-01 17:47:09,294] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-151: [2025-03-01 17:47:09,296] [INFO] [launch.py:139:main] 40 NCCL_DEBUG=INFO
c621-151: [2025-03-01 17:47:09,296] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-151: [2025-03-01 17:47:09,296] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=40
c621-151: [2025-03-01 17:47:09,296] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-151: [2025-03-01 17:47:09,296] [INFO] [launch.py:164:main] dist_world_size=64
c621-151: [2025-03-01 17:47:09,297] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-132: [2025-03-01 17:47:09,297] [INFO] [launch.py:256:main] process 990252 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-151: [2025-03-01 17:47:09,300] [INFO] [launch.py:256:main] process 336402 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:139:main] 3 NCCL_DEBUG=INFO
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=3
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:164:main] dist_world_size=64
c613-112: [2025-03-01 17:47:09,301] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-041: [2025-03-01 17:47:09,302] [INFO] [launch.py:139:main] 50 NCCL_DEBUG=INFO
c622-041: [2025-03-01 17:47:09,302] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-041: [2025-03-01 17:47:09,302] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=50
c622-041: [2025-03-01 17:47:09,302] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-041: [2025-03-01 17:47:09,303] [INFO] [launch.py:164:main] dist_world_size=64
c622-041: [2025-03-01 17:47:09,303] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-112: [2025-03-01 17:47:09,305] [INFO] [launch.py:256:main] process 1585580 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-041: [2025-03-01 17:47:09,306] [INFO] [launch.py:256:main] process 198407 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:139:main] 38 NCCL_DEBUG=INFO
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=38
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:164:main] dist_world_size=64
c621-141: [2025-03-01 17:47:09,315] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:139:main] 48 NCCL_DEBUG=INFO
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=48
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:164:main] dist_world_size=64
c622-031: [2025-03-01 17:47:09,317] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:139:main] 52 NCCL_DEBUG=INFO
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=52
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:164:main] dist_world_size=64
c622-051: [2025-03-01 17:47:09,318] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-141: [2025-03-01 17:47:09,318] [INFO] [launch.py:256:main] process 3859461 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:139:main] 44 NCCL_DEBUG=INFO
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=44
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:164:main] dist_world_size=64
c622-011: [2025-03-01 17:47:09,319] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-031: [2025-03-01 17:47:09,320] [INFO] [launch.py:256:main] process 3136042 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-051: [2025-03-01 17:47:09,322] [INFO] [launch.py:256:main] process 3538690 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-011: [2025-03-01 17:47:09,322] [INFO] [launch.py:256:main] process 710508 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-131: [2025-03-01 17:47:09,325] [INFO] [launch.py:139:main] 36 NCCL_DEBUG=INFO
c621-131: [2025-03-01 17:47:09,325] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-131: [2025-03-01 17:47:09,326] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=36
c621-131: [2025-03-01 17:47:09,326] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-131: [2025-03-01 17:47:09,326] [INFO] [launch.py:164:main] dist_world_size=64
c621-131: [2025-03-01 17:47:09,326] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-131: [2025-03-01 17:47:09,329] [INFO] [launch.py:256:main] process 2225085 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:139:main] 34 NCCL_DEBUG=INFO
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=34
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:164:main] dist_world_size=64
c621-121: [2025-03-01 17:47:09,339] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:139:main] 26 NCCL_DEBUG=INFO
c621-121: [2025-03-01 17:47:09,343] [INFO] [launch.py:256:main] process 1502414 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:139:main] 28 NCCL_DEBUG=INFO
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=26
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:164:main] dist_world_size=64
c621-081: [2025-03-01 17:47:09,343] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=28
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:164:main] dist_world_size=64
c621-091: [2025-03-01 17:47:09,343] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:139:main] 1 NCCL_DEBUG=INFO
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=1
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:164:main] dist_world_size=64
c613-102: [2025-03-01 17:47:09,346] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-081: [2025-03-01 17:47:09,346] [INFO] [launch.py:256:main] process 2075589 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-091: [2025-03-01 17:47:09,347] [INFO] [launch.py:256:main] process 102446 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:139:main] 22 NCCL_DEBUG=INFO
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=22
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:164:main] dist_world_size=64
c621-061: [2025-03-01 17:47:09,347] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-102: [2025-03-01 17:47:09,349] [INFO] [launch.py:256:main] process 1660081 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-142: [2025-03-01 17:47:09,349] [INFO] [launch.py:139:main] 9 NCCL_DEBUG=INFO
c613-142: [2025-03-01 17:47:09,350] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-142: [2025-03-01 17:47:09,350] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=9
c613-142: [2025-03-01 17:47:09,350] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-142: [2025-03-01 17:47:09,350] [INFO] [launch.py:164:main] dist_world_size=64
c613-142: [2025-03-01 17:47:09,350] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-061: [2025-03-01 17:47:09,350] [INFO] [launch.py:256:main] process 3120949 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:139:main] 30 NCCL_DEBUG=INFO
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=30
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:164:main] dist_world_size=64
c621-101: [2025-03-01 17:47:09,352] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-142: [2025-03-01 17:47:09,353] [INFO] [launch.py:256:main] process 3123258 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-101: [2025-03-01 17:47:09,355] [INFO] [launch.py:256:main] process 813656 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:139:main] 60 NCCL_DEBUG=INFO
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=60
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:164:main] dist_world_size=64
c622-091: [2025-03-01 17:47:09,358] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:139:main] 46 NCCL_DEBUG=INFO
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=46
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:164:main] dist_world_size=64
c622-021: [2025-03-01 17:47:09,361] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-091: [2025-03-01 17:47:09,361] [INFO] [launch.py:256:main] process 2780457 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:139:main] 56 NCCL_DEBUG=INFO
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=56
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:139:main] 54 NCCL_DEBUG=INFO
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:164:main] dist_world_size=64
c622-071: [2025-03-01 17:47:09,362] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=54
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:164:main] dist_world_size=64
c622-061: [2025-03-01 17:47:09,362] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-021: [2025-03-01 17:47:09,365] [INFO] [launch.py:256:main] process 1141571 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-071: [2025-03-01 17:47:09,365] [INFO] [launch.py:256:main] process 494914 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-061: [2025-03-01 17:47:09,366] [INFO] [launch.py:256:main] process 1156840 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:139:main] 42 NCCL_DEBUG=INFO
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=42
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:164:main] dist_world_size=64
c622-001: [2025-03-01 17:47:09,366] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-001: [2025-03-01 17:47:09,370] [INFO] [launch.py:256:main] process 3806195 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-012: [2025-03-01 17:47:09,371] [INFO] [launch.py:139:main] 15 NCCL_DEBUG=INFO
c619-012: [2025-03-01 17:47:09,372] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-012: [2025-03-01 17:47:09,372] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=15
c619-012: [2025-03-01 17:47:09,372] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-012: [2025-03-01 17:47:09,372] [INFO] [launch.py:164:main] dist_world_size=64
c619-012: [2025-03-01 17:47:09,372] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-012: [2025-03-01 17:47:09,375] [INFO] [launch.py:256:main] process 246412 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-002: [2025-03-01 17:47:09,476] [INFO] [launch.py:139:main] 13 NCCL_DEBUG=INFO
c619-002: [2025-03-01 17:47:09,477] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-002: [2025-03-01 17:47:09,477] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=13
c619-002: [2025-03-01 17:47:09,477] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-002: [2025-03-01 17:47:09,477] [INFO] [launch.py:164:main] dist_world_size=64
c619-002: [2025-03-01 17:47:09,477] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:139:main] 19 NCCL_DEBUG=INFO
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=19
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:164:main] dist_world_size=64
c619-032: [2025-03-01 17:47:09,479] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-002: [2025-03-01 17:47:09,480] [INFO] [launch.py:256:main] process 83791 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-032: [2025-03-01 17:47:09,482] [INFO] [launch.py:256:main] process 3546706 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-022: [2025-03-01 17:47:09,547] [INFO] [launch.py:139:main] 17 NCCL_DEBUG=INFO
c619-022: [2025-03-01 17:47:09,547] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-022: [2025-03-01 17:47:09,548] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=17
c619-022: [2025-03-01 17:47:09,548] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-022: [2025-03-01 17:47:09,548] [INFO] [launch.py:164:main] dist_world_size=64
c619-022: [2025-03-01 17:47:09,548] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-022: [2025-03-01 17:47:09,551] [INFO] [launch.py:256:main] process 804214 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-092: [2025-03-01 17:47:09,968] [INFO] [launch.py:139:main] 29 NCCL_DEBUG=INFO
c621-092: [2025-03-01 17:47:09,968] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-092: [2025-03-01 17:47:09,968] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=29
c621-092: [2025-03-01 17:47:09,968] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-092: [2025-03-01 17:47:09,968] [INFO] [launch.py:164:main] dist_world_size=64
c621-092: [2025-03-01 17:47:09,969] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:139:main] 53 NCCL_DEBUG=INFO
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=53
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:164:main] dist_world_size=64
c622-052: [2025-03-01 17:47:09,970] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-092: [2025-03-01 17:47:09,972] [INFO] [launch.py:256:main] process 1627105 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-052: [2025-03-01 17:47:09,974] [INFO] [launch.py:256:main] process 990541 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:139:main] 31 NCCL_DEBUG=INFO
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=31
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:164:main] dist_world_size=64
c621-102: [2025-03-01 17:47:09,982] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-102: [2025-03-01 17:47:09,985] [INFO] [launch.py:256:main] process 2281097 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:139:main] 4 NCCL_DEBUG=INFO
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=4
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:164:main] dist_world_size=64
c613-121: [2025-03-01 17:47:09,992] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-121: [2025-03-01 17:47:09,995] [INFO] [launch.py:256:main] process 904347 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:139:main] 55 NCCL_DEBUG=INFO
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=55
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:164:main] dist_world_size=64
c622-062: [2025-03-01 17:47:10,003] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-062: [2025-03-01 17:47:10,006] [INFO] [launch.py:256:main] process 2304142 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-141: [2025-03-01 17:47:10,018] [INFO] [launch.py:139:main] 8 NCCL_DEBUG=INFO
c613-141: [2025-03-01 17:47:10,019] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-141: [2025-03-01 17:47:10,019] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=8
c613-141: [2025-03-01 17:47:10,019] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-141: [2025-03-01 17:47:10,019] [INFO] [launch.py:164:main] dist_world_size=64
c613-141: [2025-03-01 17:47:10,019] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-141: [2025-03-01 17:47:10,022] [INFO] [launch.py:256:main] process 2104776 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-032: [2025-03-01 17:47:10,028] [INFO] [launch.py:139:main] 49 NCCL_DEBUG=INFO
c622-032: [2025-03-01 17:47:10,028] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-032: [2025-03-01 17:47:10,029] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=49
c622-032: [2025-03-01 17:47:10,029] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-032: [2025-03-01 17:47:10,029] [INFO] [launch.py:164:main] dist_world_size=64
c622-032: [2025-03-01 17:47:10,029] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-032: [2025-03-01 17:47:10,032] [INFO] [launch.py:256:main] process 1587523 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:139:main] 61 NCCL_DEBUG=INFO
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=61
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:164:main] dist_world_size=64
c622-092: [2025-03-01 17:47:10,033] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-092: [2025-03-01 17:47:10,037] [INFO] [launch.py:256:main] process 418610 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:139:main] 39 NCCL_DEBUG=INFO
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=39
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:164:main] dist_world_size=64
c621-142: [2025-03-01 17:47:10,050] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-142: [2025-03-01 17:47:10,053] [INFO] [launch.py:256:main] process 600426 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:139:main] 43 NCCL_DEBUG=INFO
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=43
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:164:main] dist_world_size=64
c622-002: [2025-03-01 17:47:10,055] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:139:main] 25 NCCL_DEBUG=INFO
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=25
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:164:main] dist_world_size=64
c621-072: [2025-03-01 17:47:10,056] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:139:main] 33 NCCL_DEBUG=INFO
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=33
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:164:main] dist_world_size=64
c621-112: [2025-03-01 17:47:10,057] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-002: [2025-03-01 17:47:10,058] [INFO] [launch.py:256:main] process 1377821 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-072: [2025-03-01 17:47:10,059] [INFO] [launch.py:256:main] process 3048145 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-112: [2025-03-01 17:47:10,060] [INFO] [launch.py:256:main] process 411646 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:139:main] 18 NCCL_DEBUG=INFO
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=18
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:164:main] dist_world_size=64
c619-031: [2025-03-01 17:47:10,072] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-062: [2025-03-01 17:47:10,073] [INFO] [launch.py:139:main] 23 NCCL_DEBUG=INFO
c621-062: [2025-03-01 17:47:10,073] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-062: [2025-03-01 17:47:10,074] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=23
c621-062: [2025-03-01 17:47:10,074] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-062: [2025-03-01 17:47:10,074] [INFO] [launch.py:164:main] dist_world_size=64
c621-062: [2025-03-01 17:47:10,074] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:139:main] 27 NCCL_DEBUG=INFO
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=27
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:164:main] dist_world_size=64
c621-082: [2025-03-01 17:47:10,074] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-031: [2025-03-01 17:47:10,075] [INFO] [launch.py:256:main] process 361171 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-072: [2025-03-01 17:47:10,075] [INFO] [launch.py:139:main] 57 NCCL_DEBUG=INFO
c622-072: [2025-03-01 17:47:10,075] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-072: [2025-03-01 17:47:10,075] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=57
c622-072: [2025-03-01 17:47:10,076] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-072: [2025-03-01 17:47:10,076] [INFO] [launch.py:164:main] dist_world_size=64
c622-072: [2025-03-01 17:47:10,076] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:139:main] 21 NCCL_DEBUG=INFO
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=21
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:164:main] dist_world_size=64
c621-052: [2025-03-01 17:47:10,076] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-062: [2025-03-01 17:47:10,077] [INFO] [launch.py:256:main] process 1471760 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-041: [2025-03-01 17:47:10,077] [INFO] [launch.py:139:main] 20 NCCL_DEBUG=INFO
c621-082: [2025-03-01 17:47:10,078] [INFO] [launch.py:256:main] process 1003419 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-041: [2025-03-01 17:47:10,077] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-041: [2025-03-01 17:47:10,077] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=20
c619-041: [2025-03-01 17:47:10,077] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-041: [2025-03-01 17:47:10,077] [INFO] [launch.py:164:main] dist_world_size=64
c619-041: [2025-03-01 17:47:10,078] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:139:main] 51 NCCL_DEBUG=INFO
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=51
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:164:main] dist_world_size=64
c622-042: [2025-03-01 17:47:10,079] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-072: [2025-03-01 17:47:10,079] [INFO] [launch.py:256:main] process 1688285 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-052: [2025-03-01 17:47:10,080] [INFO] [launch.py:256:main] process 855162 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-041: [2025-03-01 17:47:10,081] [INFO] [launch.py:256:main] process 15310 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-042: [2025-03-01 17:47:10,082] [INFO] [launch.py:256:main] process 654000 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:139:main] 37 NCCL_DEBUG=INFO
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=37
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:164:main] dist_world_size=64
c621-132: [2025-03-01 17:47:10,086] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:139:main] 35 NCCL_DEBUG=INFO
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=35
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:164:main] dist_world_size=64
c621-122: [2025-03-01 17:47:10,087] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c621-132: [2025-03-01 17:47:10,090] [INFO] [launch.py:256:main] process 518779 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:139:main] 12 NCCL_DEBUG=INFO
c621-122: [2025-03-01 17:47:10,090] [INFO] [launch.py:256:main] process 1561826 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=12
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:164:main] dist_world_size=64
c619-001: [2025-03-01 17:47:10,091] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-012: [2025-03-01 17:47:10,093] [INFO] [launch.py:139:main] 45 NCCL_DEBUG=INFO
c622-012: [2025-03-01 17:47:10,093] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-012: [2025-03-01 17:47:10,094] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=45
c622-012: [2025-03-01 17:47:10,094] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-012: [2025-03-01 17:47:10,094] [INFO] [launch.py:164:main] dist_world_size=64
c622-012: [2025-03-01 17:47:10,094] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-001: [2025-03-01 17:47:10,094] [INFO] [launch.py:256:main] process 2454729 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:139:main] 41 NCCL_DEBUG=INFO
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=41
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:164:main] dist_world_size=64
c621-152: [2025-03-01 17:47:10,095] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-012: [2025-03-01 17:47:10,097] [INFO] [launch.py:256:main] process 2903325 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c621-152: [2025-03-01 17:47:10,099] [INFO] [launch.py:256:main] process 1793414 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:139:main] 59 NCCL_DEBUG=INFO
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=59
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:164:main] dist_world_size=64
c622-082: [2025-03-01 17:47:10,099] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-011: [2025-03-01 17:47:10,102] [INFO] [launch.py:139:main] 14 NCCL_DEBUG=INFO
c619-011: [2025-03-01 17:47:10,102] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-011: [2025-03-01 17:47:10,102] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=14
c619-011: [2025-03-01 17:47:10,103] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-011: [2025-03-01 17:47:10,103] [INFO] [launch.py:164:main] dist_world_size=64
c619-011: [2025-03-01 17:47:10,103] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-082: [2025-03-01 17:47:10,103] [INFO] [launch.py:256:main] process 2786472 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-011: [2025-03-01 17:47:10,106] [INFO] [launch.py:256:main] process 219279 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-102: [2025-03-01 17:47:10,106] [INFO] [launch.py:139:main] 63 NCCL_DEBUG=INFO
c622-102: [2025-03-01 17:47:10,107] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-102: [2025-03-01 17:47:10,107] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=63
c622-102: [2025-03-01 17:47:10,107] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-102: [2025-03-01 17:47:10,107] [INFO] [launch.py:164:main] dist_world_size=64
c622-102: [2025-03-01 17:47:10,107] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-102: [2025-03-01 17:47:10,110] [INFO] [launch.py:256:main] process 2511218 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:139:main] 47 NCCL_DEBUG=INFO
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=47
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:164:main] dist_world_size=64
c622-022: [2025-03-01 17:47:10,113] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c622-022: [2025-03-01 17:47:10,116] [INFO] [launch.py:256:main] process 180805 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:139:main] 10 NCCL_DEBUG=INFO
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=10
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:164:main] dist_world_size=64
c613-151: [2025-03-01 17:47:10,132] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-151: [2025-03-01 17:47:10,135] [INFO] [launch.py:256:main] process 3397419 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-131: [2025-03-01 17:47:10,139] [INFO] [launch.py:139:main] 6 NCCL_DEBUG=INFO
c613-131: [2025-03-01 17:47:10,140] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-131: [2025-03-01 17:47:10,140] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=6
c613-131: [2025-03-01 17:47:10,140] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-131: [2025-03-01 17:47:10,140] [INFO] [launch.py:164:main] dist_world_size=64
c613-131: [2025-03-01 17:47:10,140] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-111: [2025-03-01 17:47:10,142] [INFO] [launch.py:139:main] 2 NCCL_DEBUG=INFO
c613-111: [2025-03-01 17:47:10,143] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c613-111: [2025-03-01 17:47:10,143] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=2
c613-111: [2025-03-01 17:47:10,143] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c613-111: [2025-03-01 17:47:10,143] [INFO] [launch.py:164:main] dist_world_size=64
c613-111: [2025-03-01 17:47:10,143] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c613-131: [2025-03-01 17:47:10,143] [INFO] [launch.py:256:main] process 931664 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-111: [2025-03-01 17:47:10,146] [INFO] [launch.py:256:main] process 640696 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c619-021: [2025-03-01 17:47:10,153] [INFO] [launch.py:139:main] 16 NCCL_DEBUG=INFO
c619-021: [2025-03-01 17:47:10,153] [INFO] [launch.py:146:main] WORLD INFO DICT: {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [0], 'c613-111.vista.tacc.utexas.edu': [0], 'c613-112.vista.tacc.utexas.edu': [0], 'c613-121.vista.tacc.utexas.edu': [0], 'c613-122.vista.tacc.utexas.edu': [0], 'c613-131.vista.tacc.utexas.edu': [0], 'c613-132.vista.tacc.utexas.edu': [0], 'c613-141.vista.tacc.utexas.edu': [0], 'c613-142.vista.tacc.utexas.edu': [0], 'c613-151.vista.tacc.utexas.edu': [0], 'c613-152.vista.tacc.utexas.edu': [0], 'c619-001.vista.tacc.utexas.edu': [0], 'c619-002.vista.tacc.utexas.edu': [0], 'c619-011.vista.tacc.utexas.edu': [0], 'c619-012.vista.tacc.utexas.edu': [0], 'c619-021.vista.tacc.utexas.edu': [0], 'c619-022.vista.tacc.utexas.edu': [0], 'c619-031.vista.tacc.utexas.edu': [0], 'c619-032.vista.tacc.utexas.edu': [0], 'c619-041.vista.tacc.utexas.edu': [0], 'c621-052.vista.tacc.utexas.edu': [0], 'c621-061.vista.tacc.utexas.edu': [0], 'c621-062.vista.tacc.utexas.edu': [0], 'c621-071.vista.tacc.utexas.edu': [0], 'c621-072.vista.tacc.utexas.edu': [0], 'c621-081.vista.tacc.utexas.edu': [0], 'c621-082.vista.tacc.utexas.edu': [0], 'c621-091.vista.tacc.utexas.edu': [0], 'c621-092.vista.tacc.utexas.edu': [0], 'c621-101.vista.tacc.utexas.edu': [0], 'c621-102.vista.tacc.utexas.edu': [0], 'c621-111.vista.tacc.utexas.edu': [0], 'c621-112.vista.tacc.utexas.edu': [0], 'c621-121.vista.tacc.utexas.edu': [0], 'c621-122.vista.tacc.utexas.edu': [0], 'c621-131.vista.tacc.utexas.edu': [0], 'c621-132.vista.tacc.utexas.edu': [0], 'c621-141.vista.tacc.utexas.edu': [0], 'c621-142.vista.tacc.utexas.edu': [0], 'c621-151.vista.tacc.utexas.edu': [0], 'c621-152.vista.tacc.utexas.edu': [0], 'c622-001.vista.tacc.utexas.edu': [0], 'c622-002.vista.tacc.utexas.edu': [0], 'c622-011.vista.tacc.utexas.edu': [0], 'c622-012.vista.tacc.utexas.edu': [0], 'c622-021.vista.tacc.utexas.edu': [0], 'c622-022.vista.tacc.utexas.edu': [0], 'c622-031.vista.tacc.utexas.edu': [0], 'c622-032.vista.tacc.utexas.edu': [0], 'c622-041.vista.tacc.utexas.edu': [0], 'c622-042.vista.tacc.utexas.edu': [0], 'c622-051.vista.tacc.utexas.edu': [0], 'c622-052.vista.tacc.utexas.edu': [0], 'c622-061.vista.tacc.utexas.edu': [0], 'c622-062.vista.tacc.utexas.edu': [0], 'c622-071.vista.tacc.utexas.edu': [0], 'c622-072.vista.tacc.utexas.edu': [0], 'c622-081.vista.tacc.utexas.edu': [0], 'c622-082.vista.tacc.utexas.edu': [0], 'c622-091.vista.tacc.utexas.edu': [0], 'c622-092.vista.tacc.utexas.edu': [0], 'c622-101.vista.tacc.utexas.edu': [0], 'c622-102.vista.tacc.utexas.edu': [0]}
c619-021: [2025-03-01 17:47:10,153] [INFO] [launch.py:152:main] nnodes=64, num_local_procs=1, node_rank=16
c619-021: [2025-03-01 17:47:10,154] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'c613-101.vista.tacc.utexas.edu': [0], 'c613-102.vista.tacc.utexas.edu': [1], 'c613-111.vista.tacc.utexas.edu': [2], 'c613-112.vista.tacc.utexas.edu': [3], 'c613-121.vista.tacc.utexas.edu': [4], 'c613-122.vista.tacc.utexas.edu': [5], 'c613-131.vista.tacc.utexas.edu': [6], 'c613-132.vista.tacc.utexas.edu': [7], 'c613-141.vista.tacc.utexas.edu': [8], 'c613-142.vista.tacc.utexas.edu': [9], 'c613-151.vista.tacc.utexas.edu': [10], 'c613-152.vista.tacc.utexas.edu': [11], 'c619-001.vista.tacc.utexas.edu': [12], 'c619-002.vista.tacc.utexas.edu': [13], 'c619-011.vista.tacc.utexas.edu': [14], 'c619-012.vista.tacc.utexas.edu': [15], 'c619-021.vista.tacc.utexas.edu': [16], 'c619-022.vista.tacc.utexas.edu': [17], 'c619-031.vista.tacc.utexas.edu': [18], 'c619-032.vista.tacc.utexas.edu': [19], 'c619-041.vista.tacc.utexas.edu': [20], 'c621-052.vista.tacc.utexas.edu': [21], 'c621-061.vista.tacc.utexas.edu': [22], 'c621-062.vista.tacc.utexas.edu': [23], 'c621-071.vista.tacc.utexas.edu': [24], 'c621-072.vista.tacc.utexas.edu': [25], 'c621-081.vista.tacc.utexas.edu': [26], 'c621-082.vista.tacc.utexas.edu': [27], 'c621-091.vista.tacc.utexas.edu': [28], 'c621-092.vista.tacc.utexas.edu': [29], 'c621-101.vista.tacc.utexas.edu': [30], 'c621-102.vista.tacc.utexas.edu': [31], 'c621-111.vista.tacc.utexas.edu': [32], 'c621-112.vista.tacc.utexas.edu': [33], 'c621-121.vista.tacc.utexas.edu': [34], 'c621-122.vista.tacc.utexas.edu': [35], 'c621-131.vista.tacc.utexas.edu': [36], 'c621-132.vista.tacc.utexas.edu': [37], 'c621-141.vista.tacc.utexas.edu': [38], 'c621-142.vista.tacc.utexas.edu': [39], 'c621-151.vista.tacc.utexas.edu': [40], 'c621-152.vista.tacc.utexas.edu': [41], 'c622-001.vista.tacc.utexas.edu': [42], 'c622-002.vista.tacc.utexas.edu': [43], 'c622-011.vista.tacc.utexas.edu': [44], 'c622-012.vista.tacc.utexas.edu': [45], 'c622-021.vista.tacc.utexas.edu': [46], 'c622-022.vista.tacc.utexas.edu': [47], 'c622-031.vista.tacc.utexas.edu': [48], 'c622-032.vista.tacc.utexas.edu': [49], 'c622-041.vista.tacc.utexas.edu': [50], 'c622-042.vista.tacc.utexas.edu': [51], 'c622-051.vista.tacc.utexas.edu': [52], 'c622-052.vista.tacc.utexas.edu': [53], 'c622-061.vista.tacc.utexas.edu': [54], 'c622-062.vista.tacc.utexas.edu': [55], 'c622-071.vista.tacc.utexas.edu': [56], 'c622-072.vista.tacc.utexas.edu': [57], 'c622-081.vista.tacc.utexas.edu': [58], 'c622-082.vista.tacc.utexas.edu': [59], 'c622-091.vista.tacc.utexas.edu': [60], 'c622-092.vista.tacc.utexas.edu': [61], 'c622-101.vista.tacc.utexas.edu': [62], 'c622-102.vista.tacc.utexas.edu': [63]})
c619-021: [2025-03-01 17:47:10,154] [INFO] [launch.py:164:main] dist_world_size=64
c619-021: [2025-03-01 17:47:10,154] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0
c619-021: [2025-03-01 17:47:10,157] [INFO] [launch.py:256:main] process 593675 spawned with command: ['/work/09308/zhengmk/python_vir_envs/range-topk-vista/bin/python3', '-u', '/work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/training/step2_DPO_finetuning/main.py', '--local_rank=0', '--data_path', 'local/jsonfile', '--data_split', '5,5,0', '--model_name_or_path', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_splitted_5_5/epoch_4/converted', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--max_seq_len', '2048', '--learning_rate', '9.65e-5', '--weight_decay', '0.1', '--num_train_epochs', '3', '--gradient_accumulation_steps', '16', '--lr_scheduler_type', 'cosine', '--num_warmup_steps', '100', '--seed', '1234', '--gradient_checkpointing', '--zero_stage', '3', '--deepspeed', '--dtype', 'bf16', '--print_loss', '--output_dir', '/scratch/09308/zhengmk/finetune_llama3.1_8b_Dahoas_hull_hh_rlhf_local_context_2048_hpcgpt_v3_dpo_55', '--offload', '--offload_reference_model']
c613-122: [2025-03-01 17:47:12,480] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-141: [2025-03-01 17:47:12,506] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-132: [2025-03-01 17:47:12,528] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-111: [2025-03-01 17:47:12,531] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-152: [2025-03-01 17:47:12,535] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-112: [2025-03-01 17:47:12,556] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-081: [2025-03-01 17:47:12,558] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-101: [2025-03-01 17:47:12,599] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-041: [2025-03-01 17:47:12,615] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-071: [2025-03-01 17:47:12,616] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-151: [2025-03-01 17:47:12,641] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-131: [2025-03-01 17:47:12,652] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-051: [2025-03-01 17:47:12,653] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-031: [2025-03-01 17:47:12,671] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-142: [2025-03-01 17:47:12,672] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-101: [2025-03-01 17:47:12,685] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-011: [2025-03-01 17:47:12,694] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-081: [2025-03-01 17:47:12,699] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-121: [2025-03-01 17:47:12,704] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-091: [2025-03-01 17:47:12,705] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-061: [2025-03-01 17:47:12,715] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-061: [2025-03-01 17:47:12,722] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-012: [2025-03-01 17:47:12,728] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-091: [2025-03-01 17:47:12,732] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-021: [2025-03-01 17:47:12,736] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-001: [2025-03-01 17:47:12,739] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-071: [2025-03-01 17:47:12,746] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-102: [2025-03-01 17:47:12,758] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-032: [2025-03-01 17:47:12,777] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-002: [2025-03-01 17:47:12,803] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-022: [2025-03-01 17:47:12,838] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-092: [2025-03-01 17:47:13,099] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-062: [2025-03-01 17:47:13,129] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-121: [2025-03-01 17:47:13,130] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-102: [2025-03-01 17:47:13,146] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-052: [2025-03-01 17:47:13,150] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-032: [2025-03-01 17:47:13,182] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-092: [2025-03-01 17:47:13,189] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-141: [2025-03-01 17:47:13,200] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-002: [2025-03-01 17:47:13,221] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-082: [2025-03-01 17:47:13,223] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-142: [2025-03-01 17:47:13,223] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-112: [2025-03-01 17:47:13,227] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-072: [2025-03-01 17:47:13,230] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-062: [2025-03-01 17:47:13,236] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-122: [2025-03-01 17:47:13,238] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-072: [2025-03-01 17:47:13,238] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-082: [2025-03-01 17:47:13,240] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-012: [2025-03-01 17:47:13,249] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-111: [2025-03-01 17:47:13,251] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-042: [2025-03-01 17:47:13,252] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-152: [2025-03-01 17:47:13,255] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-001: [2025-03-01 17:47:13,258] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-041: [2025-03-01 17:47:13,258] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-011: [2025-03-01 17:47:13,264] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-021: [2025-03-01 17:47:13,265] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-052: [2025-03-01 17:47:13,270] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-132: [2025-03-01 17:47:13,270] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c619-031: [2025-03-01 17:47:13,270] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-022: [2025-03-01 17:47:13,272] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-151: [2025-03-01 17:47:13,274] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c622-102: [2025-03-01 17:47:13,286] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c613-131: [2025-03-01 17:47:13,287] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
c621-141: [2025-03-01 17:47:19,681] [INFO] [comm.py:652:init_distributed] cdb=None
c613-122: [2025-03-01 17:47:19,687] [INFO] [comm.py:652:init_distributed] cdb=None
c613-132: [2025-03-01 17:47:19,751] [INFO] [comm.py:652:init_distributed] cdb=None
c613-152: [2025-03-01 17:47:19,775] [INFO] [comm.py:652:init_distributed] cdb=None
c613-112: [2025-03-01 17:47:19,794] [INFO] [comm.py:652:init_distributed] cdb=None
c621-111: [2025-03-01 17:47:19,802] [INFO] [comm.py:652:init_distributed] cdb=None
c622-081: [2025-03-01 17:47:19,802] [INFO] [comm.py:652:init_distributed] cdb=None
c622-101: [2025-03-01 17:47:19,851] [INFO] [comm.py:652:init_distributed] cdb=None
c622-041: [2025-03-01 17:47:19,867] [INFO] [comm.py:652:init_distributed] cdb=None
c613-122: [rank5]:[W301 17:47:19.588070710 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 5]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-141: [rank38]:[W301 17:47:19.385307242 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 38]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-071: [2025-03-01 17:47:19,888] [INFO] [comm.py:652:init_distributed] cdb=None
c613-132: [rank7]:[W301 17:47:19.049080177 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 7]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-131: [2025-03-01 17:47:19,896] [INFO] [comm.py:652:init_distributed] cdb=None
c613-152: [rank11]:[W301 17:47:19.742409701 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 11]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-112: [rank3]:[W301 17:47:19.889179559 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 3]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-111: [rank32]:[W301 17:47:19.917104259 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 32]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-081: [rank58]:[W301 17:47:19.900257428 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 58]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-101: [rank62]:[W301 17:47:19.971750069 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 62]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-051: [2025-03-01 17:47:19,992] [INFO] [comm.py:652:init_distributed] cdb=None
c622-041: [rank50]:[W301 17:47:19.253383852 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 50]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-101: [2025-03-01 17:47:20,015] [INFO] [comm.py:652:init_distributed] cdb=None
c621-071: [rank24]:[W301 17:47:20.034949601 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 24]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-151: [2025-03-01 17:47:20,019] [INFO] [comm.py:652:init_distributed] cdb=None
c621-131: [rank36]:[W301 17:47:20.012336707 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 36]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-142: [2025-03-01 17:47:20,111] [INFO] [comm.py:652:init_distributed] cdb=None
c622-091: [2025-03-01 17:47:20,111] [INFO] [comm.py:652:init_distributed] cdb=None
c622-051: [rank52]:[W301 17:47:20.104073231 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 52]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-031: [2025-03-01 17:47:20,132] [INFO] [comm.py:652:init_distributed] cdb=None
c621-101: [rank30]:[W301 17:47:20.103730684 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 30]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-121: [2025-03-01 17:47:20,147] [INFO] [comm.py:652:init_distributed] cdb=None
c621-151: [rank40]:[W301 17:47:20.129144390 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 40]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-061: [2025-03-01 17:47:20,166] [INFO] [comm.py:652:init_distributed] cdb=None
c622-011: [2025-03-01 17:47:20,169] [INFO] [comm.py:652:init_distributed] cdb=None
c621-091: [2025-03-01 17:47:20,174] [INFO] [comm.py:652:init_distributed] cdb=None
c621-061: [2025-03-01 17:47:20,186] [INFO] [comm.py:652:init_distributed] cdb=None
c621-081: [2025-03-01 17:47:20,189] [INFO] [comm.py:652:init_distributed] cdb=None
c619-012: [2025-03-01 17:47:20,201] [INFO] [comm.py:652:init_distributed] cdb=None
c622-001: [2025-03-01 17:47:20,205] [INFO] [comm.py:652:init_distributed] cdb=None
c622-021: [2025-03-01 17:47:20,214] [INFO] [comm.py:652:init_distributed] cdb=None
c613-142: [rank9]:[W301 17:47:20.644889074 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 9]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-102: [2025-03-01 17:47:20,234] [INFO] [comm.py:652:init_distributed] cdb=None
c619-032: [2025-03-01 17:47:20,240] [INFO] [comm.py:652:init_distributed] cdb=None
c622-091: [rank60]:[W301 17:47:20.792370633 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 60]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-071: [2025-03-01 17:47:20,243] [INFO] [comm.py:652:init_distributed] cdb=None
c619-022: [2025-03-01 17:47:20,252] [INFO] [comm.py:652:init_distributed] cdb=None
c622-031: [rank48]:[W301 17:47:20.222366390 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 48]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-002: [2025-03-01 17:47:20,270] [INFO] [comm.py:652:init_distributed] cdb=None
c621-121: [rank34]:[W301 17:47:20.251399322 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 34]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-061: [rank54]:[W301 17:47:20.206729405 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 54]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-011: [rank44]:[W301 17:47:20.840083265 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 44]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-091: [rank28]:[W301 17:47:20.263665129 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 28]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-061: [rank22]:[W301 17:47:20.603721525 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 22]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-081: [rank26]:[W301 17:47:20.278136399 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 26]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-001: [rank42]:[W301 17:47:20.320531865 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 42]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-021: [rank46]:[W301 17:47:20.734415463 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 46]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-102: [rank1]:[W301 17:47:20.286790731 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 1]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-012: [rank15]:[W301 17:47:20.761457054 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 15]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-092: [2025-03-01 17:47:20,366] [INFO] [comm.py:652:init_distributed] cdb=None
c622-071: [rank56]:[W301 17:47:20.997948797 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 56]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-102: [2025-03-01 17:47:20,380] [INFO] [comm.py:652:init_distributed] cdb=None
c622-062: [2025-03-01 17:47:20,382] [INFO] [comm.py:652:init_distributed] cdb=None
c613-121: [2025-03-01 17:47:20,384] [INFO] [comm.py:652:init_distributed] cdb=None
c622-052: [2025-03-01 17:47:20,429] [INFO] [comm.py:652:init_distributed] cdb=None
c622-092: [2025-03-01 17:47:20,452] [INFO] [comm.py:652:init_distributed] cdb=None
c622-032: [2025-03-01 17:47:20,464] [INFO] [comm.py:652:init_distributed] cdb=None
c619-002: [rank13]:[W301 17:47:20.432039881 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 13]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-022: [rank17]:[W301 17:47:20.268749633 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 17]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-032: [rank19]:[W301 17:47:20.067894941 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 19]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-092: [rank29]:[W301 17:47:20.483173648 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 29]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-102: [rank31]:[W301 17:47:20.328086439 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 31]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-062: [rank55]:[W301 17:47:20.479134457 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 55]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-121: [rank4]:[W301 17:47:20.562522848 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-141: [2025-03-01 17:47:20,529] [INFO] [comm.py:652:init_distributed] cdb=None
c622-052: [rank53]:[W301 17:47:20.516030024 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 53]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-092: [rank61]:[W301 17:47:20.547650166 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 61]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-002: [2025-03-01 17:47:20,573] [INFO] [comm.py:652:init_distributed] cdb=None
c622-072: [2025-03-01 17:47:20,580] [INFO] [comm.py:652:init_distributed] cdb=None
c621-062: [2025-03-01 17:47:20,584] [INFO] [comm.py:652:init_distributed] cdb=None
c621-142: [2025-03-01 17:47:20,592] [INFO] [comm.py:652:init_distributed] cdb=None
c622-032: [rank49]:[W301 17:47:20.558741119 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 49]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-082: [2025-03-01 17:47:20,616] [INFO] [comm.py:652:init_distributed] cdb=None
c621-122: [2025-03-01 17:47:20,621] [INFO] [comm.py:652:init_distributed] cdb=None
c622-082: [2025-03-01 17:47:20,634] [INFO] [comm.py:652:init_distributed] cdb=None
c621-072: [2025-03-01 17:47:20,634] [INFO] [comm.py:652:init_distributed] cdb=None
c621-152: [2025-03-01 17:47:20,635] [INFO] [comm.py:652:init_distributed] cdb=None
c621-112: [2025-03-01 17:47:20,635] [INFO] [comm.py:652:init_distributed] cdb=None
c619-021: [2025-03-01 17:47:20,648] [INFO] [comm.py:652:init_distributed] cdb=None
c622-022: [2025-03-01 17:47:20,651] [INFO] [comm.py:652:init_distributed] cdb=None
c613-141: [rank8]:[W301 17:47:20.756597178 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 8]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-001: [2025-03-01 17:47:20,660] [INFO] [comm.py:652:init_distributed] cdb=None
c619-041: [2025-03-01 17:47:20,660] [INFO] [comm.py:652:init_distributed] cdb=None
c621-052: [2025-03-01 17:47:20,661] [INFO] [comm.py:652:init_distributed] cdb=None
c622-012: [2025-03-01 17:47:20,662] [INFO] [comm.py:652:init_distributed] cdb=None
c622-042: [2025-03-01 17:47:20,664] [INFO] [comm.py:652:init_distributed] cdb=None
c613-111: [2025-03-01 17:47:20,668] [INFO] [comm.py:652:init_distributed] cdb=None
c619-031: [2025-03-01 17:47:20,668] [INFO] [comm.py:652:init_distributed] cdb=None
c621-132: [2025-03-01 17:47:20,670] [INFO] [comm.py:652:init_distributed] cdb=None
c622-102: [2025-03-01 17:47:20,671] [INFO] [comm.py:652:init_distributed] cdb=None
c613-151: [2025-03-01 17:47:20,673] [INFO] [comm.py:652:init_distributed] cdb=None
c619-011: [2025-03-01 17:47:20,675] [INFO] [comm.py:652:init_distributed] cdb=None
c613-131: [2025-03-01 17:47:20,675] [INFO] [comm.py:652:init_distributed] cdb=None
c622-072: [rank57]:[W301 17:47:20.650539290 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 57]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-062: [rank23]:[W301 17:47:20.776148753 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 23]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-002: [rank43]:[W301 17:47:20.679813028 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 43]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-142: [rank39]:[W301 17:47:20.041608096 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 39]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-082: [rank27]:[W301 17:47:20.695325588 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 27]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-072: [rank25]:[W301 17:47:20.089252263 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 25]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-112: [rank33]:[W301 17:47:20.592104450 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 33]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-082: [rank59]:[W301 17:47:20.582248691 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 59]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-152: [rank41]:[W301 17:47:20.737267360 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 41]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-022: [rank47]:[W301 17:47:20.764058324 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 47]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-041: [rank20]:[W301 17:47:20.005699671 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 20]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-131: [rank6]:[W301 17:47:20.583442504 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 6]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-132: [rank37]:[W301 17:47:20.856416708 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 37]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-151: [rank10]:[W301 17:47:20.032270449 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 10]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-102: [rank63]:[W301 17:47:20.798515125 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 63]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-101: [rank0]:[W301 17:47:20.725721332 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-031: [rank18]:[W301 17:47:20.520620858 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 18]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-021: [rank16]:[W301 17:47:20.878403049 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 16]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-011: [rank14]:[W301 17:47:20.839191438 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 14]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c619-001: [rank12]:[W301 17:47:20.026045770 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 12]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-012: [rank45]:[W301 17:47:20.942622069 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 45]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-052: [rank21]:[W301 17:47:20.820498143 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 21]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c622-042: [rank51]:[W301 17:47:20.003233444 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 51]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-111: [rank2]:[W301 17:47:20.968064853 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 2]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c613-101: c613-101:387979:387979 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.180<0>
c613-101: c613-101:387979:387979 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-101: c613-101:387979:387979 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-101: c613-101:387979:387979 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-101: c613-101:387979:387979 [0] NCCL INFO cudaDriverVersion 12060
c613-101: NCCL version 2.21.5+cuda12.4
c613-122: c613-122:1267677:1267677 [0] NCCL INFO cudaDriverVersion 12060
c613-152: c613-152:3770490:3770490 [0] NCCL INFO cudaDriverVersion 12060
c613-132: c613-132:990252:990252 [0] NCCL INFO cudaDriverVersion 12060
c621-131: c621-131:2225085:2225085 [0] NCCL INFO cudaDriverVersion 12060
c621-071: c621-071:2842568:2842568 [0] NCCL INFO cudaDriverVersion 12060
c621-102: c621-102:2281097:2281097 [0] NCCL INFO cudaDriverVersion 12060
c619-002: c619-002:83791:83791 [0] NCCL INFO cudaDriverVersion 12060
c622-011: c622-011:710508:710508 [0] NCCL INFO cudaDriverVersion 12060
c621-141: c621-141:3859461:3859461 [0] NCCL INFO cudaDriverVersion 12060
c621-081: c621-081:2075589:2075589 [0] NCCL INFO cudaDriverVersion 12060
c613-102: c613-102:1660081:1660081 [0] NCCL INFO cudaDriverVersion 12060
c622-081: c622-081:41435:41435 [0] NCCL INFO cudaDriverVersion 12060
c621-101: c621-101:813656:813656 [0] NCCL INFO cudaDriverVersion 12060
c621-111: c621-111:3868077:3868077 [0] NCCL INFO cudaDriverVersion 12060
c622-101: c622-101:1627370:1627370 [0] NCCL INFO cudaDriverVersion 12060
c622-072: c622-072:1688285:1688285 [0] NCCL INFO cudaDriverVersion 12060
c622-041: c622-041:198407:198407 [0] NCCL INFO cudaDriverVersion 12060
c619-012: c619-012:246412:246412 [0] NCCL INFO cudaDriverVersion 12060
c622-032: c622-032:1587523:1587523 [0] NCCL INFO cudaDriverVersion 12060
c622-102: c622-102:2511218:2511218 [0] NCCL INFO cudaDriverVersion 12060
c622-021: c622-021:1141571:1141571 [0] NCCL INFO cudaDriverVersion 12060
c613-122: c613-122:1267677:1267677 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.185<0>
c613-112: c613-112:1585580:1585580 [0] NCCL INFO cudaDriverVersion 12060
c622-071: c622-071:494914:494914 [0] NCCL INFO cudaDriverVersion 12060
c622-001: c622-001:3806195:3806195 [0] NCCL INFO cudaDriverVersion 12060
c613-151: c613-151:3397419:3397419 [0] NCCL INFO cudaDriverVersion 12060
c622-051: c622-051:3538690:3538690 [0] NCCL INFO cudaDriverVersion 12060
c621-151: c621-151:336402:336402 [0] NCCL INFO cudaDriverVersion 12060
c613-132: c613-132:990252:990252 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.187<0>
c613-142: c613-142:3123258:3123258 [0] NCCL INFO cudaDriverVersion 12060
c613-152: c613-152:3770490:3770490 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.191<0>
c621-142: c621-142:600426:600426 [0] NCCL INFO cudaDriverVersion 12060
c621-091: c621-091:102446:102446 [0] NCCL INFO cudaDriverVersion 12060
c622-031: c622-031:3136042:3136042 [0] NCCL INFO cudaDriverVersion 12060
c622-082: c622-082:2786472:2786472 [0] NCCL INFO cudaDriverVersion 12060
c621-061: c621-061:3120949:3120949 [0] NCCL INFO cudaDriverVersion 12060
c619-022: c619-022:804214:804214 [0] NCCL INFO cudaDriverVersion 12060
c621-071: c621-071:2842568:2842568 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.14<0>
c613-121: c613-121:904347:904347 [0] NCCL INFO cudaDriverVersion 12060
c621-132: c621-132:518779:518779 [0] NCCL INFO cudaDriverVersion 12060
c621-112: c621-112:411646:411646 [0] NCCL INFO cudaDriverVersion 12060
c622-022: c622-022:180805:180805 [0] NCCL INFO cudaDriverVersion 12060
c622-092: c622-092:418610:418610 [0] NCCL INFO cudaDriverVersion 12060
c621-131: c621-131:2225085:2225085 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.26<0>
c613-141: c613-141:2104776:2104776 [0] NCCL INFO cudaDriverVersion 12060
c622-061: c622-061:1156840:1156840 [0] NCCL INFO cudaDriverVersion 12060
c619-041: c619-041:15310:15310 [0] NCCL INFO cudaDriverVersion 12060
c621-121: c621-121:1502414:1502414 [0] NCCL INFO cudaDriverVersion 12060
c613-131: c613-131:931664:931664 [0] NCCL INFO cudaDriverVersion 12060
c621-092: c621-092:1627105:1627105 [0] NCCL INFO cudaDriverVersion 12060
c622-062: c622-062:2304142:2304142 [0] NCCL INFO cudaDriverVersion 12060
c621-102: c621-102:2281097:2281097 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.21<0>
c613-102: c613-102:1660081:1660081 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.181<0>
c619-002: c619-002:83791:83791 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.193<0>
c622-002: c622-002:1377821:1377821 [0] NCCL INFO cudaDriverVersion 12060
c622-041: c622-041:198407:198407 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.40<0>
c621-081: c621-081:2075589:2075589 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.16<0>
c622-081: c622-081:41435:41435 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.48<0>
c621-141: c621-141:3859461:3859461 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.28<0>
c622-011: c622-011:710508:710508 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.34<0>
c622-032: c622-032:1587523:1587523 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.39<0>
c621-062: c621-062:1471760:1471760 [0] NCCL INFO cudaDriverVersion 12060
c621-101: c621-101:813656:813656 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.20<0>
c622-101: c622-101:1627370:1627370 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.52<0>
c621-111: c621-111:3868077:3868077 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.22<0>
c621-072: c621-072:3048145:3048145 [0] NCCL INFO cudaDriverVersion 12060
c619-012: c619-012:246412:246412 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.195<0>
c622-021: c622-021:1141571:1141571 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.36<0>
c621-152: c621-152:1793414:1793414 [0] NCCL INFO cudaDriverVersion 12060
c622-072: c622-072:1688285:1688285 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.47<0>
c613-142: c613-142:3123258:3123258 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.189<0>
c622-091: c622-091:2780457:2780457 [0] NCCL INFO cudaDriverVersion 12060
c613-112: c613-112:1585580:1585580 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.183<0>
c621-091: c621-091:102446:102446 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.18<0>
c621-061: c621-061:3120949:3120949 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.12<0>
c622-102: c622-102:2511218:2511218 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.53<0>
c622-052: c622-052:990541:990541 [0] NCCL INFO cudaDriverVersion 12060
c622-001: c622-001:3806195:3806195 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.32<0>
c622-071: c622-071:494914:494914 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.46<0>
c613-151: c613-151:3397419:3397419 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.190<0>
c621-092: c621-092:1627105:1627105 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.19<0>
c621-142: c621-142:600426:600426 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.29<0>
c622-051: c622-051:3538690:3538690 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.42<0>
c622-031: c622-031:3136042:3136042 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.38<0>
c613-121: c613-121:904347:904347 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.184<0>
c619-022: c619-022:804214:804214 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.197<0>
c622-082: c622-082:2786472:2786472 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.49<0>
c622-092: c622-092:418610:418610 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.51<0>
c613-141: c613-141:2104776:2104776 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.188<0>
c621-121: c621-121:1502414:1502414 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.24<0>
c621-151: c621-151:336402:336402 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.30<0>
c622-062: c622-062:2304142:2304142 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.45<0>
c613-131: c613-131:931664:931664 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.186<0>
c621-132: c621-132:518779:518779 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.27<0>
c619-041: c619-041:15310:15310 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.200<0>
c622-022: c622-022:180805:180805 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.37<0>
c622-061: c622-061:1156840:1156840 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.44<0>
c621-112: c621-112:411646:411646 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.23<0>
c622-091: c622-091:2780457:2780457 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.50<0>
c621-152: c621-152:1793414:1793414 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.31<0>
c621-062: c621-062:1471760:1471760 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.13<0>
c622-002: c622-002:1377821:1377821 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.33<0>
c621-082: c621-082:1003419:1003419 [0] NCCL INFO cudaDriverVersion 12060
c621-072: c621-072:3048145:3048145 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.15<0>
c622-052: c622-052:990541:990541 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.43<0>
c619-032: c619-032:3546706:3546706 [0] NCCL INFO cudaDriverVersion 12060
c621-082: c621-082:1003419:1003419 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.17<0>
c619-032: c619-032:3546706:3546706 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.199<0>
c613-132: c613-132:990252:990252 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-132: c613-132:990252:990252 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-071: c621-071:2842568:2842568 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-132: c613-132:990252:990252 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-071: c621-071:2842568:2842568 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-071: c621-071:2842568:2842568 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-131: c621-131:2225085:2225085 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-131: c621-131:2225085:2225085 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-131: c621-131:2225085:2225085 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-041: c622-041:198407:198407 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-122: c613-122:1267677:1267677 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-041: c622-041:198407:198407 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-041: c622-041:198407:198407 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-081: c622-081:41435:41435 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-141: c621-141:3859461:3859461 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-122: c613-122:1267677:1267677 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-081: c622-081:41435:41435 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-081: c622-081:41435:41435 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-122: c613-122:1267677:1267677 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-032: c622-032:1587523:1587523 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-152: c613-152:3770490:3770490 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-072: c622-072:1688285:1688285 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-091: c621-091:102446:102446 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-091: c621-091:102446:102446 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-091: c621-091:102446:102446 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-141: c621-141:3859461:3859461 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-141: c621-141:3859461:3859461 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-032: c622-032:1587523:1587523 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-032: c622-032:1587523:1587523 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-102: c621-102:2281097:2281097 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-072: c622-072:1688285:1688285 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-072: c622-072:1688285:1688285 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-102: c621-102:2281097:2281097 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-152: c613-152:3770490:3770490 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-102: c621-102:2281097:2281097 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-152: c613-152:3770490:3770490 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-142: c621-142:600426:600426 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-142: c621-142:600426:600426 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-142: c621-142:600426:600426 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-002: c619-002:83791:83791 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-101: c622-101:1627370:1627370 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-002: c619-002:83791:83791 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-002: c619-002:83791:83791 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-011: c622-011:710508:710508 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-102: c613-102:1660081:1660081 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-101: c622-101:1627370:1627370 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-101: c622-101:1627370:1627370 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-102: c622-102:2511218:2511218 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-011: c622-011:710508:710508 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-011: c622-011:710508:710508 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-102: c613-102:1660081:1660081 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-102: c613-102:1660081:1660081 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-081: c621-081:2075589:2075589 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-101: c621-101:813656:813656 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-142: c613-142:3123258:3123258 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-102: c622-102:2511218:2511218 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-102: c622-102:2511218:2511218 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-092: c621-092:1627105:1627105 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-101: c621-101:813656:813656 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-101: c621-101:813656:813656 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-061: c621-061:3120949:3120949 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-001: c622-001:3806195:3806195 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-081: c621-081:2075589:2075589 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-081: c621-081:2075589:2075589 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-151: c621-151:336402:336402 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-121: c621-121:1502414:1502414 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-001: c622-001:3806195:3806195 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-151: c621-151:336402:336402 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-061: c621-061:3120949:3120949 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-012: c619-012:246412:246412 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-151: c621-151:336402:336402 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-061: c621-061:3120949:3120949 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-142: c613-142:3123258:3123258 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-092: c621-092:1627105:1627105 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-001: c622-001:3806195:3806195 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-142: c613-142:3123258:3123258 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-022: c619-022:804214:804214 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-021: c622-021:1141571:1141571 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-012: c619-012:246412:246412 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-012: c619-012:246412:246412 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-092: c621-092:1627105:1627105 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-121: c621-121:1502414:1502414 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-021: c622-021:1141571:1141571 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-071: c622-071:494914:494914 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-121: c621-121:1502414:1502414 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-021: c622-021:1141571:1141571 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-112: c621-112:411646:411646 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-022: c619-022:804214:804214 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-051: c622-051:3538690:3538690 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-092: c622-092:418610:418610 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-061: c622-061:1156840:1156840 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-071: c622-071:494914:494914 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-022: c619-022:804214:804214 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-071: c622-071:494914:494914 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-061: c622-061:1156840:1156840 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-082: c622-082:2786472:2786472 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-051: c622-051:3538690:3538690 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-112: c621-112:411646:411646 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-111: c621-111:3868077:3868077 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-112: c613-112:1585580:1585580 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-061: c622-061:1156840:1156840 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-112: c621-112:411646:411646 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-112: c613-112:1585580:1585580 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-112: c613-112:1585580:1585580 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-082: c622-082:2786472:2786472 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-041: c619-041:15310:15310 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-092: c622-092:418610:418610 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-062: c622-062:2304142:2304142 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-141: c613-141:2104776:2104776 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-092: c622-092:418610:418610 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-082: c622-082:2786472:2786472 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-031: c622-031:3136042:3136042 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-072: c621-072:3048145:3048145 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-131: c613-131:931664:931664 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-022: c622-022:180805:180805 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-111: c621-111:3868077:3868077 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-132: c621-132:518779:518779 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-072: c621-072:3048145:3048145 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-131: c613-131:931664:931664 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-131: c613-131:931664:931664 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-022: c622-022:180805:180805 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-141: c613-141:2104776:2104776 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-031: c622-031:3136042:3136042 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-152: c621-152:1793414:1793414 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-141: c613-141:2104776:2104776 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-041: c619-041:15310:15310 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-121: c613-121:904347:904347 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-151: c613-151:3397419:3397419 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-031: c622-031:3136042:3136042 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-132: c621-132:518779:518779 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-151: c613-151:3397419:3397419 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-151: c613-151:3397419:3397419 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-022: c622-022:180805:180805 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-091: c622-091:2780457:2780457 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-051: c622-051:3538690:3538690 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-082: c621-082:1003419:1003419 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-082: c621-082:1003419:1003419 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-121: c613-121:904347:904347 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-121: c613-121:904347:904347 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-002: c622-002:1377821:1377821 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-052: c622-052:990541:990541 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-032: c619-032:3546706:3546706 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-132: c621-132:518779:518779 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-152: c621-152:1793414:1793414 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-052: c622-052:990541:990541 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-152: c621-152:1793414:1793414 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-052: c622-052:990541:990541 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-041: c619-041:15310:15310 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-082: c621-082:1003419:1003419 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-111: c621-111:3868077:3868077 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-072: c621-072:3048145:3048145 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-091: c622-091:2780457:2780457 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-062: c622-062:2304142:2304142 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-002: c622-002:1377821:1377821 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-091: c622-091:2780457:2780457 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-002: c622-002:1377821:1377821 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-062: c621-062:1471760:1471760 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-062: c622-062:2304142:2304142 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-062: c621-062:1471760:1471760 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-062: c621-062:1471760:1471760 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-032: c619-032:3546706:3546706 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-032: c619-032:3546706:3546706 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-011: c619-011:219279:219279 [0] NCCL INFO cudaDriverVersion 12060
c619-031: c619-031:361171:361171 [0] NCCL INFO cudaDriverVersion 12060
c619-011: c619-011:219279:219279 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.194<0>
c619-031: c619-031:361171:361171 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.198<0>
c619-021: c619-021:593675:593675 [0] NCCL INFO cudaDriverVersion 12060
c619-001: c619-001:2454729:2454729 [0] NCCL INFO cudaDriverVersion 12060
c619-021: c619-021:593675:593675 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.196<0>
c619-011: c619-011:219279:219279 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-011: c619-011:219279:219279 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-011: c619-011:219279:219279 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-031: c619-031:361171:361171 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-031: c619-031:361171:361171 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-031: c619-031:361171:361171 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-001: c619-001:2454729:2454729 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.192<0>
c619-021: c619-021:593675:593675 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-021: c619-021:593675:593675 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-021: c619-021:593675:593675 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c619-001: c619-001:2454729:2454729 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c619-001: c619-001:2454729:2454729 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c619-001: c619-001:2454729:2454729 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-042: c622-042:654000:654000 [0] NCCL INFO cudaDriverVersion 12060
c621-052: c621-052:855162:855162 [0] NCCL INFO cudaDriverVersion 12060
c622-042: c622-042:654000:654000 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.41<0>
c621-052: c621-052:855162:855162 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.11<0>
c622-042: c622-042:654000:654000 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-052: c621-052:855162:855162 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-052: c621-052:855162:855162 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-052: c621-052:855162:855162 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-042: c622-042:654000:654000 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-042: c622-042:654000:654000 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c613-111: c613-111:640696:640696 [0] NCCL INFO cudaDriverVersion 12060
c613-111: c613-111:640696:640696 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.17.182<0>
c613-111: c613-111:640696:640696 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c613-111: c613-111:640696:640696 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c613-111: c613-111:640696:640696 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-012: c622-012:2903325:2903325 [0] NCCL INFO cudaDriverVersion 12060
c622-012: c622-012:2903325:2903325 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.35<0>
c622-012: c622-012:2903325:2903325 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c622-012: c622-012:2903325:2903325 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c622-012: c622-012:2903325:2903325 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c621-122: [rank35]:[W301 17:47:20.863005959 ProcessGroupNCCL.cpp:4268] [PG ID 0 PG GUID 0 Rank 35]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
c621-122: c621-122:1561826:1561826 [0] NCCL INFO cudaDriverVersion 12060
c621-122: c621-122:1561826:1561826 [0] NCCL INFO Bootstrap : Using ibP2s2:192.168.18.25<0>
c621-122: c621-122:1561826:1561826 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
c621-122: c621-122:1561826:1561826 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
c621-122: c621-122:1561826:1561826 [0] NCCL INFO NET/Plugin: Using internal network plugin.
c622-041: c622-041:198407:198578 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.40<0>
c622-041: c622-041:198407:198578 [0] NCCL INFO Using non-device net plugin version 0
c622-041: c622-041:198407:198578 [0] NCCL INFO Using network IB
c622-041: c622-041:198407:198578 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-011: c619-011:219279:219451 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.194<0>
c619-011: c619-011:219279:219451 [0] NCCL INFO Using non-device net plugin version 0
c619-011: c619-011:219279:219451 [0] NCCL INFO Using network IB
c619-011: c619-011:219279:219451 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.14<0>
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Using non-device net plugin version 0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Using network IB
c621-071: c621-071:2842568:2842739 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-132: c613-132:990252:990423 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.187<0>
c613-132: c613-132:990252:990423 [0] NCCL INFO Using non-device net plugin version 0
c613-132: c613-132:990252:990423 [0] NCCL INFO Using network IB
c613-132: c613-132:990252:990423 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.26<0>
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Using non-device net plugin version 0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Using network IB
c621-131: c621-131:2225085:2225256 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.49<0>
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Using non-device net plugin version 0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Using network IB
c622-082: c622-082:2786472:2786643 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-081: c622-081:41435:41606 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.48<0>
c622-081: c622-081:41435:41606 [0] NCCL INFO Using non-device net plugin version 0
c622-081: c622-081:41435:41606 [0] NCCL INFO Using network IB
c622-081: c622-081:41435:41606 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.21<0>
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Using non-device net plugin version 0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Using network IB
c621-102: c621-102:2281097:2281268 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.47<0>
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Using non-device net plugin version 0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Using network IB
c622-072: c622-072:1688285:1688456 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-091: c621-091:102446:102617 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.18<0>
c621-091: c621-091:102446:102617 [0] NCCL INFO Using non-device net plugin version 0
c621-091: c621-091:102446:102617 [0] NCCL INFO Using network IB
c621-091: c621-091:102446:102617 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-101: c613-101:387979:388152 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.180<0>
c613-101: c613-101:387979:388152 [0] NCCL INFO Using non-device net plugin version 0
c613-101: c613-101:387979:388152 [0] NCCL INFO Using network IB
c613-101: c613-101:387979:388152 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.32<0>
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Using non-device net plugin version 0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Using network IB
c622-001: c622-001:3806195:3806366 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-142: c621-142:600426:600600 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.29<0>
c621-142: c621-142:600426:600600 [0] NCCL INFO Using non-device net plugin version 0
c621-142: c621-142:600426:600600 [0] NCCL INFO Using network IB
c621-142: c621-142:600426:600600 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-022: c622-022:180805:180976 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.37<0>
c622-022: c622-022:180805:180976 [0] NCCL INFO Using non-device net plugin version 0
c622-022: c622-022:180805:180976 [0] NCCL INFO Using network IB
c622-022: c622-022:180805:180976 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.39<0>
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Using non-device net plugin version 0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Using network IB
c622-032: c622-032:1587523:1587694 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.15<0>
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Using non-device net plugin version 0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Using network IB
c621-072: c621-072:3048145:3048316 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.16<0>
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Using non-device net plugin version 0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Using network IB
c621-081: c621-081:2075589:2075760 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.185<0>
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Using non-device net plugin version 0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Using network IB
c613-122: c613-122:1267677:1267848 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-041: c619-041:15310:15481 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.200<0>
c619-041: c619-041:15310:15481 [0] NCCL INFO Using non-device net plugin version 0
c619-041: c619-041:15310:15481 [0] NCCL INFO Using network IB
c619-041: c619-041:15310:15481 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.19<0>
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Using non-device net plugin version 0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Using network IB
c621-092: c621-092:1627105:1627276 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.36<0>
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Using non-device net plugin version 0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Using network IB
c622-021: c622-021:1141571:1141742 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-132: c621-132:518779:518950 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.27<0>
c621-132: c621-132:518779:518950 [0] NCCL INFO Using non-device net plugin version 0
c621-132: c621-132:518779:518950 [0] NCCL INFO Using network IB
c621-132: c621-132:518779:518950 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.191<0>
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Using non-device net plugin version 0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Using network IB
c613-152: c613-152:3770490:3770661 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.53<0>
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Using non-device net plugin version 0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Using network IB
c622-102: c622-102:2511218:2511389 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-151: c621-151:336402:336573 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.30<0>
c621-151: c621-151:336402:336573 [0] NCCL INFO Using non-device net plugin version 0
c621-151: c621-151:336402:336573 [0] NCCL INFO Using network IB
c621-151: c621-151:336402:336573 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.192<0>
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Using non-device net plugin version 0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Using network IB
c619-001: c619-001:2454729:2454900 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-112: c621-112:411646:411817 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.23<0>
c621-112: c621-112:411646:411817 [0] NCCL INFO Using non-device net plugin version 0
c621-112: c621-112:411646:411817 [0] NCCL INFO Using network IB
c621-112: c621-112:411646:411817 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-121: c613-121:904347:904518 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.184<0>
c613-121: c613-121:904347:904518 [0] NCCL INFO Using non-device net plugin version 0
c613-121: c613-121:904347:904518 [0] NCCL INFO Using network IB
c613-121: c613-121:904347:904518 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.189<0>
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Using non-device net plugin version 0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Using network IB
c613-142: c613-142:3123258:3123429 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.44<0>
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Using non-device net plugin version 0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Using network IB
c622-061: c622-061:1156840:1157011 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.52<0>
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Using non-device net plugin version 0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Using network IB
c622-101: c622-101:1627370:1627541 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.181<0>
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Using non-device net plugin version 0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Using network IB
c613-102: c613-102:1660081:1660252 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-031: c619-031:361171:361342 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.198<0>
c619-031: c619-031:361171:361342 [0] NCCL INFO Using non-device net plugin version 0
c619-031: c619-031:361171:361342 [0] NCCL INFO Using network IB
c619-031: c619-031:361171:361342 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-022: c619-022:804214:804385 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.197<0>
c619-022: c619-022:804214:804385 [0] NCCL INFO Using non-device net plugin version 0
c619-022: c619-022:804214:804385 [0] NCCL INFO Using network IB
c619-022: c619-022:804214:804385 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.190<0>
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Using non-device net plugin version 0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Using network IB
c613-151: c613-151:3397419:3397590 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.199<0>
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Using non-device net plugin version 0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Using network IB
c619-032: c619-032:3546706:3546877 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.24<0>
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Using non-device net plugin version 0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Using network IB
c621-121: c621-121:1502414:1502585 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.183<0>
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Using non-device net plugin version 0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Using network IB
c613-112: c613-112:1585580:1585751 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.42<0>
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Using non-device net plugin version 0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Using network IB
c622-051: c622-051:3538690:3538861 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-131: c613-131:931664:931835 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.186<0>
c613-131: c613-131:931664:931835 [0] NCCL INFO Using non-device net plugin version 0
c613-131: c613-131:931664:931835 [0] NCCL INFO Using network IB
c613-131: c613-131:931664:931835 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.12<0>
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Using non-device net plugin version 0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Using network IB
c621-061: c621-061:3120949:3121120 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-002: c619-002:83791:83962 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.193<0>
c619-002: c619-002:83791:83962 [0] NCCL INFO Using non-device net plugin version 0
c619-002: c619-002:83791:83962 [0] NCCL INFO Using network IB
c619-002: c619-002:83791:83962 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.50<0>
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Using non-device net plugin version 0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Using network IB
c622-091: c622-091:2780457:2780628 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-101: c621-101:813656:813828 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.20<0>
c621-101: c621-101:813656:813828 [0] NCCL INFO Using non-device net plugin version 0
c621-101: c621-101:813656:813828 [0] NCCL INFO Using network IB
c621-141: c621-141:3859461:3859632 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.28<0>
c621-101: c621-101:813656:813828 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Using non-device net plugin version 0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Using network IB
c621-141: c621-141:3859461:3859632 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.33<0>
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Using non-device net plugin version 0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Using network IB
c622-002: c622-002:1377821:1377992 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.17<0>
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Using non-device net plugin version 0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Using network IB
c621-082: c621-082:1003419:1003590 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-011: c622-011:710508:710679 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.34<0>
c622-011: c622-011:710508:710679 [0] NCCL INFO Using non-device net plugin version 0
c622-011: c622-011:710508:710679 [0] NCCL INFO Using network IB
c622-011: c622-011:710508:710679 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-092: c622-092:418610:418781 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.51<0>
c622-092: c622-092:418610:418781 [0] NCCL INFO Using non-device net plugin version 0
c622-092: c622-092:418610:418781 [0] NCCL INFO Using network IB
c622-092: c622-092:418610:418781 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.45<0>
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Using non-device net plugin version 0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Using network IB
c622-062: c622-062:2304142:2304313 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-052: c622-052:990541:990712 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.43<0>
c622-052: c622-052:990541:990712 [0] NCCL INFO Using non-device net plugin version 0
c622-052: c622-052:990541:990712 [0] NCCL INFO Using network IB
c622-052: c622-052:990541:990712 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-111: c613-111:640696:640867 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.182<0>
c613-111: c613-111:640696:640867 [0] NCCL INFO Using non-device net plugin version 0
c613-111: c613-111:640696:640867 [0] NCCL INFO Using network IB
c613-111: c613-111:640696:640867 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.31<0>
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Using non-device net plugin version 0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Using network IB
c621-152: c621-152:1793414:1793585 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-012: c619-012:246412:246583 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.195<0>
c619-012: c619-012:246412:246583 [0] NCCL INFO Using non-device net plugin version 0
c619-012: c619-012:246412:246583 [0] NCCL INFO Using network IB
c619-012: c619-012:246412:246583 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-042: c622-042:654000:654172 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.41<0>
c622-042: c622-042:654000:654172 [0] NCCL INFO Using non-device net plugin version 0
c622-042: c622-042:654000:654172 [0] NCCL INFO Using network IB
c622-042: c622-042:654000:654172 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.22<0>
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Using non-device net plugin version 0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Using network IB
c621-111: c621-111:3868077:3868248 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.35<0>
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Using non-device net plugin version 0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Using network IB
c622-012: c622-012:2903325:2903496 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-052: c621-052:855162:855333 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.11<0>
c621-052: c621-052:855162:855333 [0] NCCL INFO Using non-device net plugin version 0
c621-052: c621-052:855162:855333 [0] NCCL INFO Using network IB
c621-052: c621-052:855162:855333 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.188<0>
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Using non-device net plugin version 0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Using network IB
c613-141: c613-141:2104776:2104947 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.13<0>
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Using non-device net plugin version 0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Using network IB
c621-062: c621-062:1471760:1471931 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.38<0>
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Using non-device net plugin version 0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Using network IB
c622-031: c622-031:3136042:3136213 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.25<0>
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Using non-device net plugin version 0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Using network IB
c621-122: c621-122:1561826:1561997 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-071: c622-071:494914:495085 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.18.46<0>
c622-071: c622-071:494914:495085 [0] NCCL INFO Using non-device net plugin version 0
c622-071: c622-071:494914:495085 [0] NCCL INFO Using network IB
c622-071: c622-071:494914:495085 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-021: c619-021:593675:593846 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [RO]; OOB ibP2s2:192.168.17.196<0>
c619-021: c619-021:593675:593846 [0] NCCL INFO Using non-device net plugin version 0
c619-021: c619-021:593675:593846 [0] NCCL INFO Using network IB
c619-021: c619-021:593675:593846 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO ncclCommInitRank comm 0xaaab01513280 rank 31 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-111: c621-111:3868077:3868248 [0] NCCL INFO ncclCommInitRank comm 0xaaaadde53570 rank 32 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-112: c621-112:411646:411817 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa972940 rank 33 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-121: c621-121:1502414:1502585 [0] NCCL INFO ncclCommInitRank comm 0xaaaac7ad29a0 rank 34 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-122: c621-122:1561826:1561997 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf42a34a0 rank 35 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-131: c621-131:2225085:2225256 [0] NCCL INFO ncclCommInitRank comm 0xaaaad9263e70 rank 36 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-132: c621-132:518779:518950 [0] NCCL INFO ncclCommInitRank comm 0xaaab08e331b0 rank 37 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-141: c621-141:3859461:3859632 [0] NCCL INFO ncclCommInitRank comm 0xaaab05fc3cd0 rank 38 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-142: c621-142:600426:600600 [0] NCCL INFO ncclCommInitRank comm 0xaaaafcef3f00 rank 39 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-151: c621-151:336402:336573 [0] NCCL INFO ncclCommInitRank comm 0xaaab31fb4310 rank 40 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-152: c621-152:1793414:1793585 [0] NCCL INFO ncclCommInitRank comm 0xaaaaefa62bc0 rank 41 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-001: c622-001:3806195:3806366 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa503ef0 rank 42 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-002: c622-002:1377821:1377992 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5e63ae0 rank 43 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-011: c622-011:710508:710679 [0] NCCL INFO ncclCommInitRank comm 0xaaab12fd3650 rank 44 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-101: c621-101:813656:813828 [0] NCCL INFO ncclCommInitRank comm 0xaaaae5fa3640 rank 30 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-092: c621-092:1627105:1627276 [0] NCCL INFO ncclCommInitRank comm 0xaaaafaf44150 rank 29 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-012: c622-012:2903325:2903496 [0] NCCL INFO ncclCommInitRank comm 0xaaaacd7e5250 rank 45 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-091: c621-091:102446:102617 [0] NCCL INFO ncclCommInitRank comm 0xaaaae7334ad0 rank 28 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-082: c621-082:1003419:1003590 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef4b43b0 rank 27 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-021: c622-021:1141571:1141742 [0] NCCL INFO ncclCommInitRank comm 0xaaaaec1b3660 rank 46 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-081: c621-081:2075589:2075760 [0] NCCL INFO ncclCommInitRank comm 0xaaaae8834680 rank 26 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-022: c622-022:180805:180976 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf9b932b0 rank 47 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-031: c622-031:3136042:3136213 [0] NCCL INFO ncclCommInitRank comm 0xaaab19de4480 rank 48 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-072: c621-072:3048145:3048316 [0] NCCL INFO ncclCommInitRank comm 0xaaaac7562e90 rank 25 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-032: c622-032:1587523:1587694 [0] NCCL INFO ncclCommInitRank comm 0xaaaaec6b4890 rank 49 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-071: c621-071:2842568:2842739 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5803340 rank 24 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-041: c622-041:198407:198578 [0] NCCL INFO ncclCommInitRank comm 0xaaaae2173010 rank 50 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-042: c622-042:654000:654172 [0] NCCL INFO ncclCommInitRank comm 0xaaaadc7a5b50 rank 51 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-062: c621-062:1471760:1471931 [0] NCCL INFO ncclCommInitRank comm 0xaaab20314540 rank 23 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-051: c622-051:3538690:3538861 [0] NCCL INFO ncclCommInitRank comm 0xaaab056632b0 rank 52 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-052: c622-052:990541:990712 [0] NCCL INFO ncclCommInitRank comm 0xaaab2f773010 rank 53 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-061: c621-061:3120949:3121120 [0] NCCL INFO ncclCommInitRank comm 0xaaab28873410 rank 22 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-061: c622-061:1156840:1157011 [0] NCCL INFO ncclCommInitRank comm 0xaaaaea363b30 rank 54 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c621-052: c621-052:855162:855333 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf3913390 rank 21 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-062: c622-062:2304142:2304313 [0] NCCL INFO ncclCommInitRank comm 0xaaaaefc52d60 rank 55 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-041: c619-041:15310:15481 [0] NCCL INFO ncclCommInitRank comm 0xaaaada3c4450 rank 20 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-071: c622-071:494914:495085 [0] NCCL INFO ncclCommInitRank comm 0xaaaadd881dc0 rank 56 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-072: c622-072:1688285:1688456 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf1bc32f0 rank 57 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-081: c622-081:41435:41606 [0] NCCL INFO ncclCommInitRank comm 0xaaaae73d3c50 rank 58 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-032: c619-032:3546706:3546877 [0] NCCL INFO ncclCommInitRank comm 0xaaab06d93bf0 rank 19 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-082: c622-082:2786472:2786643 [0] NCCL INFO ncclCommInitRank comm 0xaaab0d0846f0 rank 59 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-091: c622-091:2780457:2780628 [0] NCCL INFO ncclCommInitRank comm 0xaaaad2a14660 rank 60 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-092: c622-092:418610:418781 [0] NCCL INFO ncclCommInitRank comm 0xaaab065a30c0 rank 61 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-101: c622-101:1627370:1627541 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8c135d0 rank 62 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-031: c619-031:361171:361342 [0] NCCL INFO ncclCommInitRank comm 0xaaaad6982fb0 rank 18 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-101: c613-101:387979:388152 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf87e37d0 rank 0 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-102: c622-102:2511218:2511389 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf35a4020 rank 63 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-022: c619-022:804214:804385 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5e93920 rank 17 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-021: c619-021:593675:593846 [0] NCCL INFO ncclCommInitRank comm 0xaaaacef91690 rank 16 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-102: c613-102:1660081:1660252 [0] NCCL INFO ncclCommInitRank comm 0xaaaad5e93020 rank 1 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-111: c613-111:640696:640867 [0] NCCL INFO ncclCommInitRank comm 0xaaaaff0d4900 rank 2 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-112: c613-112:1585580:1585751 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa5643c0 rank 3 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-012: c619-012:246412:246583 [0] NCCL INFO ncclCommInitRank comm 0xaaaae5903380 rank 15 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-001: c619-001:2454729:2454900 [0] NCCL INFO ncclCommInitRank comm 0xaaab21114760 rank 12 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-121: c613-121:904347:904518 [0] NCCL INFO ncclCommInitRank comm 0xaaab03501d90 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-152: c613-152:3770490:3770661 [0] NCCL INFO ncclCommInitRank comm 0xaaaae1154720 rank 11 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-151: c613-151:3397419:3397590 [0] NCCL INFO ncclCommInitRank comm 0xaaaadaeb2c20 rank 10 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-122: c613-122:1267677:1267848 [0] NCCL INFO ncclCommInitRank comm 0xaaaacc441df0 rank 5 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-002: c619-002:83791:83962 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef322ea0 rank 13 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-131: c613-131:931664:931835 [0] NCCL INFO ncclCommInitRank comm 0xaaab00253330 rank 6 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c619-011: c619-011:219279:219451 [0] NCCL INFO ncclCommInitRank comm 0xaaab0f1f2940 rank 14 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-132: c613-132:990252:990423 [0] NCCL INFO ncclCommInitRank comm 0xaaaadc6b2810 rank 7 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-142: c613-142:3123258:3123429 [0] NCCL INFO ncclCommInitRank comm 0xaaaae9e74700 rank 9 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c613-141: c613-141:2104776:2104947 [0] NCCL INFO ncclCommInitRank comm 0xaaab1eb11de0 rank 8 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init START
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-092: c622-092:418610:418781 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-071: c622-071:494914:495085 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-081: c622-081:41435:41606 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-101: c613-101:387979:388152 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-052: c622-052:990541:990712 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-042: c622-042:654000:654172 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-022: c622-022:180805:180976 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-041: c622-041:198407:198578 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-111: c613-111:640696:640867 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-121: c613-121:904347:904518 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-131: c613-131:931664:931835 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-132: c613-132:990252:990423 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-151: c621-151:336402:336573 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-142: c621-142:600426:600600 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-132: c621-132:518779:518950 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-002: c619-002:83791:83962 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-012: c619-012:246412:246583 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-011: c619-011:219279:219451 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-021: c619-021:593675:593846 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-031: c619-031:361171:361342 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-022: c619-022:804214:804385 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-101: c621-101:813656:813828 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-112: c621-112:411646:411817 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-052: c621-052:855162:855333 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-011: c622-011:710508:710679 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-041: c619-041:15310:15481 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-091: c621-091:102446:102617 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-152: c613-152:3770490:3770661 [0] NCCL INFO comm 0xaaaae1154720 rank 11 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
c613-152: c613-152:3770490:3770661 [0] NCCL INFO P2P Chunksize set to 131072
c619-001: c619-001:2454729:2454900 [0] NCCL INFO comm 0xaaab21114760 rank 12 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
c619-001: c619-001:2454729:2454900 [0] NCCL INFO P2P Chunksize set to 131072
c613-151: c613-151:3397419:3397590 [0] NCCL INFO comm 0xaaaadaeb2c20 rank 10 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
c613-151: c613-151:3397419:3397590 [0] NCCL INFO P2P Chunksize set to 131072
c619-002: c619-002:83791:83962 [0] NCCL INFO comm 0xaaaaef322ea0 rank 13 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-011: c619-011:219279:219451 [0] NCCL INFO comm 0xaaab0f1f2940 rank 14 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO comm 0xaaaae9e74700 rank 9 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-012: c619-012:246412:246583 [0] NCCL INFO comm 0xaaaae5903380 rank 15 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-002: c619-002:83791:83962 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
c619-002: c619-002:83791:83962 [0] NCCL INFO P2P Chunksize set to 131072
c619-011: c619-011:219279:219451 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
c619-011: c619-011:219279:219451 [0] NCCL INFO P2P Chunksize set to 131072
c619-012: c619-012:246412:246583 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 23/7/-1->15->31 [3] 23/7/-1->15->31
c619-012: c619-012:246412:246583 [0] NCCL INFO P2P Chunksize set to 131072
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
c613-142: c613-142:3123258:3123429 [0] NCCL INFO P2P Chunksize set to 131072
c619-021: c619-021:593675:593846 [0] NCCL INFO comm 0xaaaacef91690 rank 16 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-021: c619-021:593675:593846 [0] NCCL INFO Trees [0] 8/24/-1->16->32 [1] 8/24/-1->16->32 [2] -1/-1/-1->16->17 [3] -1/-1/-1->16->17
c619-021: c619-021:593675:593846 [0] NCCL INFO P2P Chunksize set to 131072
c619-022: c619-022:804214:804385 [0] NCCL INFO comm 0xaaaaf5e93920 rank 17 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-022: c619-022:804214:804385 [0] NCCL INFO Trees [0] -1/-1/-1->17->18 [1] -1/-1/-1->17->18 [2] 18/16/-1->17->19 [3] 18/16/-1->17->19
c619-022: c619-022:804214:804385 [0] NCCL INFO P2P Chunksize set to 131072
c619-031: c619-031:361171:361342 [0] NCCL INFO comm 0xaaaad6982fb0 rank 18 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-031: c619-031:361171:361342 [0] NCCL INFO Trees [0] 17/19/-1->18->20 [1] 17/19/-1->18->20 [2] -1/-1/-1->18->17 [3] -1/-1/-1->18->17
c619-031: c619-031:361171:361342 [0] NCCL INFO P2P Chunksize set to 131072
c613-141: c613-141:2104776:2104947 [0] NCCL INFO comm 0xaaab1eb11de0 rank 8 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Trees [0] 4/12/-1->8->16 [1] 4/12/-1->8->16 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
c613-141: c613-141:2104776:2104947 [0] NCCL INFO P2P Chunksize set to 131072
c619-032: c619-032:3546706:3546877 [0] NCCL INFO comm 0xaaab06d93bf0 rank 19 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-041: c619-041:15310:15481 [0] NCCL INFO comm 0xaaaada3c4450 rank 20 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-132: c613-132:990252:990423 [0] NCCL INFO comm 0xaaaadc6b2810 rank 7 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-041: c619-041:15310:15481 [0] NCCL INFO Trees [0] 18/22/-1->20->24 [1] 18/22/-1->20->24 [2] -1/-1/-1->20->21 [3] -1/-1/-1->20->21
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 21/17/-1->19->23 [3] 21/17/-1->19->23
c619-032: c619-032:3546706:3546877 [0] NCCL INFO P2P Chunksize set to 131072
c613-132: c613-132:990252:990423 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
c613-132: c613-132:990252:990423 [0] NCCL INFO P2P Chunksize set to 131072
c621-052: c621-052:855162:855333 [0] NCCL INFO comm 0xaaaaf3913390 rank 21 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-041: c619-041:15310:15481 [0] NCCL INFO P2P Chunksize set to 131072
c621-052: c621-052:855162:855333 [0] NCCL INFO Trees [0] -1/-1/-1->21->22 [1] -1/-1/-1->21->22 [2] 22/20/-1->21->19 [3] 22/20/-1->21->19
c621-052: c621-052:855162:855333 [0] NCCL INFO P2P Chunksize set to 131072
c621-061: c621-061:3120949:3121120 [0] NCCL INFO comm 0xaaab28873410 rank 22 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Trees [0] 21/23/-1->22->20 [1] 21/23/-1->22->20 [2] -1/-1/-1->22->21 [3] -1/-1/-1->22->21
c621-061: c621-061:3120949:3121120 [0] NCCL INFO P2P Chunksize set to 131072
c621-062: c621-062:1471760:1471931 [0] NCCL INFO comm 0xaaab20314540 rank 23 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 27/19/-1->23->15 [3] 27/19/-1->23->15
c621-062: c621-062:1471760:1471931 [0] NCCL INFO P2P Chunksize set to 131072
c613-131: c613-131:931664:931835 [0] NCCL INFO comm 0xaaab00253330 rank 6 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO comm 0xaaaaf5803340 rank 24 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Trees [0] 20/28/-1->24->16 [1] 20/28/-1->24->16 [2] -1/-1/-1->24->25 [3] -1/-1/-1->24->25
c621-071: c621-071:2842568:2842739 [0] NCCL INFO P2P Chunksize set to 131072
c621-072: c621-072:3048145:3048316 [0] NCCL INFO comm 0xaaaac7562e90 rank 25 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-131: c613-131:931664:931835 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
c613-131: c613-131:931664:931835 [0] NCCL INFO P2P Chunksize set to 131072
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Trees [0] -1/-1/-1->25->26 [1] -1/-1/-1->25->26 [2] 26/24/-1->25->27 [3] 26/24/-1->25->27
c621-072: c621-072:3048145:3048316 [0] NCCL INFO P2P Chunksize set to 131072
c621-081: c621-081:2075589:2075760 [0] NCCL INFO comm 0xaaaae8834680 rank 26 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Trees [0] 25/27/-1->26->28 [1] 25/27/-1->26->28 [2] -1/-1/-1->26->25 [3] -1/-1/-1->26->25
c621-081: c621-081:2075589:2075760 [0] NCCL INFO P2P Chunksize set to 131072
c613-122: c613-122:1267677:1267848 [0] NCCL INFO comm 0xaaaacc441df0 rank 5 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
c621-082: c621-082:1003419:1003590 [0] NCCL INFO comm 0xaaaaef4b43b0 rank 27 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO P2P Chunksize set to 131072
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 29/25/-1->27->23 [3] 29/25/-1->27->23
c621-082: c621-082:1003419:1003590 [0] NCCL INFO P2P Chunksize set to 131072
c621-091: c621-091:102446:102617 [0] NCCL INFO comm 0xaaaae7334ad0 rank 28 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-091: c621-091:102446:102617 [0] NCCL INFO Trees [0] 26/30/-1->28->24 [1] 26/30/-1->28->24 [2] -1/-1/-1->28->29 [3] -1/-1/-1->28->29
c613-121: c613-121:904347:904518 [0] NCCL INFO comm 0xaaab03501d90 rank 4 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-091: c621-091:102446:102617 [0] NCCL INFO P2P Chunksize set to 131072
c613-121: c613-121:904347:904518 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
c613-121: c613-121:904347:904518 [0] NCCL INFO P2P Chunksize set to 131072
c621-092: c621-092:1627105:1627276 [0] NCCL INFO comm 0xaaaafaf44150 rank 29 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Trees [0] -1/-1/-1->29->30 [1] -1/-1/-1->29->30 [2] 30/28/-1->29->27 [3] 30/28/-1->29->27
c621-092: c621-092:1627105:1627276 [0] NCCL INFO P2P Chunksize set to 131072
c621-101: c621-101:813656:813828 [0] NCCL INFO comm 0xaaaae5fa3640 rank 30 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-101: c621-101:813656:813828 [0] NCCL INFO Trees [0] 29/31/-1->30->28 [1] 29/31/-1->30->28 [2] -1/-1/-1->30->29 [3] -1/-1/-1->30->29
c621-101: c621-101:813656:813828 [0] NCCL INFO P2P Chunksize set to 131072
c613-112: c613-112:1585580:1585751 [0] NCCL INFO comm 0xaaaafa5643c0 rank 3 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO comm 0xaaab01513280 rank 31 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
c613-112: c613-112:1585580:1585751 [0] NCCL INFO P2P Chunksize set to 131072
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 47/15/-1->31->63 [3] 47/15/-1->31->63
c621-111: c621-111:3868077:3868248 [0] NCCL INFO comm 0xaaaadde53570 rank 32 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO P2P Chunksize set to 131072
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Trees [0] 16/48/-1->32->0 [1] 16/48/-1->32->0 [2] -1/-1/-1->32->33 [3] -1/-1/-1->32->33
c621-111: c621-111:3868077:3868248 [0] NCCL INFO P2P Chunksize set to 131072
c613-111: c613-111:640696:640867 [0] NCCL INFO comm 0xaaaaff0d4900 rank 2 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-112: c621-112:411646:411817 [0] NCCL INFO comm 0xaaaafa972940 rank 33 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-112: c621-112:411646:411817 [0] NCCL INFO Trees [0] -1/-1/-1->33->34 [1] -1/-1/-1->33->34 [2] 34/32/-1->33->35 [3] 34/32/-1->33->35
c621-112: c621-112:411646:411817 [0] NCCL INFO P2P Chunksize set to 131072
c613-111: c613-111:640696:640867 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
c613-111: c613-111:640696:640867 [0] NCCL INFO P2P Chunksize set to 131072
c621-121: c621-121:1502414:1502585 [0] NCCL INFO comm 0xaaaac7ad29a0 rank 34 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Trees [0] 33/35/-1->34->36 [1] 33/35/-1->34->36 [2] -1/-1/-1->34->33 [3] -1/-1/-1->34->33
c621-121: c621-121:1502414:1502585 [0] NCCL INFO P2P Chunksize set to 131072
c621-122: c621-122:1561826:1561997 [0] NCCL INFO comm 0xaaaaf42a34a0 rank 35 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO comm 0xaaaad9263e70 rank 36 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Trees [0] 34/38/-1->36->40 [1] 34/38/-1->36->40 [2] -1/-1/-1->36->37 [3] -1/-1/-1->36->37
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] 37/33/-1->35->39 [3] 37/33/-1->35->39
c621-122: c621-122:1561826:1561997 [0] NCCL INFO P2P Chunksize set to 131072
c621-132: c621-132:518779:518950 [0] NCCL INFO comm 0xaaab08e331b0 rank 37 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO P2P Chunksize set to 131072
c613-102: c613-102:1660081:1660252 [0] NCCL INFO comm 0xaaaad5e93020 rank 1 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-132: c621-132:518779:518950 [0] NCCL INFO Trees [0] -1/-1/-1->37->38 [1] -1/-1/-1->37->38 [2] 38/36/-1->37->35 [3] 38/36/-1->37->35
c621-132: c621-132:518779:518950 [0] NCCL INFO P2P Chunksize set to 131072
c621-142: c621-142:600426:600600 [0] NCCL INFO comm 0xaaaafcef3f00 rank 39 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
c613-102: c613-102:1660081:1660252 [0] NCCL INFO P2P Chunksize set to 131072
c621-142: c621-142:600426:600600 [0] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] 43/35/-1->39->47 [3] 43/35/-1->39->47
c621-142: c621-142:600426:600600 [0] NCCL INFO P2P Chunksize set to 131072
c621-151: c621-151:336402:336573 [0] NCCL INFO comm 0xaaab31fb4310 rank 40 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO comm 0xaaab05fc3cd0 rank 38 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Trees [0] 37/39/-1->38->36 [1] 37/39/-1->38->36 [2] -1/-1/-1->38->37 [3] -1/-1/-1->38->37
c621-141: c621-141:3859461:3859632 [0] NCCL INFO P2P Chunksize set to 131072
c621-151: c621-151:336402:336573 [0] NCCL INFO Trees [0] 36/44/-1->40->48 [1] 36/44/-1->40->48 [2] -1/-1/-1->40->41 [3] -1/-1/-1->40->41
c621-151: c621-151:336402:336573 [0] NCCL INFO P2P Chunksize set to 131072
c621-152: c621-152:1793414:1793585 [0] NCCL INFO comm 0xaaaaefa62bc0 rank 41 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO comm 0xaaaafa503ef0 rank 42 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-101: c613-101:387979:388152 [0] NCCL INFO comm 0xaaaaf87e37d0 rank 0 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Trees [0] -1/-1/-1->41->42 [1] -1/-1/-1->41->42 [2] 42/40/-1->41->43 [3] 42/40/-1->41->43
c621-152: c621-152:1793414:1793585 [0] NCCL INFO P2P Chunksize set to 131072
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Trees [0] 41/43/-1->42->44 [1] 41/43/-1->42->44 [2] -1/-1/-1->42->41 [3] -1/-1/-1->42->41
c622-001: c622-001:3806195:3806366 [0] NCCL INFO P2P Chunksize set to 131072
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388152 [0] NCCL INFO Trees [0] 32/-1/-1->0->-1 [1] 32/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
c613-101: c613-101:387979:388152 [0] NCCL INFO P2P Chunksize set to 131072
c622-002: c622-002:1377821:1377992 [0] NCCL INFO comm 0xaaaaf5e63ae0 rank 43 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-011: c622-011:710508:710679 [0] NCCL INFO comm 0xaaab12fd3650 rank 44 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] 45/41/-1->43->39 [3] 45/41/-1->43->39
c622-002: c622-002:1377821:1377992 [0] NCCL INFO P2P Chunksize set to 131072
c622-011: c622-011:710508:710679 [0] NCCL INFO Trees [0] 42/46/-1->44->40 [1] 42/46/-1->44->40 [2] -1/-1/-1->44->45 [3] -1/-1/-1->44->45
c622-011: c622-011:710508:710679 [0] NCCL INFO P2P Chunksize set to 131072
c622-012: c622-012:2903325:2903496 [0] NCCL INFO comm 0xaaaacd7e5250 rank 45 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO comm 0xaaaaec1b3660 rank 46 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-052: c622-052:990541:990712 [0] NCCL INFO comm 0xaaab2f773010 rank 53 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Trees [0] -1/-1/-1->45->46 [1] -1/-1/-1->45->46 [2] 46/44/-1->45->43 [3] 46/44/-1->45->43
c622-012: c622-012:2903325:2903496 [0] NCCL INFO P2P Chunksize set to 131072
c622-052: c622-052:990541:990712 [0] NCCL INFO Trees [0] -1/-1/-1->53->54 [1] -1/-1/-1->53->54 [2] 54/52/-1->53->51 [3] 54/52/-1->53->51
c622-052: c622-052:990541:990712 [0] NCCL INFO P2P Chunksize set to 131072
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Trees [0] 45/47/-1->46->44 [1] 45/47/-1->46->44 [2] -1/-1/-1->46->45 [3] -1/-1/-1->46->45
c622-021: c622-021:1141571:1141742 [0] NCCL INFO P2P Chunksize set to 131072
c622-051: c622-051:3538690:3538861 [0] NCCL INFO comm 0xaaab056632b0 rank 52 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO comm 0xaaaaf35a4020 rank 63 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-022: c622-022:180805:180976 [0] NCCL INFO comm 0xaaaaf9b932b0 rank 47 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO comm 0xaaaaea363b30 rank 54 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Trees [0] 50/54/-1->52->56 [1] 50/54/-1->52->56 [2] -1/-1/-1->52->53 [3] -1/-1/-1->52->53
c622-051: c622-051:3538690:3538861 [0] NCCL INFO P2P Chunksize set to 131072
c622-042: c622-042:654000:654172 [0] NCCL INFO comm 0xaaaadc7a5b50 rank 51 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO comm 0xaaab0d0846f0 rank 59 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO comm 0xaaab19de4480 rank 48 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Trees [0] -1/-1/-1->63->62 [1] -1/-1/-1->63->62 [2] 31/-1/-1->63->-1 [3] 31/-1/-1->63->-1
c622-022: c622-022:180805:180976 [0] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] 55/39/-1->47->31 [3] 55/39/-1->47->31
c622-101: c622-101:1627370:1627541 [0] NCCL INFO comm 0xaaaaf8c135d0 rank 62 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-022: c622-022:180805:180976 [0] NCCL INFO P2P Chunksize set to 131072
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Trees [0] 40/56/-1->48->32 [1] 40/56/-1->48->32 [2] -1/-1/-1->48->49 [3] -1/-1/-1->48->49
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Trees [0] 53/55/-1->54->52 [1] 53/55/-1->54->52 [2] -1/-1/-1->54->53 [3] -1/-1/-1->54->53
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] -1/-1/-1->59->58 [2] 61/57/-1->59->55 [3] 61/57/-1->59->55
c622-062: c622-062:2304142:2304313 [0] NCCL INFO comm 0xaaaaefc52d60 rank 55 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO P2P Chunksize set to 131072
c622-102: c622-102:2511218:2511389 [0] NCCL INFO P2P Chunksize set to 131072
c622-081: c622-081:41435:41606 [0] NCCL INFO comm 0xaaaae73d3c50 rank 58 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-092: c622-092:418610:418781 [0] NCCL INFO comm 0xaaab065a30c0 rank 61 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO P2P Chunksize set to 131072
c622-082: c622-082:2786472:2786643 [0] NCCL INFO P2P Chunksize set to 131072
c622-042: c622-042:654000:654172 [0] NCCL INFO Trees [0] -1/-1/-1->51->50 [1] -1/-1/-1->51->50 [2] 53/49/-1->51->55 [3] 53/49/-1->51->55
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Trees [0] -1/-1/-1->55->54 [1] -1/-1/-1->55->54 [2] 59/51/-1->55->47 [3] 59/51/-1->55->47
c622-081: c622-081:41435:41606 [0] NCCL INFO Trees [0] 57/59/-1->58->60 [1] 57/59/-1->58->60 [2] -1/-1/-1->58->57 [3] -1/-1/-1->58->57
c622-062: c622-062:2304142:2304313 [0] NCCL INFO P2P Chunksize set to 131072
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Trees [0] 61/63/-1->62->60 [1] 61/63/-1->62->60 [2] -1/-1/-1->62->61 [3] -1/-1/-1->62->61
c622-091: c622-091:2780457:2780628 [0] NCCL INFO comm 0xaaaad2a14660 rank 60 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO P2P Chunksize set to 131072
c622-081: c622-081:41435:41606 [0] NCCL INFO P2P Chunksize set to 131072
c622-092: c622-092:418610:418781 [0] NCCL INFO Trees [0] -1/-1/-1->61->62 [1] -1/-1/-1->61->62 [2] 62/60/-1->61->59 [3] 62/60/-1->61->59
c622-092: c622-092:418610:418781 [0] NCCL INFO P2P Chunksize set to 131072
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Trees [0] 58/62/-1->60->56 [1] 58/62/-1->60->56 [2] -1/-1/-1->60->61 [3] -1/-1/-1->60->61
c622-091: c622-091:2780457:2780628 [0] NCCL INFO P2P Chunksize set to 131072
c622-032: c622-032:1587523:1587694 [0] NCCL INFO comm 0xaaaaec6b4890 rank 49 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-042: c622-042:654000:654172 [0] NCCL INFO P2P Chunksize set to 131072
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Trees [0] -1/-1/-1->49->50 [1] -1/-1/-1->49->50 [2] 50/48/-1->49->51 [3] 50/48/-1->49->51
c622-032: c622-032:1587523:1587694 [0] NCCL INFO P2P Chunksize set to 131072
c622-071: c622-071:494914:495085 [0] NCCL INFO comm 0xaaaadd881dc0 rank 56 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO comm 0xaaaaf1bc32f0 rank 57 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-041: c622-041:198407:198578 [0] NCCL INFO comm 0xaaaae2173010 rank 50 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-041: c622-041:198407:198578 [0] NCCL INFO Trees [0] 49/51/-1->50->52 [1] 49/51/-1->50->52 [2] -1/-1/-1->50->49 [3] -1/-1/-1->50->49
c622-041: c622-041:198407:198578 [0] NCCL INFO P2P Chunksize set to 131072
c622-071: c622-071:494914:495085 [0] NCCL INFO Trees [0] 52/60/-1->56->48 [1] 52/60/-1->56->48 [2] -1/-1/-1->56->57 [3] -1/-1/-1->56->57
c622-071: c622-071:494914:495085 [0] NCCL INFO P2P Chunksize set to 131072
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Trees [0] -1/-1/-1->57->58 [1] -1/-1/-1->57->58 [2] 58/56/-1->57->59 [3] 58/56/-1->57->59
c622-072: c622-072:1688285:1688456 [0] NCCL INFO P2P Chunksize set to 131072
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 02/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 03/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 00/0 : 23[0] -> 24[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 01/0 : 23[0] -> 24[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 23[0] -> 24[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 02/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 02/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 23[0] -> 24[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 03/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 03/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 02/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 00/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 00/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 02/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 03/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 01/0 : 25[0] -> 26[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 03/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 01/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 02/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 25[0] -> 26[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 02/0 : 22[0] -> 23[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 02/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 03/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 03/0 : 22[0] -> 23[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 00/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 02/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 03/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 01/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 02/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 27[0] -> 28[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 03/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 02/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 03/0 : 26[0] -> 27[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 00/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 03/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 27[0] -> 28[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 00/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 01/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 01/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 00/0 : 33[0] -> 34[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 01/0 : 33[0] -> 34[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 00/0 : 29[0] -> 30[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 33[0] -> 34[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 01/0 : 29[0] -> 30[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 00/0 : 35[0] -> 36[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 29[0] -> 30[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 33[0] -> 34[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 01/0 : 35[0] -> 36[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 35[0] -> 36[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 29[0] -> 30[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 02/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 35[0] -> 36[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 03/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 00/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 34[0] -> 35[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 01/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 00/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 34[0] -> 35[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 02/0 : 34[0] -> 35[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 02/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 00/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 03/0 : 34[0] -> 35[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 02/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 01/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 03/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 03/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 38[0] -> 39[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 00/0 : 51[0] -> 52[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 38[0] -> 39[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 00/0 : 41[0] -> 42[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 00/0 : 37[0] -> 38[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 01/0 : 51[0] -> 52[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 51[0] -> 52[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 02/0 : 38[0] -> 39[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 00/0 : 45[0] -> 46[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 00/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 01/0 : 37[0] -> 38[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 00/0 : 39[0] -> 40[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 51[0] -> 52[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 03/0 : 38[0] -> 39[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 01/0 : 45[0] -> 46[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 01/0 : 39[0] -> 40[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 01/0 : 41[0] -> 42[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 41[0] -> 42[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 02/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 02/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 37[0] -> 38[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 00/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 01/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 02/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 45[0] -> 46[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 00/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 02/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 39[0] -> 40[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 39[0] -> 40[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 41[0] -> 42[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 03/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 37[0] -> 38[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 01/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 02/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 00/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 02/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 45[0] -> 46[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 01/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 03/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 03/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 03/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 02/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 03/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 03/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 01/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 00/0 : 61[0] -> 62[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 00/0 : 43[0] -> 44[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 03/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 00/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 00/0 : 63[0] -> 0[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 01/0 : 43[0] -> 44[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 01/0 : 63[0] -> 0[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 58[0] -> 59[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 01/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 58[0] -> 59[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 02/0 : 63[0] -> 0[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 02/0 : 58[0] -> 59[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 02/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 01/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 43[0] -> 44[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 00/0 : 49[0] -> 50[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 03/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 02/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 01/0 : 49[0] -> 50[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 43[0] -> 44[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 02/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 03/0 : 58[0] -> 59[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 03/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 00/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 03/0 : 63[0] -> 0[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 49[0] -> 50[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 02/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 03/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 01/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 49[0] -> 50[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 03/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 02/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 00/0 : 55[0] -> 56[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 02/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 02/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 01/0 : 55[0] -> 56[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 03/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 03/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 03/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 55[0] -> 56[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 55[0] -> 56[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 02/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 00/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 01/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 03/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 02/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 03/0 : 50[0] -> 51[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 00/0 : 17[0] -> 18[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 01/0 : 17[0] -> 18[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 17[0] -> 18[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 17[0] -> 18[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 19[0] -> 20[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 19[0] -> 20[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Connected all rings
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 42[0] -> 44[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Connected all rings
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 41[0] -> 43[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Connected all rings
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 42[0] -> 44[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Connected all rings
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 41[0] -> 43[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 41[0] -> 43[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 41[0] -> 43[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 43[0] -> 45[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 43[0] -> 45[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 43[0] -> 41[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 01/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 02/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 03/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 44[0] -> 46[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 44[0] -> 46[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 44[0] -> 42[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Connected all rings
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Connected all rings
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 26[0] -> 28[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 26[0] -> 28[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Connected all rings
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Connected all rings
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 25[0] -> 27[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 25[0] -> 27[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Connected all rings
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 27[0] -> 29[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 27[0] -> 29[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Connected all rings
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 51[0] -> 55[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Connected all rings
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 52[0] -> 54[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 52[0] -> 54[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Connected all rings
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 34[0] -> 36[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Connected all rings
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 34[0] -> 36[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Connected all rings
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 51[0] -> 53[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 36[0] -> 38[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 51[0] -> 53[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 50[0] -> 52[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 50[0] -> 52[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 52[0] -> 54[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 52[0] -> 54[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Connected all rings
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 57[0] -> 59[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 57[0] -> 59[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Connected all rings
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 54[0] -> 52[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 54[0] -> 52[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Connected all rings
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Connected all rings
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 35[0] -> 39[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Connected all rings
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 35[0] -> 39[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 36[0] -> 38[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 39[0] -> 43[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 35[0] -> 37[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 39[0] -> 43[0] [send] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 35[0] -> 37[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 38[0] -> 36[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 38[0] -> 36[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Connected all rings
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 25[0] -> 27[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 25[0] -> 27[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 27[0] -> 29[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 27[0] -> 29[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 27[0] -> 25[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 23[0] -> 27[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 29[0] -> 27[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 23[0] -> 27[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 29[0] -> 27[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Connected all rings
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 19[0] -> 23[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 19[0] -> 23[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 23[0] -> 27[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 23[0] -> 27[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Connected all rings
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Connected all rings
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 57[0] -> 59[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 58[0] -> 60[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 57[0] -> 59[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 27[0] -> 23[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 58[0] -> 60[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 59[0] -> 61[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 60[0] -> 62[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 59[0] -> 61[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 27[0] -> 23[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 59[0] -> 57[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 59[0] -> 57[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Connected all rings
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Connected all rings
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 11[0] -> 13[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 10[0] -> 12[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 10[0] -> 12[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 11[0] -> 13[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 12[0] -> 14[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Connected all rings
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 17[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 17[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 19[0] -> 21[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 19[0] -> 21[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Connected all rings
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 26[0] -> 28[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 26[0] -> 28[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 28[0] -> 30[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 28[0] -> 30[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 28[0] -> 26[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Connected all rings
c619-031: c619-031:361171:361342 [0] NCCL INFO Connected all rings
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 12[0] -> 14[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 18[0] -> 20[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 14[0] -> 12[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 14[0] -> 12[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Connected all rings
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 17[0] -> 19[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 17[0] -> 19[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Connected all rings
c619-021: c619-021:593675:593846 [0] NCCL INFO Connected all rings
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 7[0] -> 15[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 19[0] -> 17[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 7[0] -> 15[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 19[0] -> 17[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 15[0] -> 23[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 15[0] -> 23[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Connected all rings
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 20[0] -> 22[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Connected all rings
c619-041: c619-041:15310:15481 [0] NCCL INFO Connected all rings
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 19[0] -> 21[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 18[0] -> 20[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 19[0] -> 21[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 20[0] -> 22[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 20[0] -> 22[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 21[0] -> 19[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 21[0] -> 19[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 19[0] -> 23[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 19[0] -> 23[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 20[0] -> 18[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 20[0] -> 18[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 23[0] -> 19[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 15[0] -> 23[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 23[0] -> 19[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 15[0] -> 23[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 23[0] -> 15[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 23[0] -> 15[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Connected all rings
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 43[0] -> 45[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 43[0] -> 45[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 45[0] -> 43[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 45[0] -> 43[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 39[0] -> 43[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 39[0] -> 43[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 43[0] -> 39[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 43[0] -> 39[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Connected all rings
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Connected all rings
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Connected all rings
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 49[0] -> 51[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 4[0] -> 6[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 3[0] -> 5[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 49[0] -> 51[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 3[0] -> 5[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Connected all rings
c622-022: c622-022:180805:180976 [0] NCCL INFO Connected all rings
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 39[0] -> 47[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 39[0] -> 47[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 47[0] -> 55[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 47[0] -> 55[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Connected all rings
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 15[0] -> 31[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 15[0] -> 31[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 31[0] -> 47[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 31[0] -> 47[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Connected all rings
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 33[0] -> 35[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 33[0] -> 35[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 35[0] -> 37[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 35[0] -> 37[0] [send] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 37[0] -> 35[0] [send] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 37[0] -> 35[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Connected all rings
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 34[0] -> 36[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Connected all rings
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Connected all rings
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 34[0] -> 36[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 33[0] -> 35[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 33[0] -> 35[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 36[0] -> 34[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 35[0] -> 33[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 36[0] -> 34[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 35[0] -> 33[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 35[0] -> 39[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 35[0] -> 39[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 39[0] -> 35[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 39[0] -> 35[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 39[0] -> 47[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 39[0] -> 47[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 47[0] -> 39[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 47[0] -> 39[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Connected all rings
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 28[0] -> 30[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 28[0] -> 30[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 30[0] -> 28[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 30[0] -> 28[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 43[0] -> 41[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Connected all rings
c613-132: c613-132:990252:990423 [0] NCCL INFO Connected all rings
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 9[0] -> 11[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 3[0] -> 7[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 9[0] -> 11[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 3[0] -> 7[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 11[0] -> 13[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 7[0] -> 11[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 11[0] -> 13[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 7[0] -> 11[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 13[0] -> 11[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 13[0] -> 11[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Connected all rings
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 44[0] -> 42[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 10[0] -> 12[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 10[0] -> 12[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Connected all rings
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Connected all rings
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 9[0] -> 11[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 9[0] -> 11[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 12[0] -> 10[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 12[0] -> 10[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 7[0] -> 11[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 11[0] -> 9[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 7[0] -> 11[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 11[0] -> 9[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 11[0] -> 7[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 11[0] -> 7[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Connected all rings
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 2[0] -> 4[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Connected all rings
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 59[0] -> 61[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 59[0] -> 61[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Connected all rings
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 61[0] -> 59[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 61[0] -> 59[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 1[0] -> 3[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 55[0] -> 59[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 1[0] -> 3[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 55[0] -> 59[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Connected all rings
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 01/0 : 32[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Connected all rings
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 01/0 : 0[0] -> 32[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 02/0 : 31[0] -> 63[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Connected all rings
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 28[0] -> 26[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 03/0 : 31[0] -> 63[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 02/0 : 63[0] -> 31[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 60[0] -> 62[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 03/0 : 63[0] -> 31[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 62[0] -> 60[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 62[0] -> 60[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Connected all rings
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 49[0] -> 51[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 49[0] -> 51[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 51[0] -> 53[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 51[0] -> 53[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 51[0] -> 49[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 51[0] -> 49[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 51[0] -> 55[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 51[0] -> 55[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 53[0] -> 51[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 53[0] -> 51[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 27[0] -> 25[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Connected all rings
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 1[0] -> 3[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 1[0] -> 3[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 3[0] -> 5[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 3[0] -> 5[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 3[0] -> 7[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 3[0] -> 1[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 3[0] -> 7[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 7[0] -> 3[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 7[0] -> 15[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 7[0] -> 3[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 7[0] -> 15[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 15[0] -> 7[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 15[0] -> 7[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 5[0] -> 3[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 5[0] -> 3[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Connected all rings
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 2[0] -> 4[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 4[0] -> 6[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[0] [receive] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 4[0] -> 2[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 6[0] -> 4[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 15[0] -> 31[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 15[0] -> 31[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 31[0] -> 15[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 31[0] -> 15[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Connected all rings
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 50[0] -> 52[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 50[0] -> 52[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 52[0] -> 50[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 52[0] -> 50[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Connected all rings
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 58[0] -> 60[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 58[0] -> 60[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 60[0] -> 58[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 60[0] -> 58[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 20[0] -> 22[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 22[0] -> 20[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 22[0] -> 20[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 51[0] -> 55[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 55[0] -> 59[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 55[0] -> 59[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 47[0] -> 55[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 55[0] -> 51[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 59[0] -> 55[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 47[0] -> 55[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 55[0] -> 51[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 59[0] -> 55[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 55[0] -> 47[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 55[0] -> 47[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Connected all rings
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 44[0] -> 46[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 44[0] -> 46[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 46[0] -> 44[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 46[0] -> 44[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 31[0] -> 47[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 31[0] -> 47[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 47[0] -> 31[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 47[0] -> 31[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 63[0] -> 31[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 63[0] -> 31[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 31[0] -> 63[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 31[0] -> 63[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 0[0] -> 32[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 32[0] -> 0[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 47[0] -> 31[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 47[0] -> 31[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 00/0 : 63[0] -> 62[0] [send] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Channel 01/0 : 63[0] -> 62[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 02/0 : 31[0] -> 15[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 03/0 : 31[0] -> 15[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 00/0 : 31[0] -> 30[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 55[0] -> 47[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Channel 01/0 : 31[0] -> 30[0] [send] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 55[0] -> 47[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 23[0] -> 15[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 02/0 : 47[0] -> 39[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 23[0] -> 15[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 02/0 : 33[0] -> 32[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 03/0 : 47[0] -> 39[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 02/0 : 15[0] -> 7[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Channel 03/0 : 33[0] -> 32[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 03/0 : 15[0] -> 7[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 59[0] -> 55[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 00/0 : 47[0] -> 46[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 27[0] -> 23[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 59[0] -> 55[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 43[0] -> 39[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 02/0 : 49[0] -> 48[0] [receive] via NET/IB/0
c622-022: c622-022:180805:180976 [0] NCCL INFO Channel 01/0 : 47[0] -> 46[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 02/0 : 55[0] -> 51[0] [send] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 00/0 : 15[0] -> 14[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 27[0] -> 23[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 11[0] -> 7[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 43[0] -> 39[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 03/0 : 55[0] -> 51[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246583 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Channel 03/0 : 49[0] -> 48[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 02/0 : 23[0] -> 19[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 02/0 : 39[0] -> 35[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 11[0] -> 7[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 02/0 : 17[0] -> 16[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 03/0 : 39[0] -> 35[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 03/0 : 23[0] -> 19[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 02/0 : 7[0] -> 3[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/IB/0
c619-021: c619-021:593675:593846 [0] NCCL INFO Channel 03/0 : 17[0] -> 16[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 03/0 : 7[0] -> 3[0] [send] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 00/0 : 55[0] -> 54[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Channel 01/0 : 55[0] -> 54[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 46[0] -> 44[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 29[0] -> 27[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 53[0] -> 51[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 13[0] -> 11[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 45[0] -> 43[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 00/0 : 39[0] -> 38[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 00/0 : 23[0] -> 22[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 37[0] -> 35[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 02/0 : 41[0] -> 40[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 45[0] -> 43[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 46[0] -> 44[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 02/0 : 57[0] -> 56[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 53[0] -> 51[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 13[0] -> 11[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Channel 01/0 : 23[0] -> 22[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 37[0] -> 35[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 29[0] -> 27[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 02/0 : 27[0] -> 25[0] [send] via NET/IB/0
c621-142: c621-142:600426:600600 [0] NCCL INFO Channel 01/0 : 39[0] -> 38[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 54[0] -> 52[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 02/0 : 25[0] -> 24[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 30[0] -> 28[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 62[0] -> 60[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 02/0 : 43[0] -> 41[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 38[0] -> 36[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 02/0 : 11[0] -> 9[0] [send] via NET/IB/0
c622-071: c622-071:494914:495085 [0] NCCL INFO Channel 03/0 : 57[0] -> 56[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 00/0 : 44[0] -> 42[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 02/0 : 35[0] -> 33[0] [send] via NET/IB/0
c613-132: c613-132:990252:990423 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 30[0] -> 28[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 03/0 : 11[0] -> 9[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 03/0 : 27[0] -> 25[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Channel 03/0 : 25[0] -> 24[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 21[0] -> 19[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 03/0 : 43[0] -> 41[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 02/0 : 51[0] -> 49[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 54[0] -> 52[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 01/0 : 44[0] -> 42[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 5[0] -> 3[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336573 [0] NCCL INFO Channel 03/0 : 41[0] -> 40[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 62[0] -> 60[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 02/0 : 9[0] -> 8[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 00/0 : 52[0] -> 50[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 00/0 : 28[0] -> 26[0] [send] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 03/0 : 51[0] -> 49[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 5[0] -> 3[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 14[0] -> 12[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 38[0] -> 36[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 03/0 : 35[0] -> 33[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 00/0 : 60[0] -> 58[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 02/0 : 3[0] -> 1[0] [send] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 01/0 : 28[0] -> 26[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 00/0 : 36[0] -> 34[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 21[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 02/0 : 19[0] -> 17[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 14[0] -> 12[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 01/0 : 52[0] -> 50[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 03/0 : 3[0] -> 1[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 00/0 : 12[0] -> 10[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Channel 03/0 : 9[0] -> 8[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 01/0 : 36[0] -> 34[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 03/0 : 19[0] -> 17[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 01/0 : 60[0] -> 58[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 01/0 : 12[0] -> 10[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 6[0] -> 4[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 00/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[0] [send] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 01/0 : 4[0] -> 2[0] [send] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 00/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 00/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 01/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 02/0 : 29[0] -> 28[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[0] [send] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 02/0 : 45[0] -> 44[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 47[0] -> 46[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 00/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 01/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 00/0 : 35[0] -> 34[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 31[0] -> 30[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 01/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102617 [0] NCCL INFO Channel 03/0 : 29[0] -> 28[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Channel 01/0 : 11[0] -> 10[0] [send] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 00/0 : 27[0] -> 26[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 00/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 00/0 : 51[0] -> 50[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 27[0] -> 26[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Channel 01/0 : 35[0] -> 34[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 00/0 : 43[0] -> 42[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 00/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 00/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 01/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 01/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 00/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 31[0] -> 30[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710679 [0] NCCL INFO Channel 03/0 : 45[0] -> 44[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 43[0] -> 42[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 02/0 : 53[0] -> 52[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 47[0] -> 46[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 63[0] -> 62[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 27[0] -> 26[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Channel 01/0 : 27[0] -> 26[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Channel 01/0 : 43[0] -> 42[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 02/0 : 61[0] -> 60[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 00/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 01/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 00/0 : 19[0] -> 18[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 00/0 : 30[0] -> 29[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 01/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654172 [0] NCCL INFO Channel 01/0 : 51[0] -> 50[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 00/0 : 46[0] -> 45[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 59[0] -> 58[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 51[0] -> 50[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 01/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 63[0] -> 62[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 02/0 : 53[0] -> 52[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990712 [0] NCCL INFO Channel 03/0 : 53[0] -> 52[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 39[0] -> 38[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 01/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 02/0 : 37[0] -> 36[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 15[0] -> 14[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Channel 03/0 : 61[0] -> 60[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 02/0 : 5[0] -> 4[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 01/0 : 46[0] -> 45[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 00/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 59[0] -> 58[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 02/0 : 37[0] -> 36[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 01/0 : 30[0] -> 29[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 02/0 : 30[0] -> 29[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 02/0 : 29[0] -> 28[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 00/0 : 26[0] -> 25[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 01/0 : 26[0] -> 25[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 00/0 : 62[0] -> 61[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 01/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 51[0] -> 50[0] [receive] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Channel 03/0 : 53[0] -> 52[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 39[0] -> 38[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 02/0 : 46[0] -> 45[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Channel 03/0 : 29[0] -> 28[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[0] [receive] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Channel 03/0 : 37[0] -> 36[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Channel 03/0 : 46[0] -> 45[0] [send] via NET/IB/0
c621-132: c621-132:518779:518950 [0] NCCL INFO Channel 03/0 : 37[0] -> 36[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 02/0 : 13[0] -> 12[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[0] [send] via NET/IB/0
c621-101: c621-101:813656:813828 [0] NCCL INFO Channel 03/0 : 30[0] -> 29[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 43[0] -> 42[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 00/0 : 42[0] -> 41[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 02/0 : 9[0] -> 8[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 02/0 : 25[0] -> 24[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 00/0 : 50[0] -> 49[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 01/0 : 42[0] -> 41[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 35[0] -> 34[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904518 [0] NCCL INFO Channel 03/0 : 5[0] -> 4[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 00/0 : 38[0] -> 37[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 02/0 : 42[0] -> 41[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 00/0 : 14[0] -> 13[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 01/0 : 38[0] -> 37[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 02/0 : 33[0] -> 32[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 02/0 : 13[0] -> 12[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Channel 03/0 : 25[0] -> 24[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 02/0 : 26[0] -> 25[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Channel 03/0 : 26[0] -> 25[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Channel 03/0 : 13[0] -> 12[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Channel 03/0 : 42[0] -> 41[0] [send] via NET/IB/0
c621-112: c621-112:411646:411817 [0] NCCL INFO Channel 03/0 : 33[0] -> 32[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 01/0 : 50[0] -> 49[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 02/0 : 21[0] -> 20[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 02/0 : 50[0] -> 49[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 01/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 02/0 : 38[0] -> 37[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 11[0] -> 10[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 01/0 : 14[0] -> 13[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 01/0 : 62[0] -> 61[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 02/0 : 14[0] -> 13[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 02/0 : 17[0] -> 16[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 02/0 : 62[0] -> 61[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 02/0 : 5[0] -> 4[0] [send] via NET/IB/0
c622-041: c622-041:198407:198578 [0] NCCL INFO Channel 03/0 : 50[0] -> 49[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 02/0 : 41[0] -> 40[0] [send] via NET/IB/0
c621-052: c621-052:855162:855333 [0] NCCL INFO Channel 03/0 : 21[0] -> 20[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 35[0] -> 34[0] [receive] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 00/0 : 34[0] -> 33[0] [send] via NET/IB/0
c619-002: c619-002:83791:83962 [0] NCCL INFO Channel 03/0 : 13[0] -> 12[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 01/0 : 34[0] -> 33[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Channel 03/0 : 62[0] -> 61[0] [send] via NET/IB/0
c619-022: c619-022:804214:804385 [0] NCCL INFO Channel 03/0 : 17[0] -> 16[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Channel 03/0 : 5[0] -> 4[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 02/0 : 45[0] -> 44[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Channel 03/0 : 41[0] -> 40[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Channel 03/0 : 45[0] -> 44[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 02/0 : 2[0] -> 1[0] [send] via NET/IB/0
c619-011: c619-011:219279:219451 [0] NCCL INFO Channel 03/0 : 14[0] -> 13[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 02/0 : 6[0] -> 5[0] [send] via NET/IB/0
c613-131: c613-131:931664:931835 [0] NCCL INFO Channel 03/0 : 6[0] -> 5[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 02/0 : 49[0] -> 48[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Channel 03/0 : 49[0] -> 48[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 02/0 : 34[0] -> 33[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Channel 03/0 : 38[0] -> 37[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 00/0 : 58[0] -> 57[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 01/0 : 58[0] -> 57[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 02/0 : 58[0] -> 57[0] [send] via NET/IB/0
c622-081: c622-081:41435:41606 [0] NCCL INFO Channel 03/0 : 58[0] -> 57[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 02/0 : 10[0] -> 9[0] [send] via NET/IB/0
c613-111: c613-111:640696:640867 [0] NCCL INFO Channel 03/0 : 2[0] -> 1[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Channel 03/0 : 34[0] -> 33[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Channel 03/0 : 10[0] -> 9[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 61[0] -> 59[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 61[0] -> 59[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 02/0 : 59[0] -> 57[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 03/0 : 59[0] -> 57[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 00/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 00/0 : 59[0] -> 58[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 01/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Channel 01/0 : 59[0] -> 58[0] [send] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 00/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 01/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 02/0 : 61[0] -> 60[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-092: c622-092:418610:418781 [0] NCCL INFO Channel 03/0 : 61[0] -> 60[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 02/0 : 57[0] -> 56[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Channel 03/0 : 57[0] -> 56[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 22[0] -> 20[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 22[0] -> 20[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 00/0 : 20[0] -> 18[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 01/0 : 20[0] -> 18[0] [send] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 02/0 : 21[0] -> 20[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 23[0] -> 22[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 19[0] -> 18[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15481 [0] NCCL INFO Channel 03/0 : 21[0] -> 20[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 23[0] -> 22[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 00/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 01/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 01/0 : 22[0] -> 21[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 02/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 02/0 : 22[0] -> 21[0] [send] via NET/IB/0
c619-031: c619-031:361171:361342 [0] NCCL INFO Channel 03/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Channel 03/0 : 22[0] -> 21[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 3[0] -> 1[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Channel 03/0 : 9[0] -> 8[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 55[0] -> 54[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 55[0] -> 54[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 00/0 : 54[0] -> 53[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 01/0 : 54[0] -> 53[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 02/0 : 54[0] -> 53[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Channel 03/0 : 54[0] -> 53[0] [send] via NET/IB/0
c613-101: c613-101:387979:388152 [0] NCCL INFO Connected all trees
c613-101: c613-101:387979:388152 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-101: c613-101:387979:388152 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-111: c621-111:3868077:3868248 [0] NCCL INFO Connected all trees
c621-111: c621-111:3868077:3868248 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-111: c621-111:3868077:3868248 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-031: c622-031:3136042:3136213 [0] NCCL INFO Connected all trees
c622-031: c622-031:3136042:3136213 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-031: c622-031:3136042:3136213 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-151: c621-151:336402:336573 [0] NCCL INFO Connected all trees
c613-121: c613-121:904347:904518 [0] NCCL INFO Connected all trees
c613-102: c613-102:1660081:1660252 [0] NCCL INFO Connected all trees
c621-151: c621-151:336402:336573 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-151: c621-151:336402:336573 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-121: c613-121:904347:904518 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-121: c613-121:904347:904518 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-102: c613-102:1660081:1660252 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-102: c613-102:1660081:1660252 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-111: c613-111:640696:640867 [0] NCCL INFO Connected all trees
c613-112: c613-112:1585580:1585751 [0] NCCL INFO Connected all trees
c613-112: c613-112:1585580:1585751 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-112: c613-112:1585580:1585751 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-111: c613-111:640696:640867 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-111: c613-111:640696:640867 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-122: c613-122:1267677:1267848 [0] NCCL INFO Connected all trees
c621-121: c621-121:1502414:1502585 [0] NCCL INFO Connected all trees
c613-131: c613-131:931664:931835 [0] NCCL INFO Connected all trees
c613-122: c613-122:1267677:1267848 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-122: c613-122:1267677:1267848 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-112: c621-112:411646:411817 [0] NCCL INFO Connected all trees
c621-112: c621-112:411646:411817 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-112: c621-112:411646:411817 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-121: c621-121:1502414:1502585 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-121: c621-121:1502414:1502585 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-122: c621-122:1561826:1561997 [0] NCCL INFO Connected all trees
c621-122: c621-122:1561826:1561997 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-122: c621-122:1561826:1561997 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-131: c613-131:931664:931835 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-131: c613-131:931664:931835 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-152: c621-152:1793414:1793585 [0] NCCL INFO Connected all trees
c621-152: c621-152:1793414:1793585 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-132: c613-132:990252:990423 [0] NCCL INFO Connected all trees
c622-032: c622-032:1587523:1587694 [0] NCCL INFO Connected all trees
c622-041: c622-041:198407:198578 [0] NCCL INFO Connected all trees
c613-132: c613-132:990252:990423 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-132: c613-132:990252:990423 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-001: c622-001:3806195:3806366 [0] NCCL INFO Connected all trees
c622-041: c622-041:198407:198578 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-032: c622-032:1587523:1587694 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-152: c621-152:1793414:1793585 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-032: c622-032:1587523:1587694 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-041: c622-041:198407:198578 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-001: c622-001:3806195:3806366 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-001: c622-001:3806195:3806366 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-062: c622-062:2304142:2304313 [0] NCCL INFO Connected all trees
c622-002: c622-002:1377821:1377992 [0] NCCL INFO Connected all trees
c622-042: c622-042:654000:654172 [0] NCCL INFO Connected all trees
c622-042: c622-042:654000:654172 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-002: c622-002:1377821:1377992 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-042: c622-042:654000:654172 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-091: c622-091:2780457:2780628 [0] NCCL INFO Connected all trees
c622-091: c622-091:2780457:2780628 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-002: c622-002:1377821:1377992 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-062: c622-062:2304142:2304313 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-062: c622-062:2304142:2304313 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-021: c622-021:1141571:1141742 [0] NCCL INFO Connected all trees
c622-091: c622-091:2780457:2780628 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-022: c622-022:180805:180976 [0] NCCL INFO Connected all trees
c622-082: c622-082:2786472:2786643 [0] NCCL INFO Connected all trees
c622-082: c622-082:2786472:2786643 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-142: c621-142:600426:600600 [0] NCCL INFO Connected all trees
c622-021: c622-021:1141571:1141742 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-022: c622-022:180805:180976 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-142: c621-142:600426:600600 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-021: c622-021:1141571:1141742 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-142: c621-142:600426:600600 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-022: c622-022:180805:180976 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-092: c622-092:418610:418781 [0] NCCL INFO Connected all trees
c622-092: c622-092:418610:418781 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-092: c622-092:418610:418781 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-101: c622-101:1627370:1627541 [0] NCCL INFO Connected all trees
c622-082: c622-082:2786472:2786643 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-102: c622-102:2511218:2511389 [0] NCCL INFO Connected all trees
c622-102: c622-102:2511218:2511389 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-102: c622-102:2511218:2511389 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-101: c622-101:1627370:1627541 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-101: c622-101:1627370:1627541 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-071: c622-071:494914:495085 [0] NCCL INFO Connected all trees
c622-071: c622-071:494914:495085 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-071: c622-071:494914:495085 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-081: c622-081:41435:41606 [0] NCCL INFO Connected all trees
c622-072: c622-072:1688285:1688456 [0] NCCL INFO Connected all trees
c622-072: c622-072:1688285:1688456 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-072: c622-072:1688285:1688456 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-081: c622-081:41435:41606 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-081: c622-081:41435:41606 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-152: c613-152:3770490:3770661 [0] NCCL INFO Connected all trees
c613-152: c613-152:3770490:3770661 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-152: c613-152:3770490:3770661 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-141: c613-141:2104776:2104947 [0] NCCL INFO Connected all trees
c613-141: c613-141:2104776:2104947 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-141: c613-141:2104776:2104947 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-142: c613-142:3123258:3123429 [0] NCCL INFO Connected all trees
c613-142: c613-142:3123258:3123429 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-151: c613-151:3397419:3397590 [0] NCCL INFO Connected all trees
c613-142: c613-142:3123258:3123429 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-151: c613-151:3397419:3397590 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-151: c613-151:3397419:3397590 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-011: c622-011:710508:710679 [0] NCCL INFO Connected all trees
c622-011: c622-011:710508:710679 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-011: c622-011:710508:710679 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-102: c621-102:2281097:2281268 [0] NCCL INFO Connected all trees
c621-102: c621-102:2281097:2281268 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-102: c621-102:2281097:2281268 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-132: c621-132:518779:518950 [0] NCCL INFO Connected all trees
c621-141: c621-141:3859461:3859632 [0] NCCL INFO Connected all trees
c621-131: c621-131:2225085:2225256 [0] NCCL INFO Connected all trees
c621-131: c621-131:2225085:2225256 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-131: c621-131:2225085:2225256 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-141: c621-141:3859461:3859632 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-132: c621-132:518779:518950 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-132: c621-132:518779:518950 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-141: c621-141:3859461:3859632 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-012: c619-012:246412:246583 [0] NCCL INFO Connected all trees
c619-012: c619-012:246412:246583 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-012: c619-012:246412:246583 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-062: c621-062:1471760:1471931 [0] NCCL INFO Connected all trees
c619-001: c619-001:2454729:2454900 [0] NCCL INFO Connected all trees
c619-001: c619-001:2454729:2454900 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-001: c619-001:2454729:2454900 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-002: c619-002:83791:83962 [0] NCCL INFO Connected all trees
c613-101: c613-101:387979:388152 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-101: c613-101:387979:388152 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-101: c613-101:387979:388152 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf87e37d0 rank 0 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-062: c621-062:1471760:1471931 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-062: c621-062:1471760:1471931 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-002: c619-002:83791:83962 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-002: c619-002:83791:83962 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-012: c622-012:2903325:2903496 [0] NCCL INFO Connected all trees
c622-012: c622-012:2903325:2903496 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-012: c622-012:2903325:2903496 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-011: c619-011:219279:219451 [0] NCCL INFO Connected all trees
c619-032: c619-032:3546706:3546877 [0] NCCL INFO Connected all trees
c619-011: c619-011:219279:219451 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-011: c619-011:219279:219451 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-051: c622-051:3538690:3538861 [0] NCCL INFO Connected all trees
c619-032: c619-032:3546706:3546877 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-041: c619-041:15310:15481 [0] NCCL INFO Connected all trees
c622-051: c622-051:3538690:3538861 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-051: c622-051:3538690:3538861 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-052: c622-052:990541:990712 [0] NCCL INFO Connected all trees
c619-032: c619-032:3546706:3546877 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-061: c622-061:1156840:1157011 [0] NCCL INFO Connected all trees
c619-041: c619-041:15310:15481 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-041: c619-041:15310:15481 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-061: c622-061:1156840:1157011 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-052: c622-052:990541:990712 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-052: c622-052:990541:990712 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-052: c621-052:855162:855333 [0] NCCL INFO Connected all trees
c622-031: c622-031:3136042:3136213 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-031: c622-031:3136042:3136213 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-031: c622-031:3136042:3136213 [0] NCCL INFO ncclCommInitRank comm 0xaaab19de4480 rank 48 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-082: c621-082:1003419:1003590 [0] NCCL INFO Connected all trees
c613-102: c613-102:1660081:1660252 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-102: c613-102:1660081:1660252 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-052: c621-052:855162:855333 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-052: c621-052:855162:855333 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-102: c613-102:1660081:1660252 [0] NCCL INFO ncclCommInitRank comm 0xaaaad5e93020 rank 1 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-061: c621-061:3120949:3121120 [0] NCCL INFO Connected all trees
c621-061: c621-061:3120949:3121120 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-061: c622-061:1156840:1157011 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-061: c621-061:3120949:3121120 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-082: c621-082:1003419:1003590 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-082: c621-082:1003419:1003590 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-021: c619-021:593675:593846 [0] NCCL INFO Connected all trees
c619-021: c619-021:593675:593846 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-111: c621-111:3868077:3868248 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-111: c621-111:3868077:3868248 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-071: c621-071:2842568:2842739 [0] NCCL INFO Connected all trees
c621-151: c621-151:336402:336573 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-151: c621-151:336402:336573 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-071: c621-071:2842568:2842739 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-071: c621-071:2842568:2842739 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-151: c621-151:336402:336573 [0] NCCL INFO ncclCommInitRank comm 0xaaab31fb4310 rank 40 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-112: c613-112:1585580:1585751 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-112: c613-112:1585580:1585751 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-022: c619-022:804214:804385 [0] NCCL INFO Connected all trees
c621-111: c621-111:3868077:3868248 [0] NCCL INFO ncclCommInitRank comm 0xaaaadde53570 rank 32 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-112: c613-112:1585580:1585751 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa5643c0 rank 3 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-091: c621-091:102446:102617 [0] NCCL INFO Connected all trees
c621-072: c621-072:3048145:3048316 [0] NCCL INFO Connected all trees
c619-022: c619-022:804214:804385 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-022: c619-022:804214:804385 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-062: c622-062:2304142:2304313 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-092: c621-092:1627105:1627276 [0] NCCL INFO Connected all trees
c622-062: c622-062:2304142:2304313 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-112: c621-112:411646:411817 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-021: c619-021:593675:593846 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-112: c621-112:411646:411817 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-091: c621-091:102446:102617 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-091: c621-091:102446:102617 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-072: c621-072:3048145:3048316 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-081: c621-081:2075589:2075760 [0] NCCL INFO Connected all trees
c621-072: c621-072:3048145:3048316 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-111: c613-111:640696:640867 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-092: c621-092:1627105:1627276 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-112: c621-112:411646:411817 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa972940 rank 33 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-101: c621-101:813656:813828 [0] NCCL INFO Connected all trees
c621-152: c621-152:1793414:1793585 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-092: c621-092:1627105:1627276 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-031: c619-031:361171:361342 [0] NCCL INFO Connected all trees
c622-062: c622-062:2304142:2304313 [0] NCCL INFO ncclCommInitRank comm 0xaaaaefc52d60 rank 55 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-111: c613-111:640696:640867 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-152: c621-152:1793414:1793585 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-081: c621-081:2075589:2075760 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-131: c613-131:931664:931835 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-081: c621-081:2075589:2075760 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-131: c613-131:931664:931835 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-121: c621-121:1502414:1502585 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-031: c619-031:361171:361342 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-111: c613-111:640696:640867 [0] NCCL INFO ncclCommInitRank comm 0xaaaaff0d4900 rank 2 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-121: c621-121:1502414:1502585 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-152: c621-152:1793414:1793585 [0] NCCL INFO ncclCommInitRank comm 0xaaaaefa62bc0 rank 41 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-122: c613-122:1267677:1267848 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-101: c621-101:813656:813828 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-122: c613-122:1267677:1267848 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-101: c621-101:813656:813828 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-031: c619-031:361171:361342 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-131: c613-131:931664:931835 [0] NCCL INFO ncclCommInitRank comm 0xaaab00253330 rank 6 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-121: c621-121:1502414:1502585 [0] NCCL INFO ncclCommInitRank comm 0xaaaac7ad29a0 rank 34 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-122: c621-122:1561826:1561997 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-122: c621-122:1561826:1561997 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-122: c613-122:1267677:1267848 [0] NCCL INFO ncclCommInitRank comm 0xaaaacc441df0 rank 5 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-122: c621-122:1561826:1561997 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf42a34a0 rank 35 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-132: c613-132:990252:990423 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-001: c622-001:3806195:3806366 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-041: c622-041:198407:198578 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-132: c613-132:990252:990423 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-041: c622-041:198407:198578 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-041: c622-041:198407:198578 [0] NCCL INFO ncclCommInitRank comm 0xaaaae2173010 rank 50 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-001: c622-001:3806195:3806366 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-001: c622-001:3806195:3806366 [0] NCCL INFO ncclCommInitRank comm 0xaaaafa503ef0 rank 42 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-021: c622-021:1141571:1141742 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-021: c622-021:1141571:1141742 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-132: c613-132:990252:990423 [0] NCCL INFO ncclCommInitRank comm 0xaaaadc6b2810 rank 7 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-002: c622-002:1377821:1377992 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-032: c622-032:1587523:1587694 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-032: c622-032:1587523:1587694 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-002: c622-002:1377821:1377992 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-002: c622-002:1377821:1377992 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5e63ae0 rank 43 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-021: c622-021:1141571:1141742 [0] NCCL INFO ncclCommInitRank comm 0xaaaaec1b3660 rank 46 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-032: c622-032:1587523:1587694 [0] NCCL INFO ncclCommInitRank comm 0xaaaaec6b4890 rank 49 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-022: c622-022:180805:180976 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-022: c622-022:180805:180976 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-022: c622-022:180805:180976 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf9b932b0 rank 47 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-092: c622-092:418610:418781 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-092: c622-092:418610:418781 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-082: c622-082:2786472:2786643 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-082: c622-082:2786472:2786643 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-092: c622-092:418610:418781 [0] NCCL INFO ncclCommInitRank comm 0xaaab065a30c0 rank 61 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-101: c622-101:1627370:1627541 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-101: c622-101:1627370:1627541 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-082: c622-082:2786472:2786643 [0] NCCL INFO ncclCommInitRank comm 0xaaab0d0846f0 rank 59 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-042: c622-042:654000:654172 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-042: c622-042:654000:654172 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-101: c622-101:1627370:1627541 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf8c135d0 rank 62 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-042: c622-042:654000:654172 [0] NCCL INFO ncclCommInitRank comm 0xaaaadc7a5b50 rank 51 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-081: c622-081:41435:41606 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-081: c622-081:41435:41606 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-091: c622-091:2780457:2780628 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-142: c621-142:600426:600600 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-091: c622-091:2780457:2780628 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-142: c621-142:600426:600600 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-091: c622-091:2780457:2780628 [0] NCCL INFO ncclCommInitRank comm 0xaaaad2a14660 rank 60 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-081: c622-081:41435:41606 [0] NCCL INFO ncclCommInitRank comm 0xaaaae73d3c50 rank 58 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-102: c622-102:2511218:2511389 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-102: c622-102:2511218:2511389 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-142: c621-142:600426:600600 [0] NCCL INFO ncclCommInitRank comm 0xaaaafcef3f00 rank 39 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-102: c622-102:2511218:2511389 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf35a4020 rank 63 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-072: c622-072:1688285:1688456 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-072: c622-072:1688285:1688456 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-072: c622-072:1688285:1688456 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf1bc32f0 rank 57 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-071: c622-071:494914:495085 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-071: c622-071:494914:495085 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-152: c613-152:3770490:3770661 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-152: c613-152:3770490:3770661 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-071: c622-071:494914:495085 [0] NCCL INFO ncclCommInitRank comm 0xaaaadd881dc0 rank 56 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-141: c613-141:2104776:2104947 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-141: c613-141:2104776:2104947 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-152: c613-152:3770490:3770661 [0] NCCL INFO ncclCommInitRank comm 0xaaaae1154720 rank 11 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-141: c613-141:2104776:2104947 [0] NCCL INFO ncclCommInitRank comm 0xaaab1eb11de0 rank 8 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-142: c613-142:3123258:3123429 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-142: c613-142:3123258:3123429 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-142: c613-142:3123258:3123429 [0] NCCL INFO ncclCommInitRank comm 0xaaaae9e74700 rank 9 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-151: c613-151:3397419:3397590 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-151: c613-151:3397419:3397590 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-011: c622-011:710508:710679 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-011: c622-011:710508:710679 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-151: c613-151:3397419:3397590 [0] NCCL INFO ncclCommInitRank comm 0xaaaadaeb2c20 rank 10 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-011: c622-011:710508:710679 [0] NCCL INFO ncclCommInitRank comm 0xaaab12fd3650 rank 44 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-102: c621-102:2281097:2281268 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-102: c621-102:2281097:2281268 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-102: c621-102:2281097:2281268 [0] NCCL INFO ncclCommInitRank comm 0xaaab01513280 rank 31 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-141: c621-141:3859461:3859632 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-141: c621-141:3859461:3859632 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-132: c621-132:518779:518950 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-132: c621-132:518779:518950 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c613-121: c613-121:904347:904518 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c613-121: c613-121:904347:904518 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-141: c621-141:3859461:3859632 [0] NCCL INFO ncclCommInitRank comm 0xaaab05fc3cd0 rank 38 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-132: c621-132:518779:518950 [0] NCCL INFO ncclCommInitRank comm 0xaaab08e331b0 rank 37 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-121: c613-121:904347:904518 [0] NCCL INFO ncclCommInitRank comm 0xaaab03501d90 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-001: c619-001:2454729:2454900 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-001: c619-001:2454729:2454900 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-131: c621-131:2225085:2225256 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-001: c619-001:2454729:2454900 [0] NCCL INFO ncclCommInitRank comm 0xaaab21114760 rank 12 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-131: c621-131:2225085:2225256 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-131: c621-131:2225085:2225256 [0] NCCL INFO ncclCommInitRank comm 0xaaaad9263e70 rank 36 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-012: c619-012:246412:246583 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-012: c619-012:246412:246583 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-062: c621-062:1471760:1471931 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-062: c621-062:1471760:1471931 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-012: c619-012:246412:246583 [0] NCCL INFO ncclCommInitRank comm 0xaaaae5903380 rank 15 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-062: c621-062:1471760:1471931 [0] NCCL INFO ncclCommInitRank comm 0xaaab20314540 rank 23 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-002: c619-002:83791:83962 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-002: c619-002:83791:83962 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-002: c619-002:83791:83962 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef322ea0 rank 13 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-012: c622-012:2903325:2903496 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-012: c622-012:2903325:2903496 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-012: c622-012:2903325:2903496 [0] NCCL INFO ncclCommInitRank comm 0xaaaacd7e5250 rank 45 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-032: c619-032:3546706:3546877 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-032: c619-032:3546706:3546877 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-051: c622-051:3538690:3538861 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-052: c622-052:990541:990712 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-032: c619-032:3546706:3546877 [0] NCCL INFO ncclCommInitRank comm 0xaaab06d93bf0 rank 19 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-051: c622-051:3538690:3538861 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-052: c622-052:990541:990712 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-082: c621-082:1003419:1003590 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-041: c619-041:15310:15481 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-041: c619-041:15310:15481 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-082: c621-082:1003419:1003590 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-051: c622-051:3538690:3538861 [0] NCCL INFO ncclCommInitRank comm 0xaaab056632b0 rank 52 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-041: c619-041:15310:15481 [0] NCCL INFO ncclCommInitRank comm 0xaaaada3c4450 rank 20 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-011: c619-011:219279:219451 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-011: c619-011:219279:219451 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c622-052: c622-052:990541:990712 [0] NCCL INFO ncclCommInitRank comm 0xaaab2f773010 rank 53 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-082: c621-082:1003419:1003590 [0] NCCL INFO ncclCommInitRank comm 0xaaaaef4b43b0 rank 27 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-061: c622-061:1156840:1157011 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c622-061: c622-061:1156840:1157011 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-011: c619-011:219279:219451 [0] NCCL INFO ncclCommInitRank comm 0xaaab0f1f2940 rank 14 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c622-061: c622-061:1156840:1157011 [0] NCCL INFO ncclCommInitRank comm 0xaaaaea363b30 rank 54 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-072: c621-072:3048145:3048316 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-072: c621-072:3048145:3048316 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-071: c621-071:2842568:2842739 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-071: c621-071:2842568:2842739 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-092: c621-092:1627105:1627276 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-092: c621-092:1627105:1627276 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-061: c621-061:3120949:3121120 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-061: c621-061:3120949:3121120 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-052: c621-052:855162:855333 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-052: c621-052:855162:855333 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-092: c621-092:1627105:1627276 [0] NCCL INFO ncclCommInitRank comm 0xaaaafaf44150 rank 29 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-081: c621-081:2075589:2075760 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-061: c621-061:3120949:3121120 [0] NCCL INFO ncclCommInitRank comm 0xaaab28873410 rank 22 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-072: c621-072:3048145:3048316 [0] NCCL INFO ncclCommInitRank comm 0xaaaac7562e90 rank 25 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-081: c621-081:2075589:2075760 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-081: c621-081:2075589:2075760 [0] NCCL INFO ncclCommInitRank comm 0xaaaae8834680 rank 26 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-071: c621-071:2842568:2842739 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5803340 rank 24 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-052: c621-052:855162:855333 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf3913390 rank 21 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-101: c621-101:813656:813828 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-031: c619-031:361171:361342 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-031: c619-031:361171:361342 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-021: c619-021:593675:593846 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-021: c619-021:593675:593846 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c619-022: c619-022:804214:804385 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c619-022: c619-022:804214:804385 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-101: c621-101:813656:813828 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-101: c621-101:813656:813828 [0] NCCL INFO ncclCommInitRank comm 0xaaaae5fa3640 rank 30 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-031: c619-031:361171:361342 [0] NCCL INFO ncclCommInitRank comm 0xaaaad6982fb0 rank 18 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-021: c619-021:593675:593846 [0] NCCL INFO ncclCommInitRank comm 0xaaaacef91690 rank 16 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c619-022: c619-022:804214:804385 [0] NCCL INFO ncclCommInitRank comm 0xaaaaf5e93920 rank 17 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c621-091: c621-091:102446:102617 [0] NCCL INFO TUNER/Plugin: Plugin load returned 11 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
c621-091: c621-091:102446:102617 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
c621-091: c621-091:102446:102617 [0] NCCL INFO ncclCommInitRank comm 0xaaaae7334ad0 rank 28 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 commId 0x1ca910af6376340f - Init COMPLETE
c613-101: tokenizer.eos_token: <|end_of_text|>
c613-122: [2025-03-01 17:47:22,599] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-071: [2025-03-01 17:47:22,609] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-152: [2025-03-01 17:47:22,617] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-071: [2025-03-01 17:47:22,617] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-091: [2025-03-01 17:47:22,622] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-002: [2025-03-01 17:47:22,622] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-112: [2025-03-01 17:47:22,634] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-002: [2025-03-01 17:47:22,638] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-101: [2025-03-01 17:47:22,640] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-052: [2025-03-01 17:47:22,642] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-082: [2025-03-01 17:47:22,642] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-132: [2025-03-01 17:47:22,642] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-132: [2025-03-01 17:47:22,642] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-072: [2025-03-01 17:47:22,644] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-121: [2025-03-01 17:47:22,645] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-142: [2025-03-01 17:47:22,646] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-122: [2025-03-01 17:47:22,646] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-031: [2025-03-01 17:47:22,647] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-141: [2025-03-01 17:47:22,647] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-102: [2025-03-01 17:47:22,647] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-061: [2025-03-01 17:47:22,648] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-021: [2025-03-01 17:47:22,648] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-101: [2025-03-01 17:47:22,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-142: [2025-03-01 17:47:22,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-032: [2025-03-01 17:47:22,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-072: [2025-03-01 17:47:22,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-022: [2025-03-01 17:47:22,649] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-111: [2025-03-01 17:47:22,650] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-051: [2025-03-01 17:47:22,650] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-082: [2025-03-01 17:47:22,650] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-121: [2025-03-01 17:47:22,652] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-011: [2025-03-01 17:47:22,652] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-081: [2025-03-01 17:47:22,653] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-151: [2025-03-01 17:47:22,654] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-032: [2025-03-01 17:47:22,654] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-112: [2025-03-01 17:47:22,654] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-091: [2025-03-01 17:47:22,655] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-102: [2025-03-01 17:47:22,656] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-061: [2025-03-01 17:47:22,657] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-041: [2025-03-01 17:47:22,657] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-081: [2025-03-01 17:47:22,657] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-152: [2025-03-01 17:47:22,658] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-131: [2025-03-01 17:47:22,658] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-111: [2025-03-01 17:47:22,658] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-021: [2025-03-01 17:47:22,658] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-042: [2025-03-01 17:47:22,660] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-001: [2025-03-01 17:47:22,661] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-151: [2025-03-01 17:47:22,661] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-102: [2025-03-01 17:47:22,662] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-092: [2025-03-01 17:47:22,662] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-011: [2025-03-01 17:47:22,662] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-092: [2025-03-01 17:47:22,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-001: [2025-03-01 17:47:22,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-012: [2025-03-01 17:47:22,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-062: [2025-03-01 17:47:22,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-052: [2025-03-01 17:47:22,664] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:47:22,668] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-131: [2025-03-01 17:47:22,670] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-012: [2025-03-01 17:47:22,674] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-062: [2025-03-01 17:47:22,678] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-041: [2025-03-01 17:47:22,685] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-022: [2025-03-01 17:47:22,706] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-141: [2025-03-01 17:47:22,707] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-031: [2025-03-01 17:47:22,714] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:47:24,906] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 8.03B
c619-012: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-122: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-051: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-052: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-141: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c613-121: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-071: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-151: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.56s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-032: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-082: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-042: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-111: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-082: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-152: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-061: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-112: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-002: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c619-011: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c613-112: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.56s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c619-031: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c619-021: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-011: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-052: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-131: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-131: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-081: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-132: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-012: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-151: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-021: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c619-032: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-121: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-072: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-142: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-022: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c619-001: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c619-041: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-152: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-141: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-001: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-111: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c619-002: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-092: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-072: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-071: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c619-022: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-041: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-062: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-092: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-091: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-091: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c613-122: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.55s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c621-132: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-142: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-061: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-031: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c621-062: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c613-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.57s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.66s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:06<00:26, 13.06s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c622-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.54s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.65s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.35s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.94s/it]
c622-081: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:08<00:51,  8.53s/it]Loading checkpoint shards:  29%|       | 2/7 [00:25<01:08, 13.64s/it]Loading checkpoint shards:  43%|     | 3/7 [00:42<00:59, 14.86s/it]Loading checkpoint shards:  57%|    | 4/7 [00:54<00:41, 13.83s/it]Loading checkpoint shards:  71%|  | 5/7 [01:05<00:26, 13.05s/it]Loading checkpoint shards:  86%| | 6/7 [01:18<00:12, 12.79s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 10.36s/it]Loading checkpoint shards: 100%|| 7/7 [01:23<00:00, 11.95s/it]
c613-101: [2025-03-01 17:48:48,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-111: [2025-03-01 17:48:48,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-121: [2025-03-01 17:48:48,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-112: [2025-03-01 17:48:48,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-102: [2025-03-01 17:48:48,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-131: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-141: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-012: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-152: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-032: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-132: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-011: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-002: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-122: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-031: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-061: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-022: [2025-03-01 17:48:48,898] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-021: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-041: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-062: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-101: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-122: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-111: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-082: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-081: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-112: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-152: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-072: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-091: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-092: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-151: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-131: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-141: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-121: [2025-03-01 17:48:48,899] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-102: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-032: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-052: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-011: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-002: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-142: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-132: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-012: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-072: [2025-03-01 17:48:48,900] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-022: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-021: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-041: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-051: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-031: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-042: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-001: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-001: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-142: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-052: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-082: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-092: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-091: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-061: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-071: [2025-03-01 17:48:48,901] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-102: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-151: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-101: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-062: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-071: [2025-03-01 17:48:48,902] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-081: [2025-03-01 17:48:48,903] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:48:49,030] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 582, num_elems = 16.06B
c622-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-082: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.34s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-011: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-091: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-112: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-041: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-081: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-111: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-111: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-012: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-021: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-132: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-151: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-082: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-061: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-071: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-032: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:08,  1.35s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-062: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-121: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-152: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-142: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-132: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-011: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-081: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-002: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-112: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-021: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-052: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-071: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.05it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.34s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-051: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-142: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-052: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-091: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-072: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-062: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:08,  1.35s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.01it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.61s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.75s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.34s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-032: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-131: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-022: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-042: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-141: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-102: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-012: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-122: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-122: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-022: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-031: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-151: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-031: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-002: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-092: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-001: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-121: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.00it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-141: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-092: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c621-152: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-131: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-001: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.32s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c619-041: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.31s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.00s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.09it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.06it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.60s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.76s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-072: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:07,  1.30s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.03s/it]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.10it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.05it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.59s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.77s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.34s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c622-061: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:08,  1.34s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:04,  1.01it/s]Loading checkpoint shards:  43%|     | 3/7 [00:02<00:03,  1.08it/s]Loading checkpoint shards:  57%|    | 4/7 [00:03<00:02,  1.07it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.59s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.75s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.35s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.32s/it]
c613-101: Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|        | 1/7 [00:01<00:08,  1.46s/it]Loading checkpoint shards:  29%|       | 2/7 [00:02<00:05,  1.06s/it]Loading checkpoint shards:  43%|     | 3/7 [00:03<00:03,  1.05it/s]Loading checkpoint shards:  57%|    | 4/7 [00:04<00:02,  1.04it/s]Loading checkpoint shards:  71%|  | 5/7 [00:06<00:03,  1.61s/it]Loading checkpoint shards:  86%| | 6/7 [00:08<00:01,  1.77s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.33s/it]Loading checkpoint shards: 100%|| 7/7 [00:09<00:00,  1.33s/it]
c613-101: Creating prompt dataset ['local/jsonfile'], reload=False
c613-111: Creating prompt dataset ['local/jsonfile'], reload=False
c613-102: Creating prompt dataset ['local/jsonfile'], reload=False
c613-112: Creating prompt dataset ['local/jsonfile'], reload=False
c613-121: Creating prompt dataset ['local/jsonfile'], reload=False
c613-122: Creating prompt dataset ['local/jsonfile'], reload=False
c613-132: Creating prompt dataset ['local/jsonfile'], reload=False
c613-131: Creating prompt dataset ['local/jsonfile'], reload=False
c613-142: Creating prompt dataset ['local/jsonfile'], reload=False
c613-141: Creating prompt dataset ['local/jsonfile'], reload=False
c613-151: Creating prompt dataset ['local/jsonfile'], reload=False
c613-152: Creating prompt dataset ['local/jsonfile'], reload=False
c619-001: Creating prompt dataset ['local/jsonfile'], reload=False
c619-002: Creating prompt dataset ['local/jsonfile'], reload=False
c619-011: Creating prompt dataset ['local/jsonfile'], reload=False
c619-012: Creating prompt dataset ['local/jsonfile'], reload=False
c619-021: Creating prompt dataset ['local/jsonfile'], reload=False
c619-031: Creating prompt dataset ['local/jsonfile'], reload=False
c619-022: Creating prompt dataset ['local/jsonfile'], reload=False
c619-032: Creating prompt dataset ['local/jsonfile'], reload=False
c619-041: Creating prompt dataset ['local/jsonfile'], reload=False
c621-052: Creating prompt dataset ['local/jsonfile'], reload=False
c621-061: Creating prompt dataset ['local/jsonfile'], reload=False
c621-062: Creating prompt dataset ['local/jsonfile'], reload=False
c621-071: Creating prompt dataset ['local/jsonfile'], reload=False
c621-081: Creating prompt dataset ['local/jsonfile'], reload=False
c621-091: Creating prompt dataset ['local/jsonfile'], reload=False
c621-082: Creating prompt dataset ['local/jsonfile'], reload=False
c621-072: Creating prompt dataset ['local/jsonfile'], reload=False
c621-111: Creating prompt dataset ['local/jsonfile'], reload=False
c621-092: Creating prompt dataset ['local/jsonfile'], reload=False
c621-102: Creating prompt dataset ['local/jsonfile'], reload=False
c621-121: Creating prompt dataset ['local/jsonfile'], reload=False
c621-101: Creating prompt dataset ['local/jsonfile'], reload=False
c621-112: Creating prompt dataset ['local/jsonfile'], reload=False
c621-131: Creating prompt dataset ['local/jsonfile'], reload=False
c621-142: Creating prompt dataset ['local/jsonfile'], reload=False
c621-141: Creating prompt dataset ['local/jsonfile'], reload=False
c621-151: Creating prompt dataset ['local/jsonfile'], reload=False
c622-001: Creating prompt dataset ['local/jsonfile'], reload=False
c622-031: Creating prompt dataset ['local/jsonfile'], reload=False
c622-011: Creating prompt dataset ['local/jsonfile'], reload=False
c622-021: Creating prompt dataset ['local/jsonfile'], reload=False
c621-122: Creating prompt dataset ['local/jsonfile'], reload=False
c622-022: Creating prompt dataset ['local/jsonfile'], reload=False
c622-012: Creating prompt dataset ['local/jsonfile'], reload=False
c621-132: Creating prompt dataset ['local/jsonfile'], reload=False
c622-062: Creating prompt dataset ['local/jsonfile'], reload=False
c621-152: Creating prompt dataset ['local/jsonfile'], reload=False
c622-051: Creating prompt dataset ['local/jsonfile'], reload=False
c622-081: Creating prompt dataset ['local/jsonfile'], reload=False
c622-042: Creating prompt dataset ['local/jsonfile'], reload=False
c622-002: Creating prompt dataset ['local/jsonfile'], reload=False
c622-041: Creating prompt dataset ['local/jsonfile'], reload=False
c622-072: Creating prompt dataset ['local/jsonfile'], reload=False
c622-061: Creating prompt dataset ['local/jsonfile'], reload=False
c622-101: Creating prompt dataset ['local/jsonfile'], reload=False
c622-052: Creating prompt dataset ['local/jsonfile'], reload=False
c622-071: Creating prompt dataset ['local/jsonfile'], reload=False
c622-092: Creating prompt dataset ['local/jsonfile'], reload=False
c622-032: Creating prompt dataset ['local/jsonfile'], reload=False
c622-102: Creating prompt dataset ['local/jsonfile'], reload=False
c622-082: Creating prompt dataset ['local/jsonfile'], reload=False
c622-091: Creating prompt dataset ['local/jsonfile'], reload=False
c621-112: Creating dataset jsonfile for train_phase=2 size=28887
c613-121: Creating dataset jsonfile for train_phase=2 size=28887
c622-062: Creating dataset jsonfile for train_phase=2 size=28887
c621-131: Creating dataset jsonfile for train_phase=2 size=28887
c621-082: Creating dataset jsonfile for train_phase=2 size=28887
c619-021: Creating dataset jsonfile for train_phase=2 size=28887
c621-081: Creating dataset jsonfile for train_phase=2 size=28887
c613-141: Creating dataset jsonfile for train_phase=2 size=28887
c621-111: Creating dataset jsonfile for train_phase=2 size=28887
c622-072: Creating dataset jsonfile for train_phase=2 size=28887
c613-111: Creating dataset jsonfile for train_phase=2 size=28887
c621-151: Creating dataset jsonfile for train_phase=2 size=28887
c621-142: Creating dataset jsonfile for train_phase=2 size=28887
c621-092: Creating dataset jsonfile for train_phase=2 size=28887
c621-101: Creating dataset jsonfile for train_phase=2 size=28887
c613-152: Creating dataset jsonfile for train_phase=2 size=28887
c621-152: Creating dataset jsonfile for train_phase=2 size=28887
c622-002: Creating dataset jsonfile for train_phase=2 size=28887
c613-132: Creating dataset jsonfile for train_phase=2 size=28887
c613-112: Creating dataset jsonfile for train_phase=2 size=28887
c619-002: Creating dataset jsonfile for train_phase=2 size=28887
c619-001: Creating dataset jsonfile for train_phase=2 size=28887
c621-071: Creating dataset jsonfile for train_phase=2 size=28887
c622-011: Creating dataset jsonfile for train_phase=2 size=28887
c613-122: Creating dataset jsonfile for train_phase=2 size=28887
c613-102: Creating dataset jsonfile for train_phase=2 size=28887
c621-102: Creating dataset jsonfile for train_phase=2 size=28887
c621-121: Creating dataset jsonfile for train_phase=2 size=28887
c621-062: Creating dataset jsonfile for train_phase=2 size=28887
c613-142: Creating dataset jsonfile for train_phase=2 size=28887
c621-072: Creating dataset jsonfile for train_phase=2 size=28887
c619-031: Creating dataset jsonfile for train_phase=2 size=28887
c621-122: Creating dataset jsonfile for train_phase=2 size=28887
c619-041: Creating dataset jsonfile for train_phase=2 size=28887
c619-012: Creating dataset jsonfile for train_phase=2 size=28887
c621-091: Creating dataset jsonfile for train_phase=2 size=28887
c619-032: Creating dataset jsonfile for train_phase=2 size=28887
c619-022: Creating dataset jsonfile for train_phase=2 size=28887
c621-132: Creating dataset jsonfile for train_phase=2 size=28887
c613-101: Creating dataset jsonfile for train_phase=2 size=28887
c613-151: Creating dataset jsonfile for train_phase=2 size=28887
c613-131: Creating dataset jsonfile for train_phase=2 size=28887
c619-011: Creating dataset jsonfile for train_phase=2 size=28887
c621-112: Creating dataset jsonfile for train_phase=2 size=1524
c613-121: Creating dataset jsonfile for train_phase=2 size=1524
c622-062: Creating dataset jsonfile for train_phase=2 size=1524
c622-022: Creating dataset jsonfile for train_phase=2 size=28887
c621-061: Creating dataset jsonfile for train_phase=2 size=28887
c621-131: Creating dataset jsonfile for train_phase=2 size=1524
c621-082: Creating dataset jsonfile for train_phase=2 size=1524
c619-021: Creating dataset jsonfile for train_phase=2 size=1524
c621-111: Creating dataset jsonfile for train_phase=2 size=1524
c621-081: Creating dataset jsonfile for train_phase=2 size=1524
c613-141: Creating dataset jsonfile for train_phase=2 size=1524
c622-072: Creating dataset jsonfile for train_phase=2 size=1524
c613-111: Creating dataset jsonfile for train_phase=2 size=1524
c622-012: Creating dataset jsonfile for train_phase=2 size=28887
c621-151: Creating dataset jsonfile for train_phase=2 size=1524
c621-142: Creating dataset jsonfile for train_phase=2 size=1524
c621-101: Creating dataset jsonfile for train_phase=2 size=1524
c621-092: Creating dataset jsonfile for train_phase=2 size=1524
c613-152: Creating dataset jsonfile for train_phase=2 size=1524
c622-031: Creating dataset jsonfile for train_phase=2 size=28887
c622-082: Creating dataset jsonfile for train_phase=2 size=28887
c622-032: Creating dataset jsonfile for train_phase=2 size=28887
c622-081: Creating dataset jsonfile for train_phase=2 size=28887
c621-052: Creating dataset jsonfile for train_phase=2 size=28887
c622-041: Creating dataset jsonfile for train_phase=2 size=28887
c622-002: Creating dataset jsonfile for train_phase=2 size=1524
c621-152: Creating dataset jsonfile for train_phase=2 size=1524
c622-052: Creating dataset jsonfile for train_phase=2 size=28887
c613-132: Creating dataset jsonfile for train_phase=2 size=1524
c613-112: Creating dataset jsonfile for train_phase=2 size=1524
c619-002: Creating dataset jsonfile for train_phase=2 size=1524
c622-071: Creating dataset jsonfile for train_phase=2 size=28887
c622-092: Creating dataset jsonfile for train_phase=2 size=28887
c619-001: Creating dataset jsonfile for train_phase=2 size=1524
c621-071: Creating dataset jsonfile for train_phase=2 size=1524
c622-101: Creating dataset jsonfile for train_phase=2 size=28887
c622-061: Creating dataset jsonfile for train_phase=2 size=28887
c622-011: Creating dataset jsonfile for train_phase=2 size=1524
c613-122: Creating dataset jsonfile for train_phase=2 size=1524
c622-091: Creating dataset jsonfile for train_phase=2 size=28887
c613-102: Creating dataset jsonfile for train_phase=2 size=1524
c621-102: Creating dataset jsonfile for train_phase=2 size=1524
c621-121: Creating dataset jsonfile for train_phase=2 size=1524
c621-062: Creating dataset jsonfile for train_phase=2 size=1524
c613-142: Creating dataset jsonfile for train_phase=2 size=1524
c621-072: Creating dataset jsonfile for train_phase=2 size=1524
c622-102: Creating dataset jsonfile for train_phase=2 size=28887
c619-031: Creating dataset jsonfile for train_phase=2 size=1524
c622-021: Creating dataset jsonfile for train_phase=2 size=28887
c621-122: Creating dataset jsonfile for train_phase=2 size=1524
c619-041: Creating dataset jsonfile for train_phase=2 size=1524
c619-012: Creating dataset jsonfile for train_phase=2 size=1524
c621-091: Creating dataset jsonfile for train_phase=2 size=1524
c619-032: Creating dataset jsonfile for train_phase=2 size=1524
c619-022: Creating dataset jsonfile for train_phase=2 size=1524
c613-101: Creating dataset jsonfile for train_phase=2 size=1524
c622-042: Creating dataset jsonfile for train_phase=2 size=28887
c621-132: Creating dataset jsonfile for train_phase=2 size=1524
c621-141: Creating dataset jsonfile for train_phase=2 size=28887
c622-051: Creating dataset jsonfile for train_phase=2 size=28887
c613-151: Creating dataset jsonfile for train_phase=2 size=1524
c622-001: Creating dataset jsonfile for train_phase=2 size=28887
c613-131: Creating dataset jsonfile for train_phase=2 size=1524
c619-011: Creating dataset jsonfile for train_phase=2 size=1524
c622-022: Creating dataset jsonfile for train_phase=2 size=1524
c621-061: Creating dataset jsonfile for train_phase=2 size=1524
c622-012: Creating dataset jsonfile for train_phase=2 size=1524
c622-031: Creating dataset jsonfile for train_phase=2 size=1524
c622-082: Creating dataset jsonfile for train_phase=2 size=1524
c622-032: Creating dataset jsonfile for train_phase=2 size=1524
c622-081: Creating dataset jsonfile for train_phase=2 size=1524
c621-052: Creating dataset jsonfile for train_phase=2 size=1524
c622-041: Creating dataset jsonfile for train_phase=2 size=1524
c622-052: Creating dataset jsonfile for train_phase=2 size=1524
c622-071: Creating dataset jsonfile for train_phase=2 size=1524
c622-092: Creating dataset jsonfile for train_phase=2 size=1524
c622-101: Creating dataset jsonfile for train_phase=2 size=1524
c622-061: Creating dataset jsonfile for train_phase=2 size=1524
c622-091: Creating dataset jsonfile for train_phase=2 size=1524
c622-021: Creating dataset jsonfile for train_phase=2 size=1524
c622-102: Creating dataset jsonfile for train_phase=2 size=1524
c622-042: Creating dataset jsonfile for train_phase=2 size=1524
c622-051: Creating dataset jsonfile for train_phase=2 size=1524
c621-141: Creating dataset jsonfile for train_phase=2 size=1524
c622-001: Creating dataset jsonfile for train_phase=2 size=1524
c613-101: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-101:   return torch.load(train_fname), torch.load(eval_fname)
c619-021: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-021:   return torch.load(train_fname), torch.load(eval_fname)
c613-121: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-121:   return torch.load(train_fname), torch.load(eval_fname)
c621-111: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-111:   return torch.load(train_fname), torch.load(eval_fname)
c622-022: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-022:   return torch.load(train_fname), torch.load(eval_fname)
c621-151: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-151:   return torch.load(train_fname), torch.load(eval_fname)
c621-071: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-071:   return torch.load(train_fname), torch.load(eval_fname)
c621-091: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-091:   return torch.load(train_fname), torch.load(eval_fname)
c622-102: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-102:   return torch.load(train_fname), torch.load(eval_fname)
c619-032: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-032:   return torch.load(train_fname), torch.load(eval_fname)
c621-112: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-112:   return torch.load(train_fname), torch.load(eval_fname)
c621-141: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-141:   return torch.load(train_fname), torch.load(eval_fname)
c621-101: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-101:   return torch.load(train_fname), torch.load(eval_fname)
c613-141: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-141:   return torch.load(train_fname), torch.load(eval_fname)
c613-131: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-131:   return torch.load(train_fname), torch.load(eval_fname)
c619-001: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-001:   return torch.load(train_fname), torch.load(eval_fname)
c613-111: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-111:   return torch.load(train_fname), torch.load(eval_fname)
c622-051: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-051:   return torch.load(train_fname), torch.load(eval_fname)
c621-072: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-072:   return torch.load(train_fname), torch.load(eval_fname)
c622-042: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-042:   return torch.load(train_fname), torch.load(eval_fname)
c619-011: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-011:   return torch.load(train_fname), torch.load(eval_fname)
c621-121: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-121:   return torch.load(train_fname), torch.load(eval_fname)
c622-031: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-031:   return torch.load(train_fname), torch.load(eval_fname)
c619-031: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-031:   return torch.load(train_fname), torch.load(eval_fname)
c622-002: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-002:   return torch.load(train_fname), torch.load(eval_fname)
c621-131: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-131:   return torch.load(train_fname), torch.load(eval_fname)
c622-071: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-071:   return torch.load(train_fname), torch.load(eval_fname)
c622-081: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-081:   return torch.load(train_fname), torch.load(eval_fname)
c621-092: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-092:   return torch.load(train_fname), torch.load(eval_fname)
c622-021: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-021:   return torch.load(train_fname), torch.load(eval_fname)
c619-002: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-002:   return torch.load(train_fname), torch.load(eval_fname)
c621-052: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-052:   return torch.load(train_fname), torch.load(eval_fname)
c619-012: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-012:   return torch.load(train_fname), torch.load(eval_fname)
c622-062: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-062:   return torch.load(train_fname), torch.load(eval_fname)
c622-001: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-001:   return torch.load(train_fname), torch.load(eval_fname)
c613-142: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-142:   return torch.load(train_fname), torch.load(eval_fname)
c621-081: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-081:   return torch.load(train_fname), torch.load(eval_fname)
c613-102: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-102:   return torch.load(train_fname), torch.load(eval_fname)
c622-011: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-011:   return torch.load(train_fname), torch.load(eval_fname)
c622-012: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-012:   return torch.load(train_fname), torch.load(eval_fname)
c613-132: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-132:   return torch.load(train_fname), torch.load(eval_fname)
c613-112: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-112:   return torch.load(train_fname), torch.load(eval_fname)
c622-092: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-092:   return torch.load(train_fname), torch.load(eval_fname)
c613-152: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-152:   return torch.load(train_fname), torch.load(eval_fname)
c622-101: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-101:   return torch.load(train_fname), torch.load(eval_fname)
c622-072: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-072:   return torch.load(train_fname), torch.load(eval_fname)
c621-082: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-082:   return torch.load(train_fname), torch.load(eval_fname)
c613-151: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-151:   return torch.load(train_fname), torch.load(eval_fname)
c613-122: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c613-122:   return torch.load(train_fname), torch.load(eval_fname)
c622-091: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-091:   return torch.load(train_fname), torch.load(eval_fname)
c621-061: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-061:   return torch.load(train_fname), torch.load(eval_fname)
c622-032: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-032:   return torch.load(train_fname), torch.load(eval_fname)
c622-082: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-082:   return torch.load(train_fname), torch.load(eval_fname)
c621-062: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-062:   return torch.load(train_fname), torch.load(eval_fname)
c621-102: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-102:   return torch.load(train_fname), torch.load(eval_fname)
c622-041: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-041:   return torch.load(train_fname), torch.load(eval_fname)
c621-132: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-132:   return torch.load(train_fname), torch.load(eval_fname)
c619-022: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-022:   return torch.load(train_fname), torch.load(eval_fname)
c621-152: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-152:   return torch.load(train_fname), torch.load(eval_fname)
c621-122: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-122:   return torch.load(train_fname), torch.load(eval_fname)
c621-142: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c621-142:   return torch.load(train_fname), torch.load(eval_fname)
c622-052: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-052:   return torch.load(train_fname), torch.load(eval_fname)
c622-061: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c622-061:   return torch.load(train_fname), torch.load(eval_fname)
c619-041: /work/09308/zhengmk/finetune_llama3.1_DL_assignment/DeepSpeedExamples/applications/DeepSpeed-Chat/dschat/utils/data/data_utils.py:383: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
c619-041:   return torch.load(train_fname), torch.load(eval_fname)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-112: To disable this warning, you can either:
c621-112: 	- Avoid using `tokenizers` before the fork if possible
c621-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-111: To disable this warning, you can either:
c613-111: 	- Avoid using `tokenizers` before the fork if possible
c613-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-112: To disable this warning, you can either:
c621-112: 	- Avoid using `tokenizers` before the fork if possible
c621-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-092: To disable this warning, you can either:
c621-092: 	- Avoid using `tokenizers` before the fork if possible
c621-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-112: To disable this warning, you can either:
c621-112: 	- Avoid using `tokenizers` before the fork if possible
c621-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-111: To disable this warning, you can either:
c613-111: 	- Avoid using `tokenizers` before the fork if possible
c613-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-102: To disable this warning, you can either:
c621-102: 	- Avoid using `tokenizers` before the fork if possible
c621-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-092: To disable this warning, you can either:
c621-092: 	- Avoid using `tokenizers` before the fork if possible
c621-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-111: To disable this warning, you can either:
c613-111: 	- Avoid using `tokenizers` before the fork if possible
c613-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-112: To disable this warning, you can either:
c621-112: 	- Avoid using `tokenizers` before the fork if possible
c621-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-092: To disable this warning, you can either:
c621-092: 	- Avoid using `tokenizers` before the fork if possible
c621-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-051: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-051: To disable this warning, you can either:
c622-051: 	- Avoid using `tokenizers` before the fork if possible
c622-051: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: To disable this warning, you can either:
c622-021: 	- Avoid using `tokenizers` before the fork if possible
c622-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-001: To disable this warning, you can either:
c619-001: 	- Avoid using `tokenizers` before the fork if possible
c619-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-031: To disable this warning, you can either:
c622-031: 	- Avoid using `tokenizers` before the fork if possible
c622-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-022: To disable this warning, you can either:
c622-022: 	- Avoid using `tokenizers` before the fork if possible
c622-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-102: To disable this warning, you can either:
c621-102: 	- Avoid using `tokenizers` before the fork if possible
c621-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-022: To disable this warning, you can either:
c619-022: 	- Avoid using `tokenizers` before the fork if possible
c619-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-081: To disable this warning, you can either:
c621-081: 	- Avoid using `tokenizers` before the fork if possible
c621-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-111: To disable this warning, you can either:
c613-111: 	- Avoid using `tokenizers` before the fork if possible
c613-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-112: To disable this warning, you can either:
c621-112: 	- Avoid using `tokenizers` before the fork if possible
c621-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-092: To disable this warning, you can either:
c621-092: 	- Avoid using `tokenizers` before the fork if possible
c621-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-112: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-102: To disable this warning, you can either:
c621-102: 	- Avoid using `tokenizers` before the fork if possible
c621-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-051: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-051: To disable this warning, you can either:
c622-051: 	- Avoid using `tokenizers` before the fork if possible
c622-051: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-152: To disable this warning, you can either:
c621-152: 	- Avoid using `tokenizers` before the fork if possible
c621-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: To disable this warning, you can either:
c622-021: 	- Avoid using `tokenizers` before the fork if possible
c622-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-001: To disable this warning, you can either:
c619-001: 	- Avoid using `tokenizers` before the fork if possible
c619-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-151: To disable this warning, you can either:
c621-151: 	- Avoid using `tokenizers` before the fork if possible
c621-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c613-121: Building extension module cpu_adam...
c613-121: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-051: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-051: To disable this warning, you can either:
c622-051: 	- Avoid using `tokenizers` before the fork if possible
c622-051: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-121: To disable this warning, you can either:
c613-121: 	- Avoid using `tokenizers` before the fork if possible
c613-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: To disable this warning, you can either:
c622-021: 	- Avoid using `tokenizers` before the fork if possible
c622-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-001: To disable this warning, you can either:
c619-001: 	- Avoid using `tokenizers` before the fork if possible
c619-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-031: To disable this warning, you can either:
c622-031: 	- Avoid using `tokenizers` before the fork if possible
c622-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-111: To disable this warning, you can either:
c613-111: 	- Avoid using `tokenizers` before the fork if possible
c613-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: ninja: no work to do.
c613-121: Loading extension module cpu_adam...
c622-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-022: To disable this warning, you can either:
c622-022: 	- Avoid using `tokenizers` before the fork if possible
c622-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-012: To disable this warning, you can either:
c622-012: 	- Avoid using `tokenizers` before the fork if possible
c622-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-121: Time to load cpu_adam op: 0.46130895614624023 seconds
c613-121: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-121: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-121: [2025-03-01 17:51:09,737] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-061: To disable this warning, you can either:
c621-061: 	- Avoid using `tokenizers` before the fork if possible
c621-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-092: To disable this warning, you can either:
c621-092: 	- Avoid using `tokenizers` before the fork if possible
c621-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-081: To disable this warning, you can either:
c621-081: 	- Avoid using `tokenizers` before the fork if possible
c621-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-022: To disable this warning, you can either:
c619-022: 	- Avoid using `tokenizers` before the fork if possible
c619-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-111: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-102: To disable this warning, you can either:
c621-102: 	- Avoid using `tokenizers` before the fork if possible
c621-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-031: To disable this warning, you can either:
c622-031: 	- Avoid using `tokenizers` before the fork if possible
c622-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-141: To disable this warning, you can either:
c621-141: 	- Avoid using `tokenizers` before the fork if possible
c621-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-092: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-052: To disable this warning, you can either:
c622-052: 	- Avoid using `tokenizers` before the fork if possible
c622-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-022: To disable this warning, you can either:
c622-022: 	- Avoid using `tokenizers` before the fork if possible
c622-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-022: To disable this warning, you can either:
c619-022: 	- Avoid using `tokenizers` before the fork if possible
c619-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-081: To disable this warning, you can either:
c621-081: 	- Avoid using `tokenizers` before the fork if possible
c621-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-152: To disable this warning, you can either:
c621-152: 	- Avoid using `tokenizers` before the fork if possible
c621-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-051: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-051: To disable this warning, you can either:
c622-051: 	- Avoid using `tokenizers` before the fork if possible
c622-051: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-151: To disable this warning, you can either:
c621-151: 	- Avoid using `tokenizers` before the fork if possible
c621-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-111: To disable this warning, you can either:
c621-111: 	- Avoid using `tokenizers` before the fork if possible
c621-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: To disable this warning, you can either:
c622-021: 	- Avoid using `tokenizers` before the fork if possible
c622-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-001: To disable this warning, you can either:
c619-001: 	- Avoid using `tokenizers` before the fork if possible
c619-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-152: To disable this warning, you can either:
c621-152: 	- Avoid using `tokenizers` before the fork if possible
c621-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: Building extension module cpu_adam...
c613-101: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c621-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-151: To disable this warning, you can either:
c621-151: 	- Avoid using `tokenizers` before the fork if possible
c621-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-101: To disable this warning, you can either:
c613-101: 	- Avoid using `tokenizers` before the fork if possible
c613-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-012: To disable this warning, you can either:
c622-012: 	- Avoid using `tokenizers` before the fork if possible
c622-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-102: To disable this warning, you can either:
c621-102: 	- Avoid using `tokenizers` before the fork if possible
c621-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-061: To disable this warning, you can either:
c621-061: 	- Avoid using `tokenizers` before the fork if possible
c621-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-031: To disable this warning, you can either:
c622-031: 	- Avoid using `tokenizers` before the fork if possible
c622-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-102: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-141: To disable this warning, you can either:
c621-141: 	- Avoid using `tokenizers` before the fork if possible
c621-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-022: To disable this warning, you can either:
c622-022: 	- Avoid using `tokenizers` before the fork if possible
c622-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-022: To disable this warning, you can either:
c619-022: 	- Avoid using `tokenizers` before the fork if possible
c619-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-012: To disable this warning, you can either:
c622-012: 	- Avoid using `tokenizers` before the fork if possible
c622-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: ninja: no work to do.
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-081: To disable this warning, you can either:
c621-081: 	- Avoid using `tokenizers` before the fork if possible
c621-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-052: To disable this warning, you can either:
c622-052: 	- Avoid using `tokenizers` before the fork if possible
c622-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-152: To disable this warning, you can either:
c613-152: 	- Avoid using `tokenizers` before the fork if possible
c613-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: Loading extension module cpu_adam...
c621-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-061: To disable this warning, you can either:
c621-061: 	- Avoid using `tokenizers` before the fork if possible
c621-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-052: To disable this warning, you can either:
c621-052: 	- Avoid using `tokenizers` before the fork if possible
c621-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-051: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-051: To disable this warning, you can either:
c622-051: 	- Avoid using `tokenizers` before the fork if possible
c622-051: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: Time to load cpu_adam op: 0.49010181427001953 seconds
c613-101: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-101: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-112: To disable this warning, you can either:
c613-112: 	- Avoid using `tokenizers` before the fork if possible
c613-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: [2025-03-01 17:51:09,911] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown
c613-101: [2025-03-01 17:51:09,912] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized
c613-101: [2025-03-01 17:51:09,912] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:51:09,921] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
c613-101: [2025-03-01 17:51:09,922] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
c621-082: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-101: [2025-03-01 17:51:09,922] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
c621-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-141: To disable this warning, you can either:
c621-141: 	- Avoid using `tokenizers` before the fork if possible
c621-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: [2025-03-01 17:51:09,930] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
c613-101: [2025-03-01 17:51:09,930] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
c613-101: [2025-03-01 17:51:09,930] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
c613-101: [2025-03-01 17:51:09,930] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
c622-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: To disable this warning, you can either:
c622-021: 	- Avoid using `tokenizers` before the fork if possible
c619-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-001: To disable this warning, you can either:
c619-001: 	- Avoid using `tokenizers` before the fork if possible
c619-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-051: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-052: To disable this warning, you can either:
c622-052: 	- Avoid using `tokenizers` before the fork if possible
c622-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-152: To disable this warning, you can either:
c621-152: 	- Avoid using `tokenizers` before the fork if possible
c621-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-111: To disable this warning, you can either:
c621-111: 	- Avoid using `tokenizers` before the fork if possible
c621-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-151: To disable this warning, you can either:
c621-151: 	- Avoid using `tokenizers` before the fork if possible
c621-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-021: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-001: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-062: To disable this warning, you can either:
c622-062: 	- Avoid using `tokenizers` before the fork if possible
c622-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-142: To disable this warning, you can either:
c621-142: 	- Avoid using `tokenizers` before the fork if possible
c621-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-002: To disable this warning, you can either:
c622-002: 	- Avoid using `tokenizers` before the fork if possible
c622-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-101: To disable this warning, you can either:
c622-101: 	- Avoid using `tokenizers` before the fork if possible
c622-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-031: To disable this warning, you can either:
c622-031: 	- Avoid using `tokenizers` before the fork if possible
c622-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-021: To disable this warning, you can either:
c619-021: 	- Avoid using `tokenizers` before the fork if possible
c619-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-111: To disable this warning, you can either:
c621-111: 	- Avoid using `tokenizers` before the fork if possible
c621-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-041: To disable this warning, you can either:
c619-041: 	- Avoid using `tokenizers` before the fork if possible
c619-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-151: To disable this warning, you can either:
c613-151: 	- Avoid using `tokenizers` before the fork if possible
c613-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-091: To disable this warning, you can either:
c621-091: 	- Avoid using `tokenizers` before the fork if possible
c621-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-022: To disable this warning, you can either:
c622-022: 	- Avoid using `tokenizers` before the fork if possible
c622-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-012: To disable this warning, you can either:
c622-012: 	- Avoid using `tokenizers` before the fork if possible
c622-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-022: To disable this warning, you can either:
c619-022: 	- Avoid using `tokenizers` before the fork if possible
c619-022: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-031: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-081: To disable this warning, you can either:
c621-081: 	- Avoid using `tokenizers` before the fork if possible
c621-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-062: To disable this warning, you can either:
c621-062: 	- Avoid using `tokenizers` before the fork if possible
c621-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-121: To disable this warning, you can either:
c621-121: 	- Avoid using `tokenizers` before the fork if possible
c621-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-061: To disable this warning, you can either:
c621-061: 	- Avoid using `tokenizers` before the fork if possible
c621-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-022: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-152: To disable this warning, you can either:
c613-152: 	- Avoid using `tokenizers` before the fork if possible
c613-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-022: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-112: To disable this warning, you can either:
c613-112: 	- Avoid using `tokenizers` before the fork if possible
c613-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-052: To disable this warning, you can either:
c621-052: 	- Avoid using `tokenizers` before the fork if possible
c621-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-081: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-141: To disable this warning, you can either:
c621-141: 	- Avoid using `tokenizers` before the fork if possible
c621-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-082: Building extension module cpu_adam...
c621-082: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-052: To disable this warning, you can either:
c622-052: 	- Avoid using `tokenizers` before the fork if possible
c622-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-032: To disable this warning, you can either:
c619-032: 	- Avoid using `tokenizers` before the fork if possible
c619-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-151: To disable this warning, you can either:
c621-151: 	- Avoid using `tokenizers` before the fork if possible
c621-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-152: To disable this warning, you can either:
c621-152: 	- Avoid using `tokenizers` before the fork if possible
c621-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-082: To disable this warning, you can either:
c621-082: 	- Avoid using `tokenizers` before the fork if possible
c621-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-152: To disable this warning, you can either:
c613-152: 	- Avoid using `tokenizers` before the fork if possible
c613-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-112: To disable this warning, you can either:
c613-112: 	- Avoid using `tokenizers` before the fork if possible
c613-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-052: To disable this warning, you can either:
c621-052: 	- Avoid using `tokenizers` before the fork if possible
c621-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-151: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-062: To disable this warning, you can either:
c622-062: 	- Avoid using `tokenizers` before the fork if possible
c622-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-101: To disable this warning, you can either:
c621-101: 	- Avoid using `tokenizers` before the fork if possible
c621-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-152: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-142: To disable this warning, you can either:
c621-142: 	- Avoid using `tokenizers` before the fork if possible
c621-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: ninja: no work to do.
c621-082: Loading extension module cpu_adam...
c622-011: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-111: To disable this warning, you can either:
c621-111: 	- Avoid using `tokenizers` before the fork if possible
c621-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-082: Time to load cpu_adam op: 0.49084925651550293 seconds
c621-082: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-082: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-082: [2025-03-01 17:51:10,107] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-002: To disable this warning, you can either:
c622-002: 	- Avoid using `tokenizers` before the fork if possible
c622-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-101: To disable this warning, you can either:
c622-101: 	- Avoid using `tokenizers` before the fork if possible
c622-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-012: To disable this warning, you can either:
c622-012: 	- Avoid using `tokenizers` before the fork if possible
c622-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-132: To disable this warning, you can either:
c613-132: 	- Avoid using `tokenizers` before the fork if possible
c613-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-041: To disable this warning, you can either:
c619-041: 	- Avoid using `tokenizers` before the fork if possible
c619-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-151: To disable this warning, you can either:
c613-151: 	- Avoid using `tokenizers` before the fork if possible
c613-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-062: To disable this warning, you can either:
c622-062: 	- Avoid using `tokenizers` before the fork if possible
c622-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-031: To disable this warning, you can either:
c619-031: 	- Avoid using `tokenizers` before the fork if possible
c619-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-021: To disable this warning, you can either:
c619-021: 	- Avoid using `tokenizers` before the fork if possible
c619-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-061: To disable this warning, you can either:
c621-061: 	- Avoid using `tokenizers` before the fork if possible
c621-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-091: To disable this warning, you can either:
c621-091: 	- Avoid using `tokenizers` before the fork if possible
c621-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-142: To disable this warning, you can either:
c621-142: 	- Avoid using `tokenizers` before the fork if possible
c621-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-012: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-062: To disable this warning, you can either:
c621-062: 	- Avoid using `tokenizers` before the fork if possible
c621-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-121: To disable this warning, you can either:
c621-121: 	- Avoid using `tokenizers` before the fork if possible
c621-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-002: To disable this warning, you can either:
c622-002: 	- Avoid using `tokenizers` before the fork if possible
c622-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-101: To disable this warning, you can either:
c622-101: 	- Avoid using `tokenizers` before the fork if possible
c622-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-141: To disable this warning, you can either:
c621-141: 	- Avoid using `tokenizers` before the fork if possible
c621-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-061: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-041: To disable this warning, you can either:
c619-041: 	- Avoid using `tokenizers` before the fork if possible
c619-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-151: To disable this warning, you can either:
c613-151: 	- Avoid using `tokenizers` before the fork if possible
c613-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-052: To disable this warning, you can either:
c622-052: 	- Avoid using `tokenizers` before the fork if possible
c622-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-021: To disable this warning, you can either:
c619-021: 	- Avoid using `tokenizers` before the fork if possible
c619-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-091: To disable this warning, you can either:
c621-091: 	- Avoid using `tokenizers` before the fork if possible
c621-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-141: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-152: To disable this warning, you can either:
c613-152: 	- Avoid using `tokenizers` before the fork if possible
c613-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-121: To disable this warning, you can either:
c621-121: 	- Avoid using `tokenizers` before the fork if possible
c621-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-062: To disable this warning, you can either:
c621-062: 	- Avoid using `tokenizers` before the fork if possible
c621-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-112: To disable this warning, you can either:
c613-112: 	- Avoid using `tokenizers` before the fork if possible
c613-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-052: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-032: To disable this warning, you can either:
c619-032: 	- Avoid using `tokenizers` before the fork if possible
c619-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-052: To disable this warning, you can either:
c621-052: 	- Avoid using `tokenizers` before the fork if possible
c621-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-101: To disable this warning, you can either:
c621-101: 	- Avoid using `tokenizers` before the fork if possible
c621-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-111: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-111: To disable this warning, you can either:
c621-111: 	- Avoid using `tokenizers` before the fork if possible
c621-111: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: Building extension module cpu_adam...
c622-011: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c619-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-032: To disable this warning, you can either:
c619-032: 	- Avoid using `tokenizers` before the fork if possible
c619-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-062: To disable this warning, you can either:
c622-062: 	- Avoid using `tokenizers` before the fork if possible
c622-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-011: To disable this warning, you can either:
c622-011: 	- Avoid using `tokenizers` before the fork if possible
c622-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-111: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-142: To disable this warning, you can either:
c621-142: 	- Avoid using `tokenizers` before the fork if possible
c621-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-132: To disable this warning, you can either:
c613-132: 	- Avoid using `tokenizers` before the fork if possible
c613-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-031: To disable this warning, you can either:
c619-031: 	- Avoid using `tokenizers` before the fork if possible
c619-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-101: To disable this warning, you can either:
c621-101: 	- Avoid using `tokenizers` before the fork if possible
c621-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-002: To disable this warning, you can either:
c622-002: 	- Avoid using `tokenizers` before the fork if possible
c622-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-101: To disable this warning, you can either:
c622-101: 	- Avoid using `tokenizers` before the fork if possible
c622-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: ninja: no work to do.
c622-072: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-011: Loading extension module cpu_adam...
c619-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-041: To disable this warning, you can either:
c619-041: 	- Avoid using `tokenizers` before the fork if possible
c619-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-151: To disable this warning, you can either:
c613-151: 	- Avoid using `tokenizers` before the fork if possible
c613-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-011: Time to load cpu_adam op: 0.4633064270019531 seconds
c622-011: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-011: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-011: [2025-03-01 17:51:10,275] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-111: Loading extension module cpu_adam...
c613-101: [2025-03-01 17:51:10,283] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
c619-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-021: To disable this warning, you can either:
c619-021: 	- Avoid using `tokenizers` before the fork if possible
c619-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: [2025-03-01 17:51:10,284] [INFO] [utils.py:782:see_memory_usage] MA 4.12 GB         Max_MA 6.1 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:10,284] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.5 GB, percent = 35.0%
c621-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-091: To disable this warning, you can either:
c621-091: 	- Avoid using `tokenizers` before the fork if possible
c621-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: [2025-03-01 17:51:10,286] [INFO] [stage3.py:169:__init__] Reduce bucket size 500000000
c613-101: [2025-03-01 17:51:10,286] [INFO] [stage3.py:170:__init__] Prefetch bucket size 30000000
c613-152: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-152: To disable this warning, you can either:
c613-152: 	- Avoid using `tokenizers` before the fork if possible
c613-152: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-121: To disable this warning, you can either:
c621-121: 	- Avoid using `tokenizers` before the fork if possible
c621-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-132: To disable this warning, you can either:
c613-132: 	- Avoid using `tokenizers` before the fork if possible
c613-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-031: To disable this warning, you can either:
c619-031: 	- Avoid using `tokenizers` before the fork if possible
c619-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-112: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-112: To disable this warning, you can either:
c613-112: 	- Avoid using `tokenizers` before the fork if possible
c613-112: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-062: To disable this warning, you can either:
c621-062: 	- Avoid using `tokenizers` before the fork if possible
c621-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-052: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-052: To disable this warning, you can either:
c621-052: 	- Avoid using `tokenizers` before the fork if possible
c621-052: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-111: Time to load cpu_adam op: 0.3835737705230713 seconds
c621-111: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-111: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-111: [2025-03-01 17:51:10,313] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-152: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-112: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-052: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-042: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-042: To disable this warning, you can either:
c622-042: 	- Avoid using `tokenizers` before the fork if possible
c622-042: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-032: To disable this warning, you can either:
c619-032: 	- Avoid using `tokenizers` before the fork if possible
c619-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-082: To disable this warning, you can either:
c622-082: 	- Avoid using `tokenizers` before the fork if possible
c622-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-062: To disable this warning, you can either:
c622-062: 	- Avoid using `tokenizers` before the fork if possible
c622-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-142: To disable this warning, you can either:
c621-142: 	- Avoid using `tokenizers` before the fork if possible
c621-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-062: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-101: To disable this warning, you can either:
c621-101: 	- Avoid using `tokenizers` before the fork if possible
c621-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-091: To disable this warning, you can either:
c622-091: 	- Avoid using `tokenizers` before the fork if possible
c622-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-101: To disable this warning, you can either:
c622-101: 	- Avoid using `tokenizers` before the fork if possible
c622-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-002: To disable this warning, you can either:
c622-002: 	- Avoid using `tokenizers` before the fork if possible
c622-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-041: To disable this warning, you can either:
c619-041: 	- Avoid using `tokenizers` before the fork if possible
c619-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-151: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-151: To disable this warning, you can either:
c613-151: 	- Avoid using `tokenizers` before the fork if possible
c613-151: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-142: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-072: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c622-072: Building extension module cpu_adam...
c622-072: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-091: To disable this warning, you can either:
c621-091: 	- Avoid using `tokenizers` before the fork if possible
c621-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-021: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-021: To disable this warning, you can either:
c619-021: 	- Avoid using `tokenizers` before the fork if possible
c619-021: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-101: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-121: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-121: To disable this warning, you can either:
c621-121: 	- Avoid using `tokenizers` before the fork if possible
c621-121: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-002: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-041: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-151: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-031: To disable this warning, you can either:
c619-031: 	- Avoid using `tokenizers` before the fork if possible
c619-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-072: To disable this warning, you can either:
c622-072: 	- Avoid using `tokenizers` before the fork if possible
c622-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-062: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-062: To disable this warning, you can either:
c621-062: 	- Avoid using `tokenizers` before the fork if possible
c621-062: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-132: To disable this warning, you can either:
c613-132: 	- Avoid using `tokenizers` before the fork if possible
c613-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-061: To disable this warning, you can either:
c622-061: 	- Avoid using `tokenizers` before the fork if possible
c622-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-092: To disable this warning, you can either:
c622-092: 	- Avoid using `tokenizers` before the fork if possible
c622-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-091: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-021: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-071: To disable this warning, you can either:
c621-071: 	- Avoid using `tokenizers` before the fork if possible
c621-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-121: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-062: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: ninja: no work to do.
c621-122: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-072: Loading extension module cpu_adam...
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-072: Time to load cpu_adam op: 0.48624324798583984 seconds
c622-072: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-072: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-072: [2025-03-01 17:51:10,453] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-032: To disable this warning, you can either:
c619-032: 	- Avoid using `tokenizers` before the fork if possible
c619-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-041: To disable this warning, you can either:
c622-041: 	- Avoid using `tokenizers` before the fork if possible
c622-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-042: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-042: To disable this warning, you can either:
c622-042: 	- Avoid using `tokenizers` before the fork if possible
c622-042: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-032: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-132: To disable this warning, you can either:
c621-132: 	- Avoid using `tokenizers` before the fork if possible
c621-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-082: To disable this warning, you can either:
c622-082: 	- Avoid using `tokenizers` before the fork if possible
c622-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-101: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-101: To disable this warning, you can either:
c621-101: 	- Avoid using `tokenizers` before the fork if possible
c621-101: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-091: To disable this warning, you can either:
c622-091: 	- Avoid using `tokenizers` before the fork if possible
c622-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-042: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-042: To disable this warning, you can either:
c622-042: 	- Avoid using `tokenizers` before the fork if possible
c622-042: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-101: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-131: To disable this warning, you can either:
c613-131: 	- Avoid using `tokenizers` before the fork if possible
c613-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-031: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-031: To disable this warning, you can either:
c619-031: 	- Avoid using `tokenizers` before the fork if possible
c619-031: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-082: To disable this warning, you can either:
c622-082: 	- Avoid using `tokenizers` before the fork if possible
c622-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-132: To disable this warning, you can either:
c613-132: 	- Avoid using `tokenizers` before the fork if possible
c613-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-031: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-091: To disable this warning, you can either:
c622-091: 	- Avoid using `tokenizers` before the fork if possible
c622-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-061: To disable this warning, you can either:
c622-061: 	- Avoid using `tokenizers` before the fork if possible
c622-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-071: To disable this warning, you can either:
c621-071: 	- Avoid using `tokenizers` before the fork if possible
c621-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-092: To disable this warning, you can either:
c622-092: 	- Avoid using `tokenizers` before the fork if possible
c622-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-132: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-141: To disable this warning, you can either:
c613-141: 	- Avoid using `tokenizers` before the fork if possible
c613-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-122: Building extension module cpu_adam...
c621-122: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-081: To disable this warning, you can either:
c622-081: 	- Avoid using `tokenizers` before the fork if possible
c622-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-142: To disable this warning, you can either:
c613-142: 	- Avoid using `tokenizers` before the fork if possible
c613-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-122: To disable this warning, you can either:
c621-122: 	- Avoid using `tokenizers` before the fork if possible
c621-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-041: To disable this warning, you can either:
c622-041: 	- Avoid using `tokenizers` before the fork if possible
c622-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-061: To disable this warning, you can either:
c622-061: 	- Avoid using `tokenizers` before the fork if possible
c622-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-071: To disable this warning, you can either:
c621-071: 	- Avoid using `tokenizers` before the fork if possible
c621-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-092: To disable this warning, you can either:
c622-092: 	- Avoid using `tokenizers` before the fork if possible
c622-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-101: [2025-03-01 17:51:10,607] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
c613-101: [2025-03-01 17:51:10,608] [INFO] [utils.py:782:see_memory_usage] MA 4.12 GB         Max_MA 4.12 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:10,608] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 35.0%
c621-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-132: To disable this warning, you can either:
c621-132: 	- Avoid using `tokenizers` before the fork if possible
c621-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: ninja: no work to do.
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: Loading extension module cpu_adam...
c622-042: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-042: To disable this warning, you can either:
c622-042: 	- Avoid using `tokenizers` before the fork if possible
c622-042: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-122: Time to load cpu_adam op: 0.4994533061981201 seconds
c621-122: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-122: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-122: [2025-03-01 17:51:10,626] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-041: To disable this warning, you can either:
c622-041: 	- Avoid using `tokenizers` before the fork if possible
c622-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-131: To disable this warning, you can either:
c613-131: 	- Avoid using `tokenizers` before the fork if possible
c613-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-012: To disable this warning, you can either:
c619-012: 	- Avoid using `tokenizers` before the fork if possible
c619-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-082: To disable this warning, you can either:
c622-082: 	- Avoid using `tokenizers` before the fork if possible
c622-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-132: To disable this warning, you can either:
c621-132: 	- Avoid using `tokenizers` before the fork if possible
c621-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-091: To disable this warning, you can either:
c622-091: 	- Avoid using `tokenizers` before the fork if possible
c622-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-131: To disable this warning, you can either:
c613-131: 	- Avoid using `tokenizers` before the fork if possible
c613-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-141: To disable this warning, you can either:
c613-141: 	- Avoid using `tokenizers` before the fork if possible
c613-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-142: To disable this warning, you can either:
c613-142: 	- Avoid using `tokenizers` before the fork if possible
c613-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-071: To disable this warning, you can either:
c621-071: 	- Avoid using `tokenizers` before the fork if possible
c621-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-092: To disable this warning, you can either:
c622-092: 	- Avoid using `tokenizers` before the fork if possible
c622-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-061: To disable this warning, you can either:
c622-061: 	- Avoid using `tokenizers` before the fork if possible
c622-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-081: To disable this warning, you can either:
c622-081: 	- Avoid using `tokenizers` before the fork if possible
c622-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-141: To disable this warning, you can either:
c613-141: 	- Avoid using `tokenizers` before the fork if possible
c613-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-042: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-042: To disable this warning, you can either:
c622-042: 	- Avoid using `tokenizers` before the fork if possible
c622-042: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-041: To disable this warning, you can either:
c622-041: 	- Avoid using `tokenizers` before the fork if possible
c622-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-142: To disable this warning, you can either:
c613-142: 	- Avoid using `tokenizers` before the fork if possible
c613-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-042: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-132: To disable this warning, you can either:
c621-132: 	- Avoid using `tokenizers` before the fork if possible
c621-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-081: To disable this warning, you can either:
c622-081: 	- Avoid using `tokenizers` before the fork if possible
c622-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-082: To disable this warning, you can either:
c622-082: 	- Avoid using `tokenizers` before the fork if possible
c622-082: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-012: To disable this warning, you can either:
c619-012: 	- Avoid using `tokenizers` before the fork if possible
c619-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-091: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-091: To disable this warning, you can either:
c622-091: 	- Avoid using `tokenizers` before the fork if possible
c622-091: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-082: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-131: To disable this warning, you can either:
c613-131: 	- Avoid using `tokenizers` before the fork if possible
c613-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-091: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-122: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c619-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-012: To disable this warning, you can either:
c619-012: 	- Avoid using `tokenizers` before the fork if possible
c619-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-071: To disable this warning, you can either:
c621-071: 	- Avoid using `tokenizers` before the fork if possible
c621-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: Building extension module cpu_adam...
c613-122: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-092: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-092: To disable this warning, you can either:
c622-092: 	- Avoid using `tokenizers` before the fork if possible
c622-092: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-061: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-061: To disable this warning, you can either:
c622-061: 	- Avoid using `tokenizers` before the fork if possible
c622-061: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-122: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-122: To disable this warning, you can either:
c613-122: 	- Avoid using `tokenizers` before the fork if possible
c613-122: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-071: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-141: To disable this warning, you can either:
c613-141: 	- Avoid using `tokenizers` before the fork if possible
c613-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-041: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-041: To disable this warning, you can either:
c622-041: 	- Avoid using `tokenizers` before the fork if possible
c622-041: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-092: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-061: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-142: To disable this warning, you can either:
c613-142: 	- Avoid using `tokenizers` before the fork if possible
c613-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-122: ninja: no work to do.
c613-122: Loading extension module cpu_adam...
c622-041: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-122: Time to load cpu_adam op: 0.5038750171661377 seconds
c613-122: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-122: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-132: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-132: To disable this warning, you can either:
c621-132: 	- Avoid using `tokenizers` before the fork if possible
c613-122: [2025-03-01 17:51:10,881] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-132: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-081: To disable this warning, you can either:
c622-081: 	- Avoid using `tokenizers` before the fork if possible
c622-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-132: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-131: To disable this warning, you can either:
c613-131: 	- Avoid using `tokenizers` before the fork if possible
c613-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-012: To disable this warning, you can either:
c619-012: 	- Avoid using `tokenizers` before the fork if possible
c619-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-131: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-141: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-141: To disable this warning, you can either:
c613-141: 	- Avoid using `tokenizers` before the fork if possible
c613-141: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-032: To disable this warning, you can either:
c622-032: 	- Avoid using `tokenizers` before the fork if possible
c622-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-142: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-142: To disable this warning, you can either:
c613-142: 	- Avoid using `tokenizers` before the fork if possible
c613-142: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-071: To disable this warning, you can either:
c622-071: 	- Avoid using `tokenizers` before the fork if possible
c622-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-141: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-131: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-131: Building extension module cpu_adam...
c621-131: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c613-142: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-131: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-131: To disable this warning, you can either:
c621-131: 	- Avoid using `tokenizers` before the fork if possible
c621-131: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-081: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-081: To disable this warning, you can either:
c622-081: 	- Avoid using `tokenizers` before the fork if possible
c622-081: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: ninja: no work to do.
c622-102: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-131: Loading extension module cpu_adam...
c619-012: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-012: To disable this warning, you can either:
c619-012: 	- Avoid using `tokenizers` before the fork if possible
c619-012: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-131: Time to load cpu_adam op: 0.4904186725616455 seconds
c621-131: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-131: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-131: [2025-03-01 17:51:11,052] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-081: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-012: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-032: To disable this warning, you can either:
c622-032: 	- Avoid using `tokenizers` before the fork if possible
c622-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-071: To disable this warning, you can either:
c622-071: 	- Avoid using `tokenizers` before the fork if possible
c622-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-032: To disable this warning, you can either:
c622-032: 	- Avoid using `tokenizers` before the fork if possible
c622-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-071: To disable this warning, you can either:
c622-071: 	- Avoid using `tokenizers` before the fork if possible
c622-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c622-102: Building extension module cpu_adam...
c622-102: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-102: To disable this warning, you can either:
c622-102: 	- Avoid using `tokenizers` before the fork if possible
c622-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-102: ninja: no work to do.
c622-102: Loading extension module cpu_adam...
c622-102: Time to load cpu_adam op: 0.48790907859802246 seconds
c622-102: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-102: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-102: [2025-03-01 17:51:11,221] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-102: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-032: To disable this warning, you can either:
c622-032: 	- Avoid using `tokenizers` before the fork if possible
c622-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-071: To disable this warning, you can either:
c622-071: 	- Avoid using `tokenizers` before the fork if possible
c622-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c613-102: Building extension module cpu_adam...
c613-102: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-032: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-032: To disable this warning, you can either:
c622-032: 	- Avoid using `tokenizers` before the fork if possible
c622-032: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c613-102: To disable this warning, you can either:
c613-102: 	- Avoid using `tokenizers` before the fork if possible
c613-102: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-071: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-071: To disable this warning, you can either:
c622-071: 	- Avoid using `tokenizers` before the fork if possible
c622-071: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-032: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-071: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c613-102: ninja: no work to do.
c613-102: Loading extension module cpu_adam...
c622-071: Loading extension module cpu_adam...
c613-102: Time to load cpu_adam op: 0.4976193904876709 seconds
c613-102: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-102: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-071: Time to load cpu_adam op: 0.3253493309020996 seconds
c613-102: [2025-03-01 17:51:11,415] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-071: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-071: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-071: [2025-03-01 17:51:11,415] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c622-001: Building extension module cpu_adam...
c622-001: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c622-001: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c622-001: To disable this warning, you can either:
c622-001: 	- Avoid using `tokenizers` before the fork if possible
c622-001: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-001: ninja: no work to do.
c622-001: Loading extension module cpu_adam...
c622-001: Time to load cpu_adam op: 0.48562192916870117 seconds
c622-001: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-001: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-001: [2025-03-01 17:51:11,805] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c619-011: Building extension module cpu_adam...
c619-011: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c619-011: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-011: To disable this warning, you can either:
c619-011: 	- Avoid using `tokenizers` before the fork if possible
c619-011: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: ninja: no work to do.
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-011: Loading extension module cpu_adam...
c619-011: Time to load cpu_adam op: 0.5214872360229492 seconds
c619-011: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-011: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-011: [2025-03-01 17:51:12,044] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c621-072: Building extension module cpu_adam...
c621-072: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c621-072: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c621-072: To disable this warning, you can either:
c621-072: 	- Avoid using `tokenizers` before the fork if possible
c621-072: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c621-072: ninja: no work to do.
c621-072: Loading extension module cpu_adam...
c621-072: Time to load cpu_adam op: 0.47922778129577637 seconds
c621-072: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-072: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-072: [2025-03-01 17:51:12,497] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-112: Loading extension module cpu_adam...
c621-112: Time to load cpu_adam op: 3.309546947479248 seconds
c621-112: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-112: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-112: [2025-03-01 17:51:12,662] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-111: Loading extension module cpu_adam...
c613-111: Time to load cpu_adam op: 3.319918632507324 seconds
c613-111: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-111: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-111: [2025-03-01 17:51:12,774] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-092: Loading extension module cpu_adam...
c621-092: Time to load cpu_adam op: 3.313767433166504 seconds
c621-092: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-092: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-092: [2025-03-01 17:51:12,790] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-102: Loading extension module cpu_adam...
c621-102: Time to load cpu_adam op: 3.315788507461548 seconds
c621-102: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-102: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-102: [2025-03-01 17:51:12,907] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-051: Loading extension module cpu_adam...
c622-051: Time to load cpu_adam op: 3.31866455078125 seconds
c622-051: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-051: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-051: [2025-03-01 17:51:12,956] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-021: Loading extension module cpu_adam...
c619-001: Loading extension module cpu_adam...
c622-021: Time to load cpu_adam op: 3.31618070602417 seconds
c622-021: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-021: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-001: Time to load cpu_adam op: 3.312915802001953 seconds
c619-001: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-001: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-021: [2025-03-01 17:51:12,975] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-001: [2025-03-01 17:51:12,975] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-031: Loading extension module cpu_adam...
c622-031: Time to load cpu_adam op: 3.3238537311553955 seconds
c622-031: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-031: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-031: [2025-03-01 17:51:13,027] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-022: Loading extension module cpu_adam...
c619-022: Time to load cpu_adam op: 3.319507122039795 seconds
c619-022: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-022: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-022: [2025-03-01 17:51:13,051] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-081: Loading extension module cpu_adam...
c621-081: Time to load cpu_adam op: 3.3327810764312744 seconds
c621-081: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-081: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-081: [2025-03-01 17:51:13,060] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-022: Loading extension module cpu_adam...
c622-022: Time to load cpu_adam op: 3.36354923248291 seconds
c622-022: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-022: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-022: [2025-03-01 17:51:13,079] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-151: Loading extension module cpu_adam...
c621-152: Loading extension module cpu_adam...
c621-151: Time to load cpu_adam op: 3.316041946411133 seconds
c621-151: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-151: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-151: [2025-03-01 17:51:13,104] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-152: Time to load cpu_adam op: 3.327915906906128 seconds
c621-152: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-152: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-152: [2025-03-01 17:51:13,106] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-012: Loading extension module cpu_adam...
c622-012: Time to load cpu_adam op: 3.316631317138672 seconds
c622-012: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-012: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-012: [2025-03-01 17:51:13,158] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-061: Loading extension module cpu_adam...
c621-061: Time to load cpu_adam op: 3.3257181644439697 seconds
c621-061: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-061: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-061: [2025-03-01 17:51:13,175] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-141: Loading extension module cpu_adam...
c621-141: Time to load cpu_adam op: 3.3325209617614746 seconds
c621-141: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-141: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-052: Loading extension module cpu_adam...
c621-141: [2025-03-01 17:51:13,202] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-052: Time to load cpu_adam op: 3.325252056121826 seconds
c622-052: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-052: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-052: [2025-03-01 17:51:13,207] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-152: Loading extension module cpu_adam...
c613-152: Time to load cpu_adam op: 3.3248393535614014 seconds
c613-152: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-152: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-152: [2025-03-01 17:51:13,335] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-112: Loading extension module cpu_adam...
c613-112: Time to load cpu_adam op: 3.3207521438598633 seconds
c613-112: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-112: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-112: [2025-03-01 17:51:13,340] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-052: Loading extension module cpu_adam...
c621-052: Time to load cpu_adam op: 3.338852882385254 seconds
c621-052: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-052: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-052: [2025-03-01 17:51:13,357] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-062: Loading extension module cpu_adam...
c622-062: Time to load cpu_adam op: 3.3265151977539062 seconds
c622-062: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-062: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-062: [2025-03-01 17:51:13,392] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-142: Loading extension module cpu_adam...
c621-142: Time to load cpu_adam op: 3.3369290828704834 seconds
c621-142: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-142: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-142: [2025-03-01 17:51:13,411] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-101: Loading extension module cpu_adam...
c622-101: Time to load cpu_adam op: 3.327601671218872 seconds
c622-101: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-101: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-041: Loading extension module cpu_adam...
c613-151: Loading extension module cpu_adam...
c622-002: Loading extension module cpu_adam...
c622-101: [2025-03-01 17:51:13,421] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-041: Time to load cpu_adam op: 3.3259313106536865 seconds
c619-041: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-041: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-002: Time to load cpu_adam op: 3.3360040187835693 seconds
c622-002: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-002: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-151: Time to load cpu_adam op: 3.320610284805298 seconds
c613-151: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-151: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-041: [2025-03-01 17:51:13,426] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-002: [2025-03-01 17:51:13,426] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-151: [2025-03-01 17:51:13,426] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-021: Loading extension module cpu_adam...
c621-121: Loading extension module cpu_adam...
c619-021: Time to load cpu_adam op: 3.3398351669311523 seconds
c619-021: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-021: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-021: [2025-03-01 17:51:13,445] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-121: Time to load cpu_adam op: 3.321230173110962 seconds
c621-121: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-121: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-121: [2025-03-01 17:51:13,447] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-062: Loading extension module cpu_adam...
c621-091: Loading extension module cpu_adam...
c621-062: Time to load cpu_adam op: 3.330357551574707 seconds
c621-062: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-062: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-091: Time to load cpu_adam op: 3.343583106994629 seconds
c621-091: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-091: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-062: [2025-03-01 17:51:13,455] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-091: [2025-03-01 17:51:13,456] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-032: Loading extension module cpu_adam...
c619-032: Time to load cpu_adam op: 3.330294132232666 seconds
c619-032: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-032: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-032: [2025-03-01 17:51:13,503] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-101: Loading extension module cpu_adam...
c621-101: Time to load cpu_adam op: 3.338360548019409 seconds
c621-101: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-101: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-101: [2025-03-01 17:51:13,534] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-031: Loading extension module cpu_adam...
c619-031: Time to load cpu_adam op: 3.32869029045105 seconds
c619-031: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-031: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-031: [2025-03-01 17:51:13,565] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-132: Loading extension module cpu_adam...
c613-132: Time to load cpu_adam op: 3.342038154602051 seconds
c613-132: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-132: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-132: [2025-03-01 17:51:13,575] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-042: Loading extension module cpu_adam...
c622-042: Time to load cpu_adam op: 3.33160400390625 seconds
c622-042: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-042: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-042: [2025-03-01 17:51:13,780] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-082: Loading extension module cpu_adam...
c622-082: Time to load cpu_adam op: 3.35667085647583 seconds
c622-082: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-082: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-082: [2025-03-01 17:51:13,819] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-091: Loading extension module cpu_adam...
c622-091: Time to load cpu_adam op: 3.344334363937378 seconds
c622-091: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-091: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-091: [2025-03-01 17:51:13,831] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-071: Loading extension module cpu_adam...
c621-071: Time to load cpu_adam op: 3.336047887802124 seconds
c621-071: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-071: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-071: [2025-03-01 17:51:13,873] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-092: Loading extension module cpu_adam...
c622-061: Loading extension module cpu_adam...
c622-092: Time to load cpu_adam op: 3.3445475101470947 seconds
c622-092: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-092: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-092: [2025-03-01 17:51:13,881] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-061: Time to load cpu_adam op: 3.34936261177063 seconds
c622-061: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-061: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-061: [2025-03-01 17:51:13,883] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-041: Loading extension module cpu_adam...
c622-041: Time to load cpu_adam op: 3.3240151405334473 seconds
c622-041: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-041: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-041: [2025-03-01 17:51:13,897] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-132: Loading extension module cpu_adam...
c621-132: Time to load cpu_adam op: 3.3325440883636475 seconds
c621-132: Adam Optimizer #0 is created with scalar arithmetic capability.
c621-132: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c621-132: [2025-03-01 17:51:13,926] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-131: Loading extension module cpu_adam...
c613-131: Time to load cpu_adam op: 3.330960273742676 seconds
c613-131: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-131: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-131: [2025-03-01 17:51:13,959] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-141: Loading extension module cpu_adam...
c613-141: Time to load cpu_adam op: 3.326990842819214 seconds
c613-141: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-141: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-141: [2025-03-01 17:51:14,002] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-142: Loading extension module cpu_adam...
c613-142: Time to load cpu_adam op: 3.3683760166168213 seconds
c613-142: Adam Optimizer #0 is created with scalar arithmetic capability.
c613-142: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c613-142: [2025-03-01 17:51:14,056] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-081: Loading extension module cpu_adam...
c622-081: Time to load cpu_adam op: 3.390169143676758 seconds
c622-081: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-081: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-081: [2025-03-01 17:51:14,081] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-012: Loading extension module cpu_adam...
c619-012: Time to load cpu_adam op: 3.334333658218384 seconds
c619-012: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-012: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-012: [2025-03-01 17:51:14,095] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c622-032: Loading extension module cpu_adam...
c622-032: Time to load cpu_adam op: 3.3380463123321533 seconds
c622-032: Adam Optimizer #0 is created with scalar arithmetic capability.
c622-032: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c622-032: [2025-03-01 17:51:14,415] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: Using /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124 as PyTorch extensions root...
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: Emitting ninja build file /home1/09308/zhengmk/.cache/torch_extensions/py39_cu124/cpu_adam/build.ninja...
c619-002: Building extension module cpu_adam...
c619-002: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
c619-002: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
c619-002: To disable this warning, you can either:
c619-002: 	- Avoid using `tokenizers` before the fork if possible
c619-002: 	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
c619-002: ninja: no work to do.
c619-002: Loading extension module cpu_adam...
c619-002: Time to load cpu_adam op: 0.4914100170135498 seconds
c619-002: Adam Optimizer #0 is created with scalar arithmetic capability.
c619-002: Config: alpha=0.000096, betas=(0.900000, 0.950000), weight_decay=0.000000, adam_w=1
c619-002: [2025-03-01 17:51:14,711] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: Parameter Offload: Total persistent parameters: 266240 in 65 params
c613-101: [2025-03-01 17:51:15,205] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
c613-101: [2025-03-01 17:51:15,206] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 4.12 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:15,207] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 35.0%
c613-101: [2025-03-01 17:51:15,528] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
c613-101: [2025-03-01 17:51:15,529] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:15,529] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.51 GB, percent = 35.0%
c613-101: [2025-03-01 17:51:15,913] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2
c613-101: [2025-03-01 17:51:15,915] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:15,915] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.74 GB, percent = 35.1%
c613-101: [2025-03-01 17:51:16,241] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
c613-101: [2025-03-01 17:51:16,242] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:16,242] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.74 GB, percent = 35.1%
c613-101: [2025-03-01 17:51:16,605] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
c613-101: [2025-03-01 17:51:16,606] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:16,606] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.74 GB, percent = 35.1%
c613-101: [2025-03-01 17:51:16,935] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
c613-101: [2025-03-01 17:51:16,936] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:16,937] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.74 GB, percent = 35.1%
c613-101: [2025-03-01 17:51:17,300] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
c613-101: [2025-03-01 17:51:17,301] [INFO] [utils.py:782:see_memory_usage] MA 2.16 GB         Max_MA 2.16 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:17,302] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.78 GB, percent = 35.1%
c613-101: [2025-03-01 17:51:17,302] [INFO] [stage3.py:529:_setup_for_real_optimizer] optimizer state initialized
c621-151: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-141: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-081: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-111: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-042: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-081: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-092: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-032: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-072: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-101: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-012: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-021: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-062: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-111: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-131: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-122: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-091: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-082: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-092: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-041: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-112: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-132: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-051: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-072: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-132: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-152: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-071: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-091: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-011: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-052: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-102: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-022: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-031: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-152: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-061: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-082: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-002: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-031: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-001: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-001: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-122: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-151: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-102: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-112: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-121: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-071: [2025-03-01 17:51:21,689] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-141: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-021: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-142: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-061: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-142: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-052: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-102: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-121: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-131: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-022: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-012: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-011: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-002: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c622-101: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-041: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c619-032: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c621-062: [2025-03-01 17:51:21,690] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:51:22,059] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
c613-101: [2025-03-01 17:51:22,060] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 5.05 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:22,060] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 75.03 GB, percent = 35.3%
c613-101: [2025-03-01 17:51:22,060] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
c613-101: [2025-03-01 17:51:22,060] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using client LR scheduler
c613-101: [2025-03-01 17:51:22,060] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x4004abfb7310>
c613-101: [2025-03-01 17:51:22,060] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
c613-101: [2025-03-01 17:51:22,062] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
c613-101: [2025-03-01 17:51:22,062] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
c613-101:     "partition_activations": false, 
c613-101:     "contiguous_memory_optimization": false, 
c613-101:     "cpu_checkpointing": false, 
c613-101:     "number_checkpoints": null, 
c613-101:     "synchronize_checkpoint_boundary": false, 
c613-101:     "profile": false
c613-101: }
c613-101: [2025-03-01 17:51:22,062] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
c613-101: [2025-03-01 17:51:22,062] [INFO] [config.py:1003:print]   amp_enabled .................. False
c613-101: [2025-03-01 17:51:22,062] [INFO] [config.py:1003:print]   amp_params ................... False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   autotuning_config ............ {
c613-101:     "enabled": false, 
c613-101:     "start_step": null, 
c613-101:     "end_step": null, 
c613-101:     "metric_path": null, 
c613-101:     "arg_mappings": null, 
c613-101:     "metric": "throughput", 
c613-101:     "model_info": null, 
c613-101:     "results_dir": "autotuning_results", 
c613-101:     "exps_dir": "autotuning_exps", 
c613-101:     "overwrite": true, 
c613-101:     "fast": true, 
c613-101:     "start_profile_step": 3, 
c613-101:     "end_profile_step": 5, 
c613-101:     "tuner_type": "gridsearch", 
c613-101:     "tuner_early_stopping": 5, 
c613-101:     "tuner_num_trials": 50, 
c613-101:     "model_info_path": null, 
c613-101:     "mp_size": 1, 
c613-101:     "max_train_batch_size": null, 
c613-101:     "min_train_batch_size": 1, 
c613-101:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
c613-101:     "min_train_micro_batch_size_per_gpu": 1, 
c613-101:     "num_tuning_micro_batch_sizes": 3
c613-101: }
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x4004abfb7df0>
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   communication_data_type ...... None
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
c613-101: [2025-03-01 17:51:22,063] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   disable_allgather ............ False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   dump_state ................... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
c613-101:     "enabled": false, 
c613-101:     "recompute_fwd_factor": 0.0, 
c613-101:     "profile_step": 1, 
c613-101:     "module_depth": -1, 
c613-101:     "top_modules": 1, 
c613-101:     "detailed": true, 
c613-101:     "output_file": null
c613-101: }
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   fp16_enabled ................. False
c613-101: [2025-03-01 17:51:22,064] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   global_rank .................. 0
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   graph_harvesting ............. False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   memory_breakdown ............. False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step2_tensorboard/ds_tensorboard_logs/', job_name='step2_model_tensorboard') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   nebula_config ................ {
c613-101:     "enabled": false, 
c613-101:     "persistent_storage_path": null, 
c613-101:     "persistent_time_interval": 100, 
c613-101:     "num_of_version_in_retention": 2, 
c613-101:     "enable_nebula_load": true, 
c613-101:     "load_path": null
c613-101: }
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   optimizer_name ............... None
c613-101: [2025-03-01 17:51:22,065] [INFO] [config.py:1003:print]   optimizer_params ............. None
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   pld_enabled .................. False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   pld_params ................... False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   prescale_gradients ........... False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   scheduler_name ............... None
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   scheduler_params ............. None
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   sparse_attention ............. None
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   steps_per_print .............. 10
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   train_batch_size ............. 1024
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   weight_quantization_config ... None
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   world_size ................... 64
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
c613-101: [2025-03-01 17:51:22,066] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
c613-101: [2025-03-01 17:51:22,067] [INFO] [config.py:1003:print]   zero_enabled ................. True
c613-101: [2025-03-01 17:51:22,067] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
c613-101: [2025-03-01 17:51:22,067] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
c613-101: [2025-03-01 17:51:22,067] [INFO] [config.py:989:print_user_config]   json = {
c613-101:     "train_batch_size": 1.024000e+03, 
c613-101:     "train_micro_batch_size_per_gpu": 1, 
c613-101:     "steps_per_print": 10, 
c613-101:     "zero_optimization": {
c613-101:         "stage": 3, 
c613-101:         "overlap_comm": true, 
c613-101:         "offload_param": {
c613-101:             "device": "cpu"
c613-101:         }, 
c613-101:         "offload_optimizer": {
c613-101:             "device": "cpu"
c613-101:         }, 
c613-101:         "stage3_param_persistence_threshold": 1.000000e+04, 
c613-101:         "stage3_max_live_parameters": 3.000000e+07, 
c613-101:         "stage3_prefetch_bucket_size": 3.000000e+07, 
c613-101:         "memory_efficient_linear": false
c613-101:     }, 
c613-101:     "bfloat16": {
c613-101:         "enabled": true
c613-101:     }, 
c613-101:     "gradient_clipping": 1.0, 
c613-101:     "prescale_gradients": false, 
c613-101:     "wall_clock_breakdown": false, 
c613-101:     "hybrid_engine": {
c613-101:         "enabled": false, 
c613-101:         "max_out_tokens": 512, 
c613-101:         "inference_tp_size": 1, 
c613-101:         "release_inference_cache": false, 
c613-101:         "pin_parameters": true, 
c613-101:         "tp_gather_partition_size": 8
c613-101:     }, 
c613-101:     "tensorboard": {
c613-101:         "enabled": false, 
c613-101:         "output_path": "step2_tensorboard/ds_tensorboard_logs/", 
c613-101:         "job_name": "step2_model_tensorboard"
c613-101:     }
c613-101: }
c613-101: [2025-03-01 17:51:22,067] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.3, git-hash=unknown, git-branch=unknown
c613-101: [2025-03-01 17:51:22,067] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 64
c613-101: [2025-03-01 17:51:22,077] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
c613-101: [2025-03-01 17:51:22,078] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload
c613-101: [2025-03-01 17:51:22,424] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
c613-101: [2025-03-01 17:51:22,424] [INFO] [utils.py:782:see_memory_usage] MA 3.09 GB         Max_MA 3.09 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:22,425] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 75.03 GB, percent = 35.3%
c613-101: Parameter Offload: Total persistent parameters: 266240 in 65 params
c613-101: [2025-03-01 17:51:22,795] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
c613-101: [2025-03-01 17:51:22,796] [INFO] [utils.py:782:see_memory_usage] MA 1.17 GB         Max_MA 3.11 GB         CA 8.85 GB         Max_CA 9 GB 
c613-101: [2025-03-01 17:51:22,797] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 75.03 GB, percent = 35.3%
c613-101: [2025-03-01 17:51:22,798] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
c613-101: [2025-03-01 17:51:22,798] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
c613-101:     "partition_activations": false, 
c613-101:     "contiguous_memory_optimization": false, 
c613-101:     "cpu_checkpointing": false, 
c613-101:     "number_checkpoints": null, 
c613-101:     "synchronize_checkpoint_boundary": false, 
c613-101:     "profile": false
c613-101: }
c613-101: [2025-03-01 17:51:22,798] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
c613-101: [2025-03-01 17:51:22,798] [INFO] [config.py:1003:print]   amp_enabled .................. False
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   amp_params ................... False
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   autotuning_config ............ {
c613-101:     "enabled": false, 
c613-101:     "start_step": null, 
c613-101:     "end_step": null, 
c613-101:     "metric_path": null, 
c613-101:     "arg_mappings": null, 
c613-101:     "metric": "throughput", 
c613-101:     "model_info": null, 
c613-101:     "results_dir": "autotuning_results", 
c613-101:     "exps_dir": "autotuning_exps", 
c613-101:     "overwrite": true, 
c613-101:     "fast": true, 
c613-101:     "start_profile_step": 3, 
c613-101:     "end_profile_step": 5, 
c613-101:     "tuner_type": "gridsearch", 
c613-101:     "tuner_early_stopping": 5, 
c613-101:     "tuner_num_trials": 50, 
c613-101:     "model_info_path": null, 
c613-101:     "mp_size": 1, 
c613-101:     "max_train_batch_size": null, 
c613-101:     "min_train_batch_size": 1, 
c613-101:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
c613-101:     "min_train_micro_batch_size_per_gpu": 1, 
c613-101:     "num_tuning_micro_batch_sizes": 3
c613-101: }
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x40073051caf0>
c613-101: [2025-03-01 17:51:22,799] [INFO] [config.py:1003:print]   communication_data_type ...... None
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   disable_allgather ............ False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   dump_state ................... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
c613-101: [2025-03-01 17:51:22,800] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
c613-101:     "enabled": false, 
c613-101:     "recompute_fwd_factor": 0.0, 
c613-101:     "profile_step": 1, 
c613-101:     "module_depth": -1, 
c613-101:     "top_modules": 1, 
c613-101:     "detailed": true, 
c613-101:     "output_file": null
c613-101: }
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   fp16_enabled ................. False
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   global_rank .................. 0
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   graph_harvesting ............. False
c613-101: [2025-03-01 17:51:22,801] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   memory_breakdown ............. False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   nebula_config ................ {
c613-101:     "enabled": false, 
c613-101:     "persistent_storage_path": null, 
c613-101:     "persistent_time_interval": 100, 
c613-101:     "num_of_version_in_retention": 2, 
c613-101:     "enable_nebula_load": true, 
c613-101:     "load_path": null
c613-101: }
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   optimizer_name ............... None
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   optimizer_params ............. None
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   pld_enabled .................. False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   pld_params ................... False
c613-101: [2025-03-01 17:51:22,802] [INFO] [config.py:1003:print]   prescale_gradients ........... False
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   scheduler_name ............... None
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   scheduler_params ............. None
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   sparse_attention ............. None
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   steps_per_print .............. 10
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   train_batch_size ............. 1024
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   weight_quantization_config ... None
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   world_size ................... 64
c613-101: [2025-03-01 17:51:22,803] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False
c613-101: [2025-03-01 17:51:22,804] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
c613-101: [2025-03-01 17:51:22,804] [INFO] [config.py:1003:print]   zero_enabled ................. True
c613-101: [2025-03-01 17:51:22,804] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
c613-101: [2025-03-01 17:51:22,804] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
c613-101: [2025-03-01 17:51:22,804] [INFO] [config.py:989:print_user_config]   json = {
c613-101:     "train_batch_size": 1.024000e+03, 
c613-101:     "train_micro_batch_size_per_gpu": 1, 
c613-101:     "steps_per_print": 10, 
c613-101:     "zero_optimization": {
c613-101:         "stage": 3, 
c613-101:         "stage3_param_persistence_threshold": 1.000000e+04, 
c613-101:         "offload_param": {
c613-101:             "device": "cpu"
c613-101:         }, 
c613-101:         "memory_efficient_linear": false
c613-101:     }, 
c613-101:     "bfloat16": {
c613-101:         "enabled": true
c613-101:     }, 
c613-101:     "gradient_clipping": 1.0, 
c613-101:     "prescale_gradients": false, 
c613-101:     "wall_clock_breakdown": false
c613-101: }
c613-101: ***** Running training *****
c613-101: ***** Evaluating rewards, Epoch 1/3 *****
c613-101: chosen: 0.0, rejected: 0.0, loss: 0.69140625
c613-101: Beginning of Epoch 1/3, Total Micro Batches 452
c622-071: Epoch: 0, Step: 0, Rank: 56, loss = 0.69140625
c613-151: Epoch: 0, Step: 0, Rank: 10, loss = 0.69140625
c621-111: Epoch: 0, Step: 0, Rank: 32, loss = 0.69140625
c613-101: Epoch: 0, Step: 0, Rank: 0, loss = 0.69140625
c619-001: Epoch: 0, Step: 0, Rank: 12, loss = 0.69140625
c622-052: Epoch: 0, Step: 0, Rank: 53, loss = 0.69140625
c622-062: Epoch: 0, Step: 0, Rank: 55, loss = 0.69140625
c622-061: Epoch: 0, Step: 0, Rank: 54, loss = 0.69140625
c622-032: Epoch: 0, Step: 0, Rank: 49, loss = 0.69140625
c622-081: Epoch: 0, Step: 0, Rank: 58, loss = 0.69140625
c619-031: Epoch: 0, Step: 0, Rank: 18, loss = 0.69140625
c622-002: Epoch: 0, Step: 0, Rank: 43, loss = 0.69140625
c622-031: Epoch: 0, Step: 0, Rank: 48, loss = 0.69140625
c622-012: Epoch: 0, Step: 0, Rank: 45, loss = 0.69140625
c621-081: Epoch: 0, Step: 0, Rank: 26, loss = 0.69140625
c621-112: Epoch: 0, Step: 0, Rank: 33, loss = 0.69140625
c621-121: Epoch: 0, Step: 0, Rank: 34, loss = 0.69140625
c622-101: Epoch: 0, Step: 0, Rank: 62, loss = 0.69140625
c621-131: Epoch: 0, Step: 0, Rank: 36, loss = 0.69140625
c613-152: Epoch: 0, Step: 0, Rank: 11, loss = 0.69140625
c622-041: Epoch: 0, Step: 0, Rank: 50, loss = 0.69140625
c613-142: Epoch: 0, Step: 0, Rank: 9, loss = 0.69140625
c619-012: Epoch: 0, Step: 0, Rank: 15, loss = 0.69140625
c621-132: Epoch: 0, Step: 0, Rank: 37, loss = 0.69140625
c622-092: Epoch: 0, Step: 0, Rank: 61, loss = 0.69140625
c619-021: Epoch: 0, Step: 0, Rank: 16, loss = 0.69140625
c622-022: Epoch: 0, Step: 0, Rank: 47, loss = 0.69140625
c613-131: Epoch: 0, Step: 0, Rank: 6, loss = 0.69140625
c619-002: Epoch: 0, Step: 0, Rank: 13, loss = 0.69140625
c622-072: Epoch: 0, Step: 0, Rank: 57, loss = 0.69140625
c621-102: Epoch: 0, Step: 0, Rank: 31, loss = 0.69140625
c622-042: Epoch: 0, Step: 0, Rank: 51, loss = 0.69140625
c621-082: Epoch: 0, Step: 0, Rank: 27, loss = 0.69140625
c619-041: Epoch: 0, Step: 0, Rank: 20, loss = 0.69140625
c621-091: Epoch: 0, Step: 0, Rank: 28, loss = 0.69140625
c613-132: Epoch: 0, Step: 0, Rank: 7, loss = 0.69140625
c622-051: Epoch: 0, Step: 0, Rank: 52, loss = 0.69140625
c622-102: Epoch: 0, Step: 0, Rank: 63, loss = 0.69140625
c621-052: Epoch: 0, Step: 0, Rank: 21, loss = 0.69140625
c621-061: Epoch: 0, Step: 0, Rank: 22, loss = 0.69140625
c622-001: Epoch: 0, Step: 0, Rank: 42, loss = 0.69140625
c619-032: Epoch: 0, Step: 0, Rank: 19, loss = 0.69140625
c622-011: Epoch: 0, Step: 0, Rank: 44, loss = 0.69140625
c619-022: Epoch: 0, Step: 0, Rank: 17, loss = 0.69140625
c621-071: Epoch: 0, Step: 0, Rank: 24, loss = 0.69140625
c613-112: Epoch: 0, Step: 0, Rank: 3, loss = 0.69140625
c613-122: Epoch: 0, Step: 0, Rank: 5, loss = 0.69140625
c619-011: Epoch: 0, Step: 0, Rank: 14, loss = 0.69140625
c613-121: Epoch: 0, Step: 0, Rank: 4, loss = 0.69140625
c621-101: Epoch: 0, Step: 0, Rank: 30, loss = 0.69140625
c621-092: Epoch: 0, Step: 0, Rank: 29, loss = 0.69140625
c621-151: Epoch: 0, Step: 0, Rank: 40, loss = 0.69140625
c622-021: Epoch: 0, Step: 0, Rank: 46, loss = 0.69140625
c621-141: Epoch: 0, Step: 0, Rank: 38, loss = 0.69140625
c621-062: Epoch: 0, Step: 0, Rank: 23, loss = 0.69140625
c621-152: Epoch: 0, Step: 0, Rank: 41, loss = 0.69140625
c613-111: Epoch: 0, Step: 0, Rank: 2, loss = 0.69140625
c621-122: Epoch: 0, Step: 0, Rank: 35, loss = 0.69140625
c621-142: Epoch: 0, Step: 0, Rank: 39, loss = 0.69140625
c622-082: Epoch: 0, Step: 0, Rank: 59, loss = 0.69140625
c622-091: Epoch: 0, Step: 0, Rank: 60, loss = 0.69140625
c621-072: Epoch: 0, Step: 0, Rank: 25, loss = 0.69140625
c613-102: Epoch: 0, Step: 0, Rank: 1, loss = 0.69140625
c613-141: Epoch: 0, Step: 0, Rank: 8, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6914.75341796875 | max allocated: 10410.224609375 | reserved: 18598.0 | max reserved: 18598.0
c613-101: Invalidate trace cache @ step 422 and module 0: cache has only 422 modules
c621-091: c621-091:102446:102879 [0] NCCL INFO Using non-device net plugin version 0
c621-091: c621-091:102446:102879 [0] NCCL INFO Using network IB
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Using non-device net plugin version 0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Using network IB
c621-091: c621-091:102446:102879 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Using non-device net plugin version 0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Using network IB
c621-092: c621-092:1627105:1627533 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Using non-device net plugin version 0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Using network IB
c621-081: c621-081:2075589:2076014 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-101: c621-101:813656:814085 [0] NCCL INFO Using non-device net plugin version 0
c621-101: c621-101:813656:814085 [0] NCCL INFO Using network IB
c621-101: c621-101:813656:814085 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Using non-device net plugin version 0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Using network IB
c621-102: c621-102:2281097:2281523 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Using non-device net plugin version 0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Using network IB
c621-111: c621-111:3868077:3868507 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Using non-device net plugin version 0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Using network IB
c621-112: c621-112:411646:412075 [0] NCCL INFO Using non-device net plugin version 0
c621-112: c621-112:411646:412075 [0] NCCL INFO Using network IB
c621-072: c621-072:3048145:3048578 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-112: c621-112:411646:412075 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Using non-device net plugin version 0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Using network IB
c621-071: c621-071:2842568:2842996 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Using non-device net plugin version 0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Using network IB
c621-121: c621-121:1502414:1502842 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Using non-device net plugin version 0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Using network IB
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Using non-device net plugin version 0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Using network IB
c621-131: c621-131:2225085:2225519 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Using non-device net plugin version 0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Using network IB
c621-062: c621-062:1471760:1472185 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-132: c621-132:518779:519209 [0] NCCL INFO Using non-device net plugin version 0
c621-132: c621-132:518779:519209 [0] NCCL INFO Using network IB
c621-132: c621-132:518779:519209 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-011: c619-011:219279:219711 [0] NCCL INFO Using non-device net plugin version 0
c619-011: c619-011:219279:219711 [0] NCCL INFO Using network IB
c619-012: c619-012:246412:246840 [0] NCCL INFO Using non-device net plugin version 0
c619-012: c619-012:246412:246840 [0] NCCL INFO Using network IB
c619-011: c619-011:219279:219711 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-142: c621-142:600426:600853 [0] NCCL INFO Using non-device net plugin version 0
c621-142: c621-142:600426:600853 [0] NCCL INFO Using network IB
c621-142: c621-142:600426:600853 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Using non-device net plugin version 0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Using non-device net plugin version 0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Using network IB
c621-141: c621-141:3859461:3859891 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Using network IB
c619-012: c619-012:246412:246840 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-002: c619-002:83791:84219 [0] NCCL INFO Using non-device net plugin version 0
c619-002: c619-002:83791:84219 [0] NCCL INFO Using network IB
c619-002: c619-002:83791:84219 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-021: c619-021:593675:594105 [0] NCCL INFO Using non-device net plugin version 0
c619-022: c619-022:804214:804642 [0] NCCL INFO Using non-device net plugin version 0
c619-022: c619-022:804214:804642 [0] NCCL INFO Using network IB
c619-021: c619-021:593675:594105 [0] NCCL INFO Using network IB
c621-151: c621-151:336402:336827 [0] NCCL INFO Using non-device net plugin version 0
c621-151: c621-151:336402:336827 [0] NCCL INFO Using network IB
c621-052: c621-052:855162:855591 [0] NCCL INFO Using non-device net plugin version 0
c621-052: c621-052:855162:855591 [0] NCCL INFO Using network IB
c619-021: c619-021:593675:594105 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-052: c621-052:855162:855591 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-022: c619-022:804214:804642 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-031: c619-031:361171:361598 [0] NCCL INFO Using non-device net plugin version 0
c621-151: c621-151:336402:336827 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Using non-device net plugin version 0
c619-031: c619-031:361171:361598 [0] NCCL INFO Using network IB
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Using network IB
c619-001: c619-001:2454729:2455159 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-031: c619-031:361171:361598 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Using non-device net plugin version 0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Using network IB
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Using non-device net plugin version 0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Using network IB
c619-041: c619-041:15310:15741 [0] NCCL INFO Using non-device net plugin version 0
c619-041: c619-041:15310:15741 [0] NCCL INFO Using network IB
c613-152: c613-152:3770490:3770922 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO DMA-BUF is available on GPU device 0
c619-041: c619-041:15310:15741 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Using non-device net plugin version 0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Using network IB
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Using non-device net plugin version 0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Using network IB
c621-152: c621-152:1793414:1793841 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Using non-device net plugin version 0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Using network IB
c622-001: c622-001:3806195:3806627 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Using non-device net plugin version 0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Using network IB
c622-002: c622-002:1377821:1378250 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-011: c622-011:710508:710937 [0] NCCL INFO Using non-device net plugin version 0
c622-011: c622-011:710508:710937 [0] NCCL INFO Using network IB
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Using non-device net plugin version 0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Using network IB
c622-012: c622-012:2903325:2903752 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-011: c622-011:710508:710937 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Using non-device net plugin version 0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Using network IB
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Using non-device net plugin version 0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Using network IB
c613-142: c613-142:3123258:3123686 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Using non-device net plugin version 0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Using network IB
c622-022: c622-022:180805:181232 [0] NCCL INFO Using non-device net plugin version 0
c622-022: c622-022:180805:181232 [0] NCCL INFO Using network IB
c622-031: c622-031:3136042:3136469 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-022: c622-022:180805:181232 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-131: c613-131:931664:932089 [0] NCCL INFO Using non-device net plugin version 0
c613-131: c613-131:931664:932089 [0] NCCL INFO Using network IB
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Using non-device net plugin version 0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Using network IB
c613-131: c613-131:931664:932089 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-132: c613-132:990252:990680 [0] NCCL INFO Using non-device net plugin version 0
c613-132: c613-132:990252:990680 [0] NCCL INFO Using network IB
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Using non-device net plugin version 0
c613-132: c613-132:990252:990680 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Using network IB
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Using non-device net plugin version 0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Using network IB
c622-032: c622-032:1587523:1587950 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-041: c622-041:198407:198836 [0] NCCL INFO Using non-device net plugin version 0
c622-041: c622-041:198407:198836 [0] NCCL INFO Using network IB
c622-041: c622-041:198407:198836 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-042: c622-042:654000:654431 [0] NCCL INFO Using non-device net plugin version 0
c622-042: c622-042:654000:654431 [0] NCCL INFO Using network IB
c613-121: c613-121:904347:904780 [0] NCCL INFO Using non-device net plugin version 0
c613-121: c613-121:904347:904780 [0] NCCL INFO Using network IB
c622-042: c622-042:654000:654431 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-121: c613-121:904347:904780 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Using non-device net plugin version 0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Using network IB
c622-051: c622-051:3538690:3539116 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-052: c622-052:990541:990968 [0] NCCL INFO Using non-device net plugin version 0
c622-052: c622-052:990541:990968 [0] NCCL INFO Using network IB
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Using non-device net plugin version 0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Using network IB
c622-052: c622-052:990541:990968 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-111: c613-111:640696:641123 [0] NCCL INFO Using non-device net plugin version 0
c613-111: c613-111:640696:641123 [0] NCCL INFO Using network IB
c613-111: c613-111:640696:641123 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-101: c613-101:387979:388442 [0] NCCL INFO Using non-device net plugin version 0
c613-101: c613-101:387979:388442 [0] NCCL INFO Using network IB
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Using non-device net plugin version 0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Using network IB
c622-061: c622-061:1156840:1157269 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Using non-device net plugin version 0
c613-101: c613-101:387979:388442 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Using network IB
c622-062: c622-062:2304142:2304573 [0] NCCL INFO DMA-BUF is available on GPU device 0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Using non-device net plugin version 0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Using network IB
c622-071: c622-071:494914:495339 [0] NCCL INFO Using non-device net plugin version 0
c622-071: c622-071:494914:495339 [0] NCCL INFO Using network IB
c613-102: c613-102:1660081:1660510 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-071: c622-071:494914:495339 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-081: c622-081:41435:41864 [0] NCCL INFO Using non-device net plugin version 0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Using non-device net plugin version 0
c622-081: c622-081:41435:41864 [0] NCCL INFO Using network IB
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Using network IB
c622-092: c622-092:418610:419038 [0] NCCL INFO Using non-device net plugin version 0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Using non-device net plugin version 0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Using non-device net plugin version 0
c622-092: c622-092:418610:419038 [0] NCCL INFO Using network IB
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Using network IB
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Using network IB
c622-102: c622-102:2511218:2511653 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-092: c622-092:418610:419038 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-081: c622-081:41435:41864 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Using non-device net plugin version 0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Using non-device net plugin version 0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Using network IB
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Using network IB
c622-101: c622-101:1627370:1627802 [0] NCCL INFO DMA-BUF is available on GPU device 0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO DMA-BUF is available on GPU device 0
c621-101: c621-101:813656:814085 [0] NCCL INFO bootstrapSplit: comm 0x40085c0c28b0 parent 0xaaaae5fa3640 rank 30 nranks 64 color -1443970512 key 30 prev 29 next 31 - DONE
c621-101: c621-101:813656:814085 [0] NCCL INFO ncclCommSplit comm 0x40085c0c28b0 rank 30 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae5fa3640 color -1443970512 key 30 commId 0x5dd374a04e0f27b9 - Init START
c621-102: c621-102:2281097:2281523 [0] NCCL INFO bootstrapSplit: comm 0x40086c0c2800 parent 0xaaab01513280 rank 31 nranks 64 color -1443970512 key 31 prev 30 next 32 - DONE
c621-072: c621-072:3048145:3048578 [0] NCCL INFO bootstrapSplit: comm 0x4008700c27e0 parent 0xaaaac7562e90 rank 25 nranks 64 color -1443970512 key 25 prev 24 next 26 - DONE
c621-102: c621-102:2281097:2281523 [0] NCCL INFO ncclCommSplit comm 0x40086c0c2800 rank 31 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab01513280 color -1443970512 key 31 commId 0x5dd374a04e0f27b9 - Init START
c621-072: c621-072:3048145:3048578 [0] NCCL INFO ncclCommSplit comm 0x4008700c27e0 rank 25 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac7562e90 color -1443970512 key 25 commId 0x5dd374a04e0f27b9 - Init START
c621-092: c621-092:1627105:1627533 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c28c0 parent 0xaaaafaf44150 rank 29 nranks 64 color -1443970512 key 29 prev 28 next 30 - DONE
c621-092: c621-092:1627105:1627533 [0] NCCL INFO ncclCommSplit comm 0x40087c0c28c0 rank 29 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafaf44150 color -1443970512 key 29 commId 0x5dd374a04e0f27b9 - Init START
c621-071: c621-071:2842568:2842996 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c2850 parent 0xaaaaf5803340 rank 24 nranks 64 color -1443970512 key 24 prev 23 next 25 - DONE
c621-071: c621-071:2842568:2842996 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2850 rank 24 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5803340 color -1443970512 key 24 commId 0x5dd374a04e0f27b9 - Init START
c621-081: c621-081:2075589:2076014 [0] NCCL INFO bootstrapSplit: comm 0x40085c0c2710 parent 0xaaaae8834680 rank 26 nranks 64 color -1443970512 key 26 prev 25 next 27 - DONE
c621-081: c621-081:2075589:2076014 [0] NCCL INFO ncclCommSplit comm 0x40085c0c2710 rank 26 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae8834680 color -1443970512 key 26 commId 0x5dd374a04e0f27b9 - Init START
c621-091: c621-091:102446:102879 [0] NCCL INFO bootstrapSplit: comm 0x4009b00c2820 parent 0xaaaae7334ad0 rank 28 nranks 64 color -1443970512 key 28 prev 27 next 29 - DONE
c621-091: c621-091:102446:102879 [0] NCCL INFO ncclCommSplit comm 0x4009b00c2820 rank 28 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae7334ad0 color -1443970512 key 28 commId 0x5dd374a04e0f27b9 - Init START
c621-082: c621-082:1003419:1003850 [0] NCCL INFO bootstrapSplit: comm 0x40099c0c2670 parent 0xaaaaef4b43b0 rank 27 nranks 64 color -1443970512 key 27 prev 26 next 28 - DONE
c621-082: c621-082:1003419:1003850 [0] NCCL INFO ncclCommSplit comm 0x40099c0c2670 rank 27 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef4b43b0 color -1443970512 key 27 commId 0x5dd374a04e0f27b9 - Init START
c621-111: c621-111:3868077:3868507 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2830 parent 0xaaaadde53570 rank 32 nranks 64 color -1443970512 key 32 prev 31 next 33 - DONE
c621-111: c621-111:3868077:3868507 [0] NCCL INFO ncclCommSplit comm 0x4008480c2830 rank 32 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadde53570 color -1443970512 key 32 commId 0x5dd374a04e0f27b9 - Init START
c621-112: c621-112:411646:412075 [0] NCCL INFO bootstrapSplit: comm 0x4008580c27a0 parent 0xaaaafa972940 rank 33 nranks 64 color -1443970512 key 33 prev 32 next 34 - DONE
c621-112: c621-112:411646:412075 [0] NCCL INFO ncclCommSplit comm 0x4008580c27a0 rank 33 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa972940 color -1443970512 key 33 commId 0x5dd374a04e0f27b9 - Init START
c621-121: c621-121:1502414:1502842 [0] NCCL INFO bootstrapSplit: comm 0x4008680c27d0 parent 0xaaaac7ad29a0 rank 34 nranks 64 color -1443970512 key 34 prev 33 next 35 - DONE
c621-121: c621-121:1502414:1502842 [0] NCCL INFO ncclCommSplit comm 0x4008680c27d0 rank 34 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac7ad29a0 color -1443970512 key 34 commId 0x5dd374a04e0f27b9 - Init START
c621-122: c621-122:1561826:1562255 [0] NCCL INFO bootstrapSplit: comm 0x4008540c2840 parent 0xaaaaf42a34a0 rank 35 nranks 64 color -1443970512 key 35 prev 34 next 36 - DONE
c621-122: c621-122:1561826:1562255 [0] NCCL INFO ncclCommSplit comm 0x4008540c2840 rank 35 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf42a34a0 color -1443970512 key 35 commId 0x5dd374a04e0f27b9 - Init START
c621-131: c621-131:2225085:2225519 [0] NCCL INFO bootstrapSplit: comm 0x40084c0c2890 parent 0xaaaad9263e70 rank 36 nranks 64 color -1443970512 key 36 prev 35 next 37 - DONE
c621-131: c621-131:2225085:2225519 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2890 rank 36 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad9263e70 color -1443970512 key 36 commId 0x5dd374a04e0f27b9 - Init START
c621-062: c621-062:1471760:1472185 [0] NCCL INFO bootstrapSplit: comm 0x40084c0c2750 parent 0xaaab20314540 rank 23 nranks 64 color -1443970512 key 23 prev 22 next 24 - DONE
c621-062: c621-062:1471760:1472185 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2750 rank 23 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab20314540 color -1443970512 key 23 commId 0x5dd374a04e0f27b9 - Init START
c622-072: c622-072:1688285:1688717 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c2630 parent 0xaaaaf1bc32f0 rank 57 nranks 64 color -1443970512 key 57 prev 56 next 58 - DONE
c622-081: c622-081:41435:41864 [0] NCCL INFO bootstrapSplit: comm 0x4008780c2890 parent 0xaaaae73d3c50 rank 58 nranks 64 color -1443970512 key 58 prev 57 next 59 - DONE
c622-072: c622-072:1688285:1688717 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2630 rank 57 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf1bc32f0 color -1443970512 key 57 commId 0x5dd374a04e0f27b9 - Init START
c621-132: c621-132:518779:519209 [0] NCCL INFO bootstrapSplit: comm 0x4008440c2870 parent 0xaaab08e331b0 rank 37 nranks 64 color -1443970512 key 37 prev 36 next 38 - DONE
c621-132: c621-132:518779:519209 [0] NCCL INFO ncclCommSplit comm 0x4008440c2870 rank 37 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab08e331b0 color -1443970512 key 37 commId 0x5dd374a04e0f27b9 - Init START
c622-081: c622-081:41435:41864 [0] NCCL INFO ncclCommSplit comm 0x4008780c2890 rank 58 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae73d3c50 color -1443970512 key 58 commId 0x5dd374a04e0f27b9 - Init START
c621-141: c621-141:3859461:3859891 [0] NCCL INFO bootstrapSplit: comm 0x4008780c2730 parent 0xaaab05fc3cd0 rank 38 nranks 64 color -1443970512 key 38 prev 37 next 39 - DONE
c622-082: c622-082:2786472:2786898 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2860 parent 0xaaab0d0846f0 rank 59 nranks 64 color -1443970512 key 59 prev 58 next 60 - DONE
c622-082: c622-082:2786472:2786898 [0] NCCL INFO ncclCommSplit comm 0x4008480c2860 rank 59 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0d0846f0 color -1443970512 key 59 commId 0x5dd374a04e0f27b9 - Init START
c621-141: c621-141:3859461:3859891 [0] NCCL INFO ncclCommSplit comm 0x4008780c2730 rank 38 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab05fc3cd0 color -1443970512 key 38 commId 0x5dd374a04e0f27b9 - Init START
c622-071: c622-071:494914:495339 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c2840 parent 0xaaaadd881dc0 rank 56 nranks 64 color -1443970512 key 56 prev 55 next 57 - DONE
c622-091: c622-091:2780457:2780884 [0] NCCL INFO bootstrapSplit: comm 0x4008680c2740 parent 0xaaaad2a14660 rank 60 nranks 64 color -1443970512 key 60 prev 59 next 61 - DONE
c622-091: c622-091:2780457:2780884 [0] NCCL INFO ncclCommSplit comm 0x4008680c2740 rank 60 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad2a14660 color -1443970512 key 60 commId 0x5dd374a04e0f27b9 - Init START
c622-071: c622-071:494914:495339 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2840 rank 56 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadd881dc0 color -1443970512 key 56 commId 0x5dd374a04e0f27b9 - Init START
c621-061: c621-061:3120949:3121378 [0] NCCL INFO bootstrapSplit: comm 0x40084c0c2860 parent 0xaaab28873410 rank 22 nranks 64 color -1443970512 key 22 prev 21 next 23 - DONE
c621-061: c621-061:3120949:3121378 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2860 rank 22 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab28873410 color -1443970512 key 22 commId 0x5dd374a04e0f27b9 - Init START
c622-092: c622-092:418610:419038 [0] NCCL INFO bootstrapSplit: comm 0x4009dc0c2700 parent 0xaaab065a30c0 rank 61 nranks 64 color -1443970512 key 61 prev 60 next 62 - DONE
c622-092: c622-092:418610:419038 [0] NCCL INFO ncclCommSplit comm 0x4009dc0c2700 rank 61 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab065a30c0 color -1443970512 key 61 commId 0x5dd374a04e0f27b9 - Init START
c621-142: c621-142:600426:600853 [0] NCCL INFO bootstrapSplit: comm 0x4008640c2760 parent 0xaaaafcef3f00 rank 39 nranks 64 color -1443970512 key 39 prev 38 next 40 - DONE
c621-151: c621-151:336402:336827 [0] NCCL INFO bootstrapSplit: comm 0x4008440c26e0 parent 0xaaab31fb4310 rank 40 nranks 64 color -1443970512 key 40 prev 39 next 41 - DONE
c621-142: c621-142:600426:600853 [0] NCCL INFO ncclCommSplit comm 0x4008640c2760 rank 39 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafcef3f00 color -1443970512 key 39 commId 0x5dd374a04e0f27b9 - Init START
c621-151: c621-151:336402:336827 [0] NCCL INFO ncclCommSplit comm 0x4008440c26e0 rank 40 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab31fb4310 color -1443970512 key 40 commId 0x5dd374a04e0f27b9 - Init START
c622-101: c622-101:1627370:1627802 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2730 parent 0xaaaaf8c135d0 rank 62 nranks 64 color -1443970512 key 62 prev 61 next 63 - DONE
c622-101: c622-101:1627370:1627802 [0] NCCL INFO ncclCommSplit comm 0x4008480c2730 rank 62 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8c135d0 color -1443970512 key 62 commId 0x5dd374a04e0f27b9 - Init START
c621-052: c621-052:855162:855591 [0] NCCL INFO bootstrapSplit: comm 0x4009c80c2820 parent 0xaaaaf3913390 rank 21 nranks 64 color -1443970512 key 21 prev 20 next 22 - DONE
c622-102: c622-102:2511218:2511653 [0] NCCL INFO bootstrapSplit: comm 0x4008780c2780 parent 0xaaaaf35a4020 rank 63 nranks 64 color -1443970512 key 63 prev 62 next 0 - DONE
c613-101: c613-101:387979:388442 [0] NCCL INFO bootstrapSplit: comm 0x400a0c0c2800 parent 0xaaaaf87e37d0 rank 0 nranks 64 color -1443970512 key 0 prev 63 next 1 - DONE
c622-102: c622-102:2511218:2511653 [0] NCCL INFO ncclCommSplit comm 0x4008780c2780 rank 63 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf35a4020 color -1443970512 key 63 commId 0x5dd374a04e0f27b9 - Init START
c613-101: c613-101:387979:388442 [0] NCCL INFO ncclCommSplit comm 0x400a0c0c2800 rank 0 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf87e37d0 color -1443970512 key 0 commId 0x5dd374a04e0f27b9 - Init START
c621-152: c621-152:1793414:1793841 [0] NCCL INFO bootstrapSplit: comm 0x4008500c2780 parent 0xaaaaefa62bc0 rank 41 nranks 64 color -1443970512 key 41 prev 40 next 42 - DONE
c621-052: c621-052:855162:855591 [0] NCCL INFO ncclCommSplit comm 0x4009c80c2820 rank 21 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf3913390 color -1443970512 key 21 commId 0x5dd374a04e0f27b9 - Init START
c622-001: c622-001:3806195:3806627 [0] NCCL INFO bootstrapSplit: comm 0x400a200c2730 parent 0xaaaafa503ef0 rank 42 nranks 64 color -1443970512 key 42 prev 41 next 43 - DONE
c622-001: c622-001:3806195:3806627 [0] NCCL INFO ncclCommSplit comm 0x400a200c2730 rank 42 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa503ef0 color -1443970512 key 42 commId 0x5dd374a04e0f27b9 - Init START
c621-152: c621-152:1793414:1793841 [0] NCCL INFO ncclCommSplit comm 0x4008500c2780 rank 41 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaefa62bc0 color -1443970512 key 41 commId 0x5dd374a04e0f27b9 - Init START
c622-061: c622-061:1156840:1157269 [0] NCCL INFO bootstrapSplit: comm 0x4009f00c2730 parent 0xaaaaea363b30 rank 54 nranks 64 color -1443970512 key 54 prev 53 next 55 - DONE
c622-002: c622-002:1377821:1378250 [0] NCCL INFO bootstrapSplit: comm 0x4009d00c2820 parent 0xaaaaf5e63ae0 rank 43 nranks 64 color -1443970512 key 43 prev 42 next 44 - DONE
c622-002: c622-002:1377821:1378250 [0] NCCL INFO ncclCommSplit comm 0x4009d00c2820 rank 43 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5e63ae0 color -1443970512 key 43 commId 0x5dd374a04e0f27b9 - Init START
c622-062: c622-062:2304142:2304573 [0] NCCL INFO bootstrapSplit: comm 0x4009d00c2870 parent 0xaaaaefc52d60 rank 55 nranks 64 color -1443970512 key 55 prev 54 next 56 - DONE
c622-062: c622-062:2304142:2304573 [0] NCCL INFO ncclCommSplit comm 0x4009d00c2870 rank 55 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaefc52d60 color -1443970512 key 55 commId 0x5dd374a04e0f27b9 - Init START
c622-011: c622-011:710508:710937 [0] NCCL INFO bootstrapSplit: comm 0x4008400c27a0 parent 0xaaab12fd3650 rank 44 nranks 64 color -1443970512 key 44 prev 43 next 45 - DONE
c622-011: c622-011:710508:710937 [0] NCCL INFO ncclCommSplit comm 0x4008400c27a0 rank 44 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab12fd3650 color -1443970512 key 44 commId 0x5dd374a04e0f27b9 - Init START
c619-041: c619-041:15310:15741 [0] NCCL INFO bootstrapSplit: comm 0x4008500c27a0 parent 0xaaaada3c4450 rank 20 nranks 64 color -1443970512 key 20 prev 19 next 21 - DONE
c619-041: c619-041:15310:15741 [0] NCCL INFO ncclCommSplit comm 0x4008500c27a0 rank 20 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaada3c4450 color -1443970512 key 20 commId 0x5dd374a04e0f27b9 - Init START
c622-061: c622-061:1156840:1157269 [0] NCCL INFO ncclCommSplit comm 0x4009f00c2730 rank 54 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaea363b30 color -1443970512 key 54 commId 0x5dd374a04e0f27b9 - Init START
c622-012: c622-012:2903325:2903752 [0] NCCL INFO bootstrapSplit: comm 0x4008500c2710 parent 0xaaaacd7e5250 rank 45 nranks 64 color -1443970512 key 45 prev 44 next 46 - DONE
c622-012: c622-012:2903325:2903752 [0] NCCL INFO ncclCommSplit comm 0x4008500c2710 rank 45 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacd7e5250 color -1443970512 key 45 commId 0x5dd374a04e0f27b9 - Init START
c613-102: c613-102:1660081:1660510 [0] NCCL INFO bootstrapSplit: comm 0x4009a80c2760 parent 0xaaaad5e93020 rank 1 nranks 64 color -1443970512 key 1 prev 0 next 2 - DONE
c613-102: c613-102:1660081:1660510 [0] NCCL INFO ncclCommSplit comm 0x4009a80c2760 rank 1 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad5e93020 color -1443970512 key 1 commId 0x5dd374a04e0f27b9 - Init START
c613-132: c613-132:990252:990680 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2710 parent 0xaaaadc6b2810 rank 7 nranks 64 color -1443970512 key 7 prev 6 next 8 - DONE
c613-111: c613-111:640696:641123 [0] NCCL INFO bootstrapSplit: comm 0x4008580c2820 parent 0xaaaaff0d4900 rank 2 nranks 64 color -1443970512 key 2 prev 1 next 3 - DONE
c622-021: c622-021:1141571:1142001 [0] NCCL INFO bootstrapSplit: comm 0x4008580c2dc0 parent 0xaaaaec1b3660 rank 46 nranks 64 color -1443970512 key 46 prev 45 next 47 - DONE
c613-132: c613-132:990252:990680 [0] NCCL INFO ncclCommSplit comm 0x4008480c2710 rank 7 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadc6b2810 color -1443970512 key 7 commId 0x5dd374a04e0f27b9 - Init START
c622-032: c622-032:1587523:1587950 [0] NCCL INFO bootstrapSplit: comm 0x4009fc0c2810 parent 0xaaaaec6b4890 rank 49 nranks 64 color -1443970512 key 49 prev 48 next 50 - DONE
c622-021: c622-021:1141571:1142001 [0] NCCL INFO ncclCommSplit comm 0x4008580c2dc0 rank 46 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaec1b3660 color -1443970512 key 46 commId 0x5dd374a04e0f27b9 - Init START
c613-111: c613-111:640696:641123 [0] NCCL INFO ncclCommSplit comm 0x4008580c2820 rank 2 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaff0d4900 color -1443970512 key 2 commId 0x5dd374a04e0f27b9 - Init START
c622-022: c622-022:180805:181232 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2840 parent 0xaaaaf9b932b0 rank 47 nranks 64 color -1443970512 key 47 prev 46 next 48 - DONE
c622-032: c622-032:1587523:1587950 [0] NCCL INFO ncclCommSplit comm 0x4009fc0c2810 rank 49 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaec6b4890 color -1443970512 key 49 commId 0x5dd374a04e0f27b9 - Init START
c622-031: c622-031:3136042:3136469 [0] NCCL INFO bootstrapSplit: comm 0x4008700c2850 parent 0xaaab19de4480 rank 48 nranks 64 color -1443970512 key 48 prev 47 next 49 - DONE
c622-031: c622-031:3136042:3136469 [0] NCCL INFO ncclCommSplit comm 0x4008700c2850 rank 48 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab19de4480 color -1443970512 key 48 commId 0x5dd374a04e0f27b9 - Init START
c619-032: c619-032:3546706:3547134 [0] NCCL INFO bootstrapSplit: comm 0x4009800c26c0 parent 0xaaab06d93bf0 rank 19 nranks 64 color -1443970512 key 19 prev 18 next 20 - DONE
c622-022: c622-022:180805:181232 [0] NCCL INFO ncclCommSplit comm 0x4008480c2840 rank 47 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf9b932b0 color -1443970512 key 47 commId 0x5dd374a04e0f27b9 - Init START
c613-141: c613-141:2104776:2105199 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c2740 parent 0xaaab1eb11de0 rank 8 nranks 64 color -1443970512 key 8 prev 7 next 9 - DONE
c613-141: c613-141:2104776:2105199 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2740 rank 8 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab1eb11de0 color -1443970512 key 8 commId 0x5dd374a04e0f27b9 - Init START
c622-041: c622-041:198407:198836 [0] NCCL INFO bootstrapSplit: comm 0x4008600c2770 parent 0xaaaae2173010 rank 50 nranks 64 color -1443970512 key 50 prev 49 next 51 - DONE
c613-112: c613-112:1585580:1586011 [0] NCCL INFO bootstrapSplit: comm 0x4009ac0c2820 parent 0xaaaafa5643c0 rank 3 nranks 64 color -1443970512 key 3 prev 2 next 4 - DONE
c613-112: c613-112:1585580:1586011 [0] NCCL INFO ncclCommSplit comm 0x4009ac0c2820 rank 3 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa5643c0 color -1443970512 key 3 commId 0x5dd374a04e0f27b9 - Init START
c613-131: c613-131:931664:932089 [0] NCCL INFO bootstrapSplit: comm 0x4008400c2810 parent 0xaaab00253330 rank 6 nranks 64 color -1443970512 key 6 prev 5 next 7 - DONE
c619-032: c619-032:3546706:3547134 [0] NCCL INFO ncclCommSplit comm 0x4009800c26c0 rank 19 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab06d93bf0 color -1443970512 key 19 commId 0x5dd374a04e0f27b9 - Init START
c622-052: c622-052:990541:990968 [0] NCCL INFO bootstrapSplit: comm 0x4008580c2790 parent 0xaaab2f773010 rank 53 nranks 64 color -1443970512 key 53 prev 52 next 54 - DONE
c613-131: c613-131:931664:932089 [0] NCCL INFO ncclCommSplit comm 0x4008400c2810 rank 6 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab00253330 color -1443970512 key 6 commId 0x5dd374a04e0f27b9 - Init START
c622-052: c622-052:990541:990968 [0] NCCL INFO ncclCommSplit comm 0x4008580c2790 rank 53 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab2f773010 color -1443970512 key 53 commId 0x5dd374a04e0f27b9 - Init START
c622-041: c622-041:198407:198836 [0] NCCL INFO ncclCommSplit comm 0x4008600c2770 rank 50 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae2173010 color -1443970512 key 50 commId 0x5dd374a04e0f27b9 - Init START
c622-051: c622-051:3538690:3539116 [0] NCCL INFO bootstrapSplit: comm 0x4008500c2710 parent 0xaaab056632b0 rank 52 nranks 64 color -1443970512 key 52 prev 51 next 53 - DONE
c622-051: c622-051:3538690:3539116 [0] NCCL INFO ncclCommSplit comm 0x4008500c2710 rank 52 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab056632b0 color -1443970512 key 52 commId 0x5dd374a04e0f27b9 - Init START
c622-042: c622-042:654000:654431 [0] NCCL INFO bootstrapSplit: comm 0x4009d40c2830 parent 0xaaaadc7a5b50 rank 51 nranks 64 color -1443970512 key 51 prev 50 next 52 - DONE
c622-042: c622-042:654000:654431 [0] NCCL INFO ncclCommSplit comm 0x4009d40c2830 rank 51 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadc7a5b50 color -1443970512 key 51 commId 0x5dd374a04e0f27b9 - Init START
c613-142: c613-142:3123258:3123686 [0] NCCL INFO bootstrapSplit: comm 0x4008600c2760 parent 0xaaaae9e74700 rank 9 nranks 64 color -1443970512 key 9 prev 8 next 10 - DONE
c613-142: c613-142:3123258:3123686 [0] NCCL INFO ncclCommSplit comm 0x4008600c2760 rank 9 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae9e74700 color -1443970512 key 9 commId 0x5dd374a04e0f27b9 - Init START
c613-151: c613-151:3397419:3397847 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c27e0 parent 0xaaaadaeb2c20 rank 10 nranks 64 color -1443970512 key 10 prev 9 next 11 - DONE
c613-152: c613-152:3770490:3770922 [0] NCCL INFO bootstrapSplit: comm 0x4008880c2860 parent 0xaaaae1154720 rank 11 nranks 64 color -1443970512 key 11 prev 10 next 12 - DONE
c613-152: c613-152:3770490:3770922 [0] NCCL INFO ncclCommSplit comm 0x4008880c2860 rank 11 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae1154720 color -1443970512 key 11 commId 0x5dd374a04e0f27b9 - Init START
c613-151: c613-151:3397419:3397847 [0] NCCL INFO ncclCommSplit comm 0x40087c0c27e0 rank 10 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadaeb2c20 color -1443970512 key 10 commId 0x5dd374a04e0f27b9 - Init START
c613-122: c613-122:1267677:1268109 [0] NCCL INFO bootstrapSplit: comm 0x4008480c27c0 parent 0xaaaacc441df0 rank 5 nranks 64 color -1443970512 key 5 prev 4 next 6 - DONE
c613-122: c613-122:1267677:1268109 [0] NCCL INFO ncclCommSplit comm 0x4008480c27c0 rank 5 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacc441df0 color -1443970512 key 5 commId 0x5dd374a04e0f27b9 - Init START
c619-031: c619-031:361171:361598 [0] NCCL INFO bootstrapSplit: comm 0x4008540c26e0 parent 0xaaaad6982fb0 rank 18 nranks 64 color -1443970512 key 18 prev 17 next 19 - DONE
c613-121: c613-121:904347:904780 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2760 parent 0xaaab03501d90 rank 4 nranks 64 color -1443970512 key 4 prev 3 next 5 - DONE
c613-121: c613-121:904347:904780 [0] NCCL INFO ncclCommSplit comm 0x4008480c2760 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab03501d90 color -1443970512 key 4 commId 0x5dd374a04e0f27b9 - Init START
c619-031: c619-031:361171:361598 [0] NCCL INFO ncclCommSplit comm 0x4008540c26e0 rank 18 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad6982fb0 color -1443970512 key 18 commId 0x5dd374a04e0f27b9 - Init START
c619-001: c619-001:2454729:2455159 [0] NCCL INFO bootstrapSplit: comm 0x4008580c2810 parent 0xaaab21114760 rank 12 nranks 64 color -1443970512 key 12 prev 11 next 13 - DONE
c619-022: c619-022:804214:804642 [0] NCCL INFO bootstrapSplit: comm 0x4009d00c27d0 parent 0xaaaaf5e93920 rank 17 nranks 64 color -1443970512 key 17 prev 16 next 18 - DONE
c619-001: c619-001:2454729:2455159 [0] NCCL INFO ncclCommSplit comm 0x4008580c2810 rank 12 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab21114760 color -1443970512 key 12 commId 0x5dd374a04e0f27b9 - Init START
c619-022: c619-022:804214:804642 [0] NCCL INFO ncclCommSplit comm 0x4009d00c27d0 rank 17 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5e93920 color -1443970512 key 17 commId 0x5dd374a04e0f27b9 - Init START
c619-002: c619-002:83791:84219 [0] NCCL INFO bootstrapSplit: comm 0x4008680c27d0 parent 0xaaaaef322ea0 rank 13 nranks 64 color -1443970512 key 13 prev 12 next 14 - DONE
c619-002: c619-002:83791:84219 [0] NCCL INFO ncclCommSplit comm 0x4008680c27d0 rank 13 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef322ea0 color -1443970512 key 13 commId 0x5dd374a04e0f27b9 - Init START
c619-011: c619-011:219279:219711 [0] NCCL INFO bootstrapSplit: comm 0x4008480c2810 parent 0xaaab0f1f2940 rank 14 nranks 64 color -1443970512 key 14 prev 13 next 15 - DONE
c619-011: c619-011:219279:219711 [0] NCCL INFO ncclCommSplit comm 0x4008480c2810 rank 14 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0f1f2940 color -1443970512 key 14 commId 0x5dd374a04e0f27b9 - Init START
c619-021: c619-021:593675:594105 [0] NCCL INFO bootstrapSplit: comm 0x4008440c27e0 parent 0xaaaacef91690 rank 16 nranks 64 color -1443970512 key 16 prev 15 next 17 - DONE
c619-021: c619-021:593675:594105 [0] NCCL INFO ncclCommSplit comm 0x4008440c27e0 rank 16 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacef91690 color -1443970512 key 16 commId 0x5dd374a04e0f27b9 - Init START
c619-012: c619-012:246412:246840 [0] NCCL INFO bootstrapSplit: comm 0x40087c0c2810 parent 0xaaaae5903380 rank 15 nranks 64 color -1443970512 key 15 prev 14 next 16 - DONE
c619-012: c619-012:246412:246840 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2810 rank 15 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae5903380 color -1443970512 key 15 commId 0x5dd374a04e0f27b9 - Init START
c622-052: c622-052:990541:990968 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-042: c622-042:654000:654431 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-002: c619-002:83791:84219 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-071: c622-071:494914:495339 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-011: c619-011:219279:219711 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-041: c622-041:198407:198836 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-081: c622-081:41435:41864 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-022: c622-022:180805:181232 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-112: c621-112:411646:412075 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-021: c619-021:593675:594105 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-012: c619-012:246412:246840 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-132: c621-132:518779:519209 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-151: c621-151:336402:336827 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-011: c622-011:710508:710937 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-022: c619-022:804214:804642 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-031: c619-031:361171:361598 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-142: c621-142:600426:600853 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-092: c622-092:418610:419038 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-101: c613-101:387979:388442 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-091: c621-091:102446:102879 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-052: c621-052:855162:855591 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c619-041: c619-041:15310:15741 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-101: c621-101:813656:814085 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-111: c613-111:640696:641123 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-132: c613-132:990252:990680 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-131: c613-131:931664:932089 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c613-121: c613-121:904347:904780 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffffffff,ffffffff
c622-071: c622-071:494914:495339 [0] NCCL INFO comm 0x40087c0c2840 rank 56 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO comm 0x40087c0c2630 rank 57 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-081: c622-081:41435:41864 [0] NCCL INFO comm 0x4008780c2890 rank 58 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-071: c622-071:494914:495339 [0] NCCL INFO Trees [0] 52/60/-1->56->48 [1] 52/60/-1->56->48 [2] -1/-1/-1->56->57 [3] -1/-1/-1->56->57
c622-071: c622-071:494914:495339 [0] NCCL INFO P2P Chunksize set to 131072
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Trees [0] -1/-1/-1->57->58 [1] -1/-1/-1->57->58 [2] 58/56/-1->57->59 [3] 58/56/-1->57->59
c622-072: c622-072:1688285:1688717 [0] NCCL INFO P2P Chunksize set to 131072
c622-081: c622-081:41435:41864 [0] NCCL INFO Trees [0] 57/59/-1->58->60 [1] 57/59/-1->58->60 [2] -1/-1/-1->58->57 [3] -1/-1/-1->58->57
c622-081: c622-081:41435:41864 [0] NCCL INFO P2P Chunksize set to 131072
c622-082: c622-082:2786472:2786898 [0] NCCL INFO comm 0x4008480c2860 rank 59 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Trees [0] -1/-1/-1->59->58 [1] -1/-1/-1->59->58 [2] 61/57/-1->59->55 [3] 61/57/-1->59->55
c622-082: c622-082:2786472:2786898 [0] NCCL INFO P2P Chunksize set to 131072
c622-091: c622-091:2780457:2780884 [0] NCCL INFO comm 0x4008680c2740 rank 60 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-101: c613-101:387979:388442 [0] NCCL INFO comm 0x400a0c0c2800 rank 0 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
c613-101: c613-101:387979:388442 [0] NCCL INFO Trees [0] 32/-1/-1->0->-1 [1] 32/-1/-1->0->-1 [2] -1/-1/-1->0->1 [3] -1/-1/-1->0->1
c622-102: c622-102:2511218:2511653 [0] NCCL INFO comm 0x4008780c2780 rank 63 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-101: c613-101:387979:388442 [0] NCCL INFO P2P Chunksize set to 131072
c622-092: c622-092:418610:419038 [0] NCCL INFO comm 0x4009dc0c2700 rank 61 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Trees [0] 58/62/-1->60->56 [1] 58/62/-1->60->56 [2] -1/-1/-1->60->61 [3] -1/-1/-1->60->61
c622-091: c622-091:2780457:2780884 [0] NCCL INFO P2P Chunksize set to 131072
c622-092: c622-092:418610:419038 [0] NCCL INFO Trees [0] -1/-1/-1->61->62 [1] -1/-1/-1->61->62 [2] 62/60/-1->61->59 [3] 62/60/-1->61->59
c622-092: c622-092:418610:419038 [0] NCCL INFO P2P Chunksize set to 131072
c622-101: c622-101:1627370:1627802 [0] NCCL INFO comm 0x4008480c2730 rank 62 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Trees [0] -1/-1/-1->63->62 [1] -1/-1/-1->63->62 [2] 31/-1/-1->63->-1 [3] 31/-1/-1->63->-1
c622-102: c622-102:2511218:2511653 [0] NCCL INFO P2P Chunksize set to 131072
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Trees [0] 61/63/-1->62->60 [1] 61/63/-1->62->60 [2] -1/-1/-1->62->61 [3] -1/-1/-1->62->61
c622-101: c622-101:1627370:1627802 [0] NCCL INFO P2P Chunksize set to 131072
c613-102: c613-102:1660081:1660510 [0] NCCL INFO comm 0x4009a80c2760 rank 1 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO comm 0x4008540c2840 rank 35 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO comm 0x4008680c27d0 rank 34 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Trees [0] -1/-1/-1->1->2 [1] -1/-1/-1->1->2 [2] 2/0/-1->1->3 [3] 2/0/-1->1->3
c613-102: c613-102:1660081:1660510 [0] NCCL INFO P2P Chunksize set to 131072
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] 37/33/-1->35->39 [3] 37/33/-1->35->39
c621-122: c621-122:1561826:1562255 [0] NCCL INFO P2P Chunksize set to 131072
c613-111: c613-111:640696:641123 [0] NCCL INFO comm 0x4008580c2820 rank 2 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-111: c613-111:640696:641123 [0] NCCL INFO Trees [0] 1/3/-1->2->4 [1] 1/3/-1->2->4 [2] -1/-1/-1->2->1 [3] -1/-1/-1->2->1
c613-111: c613-111:640696:641123 [0] NCCL INFO P2P Chunksize set to 131072
c622-062: c622-062:2304142:2304573 [0] NCCL INFO comm 0x4009d00c2870 rank 55 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Trees [0] 33/35/-1->34->36 [1] 33/35/-1->34->36 [2] -1/-1/-1->34->33 [3] -1/-1/-1->34->33
c621-131: c621-131:2225085:2225519 [0] NCCL INFO comm 0x40084c0c2890 rank 36 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Trees [0] 34/38/-1->36->40 [1] 34/38/-1->36->40 [2] -1/-1/-1->36->37 [3] -1/-1/-1->36->37
c613-112: c613-112:1585580:1586011 [0] NCCL INFO comm 0x4009ac0c2820 rank 3 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO P2P Chunksize set to 131072
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Trees [0] -1/-1/-1->55->54 [1] -1/-1/-1->55->54 [2] 59/51/-1->55->47 [3] 59/51/-1->55->47
c621-121: c621-121:1502414:1502842 [0] NCCL INFO P2P Chunksize set to 131072
c622-062: c622-062:2304142:2304573 [0] NCCL INFO P2P Chunksize set to 131072
c621-132: c621-132:518779:519209 [0] NCCL INFO comm 0x4008440c2870 rank 37 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 5/1/-1->3->7 [3] 5/1/-1->3->7
c621-132: c621-132:518779:519209 [0] NCCL INFO Trees [0] -1/-1/-1->37->38 [1] -1/-1/-1->37->38 [2] 38/36/-1->37->35 [3] 38/36/-1->37->35
c621-132: c621-132:518779:519209 [0] NCCL INFO P2P Chunksize set to 131072
c613-112: c613-112:1585580:1586011 [0] NCCL INFO P2P Chunksize set to 131072
c621-141: c621-141:3859461:3859891 [0] NCCL INFO comm 0x4008780c2730 rank 38 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Trees [0] 37/39/-1->38->36 [1] 37/39/-1->38->36 [2] -1/-1/-1->38->37 [3] -1/-1/-1->38->37
c613-121: c613-121:904347:904780 [0] NCCL INFO comm 0x4008480c2760 rank 4 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO P2P Chunksize set to 131072
c613-122: c613-122:1267677:1268109 [0] NCCL INFO comm 0x4008480c27c0 rank 5 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO comm 0x4009f00c2730 rank 54 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Trees [0] 53/55/-1->54->52 [1] 53/55/-1->54->52 [2] -1/-1/-1->54->53 [3] -1/-1/-1->54->53
c622-061: c622-061:1156840:1157269 [0] NCCL INFO P2P Chunksize set to 131072
c621-151: c621-151:336402:336827 [0] NCCL INFO comm 0x4008440c26e0 rank 40 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-011: c619-011:219279:219711 [0] NCCL INFO comm 0x4008480c2810 rank 14 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-121: c613-121:904347:904780 [0] NCCL INFO Trees [0] 2/6/-1->4->8 [1] 2/6/-1->4->8 [2] -1/-1/-1->4->5 [3] -1/-1/-1->4->5
c613-121: c613-121:904347:904780 [0] NCCL INFO P2P Chunksize set to 131072
c621-151: c621-151:336402:336827 [0] NCCL INFO Trees [0] 36/44/-1->40->48 [1] 36/44/-1->40->48 [2] -1/-1/-1->40->41 [3] -1/-1/-1->40->41
c621-151: c621-151:336402:336827 [0] NCCL INFO P2P Chunksize set to 131072
c621-152: c621-152:1793414:1793841 [0] NCCL INFO comm 0x4008500c2780 rank 41 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Trees [0] -1/-1/-1->5->6 [1] -1/-1/-1->5->6 [2] 6/4/-1->5->3 [3] 6/4/-1->5->3
c613-122: c613-122:1267677:1268109 [0] NCCL INFO P2P Chunksize set to 131072
c613-131: c613-131:931664:932089 [0] NCCL INFO comm 0x4008400c2810 rank 6 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-011: c619-011:219279:219711 [0] NCCL INFO Trees [0] 13/15/-1->14->12 [1] 13/15/-1->14->12 [2] -1/-1/-1->14->13 [3] -1/-1/-1->14->13
c619-011: c619-011:219279:219711 [0] NCCL INFO P2P Chunksize set to 131072
c613-132: c613-132:990252:990680 [0] NCCL INFO comm 0x4008480c2710 rank 7 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Trees [0] -1/-1/-1->41->42 [1] -1/-1/-1->41->42 [2] 42/40/-1->41->43 [3] 42/40/-1->41->43
c619-002: c619-002:83791:84219 [0] NCCL INFO comm 0x4008680c27d0 rank 13 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-131: c613-131:931664:932089 [0] NCCL INFO Trees [0] 5/7/-1->6->4 [1] 5/7/-1->6->4 [2] -1/-1/-1->6->5 [3] -1/-1/-1->6->5
c622-001: c622-001:3806195:3806627 [0] NCCL INFO comm 0x400a200c2730 rank 42 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-002: c619-002:83791:84219 [0] NCCL INFO Trees [0] -1/-1/-1->13->14 [1] -1/-1/-1->13->14 [2] 14/12/-1->13->11 [3] 14/12/-1->13->11
c621-152: c621-152:1793414:1793841 [0] NCCL INFO P2P Chunksize set to 131072
c619-012: c619-012:246412:246840 [0] NCCL INFO comm 0x40087c0c2810 rank 15 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-002: c619-002:83791:84219 [0] NCCL INFO P2P Chunksize set to 131072
c613-132: c613-132:990252:990680 [0] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] 11/3/-1->7->15 [3] 11/3/-1->7->15
c619-001: c619-001:2454729:2455159 [0] NCCL INFO comm 0x4008580c2810 rank 12 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-142: c621-142:600426:600853 [0] NCCL INFO comm 0x4008640c2760 rank 39 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-012: c619-012:246412:246840 [0] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] 23/7/-1->15->31 [3] 23/7/-1->15->31
c613-131: c613-131:931664:932089 [0] NCCL INFO P2P Chunksize set to 131072
c622-011: c622-011:710508:710937 [0] NCCL INFO comm 0x4008400c27a0 rank 44 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO comm 0x4008880c2860 rank 11 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-132: c613-132:990252:990680 [0] NCCL INFO P2P Chunksize set to 131072
c622-002: c622-002:1377821:1378250 [0] NCCL INFO comm 0x4009d00c2820 rank 43 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-012: c619-012:246412:246840 [0] NCCL INFO P2P Chunksize set to 131072
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] 13/9/-1->11->7 [3] 13/9/-1->11->7
c613-152: c613-152:3770490:3770922 [0] NCCL INFO P2P Chunksize set to 131072
c622-011: c622-011:710508:710937 [0] NCCL INFO Trees [0] 42/46/-1->44->40 [1] 42/46/-1->44->40 [2] -1/-1/-1->44->45 [3] -1/-1/-1->44->45
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Trees [0] 41/43/-1->42->44 [1] 41/43/-1->42->44 [2] -1/-1/-1->42->41 [3] -1/-1/-1->42->41
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] 45/41/-1->43->39 [3] 45/41/-1->43->39
c613-142: c613-142:3123258:3123686 [0] NCCL INFO comm 0x4008600c2760 rank 9 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO P2P Chunksize set to 131072
c622-002: c622-002:1377821:1378250 [0] NCCL INFO P2P Chunksize set to 131072
c622-011: c622-011:710508:710937 [0] NCCL INFO P2P Chunksize set to 131072
c613-141: c613-141:2104776:2105199 [0] NCCL INFO comm 0x40087c0c2740 rank 8 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO comm 0x40087c0c27e0 rank 10 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Trees [0] 10/14/-1->12->8 [1] 10/14/-1->12->8 [2] -1/-1/-1->12->13 [3] -1/-1/-1->12->13
c621-142: c621-142:600426:600853 [0] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] 43/35/-1->39->47 [3] 43/35/-1->39->47
c621-142: c621-142:600426:600853 [0] NCCL INFO P2P Chunksize set to 131072
c619-001: c619-001:2454729:2455159 [0] NCCL INFO P2P Chunksize set to 131072
c622-012: c622-012:2903325:2903752 [0] NCCL INFO comm 0x4008500c2710 rank 45 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Trees [0] 4/12/-1->8->16 [1] 4/12/-1->8->16 [2] -1/-1/-1->8->9 [3] -1/-1/-1->8->9
c619-021: c619-021:593675:594105 [0] NCCL INFO comm 0x4008440c27e0 rank 16 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO P2P Chunksize set to 131072
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Trees [0] -1/-1/-1->45->46 [1] -1/-1/-1->45->46 [2] 46/44/-1->45->43 [3] 46/44/-1->45->43
c622-012: c622-012:2903325:2903752 [0] NCCL INFO P2P Chunksize set to 131072
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Trees [0] -1/-1/-1->9->10 [1] -1/-1/-1->9->10 [2] 10/8/-1->9->11 [3] 10/8/-1->9->11
c613-142: c613-142:3123258:3123686 [0] NCCL INFO P2P Chunksize set to 131072
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Trees [0] 9/11/-1->10->12 [1] 9/11/-1->10->12 [2] -1/-1/-1->10->9 [3] -1/-1/-1->10->9
c613-151: c613-151:3397419:3397847 [0] NCCL INFO P2P Chunksize set to 131072
c622-021: c622-021:1141571:1142001 [0] NCCL INFO comm 0x4008580c2dc0 rank 46 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-052: c622-052:990541:990968 [0] NCCL INFO comm 0x4008580c2790 rank 53 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-021: c619-021:593675:594105 [0] NCCL INFO Trees [0] 8/24/-1->16->32 [1] 8/24/-1->16->32 [2] -1/-1/-1->16->17 [3] -1/-1/-1->16->17
c619-021: c619-021:593675:594105 [0] NCCL INFO P2P Chunksize set to 131072
c622-022: c622-022:180805:181232 [0] NCCL INFO comm 0x4008480c2840 rank 47 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO comm 0x4008500c2710 rank 52 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Trees [0] 45/47/-1->46->44 [1] 45/47/-1->46->44 [2] -1/-1/-1->46->45 [3] -1/-1/-1->46->45
c622-021: c622-021:1141571:1142001 [0] NCCL INFO P2P Chunksize set to 131072
c622-052: c622-052:990541:990968 [0] NCCL INFO Trees [0] -1/-1/-1->53->54 [1] -1/-1/-1->53->54 [2] 54/52/-1->53->51 [3] 54/52/-1->53->51
c622-052: c622-052:990541:990968 [0] NCCL INFO P2P Chunksize set to 131072
c622-031: c622-031:3136042:3136469 [0] NCCL INFO comm 0x4008700c2850 rank 48 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Trees [0] 50/54/-1->52->56 [1] 50/54/-1->52->56 [2] -1/-1/-1->52->53 [3] -1/-1/-1->52->53
c622-051: c622-051:3538690:3539116 [0] NCCL INFO P2P Chunksize set to 131072
c622-041: c622-041:198407:198836 [0] NCCL INFO comm 0x4008600c2770 rank 50 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-022: c622-022:180805:181232 [0] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] 55/39/-1->47->31 [3] 55/39/-1->47->31
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Trees [0] 40/56/-1->48->32 [1] 40/56/-1->48->32 [2] -1/-1/-1->48->49 [3] -1/-1/-1->48->49
c622-032: c622-032:1587523:1587950 [0] NCCL INFO comm 0x4009fc0c2810 rank 49 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-022: c622-022:180805:181232 [0] NCCL INFO P2P Chunksize set to 131072
c622-042: c622-042:654000:654431 [0] NCCL INFO comm 0x4009d40c2830 rank 51 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO P2P Chunksize set to 131072
c621-112: c621-112:411646:412075 [0] NCCL INFO comm 0x4008580c27a0 rank 33 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Trees [0] -1/-1/-1->49->50 [1] -1/-1/-1->49->50 [2] 50/48/-1->49->51 [3] 50/48/-1->49->51
c622-032: c622-032:1587523:1587950 [0] NCCL INFO P2P Chunksize set to 131072
c619-022: c619-022:804214:804642 [0] NCCL INFO comm 0x4009d00c27d0 rank 17 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c622-041: c622-041:198407:198836 [0] NCCL INFO Trees [0] 49/51/-1->50->52 [1] 49/51/-1->50->52 [2] -1/-1/-1->50->49 [3] -1/-1/-1->50->49
c622-041: c622-041:198407:198836 [0] NCCL INFO P2P Chunksize set to 131072
c619-031: c619-031:361171:361598 [0] NCCL INFO comm 0x4008540c26e0 rank 18 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-022: c619-022:804214:804642 [0] NCCL INFO Trees [0] -1/-1/-1->17->18 [1] -1/-1/-1->17->18 [2] 18/16/-1->17->19 [3] 18/16/-1->17->19
c622-042: c622-042:654000:654431 [0] NCCL INFO Trees [0] -1/-1/-1->51->50 [1] -1/-1/-1->51->50 [2] 53/49/-1->51->55 [3] 53/49/-1->51->55
c619-022: c619-022:804214:804642 [0] NCCL INFO P2P Chunksize set to 131072
c619-031: c619-031:361171:361598 [0] NCCL INFO Trees [0] 17/19/-1->18->20 [1] 17/19/-1->18->20 [2] -1/-1/-1->18->17 [3] -1/-1/-1->18->17
c619-031: c619-031:361171:361598 [0] NCCL INFO P2P Chunksize set to 131072
c622-042: c622-042:654000:654431 [0] NCCL INFO P2P Chunksize set to 131072
c621-112: c621-112:411646:412075 [0] NCCL INFO Trees [0] -1/-1/-1->33->34 [1] -1/-1/-1->33->34 [2] 34/32/-1->33->35 [3] 34/32/-1->33->35
c621-112: c621-112:411646:412075 [0] NCCL INFO P2P Chunksize set to 131072
c619-032: c619-032:3546706:3547134 [0] NCCL INFO comm 0x4009800c26c0 rank 19 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] 21/17/-1->19->23 [3] 21/17/-1->19->23
c621-111: c621-111:3868077:3868507 [0] NCCL INFO comm 0x4008480c2830 rank 32 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO P2P Chunksize set to 131072
c621-052: c621-052:855162:855591 [0] NCCL INFO comm 0x4009c80c2820 rank 21 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-041: c619-041:15310:15741 [0] NCCL INFO comm 0x4008500c27a0 rank 20 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c619-041: c619-041:15310:15741 [0] NCCL INFO Trees [0] 18/22/-1->20->24 [1] 18/22/-1->20->24 [2] -1/-1/-1->20->21 [3] -1/-1/-1->20->21
c619-041: c619-041:15310:15741 [0] NCCL INFO P2P Chunksize set to 131072
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Trees [0] 16/48/-1->32->0 [1] 16/48/-1->32->0 [2] -1/-1/-1->32->33 [3] -1/-1/-1->32->33
c621-111: c621-111:3868077:3868507 [0] NCCL INFO P2P Chunksize set to 131072
c621-052: c621-052:855162:855591 [0] NCCL INFO Trees [0] -1/-1/-1->21->22 [1] -1/-1/-1->21->22 [2] 22/20/-1->21->19 [3] 22/20/-1->21->19
c621-052: c621-052:855162:855591 [0] NCCL INFO P2P Chunksize set to 131072
c621-061: c621-061:3120949:3121378 [0] NCCL INFO comm 0x40084c0c2860 rank 22 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Trees [0] 21/23/-1->22->20 [1] 21/23/-1->22->20 [2] -1/-1/-1->22->21 [3] -1/-1/-1->22->21
c621-061: c621-061:3120949:3121378 [0] NCCL INFO P2P Chunksize set to 131072
c621-062: c621-062:1471760:1472185 [0] NCCL INFO comm 0x40084c0c2750 rank 23 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] 27/19/-1->23->15 [3] 27/19/-1->23->15
c621-062: c621-062:1471760:1472185 [0] NCCL INFO P2P Chunksize set to 131072
c621-102: c621-102:2281097:2281523 [0] NCCL INFO comm 0x40086c0c2800 rank 31 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-101: c621-101:813656:814085 [0] NCCL INFO comm 0x40085c0c28b0 rank 30 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-101: c621-101:813656:814085 [0] NCCL INFO Trees [0] 29/31/-1->30->28 [1] 29/31/-1->30->28 [2] -1/-1/-1->30->29 [3] -1/-1/-1->30->29
c621-101: c621-101:813656:814085 [0] NCCL INFO P2P Chunksize set to 131072
c621-071: c621-071:2842568:2842996 [0] NCCL INFO comm 0x40087c0c2850 rank 24 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] 47/15/-1->31->63 [3] 47/15/-1->31->63
c621-102: c621-102:2281097:2281523 [0] NCCL INFO P2P Chunksize set to 131072
c621-072: c621-072:3048145:3048578 [0] NCCL INFO comm 0x4008700c27e0 rank 25 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO comm 0x40085c0c2710 rank 26 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Trees [0] 20/28/-1->24->16 [1] 20/28/-1->24->16 [2] -1/-1/-1->24->25 [3] -1/-1/-1->24->25
c621-071: c621-071:2842568:2842996 [0] NCCL INFO P2P Chunksize set to 131072
c621-092: c621-092:1627105:1627533 [0] NCCL INFO comm 0x40087c0c28c0 rank 29 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-091: c621-091:102446:102879 [0] NCCL INFO comm 0x4009b00c2820 rank 28 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-091: c621-091:102446:102879 [0] NCCL INFO Trees [0] 26/30/-1->28->24 [1] 26/30/-1->28->24 [2] -1/-1/-1->28->29 [3] -1/-1/-1->28->29
c621-091: c621-091:102446:102879 [0] NCCL INFO P2P Chunksize set to 131072
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Trees [0] -1/-1/-1->25->26 [1] -1/-1/-1->25->26 [2] 26/24/-1->25->27 [3] 26/24/-1->25->27
c621-072: c621-072:3048145:3048578 [0] NCCL INFO P2P Chunksize set to 131072
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Trees [0] 25/27/-1->26->28 [1] 25/27/-1->26->28 [2] -1/-1/-1->26->25 [3] -1/-1/-1->26->25
c621-082: c621-082:1003419:1003850 [0] NCCL INFO comm 0x40099c0c2670 rank 27 nRanks 64 nNodes 64 localRanks 1 localRank 0 MNNVL 0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Trees [0] -1/-1/-1->29->30 [1] -1/-1/-1->29->30 [2] 30/28/-1->29->27 [3] 30/28/-1->29->27
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] 29/25/-1->27->23 [3] 29/25/-1->27->23
c621-081: c621-081:2075589:2076014 [0] NCCL INFO P2P Chunksize set to 131072
c621-092: c621-092:1627105:1627533 [0] NCCL INFO P2P Chunksize set to 131072
c621-082: c621-082:1003419:1003850 [0] NCCL INFO P2P Chunksize set to 131072
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 02/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 00/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 00/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 00/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 03/0 : 57[0] -> 58[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 01/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 01/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 02/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 58[0] -> 59[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 02/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 01/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 57[0] -> 58[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 58[0] -> 59[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 03/0 : 55[0] -> 56[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 03/0 : 63[0] -> 0[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 00/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 02/0 : 58[0] -> 59[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 58[0] -> 59[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 00/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 02/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 00/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 01/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 03/0 : 58[0] -> 59[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 00/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 01/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 03/0 : 59[0] -> 60[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 02/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 01/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 02/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 60[0] -> 61[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 02/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 00/0 : 1[0] -> 2[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 03/0 : 62[0] -> 63[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 60[0] -> 61[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 02/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 01/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 59[0] -> 60[0] [send] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 61[0] -> 62[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 03/0 : 56[0] -> 57[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 00/0 : 63[0] -> 0[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 03/0 : 37[0] -> 38[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 02/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 03/0 : 61[0] -> 62[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 61[0] -> 62[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 2[0] -> 3[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 01/0 : 1[0] -> 2[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 02/0 : 60[0] -> 61[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 03/0 : 35[0] -> 36[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 1[0] -> 2[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 01/0 : 63[0] -> 0[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 02/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 03/0 : 60[0] -> 61[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 2[0] -> 3[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 03/0 : 33[0] -> 34[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 02/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 1[0] -> 2[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 00/0 : 55[0] -> 56[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 02/0 : 63[0] -> 0[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 00/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 02/0 : 2[0] -> 3[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 00/0 : 3[0] -> 4[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 34[0] -> 35[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 62[0] -> 63[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 01/0 : 55[0] -> 56[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 38[0] -> 39[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 36[0] -> 37[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 02/0 : 62[0] -> 63[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 34[0] -> 35[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 38[0] -> 39[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 00/0 : 35[0] -> 36[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 01/0 : 35[0] -> 36[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 00/0 : 37[0] -> 38[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 55[0] -> 56[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 03/0 : 63[0] -> 0[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 01/0 : 3[0] -> 4[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 02/0 : 38[0] -> 39[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 03/0 : 2[0] -> 3[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 01/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 35[0] -> 36[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 3[0] -> 4[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 02/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 03/0 : 53[0] -> 54[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 34[0] -> 35[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 03/0 : 38[0] -> 39[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 54[0] -> 55[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 35[0] -> 36[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 3[0] -> 4[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 55[0] -> 56[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 00/0 : 5[0] -> 6[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 03/0 : 62[0] -> 63[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 01/0 : 37[0] -> 38[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 02/0 : 36[0] -> 37[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 03/0 : 39[0] -> 40[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 00/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 54[0] -> 55[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 02/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 4[0] -> 5[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 38[0] -> 39[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 37[0] -> 38[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 02/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 03/0 : 36[0] -> 37[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 02/0 : 34[0] -> 35[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 02/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 03/0 : 34[0] -> 35[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 40[0] -> 41[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 00/0 : 41[0] -> 42[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 00/0 : 39[0] -> 40[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 01/0 : 41[0] -> 42[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 02/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 00/0 : 45[0] -> 46[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 37[0] -> 38[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 03/0 : 54[0] -> 55[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 00/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 03/0 : 43[0] -> 44[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 40[0] -> 41[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 01/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 12[0] -> 13[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 41[0] -> 42[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 01/0 : 39[0] -> 40[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 02/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 02/0 : 40[0] -> 41[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 03/0 : 41[0] -> 42[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 03/0 : 5[0] -> 6[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 01/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 02/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 00/0 : 9[0] -> 10[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 01/0 : 45[0] -> 46[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 02/0 : 44[0] -> 45[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 02/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 03/0 : 51[0] -> 52[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 41[0] -> 42[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 39[0] -> 40[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 01/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 6[0] -> 7[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 00/0 : 43[0] -> 44[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 03/0 : 40[0] -> 41[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 00/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 00/0 : 11[0] -> 12[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 6[0] -> 7[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 01/0 : 43[0] -> 44[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 00/0 : 13[0] -> 14[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 43[0] -> 44[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 01/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 13[0] -> 14[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 02/0 : 12[0] -> 13[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 03/0 : 12[0] -> 13[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 01/0 : 9[0] -> 10[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 45[0] -> 46[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 45[0] -> 46[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 00/0 : 49[0] -> 50[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 00/0 : 7[0] -> 8[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 01/0 : 49[0] -> 50[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 02/0 : 52[0] -> 53[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 01/0 : 7[0] -> 8[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 7[0] -> 8[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 8[0] -> 9[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 8[0] -> 9[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 02/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 02/0 : 8[0] -> 9[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 03/0 : 52[0] -> 53[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 03/0 : 47[0] -> 48[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 7[0] -> 8[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 02/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 01/0 : 11[0] -> 12[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 03/0 : 9[0] -> 10[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 10[0] -> 11[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 02/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 10[0] -> 11[0] [send] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 02/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 03/0 : 15[0] -> 16[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 03/0 : 44[0] -> 45[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 16[0] -> 17[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 02/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 03/0 : 21[0] -> 22[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 39[0] -> 40[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 03/0 : 31[0] -> 32[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 32[0] -> 33[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 14[0] -> 15[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 14[0] -> 15[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 03/0 : 19[0] -> 20[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 22[0] -> 23[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 02/0 : 14[0] -> 15[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 00/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 48[0] -> 49[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 01/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 03/0 : 14[0] -> 15[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 00/0 : 17[0] -> 18[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 46[0] -> 47[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 11[0] -> 12[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 00/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 01/0 : 53[0] -> 54[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 53[0] -> 54[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 01/0 : 17[0] -> 18[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 00/0 : 33[0] -> 34[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 01/0 : 33[0] -> 34[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 17[0] -> 18[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 02/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 16[0] -> 17[0] [send] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 02/0 : 16[0] -> 17[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 03/0 : 16[0] -> 17[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 02/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 02/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 03/0 : 49[0] -> 50[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 03/0 : 8[0] -> 9[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 02/0 : 50[0] -> 51[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 03/0 : 50[0] -> 51[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 20[0] -> 21[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 48[0] -> 49[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 02/0 : 48[0] -> 49[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 15[0] -> 16[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 03/0 : 25[0] -> 26[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 03/0 : 48[0] -> 49[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 02/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 11[0] -> 12[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 00/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 01/0 : 47[0] -> 48[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 53[0] -> 54[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 02/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 47[0] -> 48[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 02/0 : 10[0] -> 11[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 47[0] -> 48[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 03/0 : 10[0] -> 11[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 49[0] -> 50[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 22[0] -> 23[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 42[0] -> 43[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 00/0 : 23[0] -> 24[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 42[0] -> 43[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 20[0] -> 21[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 02/0 : 6[0] -> 7[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 02/0 : 42[0] -> 43[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 03/0 : 6[0] -> 7[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 00/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 26[0] -> 27[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 00/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 43[0] -> 44[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 03/0 : 29[0] -> 30[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 02/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 03/0 : 23[0] -> 24[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 00/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 17[0] -> 18[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 24[0] -> 25[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 01/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 03/0 : 17[0] -> 18[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 18[0] -> 19[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 49[0] -> 50[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 18[0] -> 19[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 24[0] -> 25[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 03/0 : 42[0] -> 43[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 01/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 02/0 : 26[0] -> 27[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 00/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 03/0 : 27[0] -> 28[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 01/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 33[0] -> 34[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 30[0] -> 31[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 33[0] -> 34[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 30[0] -> 31[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 02/0 : 30[0] -> 31[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 03/0 : 30[0] -> 31[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 32[0] -> 33[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 02/0 : 32[0] -> 33[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 02/0 : 24[0] -> 25[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 02/0 : 20[0] -> 21[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 03/0 : 32[0] -> 33[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 22[0] -> 23[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 02/0 : 22[0] -> 23[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 03/0 : 24[0] -> 25[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 03/0 : 22[0] -> 23[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 01/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 27[0] -> 28[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 21[0] -> 22[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 21[0] -> 22[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 03/0 : 20[0] -> 21[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 03/0 : 26[0] -> 27[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 31[0] -> 32[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 00/0 : 29[0] -> 30[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 01/0 : 29[0] -> 30[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 01/0 : 23[0] -> 24[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 03/0 : 45[0] -> 46[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 29[0] -> 30[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 02/0 : 18[0] -> 19[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 50[0] -> 51[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 46[0] -> 47[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 23[0] -> 24[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 02/0 : 46[0] -> 47[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 03/0 : 46[0] -> 47[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 28[0] -> 29[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 00/0 : 51[0] -> 52[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 01/0 : 51[0] -> 52[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 51[0] -> 52[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 51[0] -> 52[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 25[0] -> 26[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 29[0] -> 30[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 18[0] -> 19[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 00/0 : 19[0] -> 20[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 01/0 : 19[0] -> 20[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 23[0] -> 24[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 28[0] -> 29[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 19[0] -> 20[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 02/0 : 28[0] -> 29[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 19[0] -> 20[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 03/0 : 28[0] -> 29[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Connected all rings
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 41[0] -> 43[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 41[0] -> 43[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Connected all rings
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 43[0] -> 45[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Connected all rings
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 36[0] -> 38[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 43[0] -> 45[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Connected all rings
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Connected all rings
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 35[0] -> 37[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 02/0 : 31[0] -> 63[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Connected all rings
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 34[0] -> 36[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 35[0] -> 37[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Connected all rings
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 03/0 : 31[0] -> 63[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 34[0] -> 36[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Connected all rings
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 42[0] -> 44[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Connected all rings
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Connected all rings
c613-121: c613-121:904347:904780 [0] NCCL INFO Connected all rings
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 17[0] -> 19[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 36[0] -> 38[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Connected all rings
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Connected all rings
c619-031: c619-031:361171:361598 [0] NCCL INFO Connected all rings
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 60[0] -> 62[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 36[0] -> 38[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 02/0 : 63[0] -> 31[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 1[0] -> 3[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 03/0 : 63[0] -> 31[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 1[0] -> 3[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 17[0] -> 19[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 2[0] -> 4[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Connected all rings
c622-011: c622-011:710508:710937 [0] NCCL INFO Connected all rings
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 01/0 : 32[0] -> 0[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 2[0] -> 4[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 2[0] -> 4[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Connected all rings
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 1[0] -> 3[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 19[0] -> 21[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Connected all rings
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 18[0] -> 20[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 1[0] -> 3[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 3[0] -> 5[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Connected all rings
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 19[0] -> 21[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 42[0] -> 44[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 52[0] -> 54[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 4[0] -> 6[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 01/0 : 0[0] -> 32[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Connected all rings
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 3[0] -> 5[0] [send] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Connected all rings
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Connected all rings
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 59[0] -> 61[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 3[0] -> 5[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 42[0] -> 44[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 17[0] -> 19[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Connected all rings
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 52[0] -> 54[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Connected all rings
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Connected all rings
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 25[0] -> 27[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Connected all rings
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 44[0] -> 46[0] [send] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 59[0] -> 61[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 17[0] -> 19[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 51[0] -> 55[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 3[0] -> 5[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 44[0] -> 46[0] [send] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 51[0] -> 53[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 41[0] -> 43[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 58[0] -> 60[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 25[0] -> 27[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Connected all rings
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 41[0] -> 43[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Connected all rings
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 57[0] -> 59[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 51[0] -> 53[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 51[0] -> 55[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 7[0] -> 15[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 55[0] -> 59[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Connected all rings
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 55[0] -> 59[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 19[0] -> 17[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 57[0] -> 59[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Connected all rings
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 58[0] -> 60[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 7[0] -> 15[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Connected all rings
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Connected all rings
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 60[0] -> 62[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 38[0] -> 36[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 15[0] -> 23[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 60[0] -> 62[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 3[0] -> 1[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 59[0] -> 61[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 12[0] -> 14[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Connected all rings
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 38[0] -> 36[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 15[0] -> 23[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 58[0] -> 60[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 35[0] -> 39[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 19[0] -> 17[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 44[0] -> 42[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 43[0] -> 41[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 3[0] -> 1[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Connected all rings
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 59[0] -> 61[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 4[0] -> 2[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Connected all rings
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 57[0] -> 59[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 35[0] -> 39[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Connected all rings
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 39[0] -> 43[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Connected all rings
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Connected all rings
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 44[0] -> 42[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Connected all rings
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 58[0] -> 60[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Connected all rings
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 43[0] -> 41[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 5[0] -> 3[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 19[0] -> 21[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 3[0] -> 7[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 26[0] -> 28[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 3[0] -> 7[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 39[0] -> 43[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 25[0] -> 27[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 19[0] -> 23[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 18[0] -> 20[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 18[0] -> 20[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 57[0] -> 59[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Connected all rings
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 20[0] -> 22[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 26[0] -> 28[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 19[0] -> 21[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 5[0] -> 3[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Connected all rings
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 20[0] -> 22[0] [receive] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 25[0] -> 27[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 20[0] -> 22[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 19[0] -> 23[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 33[0] -> 35[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 61[0] -> 59[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 49[0] -> 51[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 23[0] -> 27[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Connected all rings
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 49[0] -> 51[0] [send] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 61[0] -> 59[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Connected all rings
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 55[0] -> 59[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 62[0] -> 60[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 34[0] -> 36[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 20[0] -> 22[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 33[0] -> 35[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 27[0] -> 29[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 60[0] -> 58[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 23[0] -> 27[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 59[0] -> 57[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Connected all rings
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Connected all rings
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Connected all rings
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 27[0] -> 29[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Connected all rings
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 56[0] -> 60[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 21[0] -> 19[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 55[0] -> 59[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 62[0] -> 60[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 34[0] -> 36[0] [send] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 21[0] -> 19[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 43[0] -> 45[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 35[0] -> 37[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 60[0] -> 58[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 56[0] -> 60[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 35[0] -> 37[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 44[0] -> 46[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 39[0] -> 47[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Connected all rings
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 11[0] -> 13[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 59[0] -> 57[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 19[0] -> 23[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Connected all rings
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 20[0] -> 18[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Connected all rings
c613-131: c613-131:931664:932089 [0] NCCL INFO Connected all rings
c613-132: c613-132:990252:990680 [0] NCCL INFO Connected all rings
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 11[0] -> 13[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 43[0] -> 45[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 44[0] -> 46[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 39[0] -> 47[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 20[0] -> 24[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 50[0] -> 52[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 59[0] -> 55[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 9[0] -> 11[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 19[0] -> 23[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 20[0] -> 18[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 36[0] -> 34[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 22[0] -> 20[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 50[0] -> 52[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 22[0] -> 20[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Connected all rings
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 10[0] -> 12[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 27[0] -> 25[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 3[0] -> 7[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 4[0] -> 6[0] [receive] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 36[0] -> 34[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 47[0] -> 55[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 47[0] -> 55[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Connected all rings
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 52[0] -> 54[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 59[0] -> 55[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 49[0] -> 51[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 45[0] -> 43[0] [send] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 27[0] -> 25[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 10[0] -> 12[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 36[0] -> 40[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 50[0] -> 52[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 4[0] -> 6[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 20[0] -> 24[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 9[0] -> 11[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 11[0] -> 13[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 12[0] -> 14[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 52[0] -> 54[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 45[0] -> 43[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 36[0] -> 40[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 11[0] -> 13[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 37[0] -> 35[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 39[0] -> 43[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 49[0] -> 51[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 3[0] -> 7[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 46[0] -> 44[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 12[0] -> 14[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 7[0] -> 11[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 40[0] -> 44[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 23[0] -> 19[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 46[0] -> 44[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 37[0] -> 35[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 50[0] -> 52[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 40[0] -> 44[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 7[0] -> 11[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 51[0] -> 53[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 51[0] -> 53[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 23[0] -> 19[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 39[0] -> 43[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Connected all rings
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 6[0] -> 4[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 52[0] -> 50[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Connected all rings
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 10[0] -> 12[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 52[0] -> 56[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 10[0] -> 12[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 52[0] -> 50[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 43[0] -> 39[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 54[0] -> 52[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 52[0] -> 56[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 7[0] -> 3[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 43[0] -> 39[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 51[0] -> 49[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 40[0] -> 48[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 33[0] -> 35[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 33[0] -> 35[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Connected all rings
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Connected all rings
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 54[0] -> 52[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 14[0] -> 12[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 40[0] -> 48[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 7[0] -> 3[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 51[0] -> 49[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Connected all rings
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 14[0] -> 12[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 13[0] -> 11[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 51[0] -> 55[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 9[0] -> 11[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 9[0] -> 11[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 4[0] -> 8[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 4[0] -> 8[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 53[0] -> 51[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 26[0] -> 28[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 13[0] -> 11[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 53[0] -> 51[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 35[0] -> 33[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 12[0] -> 10[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 35[0] -> 33[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 8[0] -> 12[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 51[0] -> 55[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 12[0] -> 10[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 26[0] -> 28[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 48[0] -> 56[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 35[0] -> 39[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 8[0] -> 12[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 28[0] -> 30[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 35[0] -> 39[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 48[0] -> 56[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 28[0] -> 30[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 47[0] -> 55[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 7[0] -> 11[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 47[0] -> 55[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 55[0] -> 51[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 39[0] -> 35[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 11[0] -> 9[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 55[0] -> 51[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 39[0] -> 47[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 7[0] -> 11[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 11[0] -> 9[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 39[0] -> 35[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 39[0] -> 47[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 8[0] -> 16[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 55[0] -> 47[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 8[0] -> 16[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 11[0] -> 7[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 55[0] -> 47[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 28[0] -> 26[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 31[0] -> 47[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 7[0] -> 15[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 47[0] -> 39[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 28[0] -> 26[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 11[0] -> 7[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 7[0] -> 15[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Connected all rings
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 31[0] -> 47[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 47[0] -> 39[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 27[0] -> 29[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Connected all rings
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 27[0] -> 29[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 15[0] -> 7[0] [receive] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Connected all rings
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Connected all rings
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 15[0] -> 7[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 32[0] -> 48[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 28[0] -> 30[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 29[0] -> 27[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 15[0] -> 31[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 32[0] -> 48[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 28[0] -> 30[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 15[0] -> 31[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 29[0] -> 27[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 23[0] -> 27[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 31[0] -> 47[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 23[0] -> 27[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 31[0] -> 47[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 30[0] -> 28[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 27[0] -> 23[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 24[0] -> 28[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 30[0] -> 28[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 27[0] -> 23[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 15[0] -> 23[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 24[0] -> 28[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 15[0] -> 23[0] [receive] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 47[0] -> 31[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 47[0] -> 31[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 23[0] -> 15[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 16[0] -> 24[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 23[0] -> 15[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 15[0] -> 31[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 16[0] -> 24[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 15[0] -> 31[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [send] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 16[0] -> 32[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 31[0] -> 15[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 63[0] -> 31[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 16[0] -> 32[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 31[0] -> 15[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 63[0] -> 31[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 31[0] -> 63[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 31[0] -> 63[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 0[0] -> 32[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 0[0] -> 32[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 32[0] -> 0[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 47[0] -> 31[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 32[0] -> 0[0] [send] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 00/0 : 63[0] -> 62[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 47[0] -> 31[0] [receive] via NET/IB/0
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Channel 01/0 : 63[0] -> 62[0] [send] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 02/0 : 31[0] -> 15[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 48[0] -> 32[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 03/0 : 31[0] -> 15[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 48[0] -> 32[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [receive] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 00/0 : 32[0] -> 16[0] [send] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 01/0 : 32[0] -> 16[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 55[0] -> 47[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 00/0 : 31[0] -> 30[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 55[0] -> 47[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 23[0] -> 15[0] [receive] via NET/IB/0
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Channel 01/0 : 31[0] -> 30[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 02/0 : 47[0] -> 39[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 03/0 : 47[0] -> 39[0] [send] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 23[0] -> 15[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 02/0 : 15[0] -> 7[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 56[0] -> 48[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 03/0 : 15[0] -> 7[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 56[0] -> 48[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 02/0 : 33[0] -> 32[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 24[0] -> 16[0] [receive] via NET/IB/0
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Channel 03/0 : 33[0] -> 32[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 00/0 : 48[0] -> 40[0] [send] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 01/0 : 48[0] -> 40[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 00/0 : 47[0] -> 46[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 27[0] -> 23[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 24[0] -> 16[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 59[0] -> 55[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 00/0 : 16[0] -> 8[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 43[0] -> 39[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 00/0 : 15[0] -> 14[0] [send] via NET/IB/0
c622-022: c622-022:180805:181232 [0] NCCL INFO Channel 01/0 : 47[0] -> 46[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 27[0] -> 23[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 59[0] -> 55[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 01/0 : 16[0] -> 8[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 11[0] -> 7[0] [receive] via NET/IB/0
c619-012: c619-012:246412:246840 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 02/0 : 23[0] -> 19[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 02/0 : 55[0] -> 51[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 43[0] -> 39[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 60[0] -> 56[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 11[0] -> 7[0] [receive] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 03/0 : 55[0] -> 51[0] [send] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 03/0 : 23[0] -> 19[0] [send] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 02/0 : 39[0] -> 35[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 60[0] -> 56[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 02/0 : 49[0] -> 48[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 03/0 : 39[0] -> 35[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 02/0 : 7[0] -> 3[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 00/0 : 56[0] -> 52[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 28[0] -> 24[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 44[0] -> 40[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 02/0 : 17[0] -> 16[0] [receive] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 03/0 : 7[0] -> 3[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 28[0] -> 24[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 01/0 : 56[0] -> 52[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 12[0] -> 8[0] [receive] via NET/IB/0
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Channel 03/0 : 49[0] -> 48[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 44[0] -> 40[0] [receive] via NET/IB/0
c619-021: c619-021:593675:594105 [0] NCCL INFO Channel 03/0 : 17[0] -> 16[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 00/0 : 40[0] -> 36[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 61[0] -> 59[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 00/0 : 23[0] -> 22[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 00/0 : 55[0] -> 54[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 00/0 : 24[0] -> 20[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 01/0 : 24[0] -> 20[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 29[0] -> 27[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 12[0] -> 8[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 00/0 : 8[0] -> 4[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 61[0] -> 59[0] [receive] via NET/IB/0
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Channel 01/0 : 23[0] -> 22[0] [send] via NET/IB/0
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Channel 01/0 : 55[0] -> 54[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[0] [send] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 01/0 : 40[0] -> 36[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 02/0 : 59[0] -> 57[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 29[0] -> 27[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 02/0 : 27[0] -> 25[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 53[0] -> 51[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 01/0 : 8[0] -> 4[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 21[0] -> 19[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 13[0] -> 11[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 00/0 : 39[0] -> 38[0] [send] via NET/IB/0
c613-132: c613-132:990252:990680 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 13[0] -> 11[0] [receive] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 03/0 : 59[0] -> 57[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 45[0] -> 43[0] [receive] via NET/IB/0
c621-142: c621-142:600426:600853 [0] NCCL INFO Channel 01/0 : 39[0] -> 38[0] [send] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 03/0 : 27[0] -> 25[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 5[0] -> 3[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 53[0] -> 51[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 45[0] -> 43[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 21[0] -> 19[0] [receive] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 02/0 : 57[0] -> 56[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 46[0] -> 44[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 62[0] -> 60[0] [receive] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 02/0 : 19[0] -> 17[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 37[0] -> 35[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 37[0] -> 35[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 02/0 : 43[0] -> 41[0] [send] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 5[0] -> 3[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 02/0 : 11[0] -> 9[0] [send] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 03/0 : 11[0] -> 9[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 02/0 : 25[0] -> 24[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 14[0] -> 12[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 02/0 : 3[0] -> 1[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 30[0] -> 28[0] [receive] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 02/0 : 51[0] -> 49[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 03/0 : 19[0] -> 17[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 03/0 : 51[0] -> 49[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 03/0 : 43[0] -> 41[0] [send] via NET/IB/0
c622-071: c622-071:494914:495339 [0] NCCL INFO Channel 03/0 : 57[0] -> 56[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 46[0] -> 44[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 00/0 : 44[0] -> 42[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 62[0] -> 60[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 54[0] -> 52[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 22[0] -> 20[0] [receive] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 00/0 : 60[0] -> 58[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 02/0 : 35[0] -> 33[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 14[0] -> 12[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 03/0 : 35[0] -> 33[0] [send] via NET/IB/0
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Channel 03/0 : 25[0] -> 24[0] [receive] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 02/0 : 9[0] -> 8[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 00/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 30[0] -> 28[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 01/0 : 44[0] -> 42[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 54[0] -> 52[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 03/0 : 3[0] -> 1[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 00/0 : 12[0] -> 10[0] [send] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 22[0] -> 20[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 6[0] -> 4[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 02/0 : 41[0] -> 40[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 00/0 : 20[0] -> 18[0] [send] via NET/IB/0
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Channel 03/0 : 9[0] -> 8[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 01/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 00/0 : 27[0] -> 26[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 00/0 : 28[0] -> 26[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 01/0 : 60[0] -> 58[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 00/0 : 59[0] -> 58[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 00/0 : 52[0] -> 50[0] [send] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 00/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Channel 01/0 : 27[0] -> 26[0] [send] via NET/IB/0
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Channel 01/0 : 59[0] -> 58[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 00/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 01/0 : 28[0] -> 26[0] [send] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 01/0 : 12[0] -> 10[0] [send] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 01/0 : 52[0] -> 50[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 01/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 00/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 00/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 00/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 38[0] -> 36[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 00/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c621-151: c621-151:336402:336827 [0] NCCL INFO Channel 03/0 : 41[0] -> 40[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 01/0 : 20[0] -> 18[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 00/0 : 19[0] -> 18[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 00/0 : 51[0] -> 50[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 6[0] -> 4[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 00/0 : 4[0] -> 2[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 00/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 01/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 01/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 01/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 62[0] -> 61[0] [receive] via NET/IB/0
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Channel 01/0 : 11[0] -> 10[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 01/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 00/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 01/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 01/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 00/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 02/0 : 45[0] -> 44[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 38[0] -> 36[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 02/0 : 61[0] -> 60[0] [send] via NET/IB/0
c622-042: c622-042:654000:654431 [0] NCCL INFO Channel 01/0 : 51[0] -> 50[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 01/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 01/0 : 4[0] -> 2[0] [send] via NET/IB/0
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 00/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 47[0] -> 46[0] [receive] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 00/0 : 35[0] -> 34[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 30[0] -> 29[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 01/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 43[0] -> 42[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 01/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 63[0] -> 62[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 22[0] -> 21[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 26[0] -> 25[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 00/0 : 36[0] -> 34[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[0] [send] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 58[0] -> 57[0] [receive] via NET/IB/0
c622-092: c622-092:418610:419038 [0] NCCL INFO Channel 03/0 : 61[0] -> 60[0] [send] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 00/0 : 43[0] -> 42[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 02/0 : 29[0] -> 28[0] [send] via NET/IB/0
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Channel 03/0 : 29[0] -> 28[0] [send] via NET/IB/0
c622-011: c622-011:710508:710937 [0] NCCL INFO Channel 03/0 : 45[0] -> 44[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 02/0 : 53[0] -> 52[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 63[0] -> 62[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 15[0] -> 14[0] [receive] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 14[0] -> 13[0] [receive] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 02/0 : 21[0] -> 20[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 23[0] -> 22[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 00/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 02/0 : 25[0] -> 24[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 47[0] -> 46[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 00/0 : 46[0] -> 45[0] [send] via NET/IB/0
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Channel 01/0 : 35[0] -> 34[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 01/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 02/0 : 21[0] -> 20[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 59[0] -> 58[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 43[0] -> 42[0] [receive] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 55[0] -> 54[0] [receive] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 01/0 : 36[0] -> 34[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 54[0] -> 53[0] [receive] via NET/IB/0
c621-052: c621-052:855162:855591 [0] NCCL INFO Channel 03/0 : 21[0] -> 20[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 02/0 : 61[0] -> 60[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 31[0] -> 30[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 02/0 : 13[0] -> 12[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 02/0 : 57[0] -> 56[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 38[0] -> 37[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 19[0] -> 18[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 23[0] -> 22[0] [receive] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 18[0] -> 17[0] [receive] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 00/0 : 62[0] -> 61[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 02/0 : 17[0] -> 16[0] [send] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 50[0] -> 49[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 59[0] -> 58[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 15[0] -> 14[0] [receive] via NET/IB/0
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Channel 03/0 : 25[0] -> 24[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 46[0] -> 45[0] [receive] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 02/0 : 29[0] -> 28[0] [receive] via NET/IB/0
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Channel 03/0 : 53[0] -> 52[0] [receive] via NET/IB/0
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Channel 03/0 : 57[0] -> 56[0] [send] via NET/IB/0
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Channel 03/0 : 61[0] -> 60[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 7[0] -> 6[0] [receive] via NET/IB/0
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Channel 03/0 : 13[0] -> 12[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 00/0 : 42[0] -> 41[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 02/0 : 13[0] -> 12[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 6[0] -> 5[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 19[0] -> 18[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 02/0 : 49[0] -> 48[0] [send] via NET/IB/0
c619-022: c619-022:804214:804642 [0] NCCL INFO Channel 03/0 : 17[0] -> 16[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 01/0 : 62[0] -> 61[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c619-041: c619-041:15310:15741 [0] NCCL INFO Channel 03/0 : 21[0] -> 20[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 51[0] -> 50[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 01/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Channel 01/0 : 43[0] -> 42[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 2[0] -> 1[0] [receive] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 01/0 : 46[0] -> 45[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 00/0 : 22[0] -> 21[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 02/0 : 45[0] -> 44[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 11[0] -> 10[0] [receive] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 02/0 : 37[0] -> 36[0] [send] via NET/IB/0
c619-002: c619-002:83791:84219 [0] NCCL INFO Channel 03/0 : 13[0] -> 12[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 31[0] -> 30[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 10[0] -> 9[0] [receive] via NET/IB/0
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Channel 03/0 : 49[0] -> 48[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 00/0 : 14[0] -> 13[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 34[0] -> 33[0] [receive] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 01/0 : 14[0] -> 13[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 27[0] -> 26[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 00/0 : 58[0] -> 57[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 01/0 : 58[0] -> 57[0] [send] via NET/IB/0
c621-091: c621-091:102446:102879 [0] NCCL INFO Channel 03/0 : 29[0] -> 28[0] [receive] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 02/0 : 9[0] -> 8[0] [send] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 02/0 : 53[0] -> 52[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 7[0] -> 6[0] [receive] via NET/IB/0
c622-052: c622-052:990541:990968 [0] NCCL INFO Channel 03/0 : 53[0] -> 52[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 00/0 : 30[0] -> 29[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 51[0] -> 50[0] [receive] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 02/0 : 33[0] -> 32[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 02/0 : 5[0] -> 4[0] [receive] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 02/0 : 1[0] -> 0[0] [send] via NET/IB/0
c621-112: c621-112:411646:412075 [0] NCCL INFO Channel 03/0 : 33[0] -> 32[0] [send] via NET/IB/0
c613-121: c613-121:904347:904780 [0] NCCL INFO Channel 03/0 : 5[0] -> 4[0] [receive] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 3[0] -> 2[0] [receive] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 00/0 : 50[0] -> 49[0] [send] via NET/IB/0
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Channel 03/0 : 45[0] -> 44[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 11[0] -> 10[0] [receive] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 39[0] -> 38[0] [receive] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 01/0 : 30[0] -> 29[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 02/0 : 5[0] -> 4[0] [send] via NET/IB/0
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Channel 03/0 : 9[0] -> 8[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 02/0 : 14[0] -> 13[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 00/0 : 18[0] -> 17[0] [send] via NET/IB/0
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Channel 03/0 : 5[0] -> 4[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 00/0 : 10[0] -> 9[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 00/0 : 6[0] -> 5[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 3[0] -> 2[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 01/0 : 6[0] -> 5[0] [send] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 02/0 : 58[0] -> 57[0] [send] via NET/IB/0
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Channel 03/0 : 1[0] -> 0[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 01/0 : 50[0] -> 49[0] [send] via NET/IB/0
c621-132: c621-132:518779:519209 [0] NCCL INFO Channel 03/0 : 37[0] -> 36[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 02/0 : 50[0] -> 49[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 01/0 : 22[0] -> 21[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 35[0] -> 34[0] [receive] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 02/0 : 22[0] -> 21[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 02/0 : 37[0] -> 36[0] [receive] via NET/IB/0
c622-081: c622-081:41435:41864 [0] NCCL INFO Channel 03/0 : 58[0] -> 57[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 27[0] -> 26[0] [receive] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 01/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 00/0 : 26[0] -> 25[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 02/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 42[0] -> 41[0] [receive] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 02/0 : 6[0] -> 5[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 00/0 : 2[0] -> 1[0] [send] via NET/IB/0
c622-041: c622-041:198407:198836 [0] NCCL INFO Channel 03/0 : 50[0] -> 49[0] [send] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 02/0 : 30[0] -> 29[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 35[0] -> 34[0] [receive] via NET/IB/0
c621-101: c621-101:813656:814085 [0] NCCL INFO Channel 03/0 : 30[0] -> 29[0] [send] via NET/IB/0
c619-011: c619-011:219279:219711 [0] NCCL INFO Channel 03/0 : 14[0] -> 13[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 39[0] -> 38[0] [receive] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 01/0 : 42[0] -> 41[0] [send] via NET/IB/0
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Channel 03/0 : 22[0] -> 21[0] [send] via NET/IB/0
c619-031: c619-031:361171:361598 [0] NCCL INFO Channel 03/0 : 18[0] -> 17[0] [send] via NET/IB/0
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Channel 03/0 : 37[0] -> 36[0] [receive] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 01/0 : 10[0] -> 9[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 02/0 : 46[0] -> 45[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 02/0 : 10[0] -> 9[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 01/0 : 2[0] -> 1[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 02/0 : 41[0] -> 40[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 02/0 : 42[0] -> 41[0] [send] via NET/IB/0
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Channel 03/0 : 42[0] -> 41[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 02/0 : 62[0] -> 61[0] [send] via NET/IB/0
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Channel 03/0 : 41[0] -> 40[0] [send] via NET/IB/0
c613-131: c613-131:931664:932089 [0] NCCL INFO Channel 03/0 : 6[0] -> 5[0] [send] via NET/IB/0
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Channel 03/0 : 46[0] -> 45[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 00/0 : 34[0] -> 33[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 55[0] -> 54[0] [receive] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 01/0 : 26[0] -> 25[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 00/0 : 54[0] -> 53[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 01/0 : 54[0] -> 53[0] [send] via NET/IB/0
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Channel 03/0 : 62[0] -> 61[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 00/0 : 38[0] -> 37[0] [send] via NET/IB/0
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Channel 03/0 : 10[0] -> 9[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 02/0 : 26[0] -> 25[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 02/0 : 54[0] -> 53[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 01/0 : 38[0] -> 37[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 02/0 : 38[0] -> 37[0] [send] via NET/IB/0
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Channel 03/0 : 26[0] -> 25[0] [send] via NET/IB/0
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Channel 03/0 : 54[0] -> 53[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 02/0 : 2[0] -> 1[0] [send] via NET/IB/0
c613-111: c613-111:640696:641123 [0] NCCL INFO Channel 03/0 : 2[0] -> 1[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 01/0 : 34[0] -> 33[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 02/0 : 34[0] -> 33[0] [send] via NET/IB/0
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Channel 03/0 : 34[0] -> 33[0] [send] via NET/IB/0
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Channel 03/0 : 38[0] -> 37[0] [send] via NET/IB/0
c613-101: c613-101:387979:388442 [0] NCCL INFO Connected all trees
c613-101: c613-101:387979:388442 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-101: c613-101:387979:388442 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-021: c619-021:593675:594105 [0] NCCL INFO Connected all trees
c619-021: c619-021:593675:594105 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-111: c621-111:3868077:3868507 [0] NCCL INFO Connected all trees
c619-021: c619-021:593675:594105 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-071: c621-071:2842568:2842996 [0] NCCL INFO Connected all trees
c621-111: c621-111:3868077:3868507 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-111: c621-111:3868077:3868507 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-141: c613-141:2104776:2105199 [0] NCCL INFO Connected all trees
c621-071: c621-071:2842568:2842996 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-071: c621-071:2842568:2842996 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-102: c613-102:1660081:1660510 [0] NCCL INFO Connected all trees
c613-102: c613-102:1660081:1660510 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-031: c622-031:3136042:3136469 [0] NCCL INFO Connected all trees
c622-031: c622-031:3136042:3136469 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-031: c622-031:3136042:3136469 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-111: c613-111:640696:641123 [0] NCCL INFO Connected all trees
c613-111: c613-111:640696:641123 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-102: c613-102:1660081:1660510 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-111: c613-111:640696:641123 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-121: c613-121:904347:904780 [0] NCCL INFO Connected all trees
c613-141: c613-141:2104776:2105199 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-121: c613-121:904347:904780 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-121: c613-121:904347:904780 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-142: c613-142:3123258:3123686 [0] NCCL INFO Connected all trees
c619-022: c619-022:804214:804642 [0] NCCL INFO Connected all trees
c619-041: c619-041:15310:15741 [0] NCCL INFO Connected all trees
c613-141: c613-141:2104776:2105199 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-041: c619-041:15310:15741 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-041: c619-041:15310:15741 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-032: c619-032:3546706:3547134 [0] NCCL INFO Connected all trees
c613-151: c613-151:3397419:3397847 [0] NCCL INFO Connected all trees
c613-142: c613-142:3123258:3123686 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-151: c621-151:336402:336827 [0] NCCL INFO Connected all trees
c619-032: c619-032:3546706:3547134 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-142: c613-142:3123258:3123686 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-032: c619-032:3546706:3547134 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-071: c622-071:494914:495339 [0] NCCL INFO Connected all trees
c619-031: c619-031:361171:361598 [0] NCCL INFO Connected all trees
c613-112: c613-112:1585580:1586011 [0] NCCL INFO Connected all trees
c621-052: c621-052:855162:855591 [0] NCCL INFO Connected all trees
c619-022: c619-022:804214:804642 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-151: c613-151:3397419:3397847 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-031: c619-031:361171:361598 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-022: c619-022:804214:804642 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-031: c619-031:361171:361598 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-151: c613-151:3397419:3397847 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-001: c619-001:2454729:2455159 [0] NCCL INFO Connected all trees
c613-152: c613-152:3770490:3770922 [0] NCCL INFO Connected all trees
c621-151: c621-151:336402:336827 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-001: c619-001:2454729:2455159 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-001: c619-001:2454729:2455159 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-112: c613-112:1585580:1586011 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-071: c622-071:494914:495339 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-052: c621-052:855162:855591 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-112: c613-112:1585580:1586011 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-122: c613-122:1267677:1268109 [0] NCCL INFO Connected all trees
c621-052: c621-052:855162:855591 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-151: c621-151:336402:336827 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-072: c621-072:3048145:3048578 [0] NCCL INFO Connected all trees
c613-122: c613-122:1267677:1268109 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-122: c613-122:1267677:1268109 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-072: c621-072:3048145:3048578 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-071: c622-071:494914:495339 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-072: c621-072:3048145:3048578 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-152: c613-152:3770490:3770922 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-152: c613-152:3770490:3770922 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-002: c619-002:83791:84219 [0] NCCL INFO Connected all trees
c619-002: c619-002:83791:84219 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-002: c619-002:83791:84219 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-061: c621-061:3120949:3121378 [0] NCCL INFO Connected all trees
c621-061: c621-061:3120949:3121378 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-061: c621-061:3120949:3121378 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-011: c622-011:710508:710937 [0] NCCL INFO Connected all trees
c621-082: c621-082:1003419:1003850 [0] NCCL INFO Connected all trees
c613-131: c613-131:931664:932089 [0] NCCL INFO Connected all trees
c621-062: c621-062:1471760:1472185 [0] NCCL INFO Connected all trees
c621-082: c621-082:1003419:1003850 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c619-011: c619-011:219279:219711 [0] NCCL INFO Connected all trees
c613-131: c613-131:931664:932089 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-082: c621-082:1003419:1003850 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-152: c621-152:1793414:1793841 [0] NCCL INFO Connected all trees
c621-131: c621-131:2225085:2225519 [0] NCCL INFO Connected all trees
c613-131: c613-131:931664:932089 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-081: c621-081:2075589:2076014 [0] NCCL INFO Connected all trees
c622-072: c622-072:1688285:1688717 [0] NCCL INFO Connected all trees
c621-112: c621-112:411646:412075 [0] NCCL INFO Connected all trees
c621-091: c621-091:102446:102879 [0] NCCL INFO Connected all trees
c621-091: c621-091:102446:102879 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-011: c622-011:710508:710937 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-072: c622-072:1688285:1688717 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-112: c621-112:411646:412075 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-091: c621-091:102446:102879 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-011: c622-011:710508:710937 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-072: c622-072:1688285:1688717 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-132: c613-132:990252:990680 [0] NCCL INFO Connected all trees
c621-121: c621-121:1502414:1502842 [0] NCCL INFO Connected all trees
c621-112: c621-112:411646:412075 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-011: c619-011:219279:219711 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-032: c622-032:1587523:1587950 [0] NCCL INFO Connected all trees
c619-011: c619-011:219279:219711 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-051: c622-051:3538690:3539116 [0] NCCL INFO Connected all trees
c622-032: c622-032:1587523:1587950 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-081: c621-081:2075589:2076014 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-081: c622-081:41435:41864 [0] NCCL INFO Connected all trees
c621-152: c621-152:1793414:1793841 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c613-132: c613-132:990252:990680 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-122: c621-122:1561826:1562255 [0] NCCL INFO Connected all trees
c621-152: c621-152:1793414:1793841 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-092: c621-092:1627105:1627533 [0] NCCL INFO Connected all trees
c613-132: c613-132:990252:990680 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-062: c621-062:1471760:1472185 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-062: c621-062:1471760:1472185 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-021: c622-021:1141571:1142001 [0] NCCL INFO Connected all trees
c622-032: c622-032:1587523:1587950 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-081: c621-081:2075589:2076014 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-012: c622-012:2903325:2903752 [0] NCCL INFO Connected all trees
c621-121: c621-121:1502414:1502842 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-021: c622-021:1141571:1142001 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-082: c622-082:2786472:2786898 [0] NCCL INFO Connected all trees
c622-091: c622-091:2780457:2780884 [0] NCCL INFO Connected all trees
c622-041: c622-041:198407:198836 [0] NCCL INFO Connected all trees
c621-122: c621-122:1561826:1562255 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-091: c622-091:2780457:2780884 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-092: c621-092:1627105:1627533 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-122: c621-122:1561826:1562255 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-052: c622-052:990541:990968 [0] NCCL INFO Connected all trees
c622-042: c622-042:654000:654431 [0] NCCL INFO Connected all trees
c619-012: c619-012:246412:246840 [0] NCCL INFO Connected all trees
c621-092: c621-092:1627105:1627533 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-021: c622-021:1141571:1142001 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-042: c622-042:654000:654431 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-001: c622-001:3806195:3806627 [0] NCCL INFO Connected all trees
c622-012: c622-012:2903325:2903752 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-121: c621-121:1502414:1502842 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-101: c621-101:813656:814085 [0] NCCL INFO Connected all trees
c622-082: c622-082:2786472:2786898 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-042: c622-042:654000:654431 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-061: c622-061:1156840:1157269 [0] NCCL INFO Connected all trees
c622-001: c622-001:3806195:3806627 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-001: c622-001:3806195:3806627 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-002: c622-002:1377821:1378250 [0] NCCL INFO Connected all trees
c622-052: c622-052:990541:990968 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-052: c622-052:990541:990968 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-012: c619-012:246412:246840 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-081: c622-081:41435:41864 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-081: c622-081:41435:41864 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-002: c622-002:1377821:1378250 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-131: c621-131:2225085:2225519 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-012: c622-012:2903325:2903752 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-131: c621-131:2225085:2225519 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-092: c622-092:418610:419038 [0] NCCL INFO Connected all trees
c621-102: c621-102:2281097:2281523 [0] NCCL INFO Connected all trees
c622-091: c622-091:2780457:2780884 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-022: c622-022:180805:181232 [0] NCCL INFO Connected all trees
c621-141: c621-141:3859461:3859891 [0] NCCL INFO Connected all trees
c622-062: c622-062:2304142:2304573 [0] NCCL INFO Connected all trees
c621-132: c621-132:518779:519209 [0] NCCL INFO Connected all trees
c622-051: c622-051:3538690:3539116 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-022: c622-022:180805:181232 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-051: c622-051:3538690:3539116 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-092: c622-092:418610:419038 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-061: c622-061:1156840:1157269 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-061: c622-061:1156840:1157269 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-141: c621-141:3859461:3859891 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-101: c622-101:1627370:1627802 [0] NCCL INFO Connected all trees
c622-101: c622-101:1627370:1627802 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-101: c622-101:1627370:1627802 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-141: c621-141:3859461:3859891 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-101: c621-101:813656:814085 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-101: c621-101:813656:814085 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-142: c621-142:600426:600853 [0] NCCL INFO Connected all trees
c622-082: c622-082:2786472:2786898 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-062: c622-062:2304142:2304573 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-142: c621-142:600426:600853 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-102: c622-102:2511218:2511653 [0] NCCL INFO Connected all trees
c621-142: c621-142:600426:600853 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-062: c622-062:2304142:2304573 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-092: c622-092:418610:419038 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-022: c622-022:180805:181232 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-002: c622-002:1377821:1378250 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c619-012: c619-012:246412:246840 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-102: c621-102:2281097:2281523 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-102: c621-102:2281097:2281523 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-102: c622-102:2511218:2511653 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-041: c622-041:198407:198836 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c621-132: c621-132:518779:519209 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
c622-041: c622-041:198407:198836 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c621-132: c621-132:518779:519209 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c622-102: c622-102:2511218:2511653 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
c613-101: c613-101:387979:388442 [0] NCCL INFO ncclCommSplit comm 0x400a0c0c2800 rank 0 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf87e37d0 color -1443970512 key 0 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-021: c619-021:593675:594105 [0] NCCL INFO ncclCommSplit comm 0x4008440c27e0 rank 16 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacef91690 color -1443970512 key 16 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-111: c621-111:3868077:3868507 [0] NCCL INFO ncclCommSplit comm 0x4008480c2830 rank 32 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadde53570 color -1443970512 key 32 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-141: c613-141:2104776:2105199 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2740 rank 8 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab1eb11de0 color -1443970512 key 8 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-031: c622-031:3136042:3136469 [0] NCCL INFO ncclCommSplit comm 0x4008700c2850 rank 48 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab19de4480 color -1443970512 key 48 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-071: c621-071:2842568:2842996 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2850 rank 24 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5803340 color -1443970512 key 24 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-121: c613-121:904347:904780 [0] NCCL INFO ncclCommSplit comm 0x4008480c2760 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab03501d90 color -1443970512 key 4 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-142: c613-142:3123258:3123686 [0] NCCL INFO ncclCommSplit comm 0x4008600c2760 rank 9 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae9e74700 color -1443970512 key 9 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-102: c613-102:1660081:1660510 [0] NCCL INFO ncclCommSplit comm 0x4009a80c2760 rank 1 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad5e93020 color -1443970512 key 1 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-111: c613-111:640696:641123 [0] NCCL INFO ncclCommSplit comm 0x4008580c2820 rank 2 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaff0d4900 color -1443970512 key 2 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-032: c619-032:3546706:3547134 [0] NCCL INFO ncclCommSplit comm 0x4009800c26c0 rank 19 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab06d93bf0 color -1443970512 key 19 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-022: c619-022:804214:804642 [0] NCCL INFO ncclCommSplit comm 0x4009d00c27d0 rank 17 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5e93920 color -1443970512 key 17 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-112: c613-112:1585580:1586011 [0] NCCL INFO ncclCommSplit comm 0x4009ac0c2820 rank 3 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa5643c0 color -1443970512 key 3 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-041: c619-041:15310:15741 [0] NCCL INFO ncclCommSplit comm 0x4008500c27a0 rank 20 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaada3c4450 color -1443970512 key 20 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-052: c621-052:855162:855591 [0] NCCL INFO ncclCommSplit comm 0x4009c80c2820 rank 21 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf3913390 color -1443970512 key 21 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-152: c613-152:3770490:3770922 [0] NCCL INFO ncclCommSplit comm 0x4008880c2860 rank 11 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae1154720 color -1443970512 key 11 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-031: c619-031:361171:361598 [0] NCCL INFO ncclCommSplit comm 0x4008540c26e0 rank 18 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad6982fb0 color -1443970512 key 18 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-072: c622-072:1688285:1688717 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2630 rank 57 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf1bc32f0 color -1443970512 key 57 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-151: c613-151:3397419:3397847 [0] NCCL INFO ncclCommSplit comm 0x40087c0c27e0 rank 10 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadaeb2c20 color -1443970512 key 10 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-122: c613-122:1267677:1268109 [0] NCCL INFO ncclCommSplit comm 0x4008480c27c0 rank 5 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacc441df0 color -1443970512 key 5 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-062: c621-062:1471760:1472185 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2750 rank 23 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab20314540 color -1443970512 key 23 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-072: c621-072:3048145:3048578 [0] NCCL INFO ncclCommSplit comm 0x4008700c27e0 rank 25 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac7562e90 color -1443970512 key 25 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-131: c621-131:2225085:2225519 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2890 rank 36 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad9263e70 color -1443970512 key 36 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-001: c619-001:2454729:2455159 [0] NCCL INFO ncclCommSplit comm 0x4008580c2810 rank 12 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab21114760 color -1443970512 key 12 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-081: c621-081:2075589:2076014 [0] NCCL INFO ncclCommSplit comm 0x40085c0c2710 rank 26 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae8834680 color -1443970512 key 26 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-002: c619-002:83791:84219 [0] NCCL INFO ncclCommSplit comm 0x4008680c27d0 rank 13 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef322ea0 color -1443970512 key 13 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-151: c621-151:336402:336827 [0] NCCL INFO ncclCommSplit comm 0x4008440c26e0 rank 40 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab31fb4310 color -1443970512 key 40 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-132: c613-132:990252:990680 [0] NCCL INFO ncclCommSplit comm 0x4008480c2710 rank 7 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadc6b2810 color -1443970512 key 7 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-082: c621-082:1003419:1003850 [0] NCCL INFO ncclCommSplit comm 0x40099c0c2670 rank 27 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaef4b43b0 color -1443970512 key 27 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-071: c622-071:494914:495339 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2840 rank 56 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadd881dc0 color -1443970512 key 56 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-061: c621-061:3120949:3121378 [0] NCCL INFO ncclCommSplit comm 0x40084c0c2860 rank 22 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab28873410 color -1443970512 key 22 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-092: c621-092:1627105:1627533 [0] NCCL INFO ncclCommSplit comm 0x40087c0c28c0 rank 29 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafaf44150 color -1443970512 key 29 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-011: c622-011:710508:710937 [0] NCCL INFO ncclCommSplit comm 0x4008400c27a0 rank 44 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab12fd3650 color -1443970512 key 44 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-011: c619-011:219279:219711 [0] NCCL INFO ncclCommSplit comm 0x4008480c2810 rank 14 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0f1f2940 color -1443970512 key 14 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-022: c622-022:180805:181232 [0] NCCL INFO ncclCommSplit comm 0x4008480c2840 rank 47 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf9b932b0 color -1443970512 key 47 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-152: c621-152:1793414:1793841 [0] NCCL INFO ncclCommSplit comm 0x4008500c2780 rank 41 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaefa62bc0 color -1443970512 key 41 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-032: c622-032:1587523:1587950 [0] NCCL INFO ncclCommSplit comm 0x4009fc0c2810 rank 49 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaec6b4890 color -1443970512 key 49 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-121: c621-121:1502414:1502842 [0] NCCL INFO ncclCommSplit comm 0x4008680c27d0 rank 34 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaac7ad29a0 color -1443970512 key 34 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-091: c621-091:102446:102879 [0] NCCL INFO ncclCommSplit comm 0x4009b00c2820 rank 28 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae7334ad0 color -1443970512 key 28 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-081: c622-081:41435:41864 [0] NCCL INFO ncclCommSplit comm 0x4008780c2890 rank 58 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae73d3c50 color -1443970512 key 58 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-062: c622-062:2304142:2304573 [0] NCCL INFO ncclCommSplit comm 0x4009d00c2870 rank 55 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaefc52d60 color -1443970512 key 55 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c619-012: c619-012:246412:246840 [0] NCCL INFO ncclCommSplit comm 0x40087c0c2810 rank 15 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae5903380 color -1443970512 key 15 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-112: c621-112:411646:412075 [0] NCCL INFO ncclCommSplit comm 0x4008580c27a0 rank 33 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa972940 color -1443970512 key 33 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-002: c622-002:1377821:1378250 [0] NCCL INFO ncclCommSplit comm 0x4009d00c2820 rank 43 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf5e63ae0 color -1443970512 key 43 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-042: c622-042:654000:654431 [0] NCCL INFO ncclCommSplit comm 0x4009d40c2830 rank 51 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaadc7a5b50 color -1443970512 key 51 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-131: c613-131:931664:932089 [0] NCCL INFO ncclCommSplit comm 0x4008400c2810 rank 6 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab00253330 color -1443970512 key 6 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-091: c622-091:2780457:2780884 [0] NCCL INFO ncclCommSplit comm 0x4008680c2740 rank 60 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaad2a14660 color -1443970512 key 60 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-122: c621-122:1561826:1562255 [0] NCCL INFO ncclCommSplit comm 0x4008540c2840 rank 35 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf42a34a0 color -1443970512 key 35 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-082: c622-082:2786472:2786898 [0] NCCL INFO ncclCommSplit comm 0x4008480c2860 rank 59 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab0d0846f0 color -1443970512 key 59 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-051: c622-051:3538690:3539116 [0] NCCL INFO ncclCommSplit comm 0x4008500c2710 rank 52 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab056632b0 color -1443970512 key 52 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-021: c622-021:1141571:1142001 [0] NCCL INFO ncclCommSplit comm 0x4008580c2dc0 rank 46 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaec1b3660 color -1443970512 key 46 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-101: c621-101:813656:814085 [0] NCCL INFO ncclCommSplit comm 0x40085c0c28b0 rank 30 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae5fa3640 color -1443970512 key 30 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-001: c622-001:3806195:3806627 [0] NCCL INFO ncclCommSplit comm 0x400a200c2730 rank 42 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafa503ef0 color -1443970512 key 42 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-012: c622-012:2903325:2903752 [0] NCCL INFO ncclCommSplit comm 0x4008500c2710 rank 45 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaacd7e5250 color -1443970512 key 45 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-041: c622-041:198407:198836 [0] NCCL INFO ncclCommSplit comm 0x4008600c2770 rank 50 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaae2173010 color -1443970512 key 50 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-052: c622-052:990541:990968 [0] NCCL INFO ncclCommSplit comm 0x4008580c2790 rank 53 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab2f773010 color -1443970512 key 53 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-061: c622-061:1156840:1157269 [0] NCCL INFO ncclCommSplit comm 0x4009f00c2730 rank 54 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaea363b30 color -1443970512 key 54 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-102: c621-102:2281097:2281523 [0] NCCL INFO ncclCommSplit comm 0x40086c0c2800 rank 31 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab01513280 color -1443970512 key 31 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-142: c621-142:600426:600853 [0] NCCL INFO ncclCommSplit comm 0x4008640c2760 rank 39 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaafcef3f00 color -1443970512 key 39 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-141: c621-141:3859461:3859891 [0] NCCL INFO ncclCommSplit comm 0x4008780c2730 rank 38 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab05fc3cd0 color -1443970512 key 38 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c621-132: c621-132:518779:519209 [0] NCCL INFO ncclCommSplit comm 0x4008440c2870 rank 37 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab08e331b0 color -1443970512 key 37 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-101: c622-101:1627370:1627802 [0] NCCL INFO ncclCommSplit comm 0x4008480c2730 rank 62 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf8c135d0 color -1443970512 key 62 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-092: c622-092:418610:419038 [0] NCCL INFO ncclCommSplit comm 0x4009dc0c2700 rank 61 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaab065a30c0 color -1443970512 key 61 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c622-102: c622-102:2511218:2511653 [0] NCCL INFO ncclCommSplit comm 0x4008780c2780 rank 63 nranks 64 cudaDev 0 nvmlDev 0 busId 901000 parent 0xaaaaf35a4020 color -1443970512 key 63 commId 0x5dd374a04e0f27b9 - Init COMPLETE
c613-101: Model Parameters: 7.505 B, Latency: 13.56s, TFLOPs: 0.14, Samples/sec: 0.07, Time/seq 13.56s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 1, Rank: 43, loss = 0.69140625
c613-101: Epoch: 0, Step: 1, Rank: 0, loss = 0.69140625
c622-052: Epoch: 0, Step: 1, Rank: 53, loss = 0.69140625
c622-041: Epoch: 0, Step: 1, Rank: 50, loss = 0.69140625
c613-132: Epoch: 0, Step: 1, Rank: 7, loss = 0.69140625
c622-051: Epoch: 0, Step: 1, Rank: 52, loss = 0.69140625
c622-032: Epoch: 0, Step: 1, Rank: 49, loss = 0.69140625
c619-002: Epoch: 0, Step: 1, Rank: 13, loss = 0.69140625
c621-061: Epoch: 0, Step: 1, Rank: 22, loss = 0.69140625
c622-022: Epoch: 0, Step: 1, Rank: 47, loss = 0.69140625
c619-021: Epoch: 0, Step: 1, Rank: 16, loss = 0.69140625
c621-052: Epoch: 0, Step: 1, Rank: 21, loss = 0.69140625
c613-121: Epoch: 0, Step: 1, Rank: 4, loss = 0.69140625
c622-031: Epoch: 0, Step: 1, Rank: 48, loss = 0.69140625
c621-111: Epoch: 0, Step: 1, Rank: 32, loss = 0.69140625
c619-001: Epoch: 0, Step: 1, Rank: 12, loss = 0.69140625
c622-012: Epoch: 0, Step: 1, Rank: 45, loss = 0.69140625
c621-151: Epoch: 0, Step: 1, Rank: 40, loss = 0.69140625
c622-081: Epoch: 0, Step: 1, Rank: 58, loss = 0.69140625
c613-131: Epoch: 0, Step: 1, Rank: 6, loss = 0.69140625
c619-041: Epoch: 0, Step: 1, Rank: 20, loss = 0.69140625
c622-001: Epoch: 0, Step: 1, Rank: 42, loss = 0.69140625
c619-032: Epoch: 0, Step: 1, Rank: 19, loss = 0.69140625
c619-022: Epoch: 0, Step: 1, Rank: 17, loss = 0.69140625
c621-152: Epoch: 0, Step: 1, Rank: 41, loss = 0.69140625
c622-061: Epoch: 0, Step: 1, Rank: 54, loss = 0.69140625
c613-102: Epoch: 0, Step: 1, Rank: 1, loss = 0.69140625
c621-142: Epoch: 0, Step: 1, Rank: 39, loss = 0.69140625
c622-092: Epoch: 0, Step: 1, Rank: 61, loss = 0.69140625
c622-042: Epoch: 0, Step: 1, Rank: 51, loss = 0.69140625
c622-071: Epoch: 0, Step: 1, Rank: 56, loss = 0.69140625
c621-131: Epoch: 0, Step: 1, Rank: 36, loss = 0.69140625
c613-122: Epoch: 0, Step: 1, Rank: 5, loss = 0.69140625
c619-031: Epoch: 0, Step: 1, Rank: 18, loss = 0.69140625
c622-011: Epoch: 0, Step: 1, Rank: 44, loss = 0.69140625
c613-142: Epoch: 0, Step: 1, Rank: 9, loss = 0.69140625
c613-112: Epoch: 0, Step: 1, Rank: 3, loss = 0.69140625
c613-111: Epoch: 0, Step: 1, Rank: 2, loss = 0.69140625
c621-062: Epoch: 0, Step: 1, Rank: 23, loss = 0.69140625
c622-021: Epoch: 0, Step: 1, Rank: 46, loss = 0.69140625
c613-141: Epoch: 0, Step: 1, Rank: 8, loss = 0.69140625
c613-152: Epoch: 0, Step: 1, Rank: 11, loss = 0.69140625
c619-012: Epoch: 0, Step: 1, Rank: 15, loss = 0.69140625
c621-071: Epoch: 0, Step: 1, Rank: 24, loss = 0.69140625
c621-141: Epoch: 0, Step: 1, Rank: 38, loss = 0.69140625
c621-132: Epoch: 0, Step: 1, Rank: 37, loss = 0.69140625
c613-151: Epoch: 0, Step: 1, Rank: 10, loss = 0.69140625
c621-081: Epoch: 0, Step: 1, Rank: 26, loss = 0.69140625
c619-011: Epoch: 0, Step: 1, Rank: 14, loss = 0.69140625
c621-091: Epoch: 0, Step: 1, Rank: 28, loss = 0.69140625
c621-122: Epoch: 0, Step: 1, Rank: 35, loss = 0.69140625
c622-101: Epoch: 0, Step: 1, Rank: 62, loss = 0.69140625
c621-101: Epoch: 0, Step: 1, Rank: 30, loss = 0.69140625
c621-102: Epoch: 0, Step: 1, Rank: 31, loss = 0.69140625
c621-072: Epoch: 0, Step: 1, Rank: 25, loss = 0.69140625
c621-112: Epoch: 0, Step: 1, Rank: 33, loss = 0.69140625
c622-091: Epoch: 0, Step: 1, Rank: 60, loss = 0.69140625
c621-121: Epoch: 0, Step: 1, Rank: 34, loss = 0.69140625
c622-062: Epoch: 0, Step: 1, Rank: 55, loss = 0.69140625
c622-102: Epoch: 0, Step: 1, Rank: 63, loss = 0.69140625
c621-082: Epoch: 0, Step: 1, Rank: 27, loss = 0.69140625
c622-082: Epoch: 0, Step: 1, Rank: 59, loss = 0.69140625
c622-072: Epoch: 0, Step: 1, Rank: 57, loss = 0.69140625
c621-092: Epoch: 0, Step: 1, Rank: 29, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.736328125 | max allocated: 11956.72021484375 | reserved: 21610.0 | max reserved: 21610.0
c613-101: Model Parameters: 7.505 B, Latency: 17.73s, TFLOPs: 0.11, Samples/sec: 0.06, Time/seq 17.73s, Batch Size: 1, Sequence Length: 2048
c619-022: Epoch: 0, Step: 2, Rank: 17, loss = 0.69140625
c622-002: Epoch: 0, Step: 2, Rank: 43, loss = 0.69140625
c622-081: Epoch: 0, Step: 2, Rank: 58, loss = 0.69140625
c621-081: Epoch: 0, Step: 2, Rank: 26, loss = 0.69140625
c621-082: Epoch: 0, Step: 2, Rank: 27, loss = 0.69140625
c621-091: Epoch: 0, Step: 2, Rank: 28, loss = 0.69140625
c622-071: Epoch: 0, Step: 2, Rank: 56, loss = 0.69140625
c622-061: Epoch: 0, Step: 2, Rank: 54, loss = 0.69140625
c613-101: Epoch: 0, Step: 2, Rank: 0, loss = 0.69140625
c621-061: Epoch: 0, Step: 2, Rank: 22, loss = 0.69140625
c622-052: Epoch: 0, Step: 2, Rank: 53, loss = 0.69140625
c619-021: Epoch: 0, Step: 2, Rank: 16, loss = 0.69140625
c621-111: Epoch: 0, Step: 2, Rank: 32, loss = 0.69140625
c622-001: Epoch: 0, Step: 2, Rank: 42, loss = 0.69140625
c613-132: Epoch: 0, Step: 2, Rank: 7, loss = 0.69140625
c622-062: Epoch: 0, Step: 2, Rank: 55, loss = 0.69140625
c622-012: Epoch: 0, Step: 2, Rank: 45, loss = 0.69140625
c621-131: Epoch: 0, Step: 2, Rank: 36, loss = 0.69140625
c613-131: Epoch: 0, Step: 2, Rank: 6, loss = 0.69140625
c621-152: Epoch: 0, Step: 2, Rank: 41, loss = 0.69140625
c621-052: Epoch: 0, Step: 2, Rank: 21, loss = 0.69140625
c613-121: Epoch: 0, Step: 2, Rank: 4, loss = 0.69140625
c621-072: Epoch: 0, Step: 2, Rank: 25, loss = 0.69140625
c613-111: Epoch: 0, Step: 2, Rank: 2, loss = 0.69140625
c621-062: Epoch: 0, Step: 2, Rank: 23, loss = 0.69140625
c622-092: Epoch: 0, Step: 2, Rank: 61, loss = 0.69140625
c619-031: Epoch: 0, Step: 2, Rank: 18, loss = 0.69140625
c622-032: Epoch: 0, Step: 2, Rank: 49, loss = 0.69140625
c622-072: Epoch: 0, Step: 2, Rank: 57, loss = 0.69140625
c621-101: Epoch: 0, Step: 2, Rank: 30, loss = 0.69140625
c619-002: Epoch: 0, Step: 2, Rank: 13, loss = 0.69140625
c621-132: Epoch: 0, Step: 2, Rank: 37, loss = 0.69140625
c613-102: Epoch: 0, Step: 2, Rank: 1, loss = 0.69140625
c613-112: Epoch: 0, Step: 2, Rank: 3, loss = 0.69140625
c619-001: Epoch: 0, Step: 2, Rank: 12, loss = 0.69140625
c619-041: Epoch: 0, Step: 2, Rank: 20, loss = 0.69140625
c621-141: Epoch: 0, Step: 2, Rank: 38, loss = 0.69140625
c613-141: Epoch: 0, Step: 2, Rank: 8, loss = 0.69140625
c622-041: Epoch: 0, Step: 2, Rank: 50, loss = 0.69140625
c621-071: Epoch: 0, Step: 2, Rank: 24, loss = 0.69140625
c621-122: Epoch: 0, Step: 2, Rank: 35, loss = 0.69140625
c622-051: Epoch: 0, Step: 2, Rank: 52, loss = 0.69140625
c621-151: Epoch: 0, Step: 2, Rank: 40, loss = 0.69140625
c622-031: Epoch: 0, Step: 2, Rank: 48, loss = 0.69140625
c621-121: Epoch: 0, Step: 2, Rank: 34, loss = 0.69140625
c621-142: Epoch: 0, Step: 2, Rank: 39, loss = 0.69140625
c622-082: Epoch: 0, Step: 2, Rank: 59, loss = 0.69140625
c622-042: Epoch: 0, Step: 2, Rank: 51, loss = 0.69140625
c621-092: Epoch: 0, Step: 2, Rank: 29, loss = 0.69140625
c622-021: Epoch: 0, Step: 2, Rank: 46, loss = 0.69140625
c621-112: Epoch: 0, Step: 2, Rank: 33, loss = 0.69140625
c619-012: Epoch: 0, Step: 2, Rank: 15, loss = 0.69140625
c622-011: Epoch: 0, Step: 2, Rank: 44, loss = 0.69140625
c622-022: Epoch: 0, Step: 2, Rank: 47, loss = 0.69140625
c613-122: Epoch: 0, Step: 2, Rank: 5, loss = 0.69140625
c619-032: Epoch: 0, Step: 2, Rank: 19, loss = 0.69140625
c622-102: Epoch: 0, Step: 2, Rank: 63, loss = 0.69140625
c619-011: Epoch: 0, Step: 2, Rank: 14, loss = 0.69140625
c621-102: Epoch: 0, Step: 2, Rank: 31, loss = 0.69140625
c622-091: Epoch: 0, Step: 2, Rank: 60, loss = 0.69140625
c622-101: Epoch: 0, Step: 2, Rank: 62, loss = 0.69140625
c613-151: Epoch: 0, Step: 2, Rank: 10, loss = 0.69140625
c613-152: Epoch: 0, Step: 2, Rank: 11, loss = 0.69140625
c613-142: Epoch: 0, Step: 2, Rank: 9, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.2265625 | reserved: 21610.0 | max reserved: 21610.0
c613-101: Model Parameters: 7.505 B, Latency: 2.36s, TFLOPs: 0.80, Samples/sec: 0.42, Time/seq 2.36s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 3, Rank: 58, loss = 0.69140625
c613-101: Epoch: 0, Step: 3, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 3, Rank: 43, loss = 0.69140625
c619-001: Epoch: 0, Step: 3, Rank: 12, loss = 0.69140625
c613-102: Epoch: 0, Step: 3, Rank: 1, loss = 0.69140625
c622-101: Epoch: 0, Step: 3, Rank: 62, loss = 0.69140625
c619-021: Epoch: 0, Step: 3, Rank: 16, loss = 0.69140625
c613-111: Epoch: 0, Step: 3, Rank: 2, loss = 0.69140625
c613-121: Epoch: 0, Step: 3, Rank: 4, loss = 0.69140625
c621-151: Epoch: 0, Step: 3, Rank: 40, loss = 0.69140625
c622-102: Epoch: 0, Step: 3, Rank: 63, loss = 0.69140625
c619-002: Epoch: 0, Step: 3, Rank: 13, loss = 0.69140625
c613-122: Epoch: 0, Step: 3, Rank: 5, loss = 0.69140625
c613-131: Epoch: 0, Step: 3, Rank: 6, loss = 0.69140625
c622-052: Epoch: 0, Step: 3, Rank: 53, loss = 0.69140625
c621-111: Epoch: 0, Step: 3, Rank: 32, loss = 0.69140625
c613-132: Epoch: 0, Step: 3, Rank: 7, loss = 0.69140625
c621-091: Epoch: 0, Step: 3, Rank: 28, loss = 0.69140625
c622-071: Epoch: 0, Step: 3, Rank: 56, loss = 0.69140625
c621-081: Epoch: 0, Step: 3, Rank: 26, loss = 0.69140625
c613-112: Epoch: 0, Step: 3, Rank: 3, loss = 0.69140625
c622-001: Epoch: 0, Step: 3, Rank: 42, loss = 0.69140625
c613-142: Epoch: 0, Step: 3, Rank: 9, loss = 0.69140625
c621-112: Epoch: 0, Step: 3, Rank: 33, loss = 0.69140625
c622-022: Epoch: 0, Step: 3, Rank: 47, loss = 0.69140625
c621-052: Epoch: 0, Step: 3, Rank: 21, loss = 0.69140625
c613-141: Epoch: 0, Step: 3, Rank: 8, loss = 0.69140625
c622-012: Epoch: 0, Step: 3, Rank: 45, loss = 0.69140625
c622-061: Epoch: 0, Step: 3, Rank: 54, loss = 0.69140625
c622-092: Epoch: 0, Step: 3, Rank: 61, loss = 0.69140625
c622-041: Epoch: 0, Step: 3, Rank: 50, loss = 0.69140625
c622-042: Epoch: 0, Step: 3, Rank: 51, loss = 0.69140625
c621-082: Epoch: 0, Step: 3, Rank: 27, loss = 0.69140625
c622-021: Epoch: 0, Step: 3, Rank: 46, loss = 0.69140625
c619-041: Epoch: 0, Step: 3, Rank: 20, loss = 0.69140625
c621-131: Epoch: 0, Step: 3, Rank: 36, loss = 0.69140625
c621-101: Epoch: 0, Step: 3, Rank: 30, loss = 0.69140625
c613-151: Epoch: 0, Step: 3, Rank: 10, loss = 0.69140625
c621-061: Epoch: 0, Step: 3, Rank: 22, loss = 0.69140625
c621-141: Epoch: 0, Step: 3, Rank: 38, loss = 0.69140625
c621-132: Epoch: 0, Step: 3, Rank: 37, loss = 0.69140625
c621-152: Epoch: 0, Step: 3, Rank: 41, loss = 0.69140625
c622-032: Epoch: 0, Step: 3, Rank: 49, loss = 0.69140625
c622-062: Epoch: 0, Step: 3, Rank: 55, loss = 0.69140625
c622-072: Epoch: 0, Step: 3, Rank: 57, loss = 0.69140625
c622-082: Epoch: 0, Step: 3, Rank: 59, loss = 0.69140625
c622-031: Epoch: 0, Step: 3, Rank: 48, loss = 0.69140625
c621-072: Epoch: 0, Step: 3, Rank: 25, loss = 0.69140625
c621-071: Epoch: 0, Step: 3, Rank: 24, loss = 0.69140625
c619-011: Epoch: 0, Step: 3, Rank: 14, loss = 0.69140625
c621-122: Epoch: 0, Step: 3, Rank: 35, loss = 0.69140625
c621-142: Epoch: 0, Step: 3, Rank: 39, loss = 0.69140625
c613-152: Epoch: 0, Step: 3, Rank: 11, loss = 0.69140625
c619-031: Epoch: 0, Step: 3, Rank: 18, loss = 0.69140625
c621-062: Epoch: 0, Step: 3, Rank: 23, loss = 0.69140625
c622-011: Epoch: 0, Step: 3, Rank: 44, loss = 0.69140625
c622-051: Epoch: 0, Step: 3, Rank: 52, loss = 0.69140625
c619-022: Epoch: 0, Step: 3, Rank: 17, loss = 0.69140625
c621-092: Epoch: 0, Step: 3, Rank: 29, loss = 0.69140625
c619-032: Epoch: 0, Step: 3, Rank: 19, loss = 0.69140625
c621-121: Epoch: 0, Step: 3, Rank: 34, loss = 0.69140625
c621-102: Epoch: 0, Step: 3, Rank: 31, loss = 0.69140625
c622-091: Epoch: 0, Step: 3, Rank: 60, loss = 0.69140625
c619-012: Epoch: 0, Step: 3, Rank: 15, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22705078125 | reserved: 21610.0 | max reserved: 21610.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c621-112: Epoch: 0, Step: 4, Rank: 33, loss = 0.69140625
c622-032: Epoch: 0, Step: 4, Rank: 49, loss = 0.69140625
c621-111: Epoch: 0, Step: 4, Rank: 32, loss = 0.69140625
c619-021: Epoch: 0, Step: 4, Rank: 16, loss = 0.69140625
c621-062: Epoch: 0, Step: 4, Rank: 23, loss = 0.69140625
c619-031: Epoch: 0, Step: 4, Rank: 18, loss = 0.69140625
c622-031: Epoch: 0, Step: 4, Rank: 48, loss = 0.69140625
c622-052: Epoch: 0, Step: 4, Rank: 53, loss = 0.69140625
c621-061: Epoch: 0, Step: 4, Rank: 22, loss = 0.69140625
c621-091: Epoch: 0, Step: 4, Rank: 28, loss = 0.69140625
c621-052: Epoch: 0, Step: 4, Rank: 21, loss = 0.69140625
c619-002: Epoch: 0, Step: 4, Rank: 13, loss = 0.69140625
c621-151: Epoch: 0, Step: 4, Rank: 40, loss = 0.69140625
c622-002: Epoch: 0, Step: 4, Rank: 43, loss = 0.69140625
c622-042: Epoch: 0, Step: 4, Rank: 51, loss = 0.69140625
c621-081: Epoch: 0, Step: 4, Rank: 26, loss = 0.69140625
c621-132: Epoch: 0, Step: 4, Rank: 37, loss = 0.69140625
c621-141: Epoch: 0, Step: 4, Rank: 38, loss = 0.69140625
c622-022: Epoch: 0, Step: 4, Rank: 47, loss = 0.69140625
c622-012: Epoch: 0, Step: 4, Rank: 45, loss = 0.69140625
c621-071: Epoch: 0, Step: 4, Rank: 24, loss = 0.69140625
c622-041: Epoch: 0, Step: 4, Rank: 50, loss = 0.69140625
c619-032: Epoch: 0, Step: 4, Rank: 19, loss = 0.69140625
c622-001: Epoch: 0, Step: 4, Rank: 42, loss = 0.69140625
c621-072: Epoch: 0, Step: 4, Rank: 25, loss = 0.69140625
c622-021: Epoch: 0, Step: 4, Rank: 46, loss = 0.69140625
c621-082: Epoch: 0, Step: 4, Rank: 27, loss = 0.69140625
c619-001: Epoch: 0, Step: 4, Rank: 12, loss = 0.69140625
c621-131: Epoch: 0, Step: 4, Rank: 36, loss = 0.69140625
c621-142: Epoch: 0, Step: 4, Rank: 39, loss = 0.69140625
c622-051: Epoch: 0, Step: 4, Rank: 52, loss = 0.69140625
c621-152: Epoch: 0, Step: 4, Rank: 41, loss = 0.69140625
c613-151: Epoch: 0, Step: 4, Rank: 10, loss = 0.69140625
c622-101: Epoch: 0, Step: 4, Rank: 62, loss = 0.69140625
c621-101: Epoch: 0, Step: 4, Rank: 30, loss = 0.69140625
c619-022: Epoch: 0, Step: 4, Rank: 17, loss = 0.69140625
c613-101: Epoch: 0, Step: 4, Rank: 0, loss = 0.69140625
c621-121: Epoch: 0, Step: 4, Rank: 34, loss = 0.69140625
c622-011: Epoch: 0, Step: 4, Rank: 44, loss = 0.69140625
c619-041: Epoch: 0, Step: 4, Rank: 20, loss = 0.69140625
c613-111: Epoch: 0, Step: 4, Rank: 2, loss = 0.69140625
c613-132: Epoch: 0, Step: 4, Rank: 7, loss = 0.69140625
c613-102: Epoch: 0, Step: 4, Rank: 1, loss = 0.69140625
c613-131: Epoch: 0, Step: 4, Rank: 6, loss = 0.69140625
c613-122: Epoch: 0, Step: 4, Rank: 5, loss = 0.69140625
c613-112: Epoch: 0, Step: 4, Rank: 3, loss = 0.69140625
c619-011: Epoch: 0, Step: 4, Rank: 14, loss = 0.69140625
c613-142: Epoch: 0, Step: 4, Rank: 9, loss = 0.69140625
c621-092: Epoch: 0, Step: 4, Rank: 29, loss = 0.69140625
c621-122: Epoch: 0, Step: 4, Rank: 35, loss = 0.69140625
c613-152: Epoch: 0, Step: 4, Rank: 11, loss = 0.69140625
c621-102: Epoch: 0, Step: 4, Rank: 31, loss = 0.69140625
c619-012: Epoch: 0, Step: 4, Rank: 15, loss = 0.69140625
c622-092: Epoch: 0, Step: 4, Rank: 61, loss = 0.69140625
c613-141: Epoch: 0, Step: 4, Rank: 8, loss = 0.69140625
c622-081: Epoch: 0, Step: 4, Rank: 58, loss = 0.69140625
c622-102: Epoch: 0, Step: 4, Rank: 63, loss = 0.69140625
c613-121: Epoch: 0, Step: 4, Rank: 4, loss = 0.69140625
c622-061: Epoch: 0, Step: 4, Rank: 54, loss = 0.69140625
c622-071: Epoch: 0, Step: 4, Rank: 56, loss = 0.69140625
c622-082: Epoch: 0, Step: 4, Rank: 59, loss = 0.69140625
c622-072: Epoch: 0, Step: 4, Rank: 57, loss = 0.69140625
c622-091: Epoch: 0, Step: 4, Rank: 60, loss = 0.69140625
c622-062: Epoch: 0, Step: 4, Rank: 55, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22705078125 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c619-001: Epoch: 0, Step: 5, Rank: 12, loss = 0.69140625
c619-002: Epoch: 0, Step: 5, Rank: 13, loss = 0.69140625
c613-101: Epoch: 0, Step: 5, Rank: 0, loss = 0.69140625
c622-102: Epoch: 0, Step: 5, Rank: 63, loss = 0.69140625
c613-151: Epoch: 0, Step: 5, Rank: 10, loss = 0.69140625
c619-021: Epoch: 0, Step: 5, Rank: 16, loss = 0.69140625
c622-081: Epoch: 0, Step: 5, Rank: 58, loss = 0.69140625
c613-142: Epoch: 0, Step: 5, Rank: 9, loss = 0.69140625
c622-052: Epoch: 0, Step: 5, Rank: 53, loss = 0.69140625
c622-051: Epoch: 0, Step: 5, Rank: 52, loss = 0.69140625
c613-132: Epoch: 0, Step: 5, Rank: 7, loss = 0.69140625
c622-002: Epoch: 0, Step: 5, Rank: 43, loss = 0.69140625
c613-111: Epoch: 0, Step: 5, Rank: 2, loss = 0.69140625
c621-052: Epoch: 0, Step: 5, Rank: 21, loss = 0.69140625
c619-041: Epoch: 0, Step: 5, Rank: 20, loss = 0.69140625
c621-111: Epoch: 0, Step: 5, Rank: 32, loss = 0.69140625
c613-112: Epoch: 0, Step: 5, Rank: 3, loss = 0.69140625
c619-031: Epoch: 0, Step: 5, Rank: 18, loss = 0.69140625
c622-092: Epoch: 0, Step: 5, Rank: 61, loss = 0.69140625
c622-001: Epoch: 0, Step: 5, Rank: 42, loss = 0.69140625
c621-081: Epoch: 0, Step: 5, Rank: 26, loss = 0.69140625
c619-032: Epoch: 0, Step: 5, Rank: 19, loss = 0.69140625
c622-012: Epoch: 0, Step: 5, Rank: 45, loss = 0.69140625
c613-141: Epoch: 0, Step: 5, Rank: 8, loss = 0.69140625
c621-112: Epoch: 0, Step: 5, Rank: 33, loss = 0.69140625
c621-061: Epoch: 0, Step: 5, Rank: 22, loss = 0.69140625
c622-101: Epoch: 0, Step: 5, Rank: 62, loss = 0.69140625
c613-131: Epoch: 0, Step: 5, Rank: 6, loss = 0.69140625
c621-071: Epoch: 0, Step: 5, Rank: 24, loss = 0.69140625
c619-022: Epoch: 0, Step: 5, Rank: 17, loss = 0.69140625
c621-091: Epoch: 0, Step: 5, Rank: 28, loss = 0.69140625
c621-131: Epoch: 0, Step: 5, Rank: 36, loss = 0.69140625
c613-102: Epoch: 0, Step: 5, Rank: 1, loss = 0.69140625
c621-062: Epoch: 0, Step: 5, Rank: 23, loss = 0.69140625
c621-121: Epoch: 0, Step: 5, Rank: 34, loss = 0.69140625
c613-121: Epoch: 0, Step: 5, Rank: 4, loss = 0.69140625
c622-062: Epoch: 0, Step: 5, Rank: 55, loss = 0.69140625
c622-041: Epoch: 0, Step: 5, Rank: 50, loss = 0.69140625
c622-061: Epoch: 0, Step: 5, Rank: 54, loss = 0.69140625
c621-082: Epoch: 0, Step: 5, Rank: 27, loss = 0.69140625
c613-152: Epoch: 0, Step: 5, Rank: 11, loss = 0.69140625
c621-101: Epoch: 0, Step: 5, Rank: 30, loss = 0.69140625
c621-141: Epoch: 0, Step: 5, Rank: 38, loss = 0.69140625
c622-071: Epoch: 0, Step: 5, Rank: 56, loss = 0.69140625
c622-022: Epoch: 0, Step: 5, Rank: 47, loss = 0.69140625
c621-132: Epoch: 0, Step: 5, Rank: 37, loss = 0.69140625
c621-151: Epoch: 0, Step: 5, Rank: 40, loss = 0.69140625
c621-102: Epoch: 0, Step: 5, Rank: 31, loss = 0.69140625
c622-031: Epoch: 0, Step: 5, Rank: 48, loss = 0.69140625
c622-082: Epoch: 0, Step: 5, Rank: 59, loss = 0.69140625
c622-042: Epoch: 0, Step: 5, Rank: 51, loss = 0.69140625
c621-122: Epoch: 0, Step: 5, Rank: 35, loss = 0.69140625
c622-072: Epoch: 0, Step: 5, Rank: 57, loss = 0.69140625
c613-122: Epoch: 0, Step: 5, Rank: 5, loss = 0.69140625
c619-012: Epoch: 0, Step: 5, Rank: 15, loss = 0.69140625
c619-011: Epoch: 0, Step: 5, Rank: 14, loss = 0.69140625
c622-091: Epoch: 0, Step: 5, Rank: 60, loss = 0.69140625
c622-021: Epoch: 0, Step: 5, Rank: 46, loss = 0.69140625
c621-142: Epoch: 0, Step: 5, Rank: 39, loss = 0.69140625
c621-072: Epoch: 0, Step: 5, Rank: 25, loss = 0.69140625
c622-032: Epoch: 0, Step: 5, Rank: 49, loss = 0.69140625
c621-152: Epoch: 0, Step: 5, Rank: 41, loss = 0.69140625
c622-011: Epoch: 0, Step: 5, Rank: 44, loss = 0.69140625
c621-092: Epoch: 0, Step: 5, Rank: 29, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2451171875 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 6, Rank: 53, loss = 0.69140625
c621-111: Epoch: 0, Step: 6, Rank: 32, loss = 0.69140625
c622-002: Epoch: 0, Step: 6, Rank: 43, loss = 0.69140625
c622-062: Epoch: 0, Step: 6, Rank: 55, loss = 0.69140625
c622-032: Epoch: 0, Step: 6, Rank: 49, loss = 0.69140625
c622-061: Epoch: 0, Step: 6, Rank: 54, loss = 0.69140625
c621-091: Epoch: 0, Step: 6, Rank: 28, loss = 0.69140625
c613-101: Epoch: 0, Step: 6, Rank: 0, loss = 0.69140625
c622-051: Epoch: 0, Step: 6, Rank: 52, loss = 0.69140625
c619-001: Epoch: 0, Step: 6, Rank: 12, loss = 0.69140625
c619-021: Epoch: 0, Step: 6, Rank: 16, loss = 0.69140625
c621-081: Epoch: 0, Step: 6, Rank: 26, loss = 0.69140625
c621-072: Epoch: 0, Step: 6, Rank: 25, loss = 0.69140625
c621-101: Epoch: 0, Step: 6, Rank: 30, loss = 0.69140625
c621-121: Epoch: 0, Step: 6, Rank: 34, loss = 0.69140625
c621-082: Epoch: 0, Step: 6, Rank: 27, loss = 0.69140625
c622-081: Epoch: 0, Step: 6, Rank: 58, loss = 0.69140625
c621-112: Epoch: 0, Step: 6, Rank: 33, loss = 0.69140625
c622-041: Epoch: 0, Step: 6, Rank: 50, loss = 0.69140625
c622-001: Epoch: 0, Step: 6, Rank: 42, loss = 0.69140625
c621-131: Epoch: 0, Step: 6, Rank: 36, loss = 0.69140625
c613-151: Epoch: 0, Step: 6, Rank: 10, loss = 0.69140625
c622-042: Epoch: 0, Step: 6, Rank: 51, loss = 0.69140625
c621-122: Epoch: 0, Step: 6, Rank: 35, loss = 0.69140625
c619-002: Epoch: 0, Step: 6, Rank: 13, loss = 0.69140625
c613-142: Epoch: 0, Step: 6, Rank: 9, loss = 0.69140625
c621-151: Epoch: 0, Step: 6, Rank: 40, loss = 0.69140625
c619-031: Epoch: 0, Step: 6, Rank: 18, loss = 0.69140625
c622-071: Epoch: 0, Step: 6, Rank: 56, loss = 0.69140625
c613-112: Epoch: 0, Step: 6, Rank: 3, loss = 0.69140625
c622-031: Epoch: 0, Step: 6, Rank: 48, loss = 0.69140625
c622-012: Epoch: 0, Step: 6, Rank: 45, loss = 0.69140625
c613-131: Epoch: 0, Step: 6, Rank: 6, loss = 0.69140625
c621-102: Epoch: 0, Step: 6, Rank: 31, loss = 0.69140625
c619-022: Epoch: 0, Step: 6, Rank: 17, loss = 0.69140625
c622-092: Epoch: 0, Step: 6, Rank: 61, loss = 0.69140625
c619-012: Epoch: 0, Step: 6, Rank: 15, loss = 0.69140625
c621-071: Epoch: 0, Step: 6, Rank: 24, loss = 0.69140625
c613-121: Epoch: 0, Step: 6, Rank: 4, loss = 0.69140625
c621-132: Epoch: 0, Step: 6, Rank: 37, loss = 0.69140625
c621-092: Epoch: 0, Step: 6, Rank: 29, loss = 0.69140625
c621-141: Epoch: 0, Step: 6, Rank: 38, loss = 0.69140625
c621-152: Epoch: 0, Step: 6, Rank: 41, loss = 0.69140625
c613-102: Epoch: 0, Step: 6, Rank: 1, loss = 0.69140625
c621-142: Epoch: 0, Step: 6, Rank: 39, loss = 0.69140625
c613-111: Epoch: 0, Step: 6, Rank: 2, loss = 0.69140625
c622-101: Epoch: 0, Step: 6, Rank: 62, loss = 0.69140625
c613-152: Epoch: 0, Step: 6, Rank: 11, loss = 0.69140625
c622-011: Epoch: 0, Step: 6, Rank: 44, loss = 0.69140625
c619-041: Epoch: 0, Step: 6, Rank: 20, loss = 0.69140625
c613-122: Epoch: 0, Step: 6, Rank: 5, loss = 0.69140625
c622-021: Epoch: 0, Step: 6, Rank: 46, loss = 0.69140625
c622-022: Epoch: 0, Step: 6, Rank: 47, loss = 0.69140625
c613-132: Epoch: 0, Step: 6, Rank: 7, loss = 0.69140625
c613-141: Epoch: 0, Step: 6, Rank: 8, loss = 0.69140625
c621-062: Epoch: 0, Step: 6, Rank: 23, loss = 0.69140625
c621-052: Epoch: 0, Step: 6, Rank: 21, loss = 0.69140625
c619-032: Epoch: 0, Step: 6, Rank: 19, loss = 0.69140625
c622-091: Epoch: 0, Step: 6, Rank: 60, loss = 0.69140625
c622-102: Epoch: 0, Step: 6, Rank: 63, loss = 0.69140625
c622-082: Epoch: 0, Step: 6, Rank: 59, loss = 0.69140625
c619-011: Epoch: 0, Step: 6, Rank: 14, loss = 0.69140625
c622-072: Epoch: 0, Step: 6, Rank: 57, loss = 0.69140625
c621-061: Epoch: 0, Step: 6, Rank: 22, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 7, Rank: 43, loss = 0.69140625
c621-151: Epoch: 0, Step: 7, Rank: 40, loss = 0.69140625
c622-092: Epoch: 0, Step: 7, Rank: 61, loss = 0.69140625
c619-021: Epoch: 0, Step: 7, Rank: 16, loss = 0.69140625
c621-132: Epoch: 0, Step: 7, Rank: 37, loss = 0.69140625
c613-101: Epoch: 0, Step: 7, Rank: 0, loss = 0.69140625
c622-012: Epoch: 0, Step: 7, Rank: 45, loss = 0.69140625
c622-081: Epoch: 0, Step: 7, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 7, Rank: 53, loss = 0.69140625
c621-121: Epoch: 0, Step: 7, Rank: 34, loss = 0.69140625
c621-131: Epoch: 0, Step: 7, Rank: 36, loss = 0.69140625
c621-142: Epoch: 0, Step: 7, Rank: 39, loss = 0.69140625
c613-151: Epoch: 0, Step: 7, Rank: 10, loss = 0.69140625
c621-152: Epoch: 0, Step: 7, Rank: 41, loss = 0.69140625
c621-141: Epoch: 0, Step: 7, Rank: 38, loss = 0.69140625
c619-001: Epoch: 0, Step: 7, Rank: 12, loss = 0.69140625
c621-111: Epoch: 0, Step: 7, Rank: 32, loss = 0.69140625
c619-031: Epoch: 0, Step: 7, Rank: 18, loss = 0.69140625
c613-111: Epoch: 0, Step: 7, Rank: 2, loss = 0.69140625
c613-131: Epoch: 0, Step: 7, Rank: 6, loss = 0.69140625
c622-061: Epoch: 0, Step: 7, Rank: 54, loss = 0.69140625
c622-031: Epoch: 0, Step: 7, Rank: 48, loss = 0.69140625
c622-032: Epoch: 0, Step: 7, Rank: 49, loss = 0.69140625
c622-051: Epoch: 0, Step: 7, Rank: 52, loss = 0.69140625
c619-002: Epoch: 0, Step: 7, Rank: 13, loss = 0.69140625
c622-001: Epoch: 0, Step: 7, Rank: 42, loss = 0.69140625
c613-112: Epoch: 0, Step: 7, Rank: 3, loss = 0.69140625
c613-121: Epoch: 0, Step: 7, Rank: 4, loss = 0.69140625
c622-041: Epoch: 0, Step: 7, Rank: 50, loss = 0.69140625
c621-122: Epoch: 0, Step: 7, Rank: 35, loss = 0.69140625
c622-022: Epoch: 0, Step: 7, Rank: 47, loss = 0.69140625
c622-102: Epoch: 0, Step: 7, Rank: 63, loss = 0.69140625
c621-112: Epoch: 0, Step: 7, Rank: 33, loss = 0.69140625
c619-041: Epoch: 0, Step: 7, Rank: 20, loss = 0.69140625
c622-091: Epoch: 0, Step: 7, Rank: 60, loss = 0.69140625
c622-011: Epoch: 0, Step: 7, Rank: 44, loss = 0.69140625
c622-021: Epoch: 0, Step: 7, Rank: 46, loss = 0.69140625
c613-141: Epoch: 0, Step: 7, Rank: 8, loss = 0.69140625
c621-072: Epoch: 0, Step: 7, Rank: 25, loss = 0.69140625
c621-082: Epoch: 0, Step: 7, Rank: 27, loss = 0.69140625
c613-102: Epoch: 0, Step: 7, Rank: 1, loss = 0.69140625
c619-032: Epoch: 0, Step: 7, Rank: 19, loss = 0.69140625
c613-132: Epoch: 0, Step: 7, Rank: 7, loss = 0.69140625
c613-142: Epoch: 0, Step: 7, Rank: 9, loss = 0.69140625
c621-091: Epoch: 0, Step: 7, Rank: 28, loss = 0.69140625
c621-062: Epoch: 0, Step: 7, Rank: 23, loss = 0.69140625
c621-101: Epoch: 0, Step: 7, Rank: 30, loss = 0.69140625
c622-071: Epoch: 0, Step: 7, Rank: 56, loss = 0.69140625
c621-061: Epoch: 0, Step: 7, Rank: 22, loss = 0.69140625
c622-082: Epoch: 0, Step: 7, Rank: 59, loss = 0.69140625
c622-042: Epoch: 0, Step: 7, Rank: 51, loss = 0.69140625
c619-022: Epoch: 0, Step: 7, Rank: 17, loss = 0.69140625
c621-052: Epoch: 0, Step: 7, Rank: 21, loss = 0.69140625
c619-012: Epoch: 0, Step: 7, Rank: 15, loss = 0.69140625
c621-102: Epoch: 0, Step: 7, Rank: 31, loss = 0.69140625
c613-152: Epoch: 0, Step: 7, Rank: 11, loss = 0.69140625
c619-011: Epoch: 0, Step: 7, Rank: 14, loss = 0.69140625
c622-062: Epoch: 0, Step: 7, Rank: 55, loss = 0.69140625
c621-081: Epoch: 0, Step: 7, Rank: 26, loss = 0.69140625
c622-072: Epoch: 0, Step: 7, Rank: 57, loss = 0.69140625
c613-122: Epoch: 0, Step: 7, Rank: 5, loss = 0.69140625
c621-071: Epoch: 0, Step: 7, Rank: 24, loss = 0.69140625
c622-101: Epoch: 0, Step: 7, Rank: 62, loss = 0.69140625
c621-092: Epoch: 0, Step: 7, Rank: 29, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 8, Rank: 0, loss = 0.69140625
c622-052: Epoch: 0, Step: 8, Rank: 53, loss = 0.69140625
c622-002: Epoch: 0, Step: 8, Rank: 43, loss = 0.69140625
c622-001: Epoch: 0, Step: 8, Rank: 42, loss = 0.69140625
c619-021: Epoch: 0, Step: 8, Rank: 16, loss = 0.69140625
c621-111: Epoch: 0, Step: 8, Rank: 32, loss = 0.69140625
c621-091: Epoch: 0, Step: 8, Rank: 28, loss = 0.69140625
c619-001: Epoch: 0, Step: 8, Rank: 12, loss = 0.69140625
c619-002: Epoch: 0, Step: 8, Rank: 13, loss = 0.69140625
c619-031: Epoch: 0, Step: 8, Rank: 18, loss = 0.69140625
c621-152: Epoch: 0, Step: 8, Rank: 41, loss = 0.69140625
c621-072: Epoch: 0, Step: 8, Rank: 25, loss = 0.69140625
c622-092: Epoch: 0, Step: 8, Rank: 61, loss = 0.69140625
c622-032: Epoch: 0, Step: 8, Rank: 49, loss = 0.69140625
c622-051: Epoch: 0, Step: 8, Rank: 52, loss = 0.69140625
c622-041: Epoch: 0, Step: 8, Rank: 50, loss = 0.69140625
c621-131: Epoch: 0, Step: 8, Rank: 36, loss = 0.69140625
c621-071: Epoch: 0, Step: 8, Rank: 24, loss = 0.69140625
c621-081: Epoch: 0, Step: 8, Rank: 26, loss = 0.69140625
c621-062: Epoch: 0, Step: 8, Rank: 23, loss = 0.69140625
c621-151: Epoch: 0, Step: 8, Rank: 40, loss = 0.69140625
c621-082: Epoch: 0, Step: 8, Rank: 27, loss = 0.69140625
c619-012: Epoch: 0, Step: 8, Rank: 15, loss = 0.69140625
c622-012: Epoch: 0, Step: 8, Rank: 45, loss = 0.69140625
c619-022: Epoch: 0, Step: 8, Rank: 17, loss = 0.69140625
c622-101: Epoch: 0, Step: 8, Rank: 62, loss = 0.69140625
c613-152: Epoch: 0, Step: 8, Rank: 11, loss = 0.69140625
c621-052: Epoch: 0, Step: 8, Rank: 21, loss = 0.69140625
c622-011: Epoch: 0, Step: 8, Rank: 44, loss = 0.69140625
c622-102: Epoch: 0, Step: 8, Rank: 63, loss = 0.69140625
c622-042: Epoch: 0, Step: 8, Rank: 51, loss = 0.69140625
c621-121: Epoch: 0, Step: 8, Rank: 34, loss = 0.69140625
c622-081: Epoch: 0, Step: 8, Rank: 58, loss = 0.69140625
c619-011: Epoch: 0, Step: 8, Rank: 14, loss = 0.69140625
c621-101: Epoch: 0, Step: 8, Rank: 30, loss = 0.69140625
c613-151: Epoch: 0, Step: 8, Rank: 10, loss = 0.69140625
c619-032: Epoch: 0, Step: 8, Rank: 19, loss = 0.69140625
c621-102: Epoch: 0, Step: 8, Rank: 31, loss = 0.69140625
c613-142: Epoch: 0, Step: 8, Rank: 9, loss = 0.69140625
c619-041: Epoch: 0, Step: 8, Rank: 20, loss = 0.69140625
c621-141: Epoch: 0, Step: 8, Rank: 38, loss = 0.69140625
c621-112: Epoch: 0, Step: 8, Rank: 33, loss = 0.69140625
c621-122: Epoch: 0, Step: 8, Rank: 35, loss = 0.69140625
c622-061: Epoch: 0, Step: 8, Rank: 54, loss = 0.69140625
c621-142: Epoch: 0, Step: 8, Rank: 39, loss = 0.69140625
c621-132: Epoch: 0, Step: 8, Rank: 37, loss = 0.69140625
c613-131: Epoch: 0, Step: 8, Rank: 6, loss = 0.69140625
c622-021: Epoch: 0, Step: 8, Rank: 46, loss = 0.69140625
c622-082: Epoch: 0, Step: 8, Rank: 59, loss = 0.69140625
c621-061: Epoch: 0, Step: 8, Rank: 22, loss = 0.69140625
c613-121: Epoch: 0, Step: 8, Rank: 4, loss = 0.69140625
c621-092: Epoch: 0, Step: 8, Rank: 29, loss = 0.69140625
c622-031: Epoch: 0, Step: 8, Rank: 48, loss = 0.69140625
c613-111: Epoch: 0, Step: 8, Rank: 2, loss = 0.69140625
c622-071: Epoch: 0, Step: 8, Rank: 56, loss = 0.69140625
c622-091: Epoch: 0, Step: 8, Rank: 60, loss = 0.69140625
c622-022: Epoch: 0, Step: 8, Rank: 47, loss = 0.69140625
c622-062: Epoch: 0, Step: 8, Rank: 55, loss = 0.69140625
c613-102: Epoch: 0, Step: 8, Rank: 1, loss = 0.69140625
c613-141: Epoch: 0, Step: 8, Rank: 8, loss = 0.69140625
c613-132: Epoch: 0, Step: 8, Rank: 7, loss = 0.69140625
c613-122: Epoch: 0, Step: 8, Rank: 5, loss = 0.69140625
c613-112: Epoch: 0, Step: 8, Rank: 3, loss = 0.69140625
c622-072: Epoch: 0, Step: 8, Rank: 57, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 9, Rank: 58, loss = 0.69140625
c619-021: Epoch: 0, Step: 9, Rank: 16, loss = 0.69140625
c619-001: Epoch: 0, Step: 9, Rank: 12, loss = 0.69140625
c613-101: Epoch: 0, Step: 9, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 9, Rank: 43, loss = 0.69140625
c622-082: Epoch: 0, Step: 9, Rank: 59, loss = 0.69140625
c621-072: Epoch: 0, Step: 9, Rank: 25, loss = 0.69140625
c621-121: Epoch: 0, Step: 9, Rank: 34, loss = 0.69140625
c622-102: Epoch: 0, Step: 9, Rank: 63, loss = 0.69140625
c622-101: Epoch: 0, Step: 9, Rank: 62, loss = 0.69140625
c621-082: Epoch: 0, Step: 9, Rank: 27, loss = 0.69140625
c622-092: Epoch: 0, Step: 9, Rank: 61, loss = 0.69140625
c613-151: Epoch: 0, Step: 9, Rank: 10, loss = 0.69140625
c621-151: Epoch: 0, Step: 9, Rank: 40, loss = 0.69140625
c619-002: Epoch: 0, Step: 9, Rank: 13, loss = 0.69140625
c621-112: Epoch: 0, Step: 9, Rank: 33, loss = 0.69140625
c621-111: Epoch: 0, Step: 9, Rank: 32, loss = 0.69140625
c613-132: Epoch: 0, Step: 9, Rank: 7, loss = 0.69140625
c613-141: Epoch: 0, Step: 9, Rank: 8, loss = 0.69140625
c622-052: Epoch: 0, Step: 9, Rank: 53, loss = 0.69140625
c621-142: Epoch: 0, Step: 9, Rank: 39, loss = 0.69140625
c621-071: Epoch: 0, Step: 9, Rank: 24, loss = 0.69140625
c622-031: Epoch: 0, Step: 9, Rank: 48, loss = 0.69140625
c621-101: Epoch: 0, Step: 9, Rank: 30, loss = 0.69140625
c621-131: Epoch: 0, Step: 9, Rank: 36, loss = 0.69140625
c613-142: Epoch: 0, Step: 9, Rank: 9, loss = 0.69140625
c622-012: Epoch: 0, Step: 9, Rank: 45, loss = 0.69140625
c619-041: Epoch: 0, Step: 9, Rank: 20, loss = 0.69140625
c621-052: Epoch: 0, Step: 9, Rank: 21, loss = 0.69140625
c613-152: Epoch: 0, Step: 9, Rank: 11, loss = 0.69140625
c613-111: Epoch: 0, Step: 9, Rank: 2, loss = 0.69140625
c613-122: Epoch: 0, Step: 9, Rank: 5, loss = 0.69140625
c621-081: Epoch: 0, Step: 9, Rank: 26, loss = 0.69140625
c622-032: Epoch: 0, Step: 9, Rank: 49, loss = 0.69140625
c622-071: Epoch: 0, Step: 9, Rank: 56, loss = 0.69140625
c622-022: Epoch: 0, Step: 9, Rank: 47, loss = 0.69140625
c613-131: Epoch: 0, Step: 9, Rank: 6, loss = 0.69140625
c622-062: Epoch: 0, Step: 9, Rank: 55, loss = 0.69140625
c621-061: Epoch: 0, Step: 9, Rank: 22, loss = 0.69140625
c613-102: Epoch: 0, Step: 9, Rank: 1, loss = 0.69140625
c622-001: Epoch: 0, Step: 9, Rank: 42, loss = 0.69140625
c622-061: Epoch: 0, Step: 9, Rank: 54, loss = 0.69140625
c622-011: Epoch: 0, Step: 9, Rank: 44, loss = 0.69140625
c613-112: Epoch: 0, Step: 9, Rank: 3, loss = 0.69140625
c622-041: Epoch: 0, Step: 9, Rank: 50, loss = 0.69140625
c622-021: Epoch: 0, Step: 9, Rank: 46, loss = 0.69140625
c619-012: Epoch: 0, Step: 9, Rank: 15, loss = 0.69140625
c619-022: Epoch: 0, Step: 9, Rank: 17, loss = 0.69140625
c622-051: Epoch: 0, Step: 9, Rank: 52, loss = 0.69140625
c619-031: Epoch: 0, Step: 9, Rank: 18, loss = 0.69140625
c619-011: Epoch: 0, Step: 9, Rank: 14, loss = 0.69140625
c621-152: Epoch: 0, Step: 9, Rank: 41, loss = 0.69140625
c622-042: Epoch: 0, Step: 9, Rank: 51, loss = 0.69140625
c621-062: Epoch: 0, Step: 9, Rank: 23, loss = 0.69140625
c621-092: Epoch: 0, Step: 9, Rank: 29, loss = 0.69140625
c613-121: Epoch: 0, Step: 9, Rank: 4, loss = 0.69140625
c621-091: Epoch: 0, Step: 9, Rank: 28, loss = 0.69140625
c622-091: Epoch: 0, Step: 9, Rank: 60, loss = 0.69140625
c621-102: Epoch: 0, Step: 9, Rank: 31, loss = 0.69140625
c621-122: Epoch: 0, Step: 9, Rank: 35, loss = 0.69140625
c622-072: Epoch: 0, Step: 9, Rank: 57, loss = 0.69140625
c621-132: Epoch: 0, Step: 9, Rank: 37, loss = 0.69140625
c619-032: Epoch: 0, Step: 9, Rank: 19, loss = 0.69140625
c621-141: Epoch: 0, Step: 9, Rank: 38, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 10, Rank: 53, loss = 0.69140625
c619-002: Epoch: 0, Step: 10, Rank: 13, loss = 0.69140625
c622-051: Epoch: 0, Step: 10, Rank: 52, loss = 0.69140625
c619-001: Epoch: 0, Step: 10, Rank: 12, loss = 0.69140625
c621-111: Epoch: 0, Step: 10, Rank: 32, loss = 0.69140625
c619-022: Epoch: 0, Step: 10, Rank: 17, loss = 0.69140625
c621-072: Epoch: 0, Step: 10, Rank: 25, loss = 0.69140625
c619-021: Epoch: 0, Step: 10, Rank: 16, loss = 0.69140625
c619-041: Epoch: 0, Step: 10, Rank: 20, loss = 0.69140625
c619-011: Epoch: 0, Step: 10, Rank: 14, loss = 0.69140625
c621-052: Epoch: 0, Step: 10, Rank: 21, loss = 0.69140625
c619-012: Epoch: 0, Step: 10, Rank: 15, loss = 0.69140625
c613-151: Epoch: 0, Step: 10, Rank: 10, loss = 0.69140625
c613-152: Epoch: 0, Step: 10, Rank: 11, loss = 0.69140625
c622-002: Epoch: 0, Step: 10, Rank: 43, loss = 0.69140625
c619-032: Epoch: 0, Step: 10, Rank: 19, loss = 0.69140625
c622-032: Epoch: 0, Step: 10, Rank: 49, loss = 0.69140625
c622-081: Epoch: 0, Step: 10, Rank: 58, loss = 0.69140625
c621-082: Epoch: 0, Step: 10, Rank: 27, loss = 0.69140625
c619-031: Epoch: 0, Step: 10, Rank: 18, loss = 0.69140625
c621-061: Epoch: 0, Step: 10, Rank: 22, loss = 0.69140625
c613-101: Epoch: 0, Step: 10, Rank: 0, loss = 0.69140625
c621-081: Epoch: 0, Step: 10, Rank: 26, loss = 0.69140625
c613-132: Epoch: 0, Step: 10, Rank: 7, loss = 0.69140625
c622-041: Epoch: 0, Step: 10, Rank: 50, loss = 0.69140625
c622-061: Epoch: 0, Step: 10, Rank: 54, loss = 0.69140625
c613-142: Epoch: 0, Step: 10, Rank: 9, loss = 0.69140625
c621-071: Epoch: 0, Step: 10, Rank: 24, loss = 0.69140625
c622-012: Epoch: 0, Step: 10, Rank: 45, loss = 0.69140625
c622-042: Epoch: 0, Step: 10, Rank: 51, loss = 0.69140625
c621-112: Epoch: 0, Step: 10, Rank: 33, loss = 0.69140625
c622-001: Epoch: 0, Step: 10, Rank: 42, loss = 0.69140625
c621-121: Epoch: 0, Step: 10, Rank: 34, loss = 0.69140625
c621-091: Epoch: 0, Step: 10, Rank: 28, loss = 0.69140625
c613-112: Epoch: 0, Step: 10, Rank: 3, loss = 0.69140625
c613-111: Epoch: 0, Step: 10, Rank: 2, loss = 0.69140625
c621-062: Epoch: 0, Step: 10, Rank: 23, loss = 0.69140625
c613-121: Epoch: 0, Step: 10, Rank: 4, loss = 0.69140625
c621-122: Epoch: 0, Step: 10, Rank: 35, loss = 0.69140625
c622-022: Epoch: 0, Step: 10, Rank: 47, loss = 0.69140625
c621-151: Epoch: 0, Step: 10, Rank: 40, loss = 0.69140625
c613-141: Epoch: 0, Step: 10, Rank: 8, loss = 0.69140625
c613-131: Epoch: 0, Step: 10, Rank: 6, loss = 0.69140625
c621-092: Epoch: 0, Step: 10, Rank: 29, loss = 0.69140625
c621-101: Epoch: 0, Step: 10, Rank: 30, loss = 0.69140625
c613-102: Epoch: 0, Step: 10, Rank: 1, loss = 0.69140625
c622-102: Epoch: 0, Step: 10, Rank: 63, loss = 0.69140625
c621-102: Epoch: 0, Step: 10, Rank: 31, loss = 0.69140625
c621-141: Epoch: 0, Step: 10, Rank: 38, loss = 0.69140625
c622-031: Epoch: 0, Step: 10, Rank: 48, loss = 0.69140625
c621-131: Epoch: 0, Step: 10, Rank: 36, loss = 0.69140625
c621-152: Epoch: 0, Step: 10, Rank: 41, loss = 0.69140625
c622-011: Epoch: 0, Step: 10, Rank: 44, loss = 0.69140625
c622-021: Epoch: 0, Step: 10, Rank: 46, loss = 0.69140625
c613-122: Epoch: 0, Step: 10, Rank: 5, loss = 0.69140625
c622-071: Epoch: 0, Step: 10, Rank: 56, loss = 0.69140625
c621-132: Epoch: 0, Step: 10, Rank: 37, loss = 0.69140625
c621-142: Epoch: 0, Step: 10, Rank: 39, loss = 0.69140625
c622-101: Epoch: 0, Step: 10, Rank: 62, loss = 0.69140625
c622-082: Epoch: 0, Step: 10, Rank: 59, loss = 0.69140625
c622-092: Epoch: 0, Step: 10, Rank: 61, loss = 0.69140625
c622-062: Epoch: 0, Step: 10, Rank: 55, loss = 0.69140625
c622-091: Epoch: 0, Step: 10, Rank: 60, loss = 0.69140625
c622-072: Epoch: 0, Step: 10, Rank: 57, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 11, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 11, Rank: 43, loss = 0.69140625
c619-001: Epoch: 0, Step: 11, Rank: 12, loss = 0.69140625
c621-112: Epoch: 0, Step: 11, Rank: 33, loss = 0.69140625
c621-072: Epoch: 0, Step: 11, Rank: 25, loss = 0.69140625
c619-021: Epoch: 0, Step: 11, Rank: 16, loss = 0.69140625
c621-081: Epoch: 0, Step: 11, Rank: 26, loss = 0.69140625
c619-002: Epoch: 0, Step: 11, Rank: 13, loss = 0.69140625
c621-121: Epoch: 0, Step: 11, Rank: 34, loss = 0.69140625
c622-101: Epoch: 0, Step: 11, Rank: 62, loss = 0.69140625
c621-082: Epoch: 0, Step: 11, Rank: 27, loss = 0.69140625
c613-152: Epoch: 0, Step: 11, Rank: 11, loss = 0.69140625
c621-111: Epoch: 0, Step: 11, Rank: 32, loss = 0.69140625
c621-052: Epoch: 0, Step: 11, Rank: 21, loss = 0.69140625
c621-091: Epoch: 0, Step: 11, Rank: 28, loss = 0.69140625
c619-041: Epoch: 0, Step: 11, Rank: 20, loss = 0.69140625
c621-151: Epoch: 0, Step: 11, Rank: 40, loss = 0.69140625
c613-132: Epoch: 0, Step: 11, Rank: 7, loss = 0.69140625
c621-062: Epoch: 0, Step: 11, Rank: 23, loss = 0.69140625
c621-101: Epoch: 0, Step: 11, Rank: 30, loss = 0.69140625
c619-031: Epoch: 0, Step: 11, Rank: 18, loss = 0.69140625
c621-131: Epoch: 0, Step: 11, Rank: 36, loss = 0.69140625
c621-142: Epoch: 0, Step: 11, Rank: 39, loss = 0.69140625
c613-131: Epoch: 0, Step: 11, Rank: 6, loss = 0.69140625
c622-011: Epoch: 0, Step: 11, Rank: 44, loss = 0.69140625
c621-061: Epoch: 0, Step: 11, Rank: 22, loss = 0.69140625
c613-151: Epoch: 0, Step: 11, Rank: 10, loss = 0.69140625
c622-001: Epoch: 0, Step: 11, Rank: 42, loss = 0.69140625
c622-052: Epoch: 0, Step: 11, Rank: 53, loss = 0.69140625
c613-112: Epoch: 0, Step: 11, Rank: 3, loss = 0.69140625
c622-102: Epoch: 0, Step: 11, Rank: 63, loss = 0.69140625
c613-142: Epoch: 0, Step: 11, Rank: 9, loss = 0.69140625
c621-152: Epoch: 0, Step: 11, Rank: 41, loss = 0.69140625
c621-092: Epoch: 0, Step: 11, Rank: 29, loss = 0.69140625
c613-111: Epoch: 0, Step: 11, Rank: 2, loss = 0.69140625
c622-012: Epoch: 0, Step: 11, Rank: 45, loss = 0.69140625
c621-071: Epoch: 0, Step: 11, Rank: 24, loss = 0.69140625
c619-012: Epoch: 0, Step: 11, Rank: 15, loss = 0.69140625
c619-032: Epoch: 0, Step: 11, Rank: 19, loss = 0.69140625
c622-031: Epoch: 0, Step: 11, Rank: 48, loss = 0.69140625
c619-022: Epoch: 0, Step: 11, Rank: 17, loss = 0.69140625
c622-061: Epoch: 0, Step: 11, Rank: 54, loss = 0.69140625
c613-141: Epoch: 0, Step: 11, Rank: 8, loss = 0.69140625
c622-092: Epoch: 0, Step: 11, Rank: 61, loss = 0.69140625
c621-122: Epoch: 0, Step: 11, Rank: 35, loss = 0.69140625
c621-141: Epoch: 0, Step: 11, Rank: 38, loss = 0.69140625
c613-102: Epoch: 0, Step: 11, Rank: 1, loss = 0.69140625
c613-121: Epoch: 0, Step: 11, Rank: 4, loss = 0.69140625
c622-051: Epoch: 0, Step: 11, Rank: 52, loss = 0.69140625
c622-042: Epoch: 0, Step: 11, Rank: 51, loss = 0.69140625
c613-122: Epoch: 0, Step: 11, Rank: 5, loss = 0.69140625
c621-132: Epoch: 0, Step: 11, Rank: 37, loss = 0.69140625
c619-011: Epoch: 0, Step: 11, Rank: 14, loss = 0.69140625
c622-021: Epoch: 0, Step: 11, Rank: 46, loss = 0.69140625
c621-102: Epoch: 0, Step: 11, Rank: 31, loss = 0.69140625
c622-022: Epoch: 0, Step: 11, Rank: 47, loss = 0.69140625
c622-032: Epoch: 0, Step: 11, Rank: 49, loss = 0.69140625
c622-041: Epoch: 0, Step: 11, Rank: 50, loss = 0.69140625
c622-081: Epoch: 0, Step: 11, Rank: 58, loss = 0.69140625
c622-091: Epoch: 0, Step: 11, Rank: 60, loss = 0.69140625
c622-082: Epoch: 0, Step: 11, Rank: 59, loss = 0.69140625
c622-071: Epoch: 0, Step: 11, Rank: 56, loss = 0.69140625
c622-072: Epoch: 0, Step: 11, Rank: 57, loss = 0.69140625
c622-062: Epoch: 0, Step: 11, Rank: 55, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 12, Rank: 0, loss = 0.69140625
c622-102: Epoch: 0, Step: 12, Rank: 63, loss = 0.69140625
c613-102: Epoch: 0, Step: 12, Rank: 1, loss = 0.69140625
c622-081: Epoch: 0, Step: 12, Rank: 58, loss = 0.69140625
c622-092: Epoch: 0, Step: 12, Rank: 61, loss = 0.69140625
c622-101: Epoch: 0, Step: 12, Rank: 62, loss = 0.69140625
c613-111: Epoch: 0, Step: 12, Rank: 2, loss = 0.69140625
c613-112: Epoch: 0, Step: 12, Rank: 3, loss = 0.69140625
c613-152: Epoch: 0, Step: 12, Rank: 11, loss = 0.69140625
c622-071: Epoch: 0, Step: 12, Rank: 56, loss = 0.69140625
c613-132: Epoch: 0, Step: 12, Rank: 7, loss = 0.69140625
c613-131: Epoch: 0, Step: 12, Rank: 6, loss = 0.69140625
c613-122: Epoch: 0, Step: 12, Rank: 5, loss = 0.69140625
c613-151: Epoch: 0, Step: 12, Rank: 10, loss = 0.69140625
c619-001: Epoch: 0, Step: 12, Rank: 12, loss = 0.69140625
c613-141: Epoch: 0, Step: 12, Rank: 8, loss = 0.69140625
c619-021: Epoch: 0, Step: 12, Rank: 16, loss = 0.69140625
c622-072: Epoch: 0, Step: 12, Rank: 57, loss = 0.69140625
c613-121: Epoch: 0, Step: 12, Rank: 4, loss = 0.69140625
c619-002: Epoch: 0, Step: 12, Rank: 13, loss = 0.69140625
c613-142: Epoch: 0, Step: 12, Rank: 9, loss = 0.69140625
c622-082: Epoch: 0, Step: 12, Rank: 59, loss = 0.69140625
c622-062: Epoch: 0, Step: 12, Rank: 55, loss = 0.69140625
c622-052: Epoch: 0, Step: 12, Rank: 53, loss = 0.69140625
c619-031: Epoch: 0, Step: 12, Rank: 18, loss = 0.69140625
c622-002: Epoch: 0, Step: 12, Rank: 43, loss = 0.69140625
c621-072: Epoch: 0, Step: 12, Rank: 25, loss = 0.69140625
c621-081: Epoch: 0, Step: 12, Rank: 26, loss = 0.69140625
c622-032: Epoch: 0, Step: 12, Rank: 49, loss = 0.69140625
c622-091: Epoch: 0, Step: 12, Rank: 60, loss = 0.69140625
c619-011: Epoch: 0, Step: 12, Rank: 14, loss = 0.69140625
c622-031: Epoch: 0, Step: 12, Rank: 48, loss = 0.69140625
c622-061: Epoch: 0, Step: 12, Rank: 54, loss = 0.69140625
c622-012: Epoch: 0, Step: 12, Rank: 45, loss = 0.69140625
c621-121: Epoch: 0, Step: 12, Rank: 34, loss = 0.69140625
c619-012: Epoch: 0, Step: 12, Rank: 15, loss = 0.69140625
c621-082: Epoch: 0, Step: 12, Rank: 27, loss = 0.69140625
c619-022: Epoch: 0, Step: 12, Rank: 17, loss = 0.69140625
c622-001: Epoch: 0, Step: 12, Rank: 42, loss = 0.69140625
c619-041: Epoch: 0, Step: 12, Rank: 20, loss = 0.69140625
c619-032: Epoch: 0, Step: 12, Rank: 19, loss = 0.69140625
c621-142: Epoch: 0, Step: 12, Rank: 39, loss = 0.69140625
c621-131: Epoch: 0, Step: 12, Rank: 36, loss = 0.69140625
c621-112: Epoch: 0, Step: 12, Rank: 33, loss = 0.69140625
c621-052: Epoch: 0, Step: 12, Rank: 21, loss = 0.69140625
c621-092: Epoch: 0, Step: 12, Rank: 29, loss = 0.69140625
c622-022: Epoch: 0, Step: 12, Rank: 47, loss = 0.69140625
c621-091: Epoch: 0, Step: 12, Rank: 28, loss = 0.69140625
c621-111: Epoch: 0, Step: 12, Rank: 32, loss = 0.69140625
c621-061: Epoch: 0, Step: 12, Rank: 22, loss = 0.69140625
c621-101: Epoch: 0, Step: 12, Rank: 30, loss = 0.69140625
c622-021: Epoch: 0, Step: 12, Rank: 46, loss = 0.69140625
c621-062: Epoch: 0, Step: 12, Rank: 23, loss = 0.69140625
c621-071: Epoch: 0, Step: 12, Rank: 24, loss = 0.69140625
c621-132: Epoch: 0, Step: 12, Rank: 37, loss = 0.69140625
c621-141: Epoch: 0, Step: 12, Rank: 38, loss = 0.69140625
c621-151: Epoch: 0, Step: 12, Rank: 40, loss = 0.69140625
c621-122: Epoch: 0, Step: 12, Rank: 35, loss = 0.69140625
c621-152: Epoch: 0, Step: 12, Rank: 41, loss = 0.69140625
c622-041: Epoch: 0, Step: 12, Rank: 50, loss = 0.69140625
c622-051: Epoch: 0, Step: 12, Rank: 52, loss = 0.69140625
c622-042: Epoch: 0, Step: 12, Rank: 51, loss = 0.69140625
c622-011: Epoch: 0, Step: 12, Rank: 44, loss = 0.69140625
c621-102: Epoch: 0, Step: 12, Rank: 31, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 13, Rank: 43, loss = 0.69140625
c619-002: Epoch: 0, Step: 13, Rank: 13, loss = 0.69140625
c619-001: Epoch: 0, Step: 13, Rank: 12, loss = 0.69140625
c613-101: Epoch: 0, Step: 13, Rank: 0, loss = 0.69140625
c622-001: Epoch: 0, Step: 13, Rank: 42, loss = 0.69140625
c622-092: Epoch: 0, Step: 13, Rank: 61, loss = 0.69140625
c622-081: Epoch: 0, Step: 13, Rank: 58, loss = 0.69140625
c621-111: Epoch: 0, Step: 13, Rank: 32, loss = 0.69140625
c622-101: Epoch: 0, Step: 13, Rank: 62, loss = 0.69140625
c619-021: Epoch: 0, Step: 13, Rank: 16, loss = 0.69140625
c621-131: Epoch: 0, Step: 13, Rank: 36, loss = 0.69140625
c622-102: Epoch: 0, Step: 13, Rank: 63, loss = 0.69140625
c613-152: Epoch: 0, Step: 13, Rank: 11, loss = 0.69140625
c621-072: Epoch: 0, Step: 13, Rank: 25, loss = 0.69140625
c621-062: Epoch: 0, Step: 13, Rank: 23, loss = 0.69140625
c622-052: Epoch: 0, Step: 13, Rank: 53, loss = 0.69140625
c621-071: Epoch: 0, Step: 13, Rank: 24, loss = 0.69140625
c621-142: Epoch: 0, Step: 13, Rank: 39, loss = 0.69140625
c613-132: Epoch: 0, Step: 13, Rank: 7, loss = 0.69140625
c621-061: Epoch: 0, Step: 13, Rank: 22, loss = 0.69140625
c613-111: Epoch: 0, Step: 13, Rank: 2, loss = 0.69140625
c619-031: Epoch: 0, Step: 13, Rank: 18, loss = 0.69140625
c621-081: Epoch: 0, Step: 13, Rank: 26, loss = 0.69140625
c621-151: Epoch: 0, Step: 13, Rank: 40, loss = 0.69140625
c622-071: Epoch: 0, Step: 13, Rank: 56, loss = 0.69140625
c622-031: Epoch: 0, Step: 13, Rank: 48, loss = 0.69140625
c622-032: Epoch: 0, Step: 13, Rank: 49, loss = 0.69140625
c622-012: Epoch: 0, Step: 13, Rank: 45, loss = 0.69140625
c613-112: Epoch: 0, Step: 13, Rank: 3, loss = 0.69140625
c619-041: Epoch: 0, Step: 13, Rank: 20, loss = 0.69140625
c622-042: Epoch: 0, Step: 13, Rank: 51, loss = 0.69140625
c621-101: Epoch: 0, Step: 13, Rank: 30, loss = 0.69140625
c622-041: Epoch: 0, Step: 13, Rank: 50, loss = 0.69140625
c613-142: Epoch: 0, Step: 13, Rank: 9, loss = 0.69140625
c621-152: Epoch: 0, Step: 13, Rank: 41, loss = 0.69140625
c622-022: Epoch: 0, Step: 13, Rank: 47, loss = 0.69140625
c622-011: Epoch: 0, Step: 13, Rank: 44, loss = 0.69140625
c613-131: Epoch: 0, Step: 13, Rank: 6, loss = 0.69140625
c621-091: Epoch: 0, Step: 13, Rank: 28, loss = 0.69140625
c621-082: Epoch: 0, Step: 13, Rank: 27, loss = 0.69140625
c621-121: Epoch: 0, Step: 13, Rank: 34, loss = 0.69140625
c619-032: Epoch: 0, Step: 13, Rank: 19, loss = 0.69140625
c622-061: Epoch: 0, Step: 13, Rank: 54, loss = 0.69140625
c621-092: Epoch: 0, Step: 13, Rank: 29, loss = 0.69140625
c613-141: Epoch: 0, Step: 13, Rank: 8, loss = 0.69140625
c619-022: Epoch: 0, Step: 13, Rank: 17, loss = 0.69140625
c621-112: Epoch: 0, Step: 13, Rank: 33, loss = 0.69140625
c613-121: Epoch: 0, Step: 13, Rank: 4, loss = 0.69140625
c622-072: Epoch: 0, Step: 13, Rank: 57, loss = 0.69140625
c613-122: Epoch: 0, Step: 13, Rank: 5, loss = 0.69140625
c622-082: Epoch: 0, Step: 13, Rank: 59, loss = 0.69140625
c613-102: Epoch: 0, Step: 13, Rank: 1, loss = 0.69140625
c622-021: Epoch: 0, Step: 13, Rank: 46, loss = 0.69140625
c619-011: Epoch: 0, Step: 13, Rank: 14, loss = 0.69140625
c622-051: Epoch: 0, Step: 13, Rank: 52, loss = 0.69140625
c621-052: Epoch: 0, Step: 13, Rank: 21, loss = 0.69140625
c621-102: Epoch: 0, Step: 13, Rank: 31, loss = 0.69140625
c621-132: Epoch: 0, Step: 13, Rank: 37, loss = 0.69140625
c619-012: Epoch: 0, Step: 13, Rank: 15, loss = 0.69140625
c622-091: Epoch: 0, Step: 13, Rank: 60, loss = 0.69140625
c621-141: Epoch: 0, Step: 13, Rank: 38, loss = 0.69140625
c622-062: Epoch: 0, Step: 13, Rank: 55, loss = 0.69140625
c613-151: Epoch: 0, Step: 13, Rank: 10, loss = 0.69140625
c621-122: Epoch: 0, Step: 13, Rank: 35, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 14, Rank: 32, loss = 0.69140625
c621-151: Epoch: 0, Step: 14, Rank: 40, loss = 0.69140625
c622-012: Epoch: 0, Step: 14, Rank: 45, loss = 0.69140625
c622-001: Epoch: 0, Step: 14, Rank: 42, loss = 0.69140625
c621-131: Epoch: 0, Step: 14, Rank: 36, loss = 0.69140625
c622-002: Epoch: 0, Step: 14, Rank: 43, loss = 0.69140625
c622-022: Epoch: 0, Step: 14, Rank: 47, loss = 0.69140625
c621-152: Epoch: 0, Step: 14, Rank: 41, loss = 0.69140625
c621-142: Epoch: 0, Step: 14, Rank: 39, loss = 0.69140625
c621-101: Epoch: 0, Step: 14, Rank: 30, loss = 0.69140625
c622-052: Epoch: 0, Step: 14, Rank: 53, loss = 0.69140625
c622-041: Epoch: 0, Step: 14, Rank: 50, loss = 0.69140625
c622-021: Epoch: 0, Step: 14, Rank: 46, loss = 0.69140625
c622-031: Epoch: 0, Step: 14, Rank: 48, loss = 0.69140625
c621-112: Epoch: 0, Step: 14, Rank: 33, loss = 0.69140625
c621-141: Epoch: 0, Step: 14, Rank: 38, loss = 0.69140625
c621-121: Epoch: 0, Step: 14, Rank: 34, loss = 0.69140625
c622-051: Epoch: 0, Step: 14, Rank: 52, loss = 0.69140625
c621-102: Epoch: 0, Step: 14, Rank: 31, loss = 0.69140625
c621-092: Epoch: 0, Step: 14, Rank: 29, loss = 0.69140625
c621-091: Epoch: 0, Step: 14, Rank: 28, loss = 0.69140625
c621-082: Epoch: 0, Step: 14, Rank: 27, loss = 0.69140625
c621-122: Epoch: 0, Step: 14, Rank: 35, loss = 0.69140625
c621-132: Epoch: 0, Step: 14, Rank: 37, loss = 0.69140625
c621-081: Epoch: 0, Step: 14, Rank: 26, loss = 0.69140625
c622-032: Epoch: 0, Step: 14, Rank: 49, loss = 0.69140625
c622-042: Epoch: 0, Step: 14, Rank: 51, loss = 0.69140625
c621-072: Epoch: 0, Step: 14, Rank: 25, loss = 0.69140625
c622-011: Epoch: 0, Step: 14, Rank: 44, loss = 0.69140625
c622-061: Epoch: 0, Step: 14, Rank: 54, loss = 0.69140625
c622-062: Epoch: 0, Step: 14, Rank: 55, loss = 0.69140625
c621-071: Epoch: 0, Step: 14, Rank: 24, loss = 0.69140625
c621-061: Epoch: 0, Step: 14, Rank: 22, loss = 0.69140625
c621-052: Epoch: 0, Step: 14, Rank: 21, loss = 0.69140625
c621-062: Epoch: 0, Step: 14, Rank: 23, loss = 0.69140625
c619-041: Epoch: 0, Step: 14, Rank: 20, loss = 0.69140625
c619-032: Epoch: 0, Step: 14, Rank: 19, loss = 0.69140625
c622-071: Epoch: 0, Step: 14, Rank: 56, loss = 0.69140625
c619-031: Epoch: 0, Step: 14, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 14, Rank: 16, loss = 0.69140625
c619-022: Epoch: 0, Step: 14, Rank: 17, loss = 0.69140625
c619-002: Epoch: 0, Step: 14, Rank: 13, loss = 0.69140625
c619-001: Epoch: 0, Step: 14, Rank: 12, loss = 0.69140625
c619-012: Epoch: 0, Step: 14, Rank: 15, loss = 0.69140625
c622-081: Epoch: 0, Step: 14, Rank: 58, loss = 0.69140625
c619-011: Epoch: 0, Step: 14, Rank: 14, loss = 0.69140625
c613-152: Epoch: 0, Step: 14, Rank: 11, loss = 0.69140625
c622-072: Epoch: 0, Step: 14, Rank: 57, loss = 0.69140625
c613-151: Epoch: 0, Step: 14, Rank: 10, loss = 0.69140625
c613-142: Epoch: 0, Step: 14, Rank: 9, loss = 0.69140625
c613-132: Epoch: 0, Step: 14, Rank: 7, loss = 0.69140625
c613-141: Epoch: 0, Step: 14, Rank: 8, loss = 0.69140625
c613-131: Epoch: 0, Step: 14, Rank: 6, loss = 0.69140625
c613-122: Epoch: 0, Step: 14, Rank: 5, loss = 0.69140625
c613-112: Epoch: 0, Step: 14, Rank: 3, loss = 0.69140625
c613-111: Epoch: 0, Step: 14, Rank: 2, loss = 0.69140625
c613-121: Epoch: 0, Step: 14, Rank: 4, loss = 0.69140625
c622-082: Epoch: 0, Step: 14, Rank: 59, loss = 0.69140625
c613-101: Epoch: 0, Step: 14, Rank: 0, loss = 0.69140625
c613-102: Epoch: 0, Step: 14, Rank: 1, loss = 0.69140625
c622-102: Epoch: 0, Step: 14, Rank: 63, loss = 0.69140625
c622-101: Epoch: 0, Step: 14, Rank: 62, loss = 0.69140625
c622-091: Epoch: 0, Step: 14, Rank: 60, loss = 0.69140625
c622-092: Epoch: 0, Step: 14, Rank: 61, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 15, Rank: 43, loss = 0.69140625
c613-101: Epoch: 0, Step: 15, Rank: 0, loss = 0.69140625
c622-081: Epoch: 0, Step: 15, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 15, Rank: 53, loss = 0.69140625
c613-121: Epoch: 0, Step: 15, Rank: 4, loss = 0.69140625
c622-032: Epoch: 0, Step: 15, Rank: 49, loss = 0.69140625
c622-031: Epoch: 0, Step: 15, Rank: 48, loss = 0.69140625
c622-041: Epoch: 0, Step: 15, Rank: 50, loss = 0.69140625
c613-112: Epoch: 0, Step: 15, Rank: 3, loss = 0.69140625
c613-111: Epoch: 0, Step: 15, Rank: 2, loss = 0.69140625
c619-001: Epoch: 0, Step: 15, Rank: 12, loss = 0.69140625
c613-132: Epoch: 0, Step: 15, Rank: 7, loss = 0.69140625
c621-072: Epoch: 0, Step: 15, Rank: 25, loss = 0.69140625
c619-002: Epoch: 0, Step: 15, Rank: 13, loss = 0.69140625
c622-051: Epoch: 0, Step: 15, Rank: 52, loss = 0.69140625
c622-101: Epoch: 0, Step: 15, Rank: 62, loss = 0.69140625
c613-131: Epoch: 0, Step: 15, Rank: 6, loss = 0.69140625
c619-031: Epoch: 0, Step: 15, Rank: 18, loss = 0.69140625
c622-001: Epoch: 0, Step: 15, Rank: 42, loss = 0.69140625
c622-092: Epoch: 0, Step: 15, Rank: 61, loss = 0.69140625
c622-012: Epoch: 0, Step: 15, Rank: 45, loss = 0.69140625
c619-021: Epoch: 0, Step: 15, Rank: 16, loss = 0.69140625
c622-062: Epoch: 0, Step: 15, Rank: 55, loss = 0.69140625
c622-061: Epoch: 0, Step: 15, Rank: 54, loss = 0.69140625
c621-151: Epoch: 0, Step: 15, Rank: 40, loss = 0.69140625
c621-082: Epoch: 0, Step: 15, Rank: 27, loss = 0.69140625
c613-122: Epoch: 0, Step: 15, Rank: 5, loss = 0.69140625
c621-071: Epoch: 0, Step: 15, Rank: 24, loss = 0.69140625
c613-102: Epoch: 0, Step: 15, Rank: 1, loss = 0.69140625
c622-022: Epoch: 0, Step: 15, Rank: 47, loss = 0.69140625
c621-131: Epoch: 0, Step: 15, Rank: 36, loss = 0.69140625
c622-102: Epoch: 0, Step: 15, Rank: 63, loss = 0.69140625
c622-021: Epoch: 0, Step: 15, Rank: 46, loss = 0.69140625
c619-032: Epoch: 0, Step: 15, Rank: 19, loss = 0.69140625
c621-101: Epoch: 0, Step: 15, Rank: 30, loss = 0.69140625
c621-091: Epoch: 0, Step: 15, Rank: 28, loss = 0.69140625
c613-152: Epoch: 0, Step: 15, Rank: 11, loss = 0.69140625
c621-112: Epoch: 0, Step: 15, Rank: 33, loss = 0.69140625
c622-071: Epoch: 0, Step: 15, Rank: 56, loss = 0.69140625
c613-142: Epoch: 0, Step: 15, Rank: 9, loss = 0.69140625
c621-111: Epoch: 0, Step: 15, Rank: 32, loss = 0.69140625
c621-052: Epoch: 0, Step: 15, Rank: 21, loss = 0.69140625
c621-081: Epoch: 0, Step: 15, Rank: 26, loss = 0.69140625
c613-151: Epoch: 0, Step: 15, Rank: 10, loss = 0.69140625
c622-011: Epoch: 0, Step: 15, Rank: 44, loss = 0.69140625
c621-152: Epoch: 0, Step: 15, Rank: 41, loss = 0.69140625
c619-022: Epoch: 0, Step: 15, Rank: 17, loss = 0.69140625
c621-061: Epoch: 0, Step: 15, Rank: 22, loss = 0.69140625
c621-121: Epoch: 0, Step: 15, Rank: 34, loss = 0.69140625
c622-042: Epoch: 0, Step: 15, Rank: 51, loss = 0.69140625
c621-092: Epoch: 0, Step: 15, Rank: 29, loss = 0.69140625
c622-072: Epoch: 0, Step: 15, Rank: 57, loss = 0.69140625
c621-142: Epoch: 0, Step: 15, Rank: 39, loss = 0.69140625
c619-041: Epoch: 0, Step: 15, Rank: 20, loss = 0.69140625
c621-062: Epoch: 0, Step: 15, Rank: 23, loss = 0.69140625
c613-141: Epoch: 0, Step: 15, Rank: 8, loss = 0.69140625
c622-082: Epoch: 0, Step: 15, Rank: 59, loss = 0.69140625
c619-012: Epoch: 0, Step: 15, Rank: 15, loss = 0.69140625
c619-011: Epoch: 0, Step: 15, Rank: 14, loss = 0.69140625
c622-091: Epoch: 0, Step: 15, Rank: 60, loss = 0.69140625
c621-122: Epoch: 0, Step: 15, Rank: 35, loss = 0.69140625
c621-132: Epoch: 0, Step: 15, Rank: 37, loss = 0.69140625
c621-141: Epoch: 0, Step: 15, Rank: 38, loss = 0.69140625
c621-102: Epoch: 0, Step: 15, Rank: 31, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.22900390625 | reserved: 22634.0 | max reserved: 22634.0
c613-101: Model Parameters: 7.505 B, Latency: 2.67s, TFLOPs: 0.71, Samples/sec: 0.38, Time/seq 2.67s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 16, Rank: 32, loss = 0.69140625
c621-081: Epoch: 0, Step: 16, Rank: 26, loss = 0.69140625
c613-101: Epoch: 0, Step: 16, Rank: 0, loss = 0.69140625
c621-132: Epoch: 0, Step: 16, Rank: 37, loss = 0.69140625
c621-121: Epoch: 0, Step: 16, Rank: 34, loss = 0.69140625
c622-002: Epoch: 0, Step: 16, Rank: 43, loss = 0.69140625
c619-021: Epoch: 0, Step: 16, Rank: 16, loss = 0.69140625
c621-131: Epoch: 0, Step: 16, Rank: 36, loss = 0.69140625
c621-102: Epoch: 0, Step: 16, Rank: 31, loss = 0.69140625
c613-132: Epoch: 0, Step: 16, Rank: 7, loss = 0.69140625
c619-031: Epoch: 0, Step: 16, Rank: 18, loss = 0.69140625
c622-052: Epoch: 0, Step: 16, Rank: 53, loss = 0.69140625
c621-112: Epoch: 0, Step: 16, Rank: 33, loss = 0.69140625
c621-052: Epoch: 0, Step: 16, Rank: 21, loss = 0.69140625
c621-142: Epoch: 0, Step: 16, Rank: 39, loss = 0.69140625
c613-131: Epoch: 0, Step: 16, Rank: 6, loss = 0.69140625
c621-082: Epoch: 0, Step: 16, Rank: 27, loss = 0.69140625
c619-002: Epoch: 0, Step: 16, Rank: 13, loss = 0.69140625
c621-061: Epoch: 0, Step: 16, Rank: 22, loss = 0.69140625
c622-022: Epoch: 0, Step: 16, Rank: 47, loss = 0.69140625
c622-001: Epoch: 0, Step: 16, Rank: 42, loss = 0.69140625
c619-041: Epoch: 0, Step: 16, Rank: 20, loss = 0.69140625
c619-001: Epoch: 0, Step: 16, Rank: 12, loss = 0.69140625
c621-091: Epoch: 0, Step: 16, Rank: 28, loss = 0.69140625
c622-032: Epoch: 0, Step: 16, Rank: 49, loss = 0.69140625
c622-031: Epoch: 0, Step: 16, Rank: 48, loss = 0.69140625
c619-032: Epoch: 0, Step: 16, Rank: 19, loss = 0.69140625
c619-022: Epoch: 0, Step: 16, Rank: 17, loss = 0.69140625
c613-151: Epoch: 0, Step: 16, Rank: 10, loss = 0.69140625
c622-041: Epoch: 0, Step: 16, Rank: 50, loss = 0.69140625
c621-072: Epoch: 0, Step: 16, Rank: 25, loss = 0.69140625
c622-012: Epoch: 0, Step: 16, Rank: 45, loss = 0.69140625
c621-071: Epoch: 0, Step: 16, Rank: 24, loss = 0.69140625
c621-152: Epoch: 0, Step: 16, Rank: 41, loss = 0.69140625
c613-122: Epoch: 0, Step: 16, Rank: 5, loss = 0.69140625
c613-111: Epoch: 0, Step: 16, Rank: 2, loss = 0.69140625
c621-101: Epoch: 0, Step: 16, Rank: 30, loss = 0.69140625
c621-141: Epoch: 0, Step: 16, Rank: 38, loss = 0.69140625
c621-122: Epoch: 0, Step: 16, Rank: 35, loss = 0.69140625
c613-112: Epoch: 0, Step: 16, Rank: 3, loss = 0.69140625
c613-152: Epoch: 0, Step: 16, Rank: 11, loss = 0.69140625
c622-051: Epoch: 0, Step: 16, Rank: 52, loss = 0.69140625
c622-092: Epoch: 0, Step: 16, Rank: 61, loss = 0.69140625
c622-021: Epoch: 0, Step: 16, Rank: 46, loss = 0.69140625
c621-151: Epoch: 0, Step: 16, Rank: 40, loss = 0.69140625
c613-142: Epoch: 0, Step: 16, Rank: 9, loss = 0.69140625
c622-042: Epoch: 0, Step: 16, Rank: 51, loss = 0.69140625
c613-121: Epoch: 0, Step: 16, Rank: 4, loss = 0.69140625
c613-102: Epoch: 0, Step: 16, Rank: 1, loss = 0.69140625
c622-071: Epoch: 0, Step: 16, Rank: 56, loss = 0.69140625
c621-092: Epoch: 0, Step: 16, Rank: 29, loss = 0.69140625
c622-102: Epoch: 0, Step: 16, Rank: 63, loss = 0.69140625
c621-062: Epoch: 0, Step: 16, Rank: 23, loss = 0.69140625
c622-101: Epoch: 0, Step: 16, Rank: 62, loss = 0.69140625
c619-012: Epoch: 0, Step: 16, Rank: 15, loss = 0.69140625
c619-011: Epoch: 0, Step: 16, Rank: 14, loss = 0.69140625
c622-011: Epoch: 0, Step: 16, Rank: 44, loss = 0.69140625
c622-062: Epoch: 0, Step: 16, Rank: 55, loss = 0.69140625
c613-141: Epoch: 0, Step: 16, Rank: 8, loss = 0.69140625
c622-091: Epoch: 0, Step: 16, Rank: 60, loss = 0.69140625
c622-081: Epoch: 0, Step: 16, Rank: 58, loss = 0.69140625
c622-061: Epoch: 0, Step: 16, Rank: 54, loss = 0.69140625
c622-072: Epoch: 0, Step: 16, Rank: 57, loss = 0.69140625
c622-082: Epoch: 0, Step: 16, Rank: 59, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2421875 | max allocated: 11957.22900390625 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 17, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 17, Rank: 43, loss = 0.69140625
c621-111: Epoch: 0, Step: 17, Rank: 32, loss = 0.69140625
c619-002: Epoch: 0, Step: 17, Rank: 13, loss = 0.69140625
c621-081: Epoch: 0, Step: 17, Rank: 26, loss = 0.69140625
c622-081: Epoch: 0, Step: 17, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 17, Rank: 53, loss = 0.69140625
c621-142: Epoch: 0, Step: 17, Rank: 39, loss = 0.69140625
c619-031: Epoch: 0, Step: 17, Rank: 18, loss = 0.69140625
c621-151: Epoch: 0, Step: 17, Rank: 40, loss = 0.69140625
c619-021: Epoch: 0, Step: 17, Rank: 16, loss = 0.69140625
c621-132: Epoch: 0, Step: 17, Rank: 37, loss = 0.69140625
c622-001: Epoch: 0, Step: 17, Rank: 42, loss = 0.69140625
c621-121: Epoch: 0, Step: 17, Rank: 34, loss = 0.69140625
c613-132: Epoch: 0, Step: 17, Rank: 7, loss = 0.69140625
c613-111: Epoch: 0, Step: 17, Rank: 2, loss = 0.69140625
c622-041: Epoch: 0, Step: 17, Rank: 50, loss = 0.69140625
c621-091: Epoch: 0, Step: 17, Rank: 28, loss = 0.69140625
c621-082: Epoch: 0, Step: 17, Rank: 27, loss = 0.69140625
c621-122: Epoch: 0, Step: 17, Rank: 35, loss = 0.69140625
c621-131: Epoch: 0, Step: 17, Rank: 36, loss = 0.69140625
c621-092: Epoch: 0, Step: 17, Rank: 29, loss = 0.69140625
c622-101: Epoch: 0, Step: 17, Rank: 62, loss = 0.69140625
c622-031: Epoch: 0, Step: 17, Rank: 48, loss = 0.69140625
c613-121: Epoch: 0, Step: 17, Rank: 4, loss = 0.69140625
c621-152: Epoch: 0, Step: 17, Rank: 41, loss = 0.69140625
c619-011: Epoch: 0, Step: 17, Rank: 14, loss = 0.69140625
c613-112: Epoch: 0, Step: 17, Rank: 3, loss = 0.69140625
c619-041: Epoch: 0, Step: 17, Rank: 20, loss = 0.69140625
c613-122: Epoch: 0, Step: 17, Rank: 5, loss = 0.69140625
c619-001: Epoch: 0, Step: 17, Rank: 12, loss = 0.69140625
c622-032: Epoch: 0, Step: 17, Rank: 49, loss = 0.69140625
c613-131: Epoch: 0, Step: 17, Rank: 6, loss = 0.69140625
c621-071: Epoch: 0, Step: 17, Rank: 24, loss = 0.69140625
c622-012: Epoch: 0, Step: 17, Rank: 45, loss = 0.69140625
c613-152: Epoch: 0, Step: 17, Rank: 11, loss = 0.69140625
c622-071: Epoch: 0, Step: 17, Rank: 56, loss = 0.69140625
c613-151: Epoch: 0, Step: 17, Rank: 10, loss = 0.69140625
c622-092: Epoch: 0, Step: 17, Rank: 61, loss = 0.69140625
c621-102: Epoch: 0, Step: 17, Rank: 31, loss = 0.69140625
c613-102: Epoch: 0, Step: 17, Rank: 1, loss = 0.69140625
c619-012: Epoch: 0, Step: 17, Rank: 15, loss = 0.69140625
c621-101: Epoch: 0, Step: 17, Rank: 30, loss = 0.69140625
c613-141: Epoch: 0, Step: 17, Rank: 8, loss = 0.69140625
c621-141: Epoch: 0, Step: 17, Rank: 38, loss = 0.69140625
c619-032: Epoch: 0, Step: 17, Rank: 19, loss = 0.69140625
c621-112: Epoch: 0, Step: 17, Rank: 33, loss = 0.69140625
c622-042: Epoch: 0, Step: 17, Rank: 51, loss = 0.69140625
c613-142: Epoch: 0, Step: 17, Rank: 9, loss = 0.69140625
c621-052: Epoch: 0, Step: 17, Rank: 21, loss = 0.69140625
c621-061: Epoch: 0, Step: 17, Rank: 22, loss = 0.69140625
c622-102: Epoch: 0, Step: 17, Rank: 63, loss = 0.69140625
c622-091: Epoch: 0, Step: 17, Rank: 60, loss = 0.69140625
c622-011: Epoch: 0, Step: 17, Rank: 44, loss = 0.69140625
c622-051: Epoch: 0, Step: 17, Rank: 52, loss = 0.69140625
c621-072: Epoch: 0, Step: 17, Rank: 25, loss = 0.69140625
c619-022: Epoch: 0, Step: 17, Rank: 17, loss = 0.69140625
c622-022: Epoch: 0, Step: 17, Rank: 47, loss = 0.69140625
c622-021: Epoch: 0, Step: 17, Rank: 46, loss = 0.69140625
c622-072: Epoch: 0, Step: 17, Rank: 57, loss = 0.69140625
c621-062: Epoch: 0, Step: 17, Rank: 23, loss = 0.69140625
c622-061: Epoch: 0, Step: 17, Rank: 54, loss = 0.69140625
c622-082: Epoch: 0, Step: 17, Rank: 59, loss = 0.69140625
c622-062: Epoch: 0, Step: 17, Rank: 55, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24755859375 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.98s, TFLOPs: 0.95, Samples/sec: 0.51, Time/seq 1.98s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 18, Rank: 26, loss = 0.69140625
c622-002: Epoch: 0, Step: 18, Rank: 43, loss = 0.69140625
c619-032: Epoch: 0, Step: 18, Rank: 19, loss = 0.69140625
c613-132: Epoch: 0, Step: 18, Rank: 7, loss = 0.69140625
c622-052: Epoch: 0, Step: 18, Rank: 53, loss = 0.69140625
c622-001: Epoch: 0, Step: 18, Rank: 42, loss = 0.69140625
c621-142: Epoch: 0, Step: 18, Rank: 39, loss = 0.69140625
c621-151: Epoch: 0, Step: 18, Rank: 40, loss = 0.69140625
c621-111: Epoch: 0, Step: 18, Rank: 32, loss = 0.69140625
c613-141: Epoch: 0, Step: 18, Rank: 8, loss = 0.69140625
c613-142: Epoch: 0, Step: 18, Rank: 9, loss = 0.69140625
c621-152: Epoch: 0, Step: 18, Rank: 41, loss = 0.69140625
c613-101: Epoch: 0, Step: 18, Rank: 0, loss = 0.69140625
c619-021: Epoch: 0, Step: 18, Rank: 16, loss = 0.69140625
c621-132: Epoch: 0, Step: 18, Rank: 37, loss = 0.69140625
c621-052: Epoch: 0, Step: 18, Rank: 21, loss = 0.69140625
c622-051: Epoch: 0, Step: 18, Rank: 52, loss = 0.69140625
c619-041: Epoch: 0, Step: 18, Rank: 20, loss = 0.69140625
c613-131: Epoch: 0, Step: 18, Rank: 6, loss = 0.69140625
c622-011: Epoch: 0, Step: 18, Rank: 44, loss = 0.69140625
c619-001: Epoch: 0, Step: 18, Rank: 12, loss = 0.69140625
c621-061: Epoch: 0, Step: 18, Rank: 22, loss = 0.69140625
c619-031: Epoch: 0, Step: 18, Rank: 18, loss = 0.69140625
c619-011: Epoch: 0, Step: 18, Rank: 14, loss = 0.69140625
c621-122: Epoch: 0, Step: 18, Rank: 35, loss = 0.69140625
c622-101: Epoch: 0, Step: 18, Rank: 62, loss = 0.69140625
c619-002: Epoch: 0, Step: 18, Rank: 13, loss = 0.69140625
c621-131: Epoch: 0, Step: 18, Rank: 36, loss = 0.69140625
c613-111: Epoch: 0, Step: 18, Rank: 2, loss = 0.69140625
c622-012: Epoch: 0, Step: 18, Rank: 45, loss = 0.69140625
c613-151: Epoch: 0, Step: 18, Rank: 10, loss = 0.69140625
c622-032: Epoch: 0, Step: 18, Rank: 49, loss = 0.69140625
c622-031: Epoch: 0, Step: 18, Rank: 48, loss = 0.69140625
c613-122: Epoch: 0, Step: 18, Rank: 5, loss = 0.69140625
c621-121: Epoch: 0, Step: 18, Rank: 34, loss = 0.69140625
c621-082: Epoch: 0, Step: 18, Rank: 27, loss = 0.69140625
c622-022: Epoch: 0, Step: 18, Rank: 47, loss = 0.69140625
c622-042: Epoch: 0, Step: 18, Rank: 51, loss = 0.69140625
c621-112: Epoch: 0, Step: 18, Rank: 33, loss = 0.69140625
c622-041: Epoch: 0, Step: 18, Rank: 50, loss = 0.69140625
c622-061: Epoch: 0, Step: 18, Rank: 54, loss = 0.69140625
c621-091: Epoch: 0, Step: 18, Rank: 28, loss = 0.69140625
c613-152: Epoch: 0, Step: 18, Rank: 11, loss = 0.69140625
c613-121: Epoch: 0, Step: 18, Rank: 4, loss = 0.69140625
c621-071: Epoch: 0, Step: 18, Rank: 24, loss = 0.69140625
c621-072: Epoch: 0, Step: 18, Rank: 25, loss = 0.69140625
c613-112: Epoch: 0, Step: 18, Rank: 3, loss = 0.69140625
c622-021: Epoch: 0, Step: 18, Rank: 46, loss = 0.69140625
c621-141: Epoch: 0, Step: 18, Rank: 38, loss = 0.69140625
c619-012: Epoch: 0, Step: 18, Rank: 15, loss = 0.69140625
c619-022: Epoch: 0, Step: 18, Rank: 17, loss = 0.69140625
c622-081: Epoch: 0, Step: 18, Rank: 58, loss = 0.69140625
c621-101: Epoch: 0, Step: 18, Rank: 30, loss = 0.69140625
c621-102: Epoch: 0, Step: 18, Rank: 31, loss = 0.69140625
c622-102: Epoch: 0, Step: 18, Rank: 63, loss = 0.69140625
c622-092: Epoch: 0, Step: 18, Rank: 61, loss = 0.69140625
c621-092: Epoch: 0, Step: 18, Rank: 29, loss = 0.69140625
c621-062: Epoch: 0, Step: 18, Rank: 23, loss = 0.69140625
c613-102: Epoch: 0, Step: 18, Rank: 1, loss = 0.69140625
c622-071: Epoch: 0, Step: 18, Rank: 56, loss = 0.69140625
c622-082: Epoch: 0, Step: 18, Rank: 59, loss = 0.69140625
c622-072: Epoch: 0, Step: 18, Rank: 57, loss = 0.69140625
c622-091: Epoch: 0, Step: 18, Rank: 60, loss = 0.69140625
c622-062: Epoch: 0, Step: 18, Rank: 55, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 19, Rank: 26, loss = 0.69140625
c619-002: Epoch: 0, Step: 19, Rank: 13, loss = 0.69140625
c619-021: Epoch: 0, Step: 19, Rank: 16, loss = 0.69140625
c621-052: Epoch: 0, Step: 19, Rank: 21, loss = 0.69140625
c621-072: Epoch: 0, Step: 19, Rank: 25, loss = 0.69140625
c619-032: Epoch: 0, Step: 19, Rank: 19, loss = 0.69140625
c621-131: Epoch: 0, Step: 19, Rank: 36, loss = 0.69140625
c621-121: Epoch: 0, Step: 19, Rank: 34, loss = 0.69140625
c621-122: Epoch: 0, Step: 19, Rank: 35, loss = 0.69140625
c622-002: Epoch: 0, Step: 19, Rank: 43, loss = 0.69140625
c621-091: Epoch: 0, Step: 19, Rank: 28, loss = 0.69140625
c622-052: Epoch: 0, Step: 19, Rank: 53, loss = 0.69140625
c621-111: Epoch: 0, Step: 19, Rank: 32, loss = 0.69140625
c621-151: Epoch: 0, Step: 19, Rank: 40, loss = 0.69140625
c621-071: Epoch: 0, Step: 19, Rank: 24, loss = 0.69140625
c619-001: Epoch: 0, Step: 19, Rank: 12, loss = 0.69140625
c621-112: Epoch: 0, Step: 19, Rank: 33, loss = 0.69140625
c619-041: Epoch: 0, Step: 19, Rank: 20, loss = 0.69140625
c619-031: Epoch: 0, Step: 19, Rank: 18, loss = 0.69140625
c613-142: Epoch: 0, Step: 19, Rank: 9, loss = 0.69140625
c621-152: Epoch: 0, Step: 19, Rank: 41, loss = 0.69140625
c619-012: Epoch: 0, Step: 19, Rank: 15, loss = 0.69140625
c621-132: Epoch: 0, Step: 19, Rank: 37, loss = 0.69140625
c613-101: Epoch: 0, Step: 19, Rank: 0, loss = 0.69140625
c621-142: Epoch: 0, Step: 19, Rank: 39, loss = 0.69140625
c613-111: Epoch: 0, Step: 19, Rank: 2, loss = 0.69140625
c621-061: Epoch: 0, Step: 19, Rank: 22, loss = 0.69140625
c621-062: Epoch: 0, Step: 19, Rank: 23, loss = 0.69140625
c622-011: Epoch: 0, Step: 19, Rank: 44, loss = 0.69140625
c622-001: Epoch: 0, Step: 19, Rank: 42, loss = 0.69140625
c619-022: Epoch: 0, Step: 19, Rank: 17, loss = 0.69140625
c613-132: Epoch: 0, Step: 19, Rank: 7, loss = 0.69140625
c619-011: Epoch: 0, Step: 19, Rank: 14, loss = 0.69140625
c613-152: Epoch: 0, Step: 19, Rank: 11, loss = 0.69140625
c621-141: Epoch: 0, Step: 19, Rank: 38, loss = 0.69140625
c622-032: Epoch: 0, Step: 19, Rank: 49, loss = 0.69140625
c622-041: Epoch: 0, Step: 19, Rank: 50, loss = 0.69140625
c613-112: Epoch: 0, Step: 19, Rank: 3, loss = 0.69140625
c613-131: Epoch: 0, Step: 19, Rank: 6, loss = 0.69140625
c622-061: Epoch: 0, Step: 19, Rank: 54, loss = 0.69140625
c613-122: Epoch: 0, Step: 19, Rank: 5, loss = 0.69140625
c622-012: Epoch: 0, Step: 19, Rank: 45, loss = 0.69140625
c613-151: Epoch: 0, Step: 19, Rank: 10, loss = 0.69140625
c621-092: Epoch: 0, Step: 19, Rank: 29, loss = 0.69140625
c621-102: Epoch: 0, Step: 19, Rank: 31, loss = 0.69140625
c622-031: Epoch: 0, Step: 19, Rank: 48, loss = 0.69140625
c621-082: Epoch: 0, Step: 19, Rank: 27, loss = 0.69140625
c622-081: Epoch: 0, Step: 19, Rank: 58, loss = 0.69140625
c613-141: Epoch: 0, Step: 19, Rank: 8, loss = 0.69140625
c622-042: Epoch: 0, Step: 19, Rank: 51, loss = 0.69140625
c622-051: Epoch: 0, Step: 19, Rank: 52, loss = 0.69140625
c622-022: Epoch: 0, Step: 19, Rank: 47, loss = 0.69140625
c622-021: Epoch: 0, Step: 19, Rank: 46, loss = 0.69140625
c613-121: Epoch: 0, Step: 19, Rank: 4, loss = 0.69140625
c621-101: Epoch: 0, Step: 19, Rank: 30, loss = 0.69140625
c622-101: Epoch: 0, Step: 19, Rank: 62, loss = 0.69140625
c622-062: Epoch: 0, Step: 19, Rank: 55, loss = 0.69140625
c622-102: Epoch: 0, Step: 19, Rank: 63, loss = 0.69140625
c622-071: Epoch: 0, Step: 19, Rank: 56, loss = 0.69140625
c622-092: Epoch: 0, Step: 19, Rank: 61, loss = 0.69140625
c613-102: Epoch: 0, Step: 19, Rank: 1, loss = 0.69140625
c622-072: Epoch: 0, Step: 19, Rank: 57, loss = 0.69140625
c622-082: Epoch: 0, Step: 19, Rank: 59, loss = 0.69140625
c622-091: Epoch: 0, Step: 19, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 20, Rank: 43, loss = 0.69140625
c619-031: Epoch: 0, Step: 20, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 20, Rank: 16, loss = 0.69140625
c619-001: Epoch: 0, Step: 20, Rank: 12, loss = 0.69140625
c619-002: Epoch: 0, Step: 20, Rank: 13, loss = 0.69140625
c621-142: Epoch: 0, Step: 20, Rank: 39, loss = 0.69140625
c613-101: Epoch: 0, Step: 20, Rank: 0, loss = 0.69140625
c621-131: Epoch: 0, Step: 20, Rank: 36, loss = 0.69140625
c621-151: Epoch: 0, Step: 20, Rank: 40, loss = 0.69140625
c619-011: Epoch: 0, Step: 20, Rank: 14, loss = 0.69140625
c613-152: Epoch: 0, Step: 20, Rank: 11, loss = 0.69140625
c613-141: Epoch: 0, Step: 20, Rank: 8, loss = 0.69140625
c619-041: Epoch: 0, Step: 20, Rank: 20, loss = 0.69140625
c621-061: Epoch: 0, Step: 20, Rank: 22, loss = 0.69140625
c613-142: Epoch: 0, Step: 20, Rank: 9, loss = 0.69140625
c613-132: Epoch: 0, Step: 20, Rank: 7, loss = 0.69140625
c619-012: Epoch: 0, Step: 20, Rank: 15, loss = 0.69140625
c622-012: Epoch: 0, Step: 20, Rank: 45, loss = 0.69140625
c621-111: Epoch: 0, Step: 20, Rank: 32, loss = 0.69140625
c621-091: Epoch: 0, Step: 20, Rank: 28, loss = 0.69140625
c613-131: Epoch: 0, Step: 20, Rank: 6, loss = 0.69140625
c622-092: Epoch: 0, Step: 20, Rank: 61, loss = 0.69140625
c621-052: Epoch: 0, Step: 20, Rank: 21, loss = 0.69140625
c622-011: Epoch: 0, Step: 20, Rank: 44, loss = 0.69140625
c613-111: Epoch: 0, Step: 20, Rank: 2, loss = 0.69140625
c619-032: Epoch: 0, Step: 20, Rank: 19, loss = 0.69140625
c613-121: Epoch: 0, Step: 20, Rank: 4, loss = 0.69140625
c622-041: Epoch: 0, Step: 20, Rank: 50, loss = 0.69140625
c622-081: Epoch: 0, Step: 20, Rank: 58, loss = 0.69140625
c621-132: Epoch: 0, Step: 20, Rank: 37, loss = 0.69140625
c621-082: Epoch: 0, Step: 20, Rank: 27, loss = 0.69140625
c621-081: Epoch: 0, Step: 20, Rank: 26, loss = 0.69140625
c622-001: Epoch: 0, Step: 20, Rank: 42, loss = 0.69140625
c621-092: Epoch: 0, Step: 20, Rank: 29, loss = 0.69140625
c622-021: Epoch: 0, Step: 20, Rank: 46, loss = 0.69140625
c621-121: Epoch: 0, Step: 20, Rank: 34, loss = 0.69140625
c619-022: Epoch: 0, Step: 20, Rank: 17, loss = 0.69140625
c622-031: Epoch: 0, Step: 20, Rank: 48, loss = 0.69140625
c621-071: Epoch: 0, Step: 20, Rank: 24, loss = 0.69140625
c613-112: Epoch: 0, Step: 20, Rank: 3, loss = 0.69140625
c613-151: Epoch: 0, Step: 20, Rank: 10, loss = 0.69140625
c621-152: Epoch: 0, Step: 20, Rank: 41, loss = 0.69140625
c621-122: Epoch: 0, Step: 20, Rank: 35, loss = 0.69140625
c613-102: Epoch: 0, Step: 20, Rank: 1, loss = 0.69140625
c621-101: Epoch: 0, Step: 20, Rank: 30, loss = 0.69140625
c621-062: Epoch: 0, Step: 20, Rank: 23, loss = 0.69140625
c622-022: Epoch: 0, Step: 20, Rank: 47, loss = 0.69140625
c622-052: Epoch: 0, Step: 20, Rank: 53, loss = 0.69140625
c622-071: Epoch: 0, Step: 20, Rank: 56, loss = 0.69140625
c622-061: Epoch: 0, Step: 20, Rank: 54, loss = 0.69140625
c621-072: Epoch: 0, Step: 20, Rank: 25, loss = 0.69140625
c622-102: Epoch: 0, Step: 20, Rank: 63, loss = 0.69140625
c622-101: Epoch: 0, Step: 20, Rank: 62, loss = 0.69140625
c621-112: Epoch: 0, Step: 20, Rank: 33, loss = 0.69140625
c622-032: Epoch: 0, Step: 20, Rank: 49, loss = 0.69140625
c621-102: Epoch: 0, Step: 20, Rank: 31, loss = 0.69140625
c622-051: Epoch: 0, Step: 20, Rank: 52, loss = 0.69140625
c613-122: Epoch: 0, Step: 20, Rank: 5, loss = 0.69140625
c621-141: Epoch: 0, Step: 20, Rank: 38, loss = 0.69140625
c622-091: Epoch: 0, Step: 20, Rank: 60, loss = 0.69140625
c622-042: Epoch: 0, Step: 20, Rank: 51, loss = 0.69140625
c622-082: Epoch: 0, Step: 20, Rank: 59, loss = 0.69140625
c622-072: Epoch: 0, Step: 20, Rank: 57, loss = 0.69140625
c622-062: Epoch: 0, Step: 20, Rank: 55, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 21, Rank: 16, loss = 0.69140625
c613-101: Epoch: 0, Step: 21, Rank: 0, loss = 0.69140625
c619-002: Epoch: 0, Step: 21, Rank: 13, loss = 0.69140625
c619-001: Epoch: 0, Step: 21, Rank: 12, loss = 0.69140625
c622-101: Epoch: 0, Step: 21, Rank: 62, loss = 0.69140625
c613-151: Epoch: 0, Step: 21, Rank: 10, loss = 0.69140625
c622-081: Epoch: 0, Step: 21, Rank: 58, loss = 0.69140625
c622-092: Epoch: 0, Step: 21, Rank: 61, loss = 0.69140625
c613-132: Epoch: 0, Step: 21, Rank: 7, loss = 0.69140625
c613-111: Epoch: 0, Step: 21, Rank: 2, loss = 0.69140625
c622-102: Epoch: 0, Step: 21, Rank: 63, loss = 0.69140625
c613-121: Epoch: 0, Step: 21, Rank: 4, loss = 0.69140625
c613-112: Epoch: 0, Step: 21, Rank: 3, loss = 0.69140625
c613-131: Epoch: 0, Step: 21, Rank: 6, loss = 0.69140625
c619-031: Epoch: 0, Step: 21, Rank: 18, loss = 0.69140625
c619-022: Epoch: 0, Step: 21, Rank: 17, loss = 0.69140625
c619-012: Epoch: 0, Step: 21, Rank: 15, loss = 0.69140625
c613-152: Epoch: 0, Step: 21, Rank: 11, loss = 0.69140625
c622-071: Epoch: 0, Step: 21, Rank: 56, loss = 0.69140625
c622-052: Epoch: 0, Step: 21, Rank: 53, loss = 0.69140625
c622-002: Epoch: 0, Step: 21, Rank: 43, loss = 0.69140625
c613-102: Epoch: 0, Step: 21, Rank: 1, loss = 0.69140625
c613-142: Epoch: 0, Step: 21, Rank: 9, loss = 0.69140625
c621-111: Epoch: 0, Step: 21, Rank: 32, loss = 0.69140625
c619-011: Epoch: 0, Step: 21, Rank: 14, loss = 0.69140625
c613-141: Epoch: 0, Step: 21, Rank: 8, loss = 0.69140625
c622-072: Epoch: 0, Step: 21, Rank: 57, loss = 0.69140625
c613-122: Epoch: 0, Step: 21, Rank: 5, loss = 0.69140625
c622-061: Epoch: 0, Step: 21, Rank: 54, loss = 0.69140625
c621-151: Epoch: 0, Step: 21, Rank: 40, loss = 0.69140625
c621-101: Epoch: 0, Step: 21, Rank: 30, loss = 0.69140625
c621-102: Epoch: 0, Step: 21, Rank: 31, loss = 0.69140625
c622-082: Epoch: 0, Step: 21, Rank: 59, loss = 0.69140625
c621-132: Epoch: 0, Step: 21, Rank: 37, loss = 0.69140625
c622-051: Epoch: 0, Step: 21, Rank: 52, loss = 0.69140625
c622-062: Epoch: 0, Step: 21, Rank: 55, loss = 0.69140625
c622-091: Epoch: 0, Step: 21, Rank: 60, loss = 0.69140625
c621-131: Epoch: 0, Step: 21, Rank: 36, loss = 0.69140625
c621-092: Epoch: 0, Step: 21, Rank: 29, loss = 0.69140625
c622-041: Epoch: 0, Step: 21, Rank: 50, loss = 0.69140625
c622-012: Epoch: 0, Step: 21, Rank: 45, loss = 0.69140625
c622-011: Epoch: 0, Step: 21, Rank: 44, loss = 0.69140625
c622-001: Epoch: 0, Step: 21, Rank: 42, loss = 0.69140625
c621-081: Epoch: 0, Step: 21, Rank: 26, loss = 0.69140625
c622-031: Epoch: 0, Step: 21, Rank: 48, loss = 0.69140625
c621-142: Epoch: 0, Step: 21, Rank: 39, loss = 0.69140625
c621-121: Epoch: 0, Step: 21, Rank: 34, loss = 0.69140625
c621-112: Epoch: 0, Step: 21, Rank: 33, loss = 0.69140625
c621-152: Epoch: 0, Step: 21, Rank: 41, loss = 0.69140625
c622-032: Epoch: 0, Step: 21, Rank: 49, loss = 0.69140625
c621-091: Epoch: 0, Step: 21, Rank: 28, loss = 0.69140625
c621-082: Epoch: 0, Step: 21, Rank: 27, loss = 0.69140625
c622-022: Epoch: 0, Step: 21, Rank: 47, loss = 0.69140625
c621-141: Epoch: 0, Step: 21, Rank: 38, loss = 0.69140625
c622-042: Epoch: 0, Step: 21, Rank: 51, loss = 0.69140625
c621-122: Epoch: 0, Step: 21, Rank: 35, loss = 0.69140625
c622-021: Epoch: 0, Step: 21, Rank: 46, loss = 0.69140625
c621-072: Epoch: 0, Step: 21, Rank: 25, loss = 0.69140625
c621-062: Epoch: 0, Step: 21, Rank: 23, loss = 0.69140625
c619-032: Epoch: 0, Step: 21, Rank: 19, loss = 0.69140625
c621-071: Epoch: 0, Step: 21, Rank: 24, loss = 0.69140625
c621-061: Epoch: 0, Step: 21, Rank: 22, loss = 0.69140625
c621-052: Epoch: 0, Step: 21, Rank: 21, loss = 0.69140625
c619-041: Epoch: 0, Step: 21, Rank: 20, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 22, Rank: 32, loss = 0.69140625
c613-101: Epoch: 0, Step: 22, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 22, Rank: 43, loss = 0.69140625
c621-121: Epoch: 0, Step: 22, Rank: 34, loss = 0.69140625
c621-132: Epoch: 0, Step: 22, Rank: 37, loss = 0.69140625
c619-002: Epoch: 0, Step: 22, Rank: 13, loss = 0.69140625
c622-052: Epoch: 0, Step: 22, Rank: 53, loss = 0.69140625
c622-011: Epoch: 0, Step: 22, Rank: 44, loss = 0.69140625
c621-081: Epoch: 0, Step: 22, Rank: 26, loss = 0.69140625
c622-071: Epoch: 0, Step: 22, Rank: 56, loss = 0.69140625
c621-131: Epoch: 0, Step: 22, Rank: 36, loss = 0.69140625
c619-001: Epoch: 0, Step: 22, Rank: 12, loss = 0.69140625
c622-081: Epoch: 0, Step: 22, Rank: 58, loss = 0.69140625
c621-142: Epoch: 0, Step: 22, Rank: 39, loss = 0.69140625
c621-151: Epoch: 0, Step: 22, Rank: 40, loss = 0.69140625
c622-041: Epoch: 0, Step: 22, Rank: 50, loss = 0.69140625
c622-001: Epoch: 0, Step: 22, Rank: 42, loss = 0.69140625
c613-112: Epoch: 0, Step: 22, Rank: 3, loss = 0.69140625
c613-111: Epoch: 0, Step: 22, Rank: 2, loss = 0.69140625
c621-091: Epoch: 0, Step: 22, Rank: 28, loss = 0.69140625
c621-052: Epoch: 0, Step: 22, Rank: 21, loss = 0.69140625
c621-122: Epoch: 0, Step: 22, Rank: 35, loss = 0.69140625
c622-032: Epoch: 0, Step: 22, Rank: 49, loss = 0.69140625
c622-012: Epoch: 0, Step: 22, Rank: 45, loss = 0.69140625
c622-101: Epoch: 0, Step: 22, Rank: 62, loss = 0.69140625
c621-082: Epoch: 0, Step: 22, Rank: 27, loss = 0.69140625
c621-061: Epoch: 0, Step: 22, Rank: 22, loss = 0.69140625
c621-102: Epoch: 0, Step: 22, Rank: 31, loss = 0.69140625
c622-031: Epoch: 0, Step: 22, Rank: 48, loss = 0.69140625
c613-121: Epoch: 0, Step: 22, Rank: 4, loss = 0.69140625
c613-131: Epoch: 0, Step: 22, Rank: 6, loss = 0.69140625
c613-102: Epoch: 0, Step: 22, Rank: 1, loss = 0.69140625
c621-101: Epoch: 0, Step: 22, Rank: 30, loss = 0.69140625
c622-082: Epoch: 0, Step: 22, Rank: 59, loss = 0.69140625
c619-041: Epoch: 0, Step: 22, Rank: 20, loss = 0.69140625
c622-062: Epoch: 0, Step: 22, Rank: 55, loss = 0.69140625
c621-072: Epoch: 0, Step: 22, Rank: 25, loss = 0.69140625
c622-072: Epoch: 0, Step: 22, Rank: 57, loss = 0.69140625
c613-132: Epoch: 0, Step: 22, Rank: 7, loss = 0.69140625
c622-092: Epoch: 0, Step: 22, Rank: 61, loss = 0.69140625
c621-152: Epoch: 0, Step: 22, Rank: 41, loss = 0.69140625
c622-021: Epoch: 0, Step: 22, Rank: 46, loss = 0.69140625
c619-012: Epoch: 0, Step: 22, Rank: 15, loss = 0.69140625
c622-051: Epoch: 0, Step: 22, Rank: 52, loss = 0.69140625
c619-021: Epoch: 0, Step: 22, Rank: 16, loss = 0.69140625
c619-032: Epoch: 0, Step: 22, Rank: 19, loss = 0.69140625
c622-022: Epoch: 0, Step: 22, Rank: 47, loss = 0.69140625
c622-102: Epoch: 0, Step: 22, Rank: 63, loss = 0.69140625
c613-152: Epoch: 0, Step: 22, Rank: 11, loss = 0.69140625
c613-141: Epoch: 0, Step: 22, Rank: 8, loss = 0.69140625
c619-031: Epoch: 0, Step: 22, Rank: 18, loss = 0.69140625
c622-042: Epoch: 0, Step: 22, Rank: 51, loss = 0.69140625
c621-092: Epoch: 0, Step: 22, Rank: 29, loss = 0.69140625
c622-061: Epoch: 0, Step: 22, Rank: 54, loss = 0.69140625
c621-071: Epoch: 0, Step: 22, Rank: 24, loss = 0.69140625
c619-011: Epoch: 0, Step: 22, Rank: 14, loss = 0.69140625
c613-151: Epoch: 0, Step: 22, Rank: 10, loss = 0.69140625
c619-022: Epoch: 0, Step: 22, Rank: 17, loss = 0.69140625
c613-142: Epoch: 0, Step: 22, Rank: 9, loss = 0.69140625
c621-112: Epoch: 0, Step: 22, Rank: 33, loss = 0.69140625
c621-141: Epoch: 0, Step: 22, Rank: 38, loss = 0.69140625
c622-091: Epoch: 0, Step: 22, Rank: 60, loss = 0.69140625
c613-122: Epoch: 0, Step: 22, Rank: 5, loss = 0.69140625
c621-062: Epoch: 0, Step: 22, Rank: 23, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 23, Rank: 16, loss = 0.69140625
c622-032: Epoch: 0, Step: 23, Rank: 49, loss = 0.69140625
c613-101: Epoch: 0, Step: 23, Rank: 0, loss = 0.69140625
c621-132: Epoch: 0, Step: 23, Rank: 37, loss = 0.69140625
c622-012: Epoch: 0, Step: 23, Rank: 45, loss = 0.69140625
c613-132: Epoch: 0, Step: 23, Rank: 7, loss = 0.69140625
c619-031: Epoch: 0, Step: 23, Rank: 18, loss = 0.69140625
c621-121: Epoch: 0, Step: 23, Rank: 34, loss = 0.69140625
c621-112: Epoch: 0, Step: 23, Rank: 33, loss = 0.69140625
c622-101: Epoch: 0, Step: 23, Rank: 62, loss = 0.69140625
c621-151: Epoch: 0, Step: 23, Rank: 40, loss = 0.69140625
c621-111: Epoch: 0, Step: 23, Rank: 32, loss = 0.69140625
c619-002: Epoch: 0, Step: 23, Rank: 13, loss = 0.69140625
c622-041: Epoch: 0, Step: 23, Rank: 50, loss = 0.69140625
c621-131: Epoch: 0, Step: 23, Rank: 36, loss = 0.69140625
c622-052: Epoch: 0, Step: 23, Rank: 53, loss = 0.69140625
c622-002: Epoch: 0, Step: 23, Rank: 43, loss = 0.69140625
c619-001: Epoch: 0, Step: 23, Rank: 12, loss = 0.69140625
c622-061: Epoch: 0, Step: 23, Rank: 54, loss = 0.69140625
c621-092: Epoch: 0, Step: 23, Rank: 29, loss = 0.69140625
c622-071: Epoch: 0, Step: 23, Rank: 56, loss = 0.69140625
c622-062: Epoch: 0, Step: 23, Rank: 55, loss = 0.69140625
c621-122: Epoch: 0, Step: 23, Rank: 35, loss = 0.69140625
c621-091: Epoch: 0, Step: 23, Rank: 28, loss = 0.69140625
c621-081: Epoch: 0, Step: 23, Rank: 26, loss = 0.69140625
c622-092: Epoch: 0, Step: 23, Rank: 61, loss = 0.69140625
c622-011: Epoch: 0, Step: 23, Rank: 44, loss = 0.69140625
c622-051: Epoch: 0, Step: 23, Rank: 52, loss = 0.69140625
c613-141: Epoch: 0, Step: 23, Rank: 8, loss = 0.69140625
c621-152: Epoch: 0, Step: 23, Rank: 41, loss = 0.69140625
c622-031: Epoch: 0, Step: 23, Rank: 48, loss = 0.69140625
c622-102: Epoch: 0, Step: 23, Rank: 63, loss = 0.69140625
c621-082: Epoch: 0, Step: 23, Rank: 27, loss = 0.69140625
c622-022: Epoch: 0, Step: 23, Rank: 47, loss = 0.69140625
c622-081: Epoch: 0, Step: 23, Rank: 58, loss = 0.69140625
c622-072: Epoch: 0, Step: 23, Rank: 57, loss = 0.69140625
c613-151: Epoch: 0, Step: 23, Rank: 10, loss = 0.69140625
c613-131: Epoch: 0, Step: 23, Rank: 6, loss = 0.69140625
c613-152: Epoch: 0, Step: 23, Rank: 11, loss = 0.69140625
c613-121: Epoch: 0, Step: 23, Rank: 4, loss = 0.69140625
c621-102: Epoch: 0, Step: 23, Rank: 31, loss = 0.69140625
c621-072: Epoch: 0, Step: 23, Rank: 25, loss = 0.69140625
c619-012: Epoch: 0, Step: 23, Rank: 15, loss = 0.69140625
c613-112: Epoch: 0, Step: 23, Rank: 3, loss = 0.69140625
c619-041: Epoch: 0, Step: 23, Rank: 20, loss = 0.69140625
c622-001: Epoch: 0, Step: 23, Rank: 42, loss = 0.69140625
c621-101: Epoch: 0, Step: 23, Rank: 30, loss = 0.69140625
c613-111: Epoch: 0, Step: 23, Rank: 2, loss = 0.69140625
c613-122: Epoch: 0, Step: 23, Rank: 5, loss = 0.69140625
c622-082: Epoch: 0, Step: 23, Rank: 59, loss = 0.69140625
c621-061: Epoch: 0, Step: 23, Rank: 22, loss = 0.69140625
c621-071: Epoch: 0, Step: 23, Rank: 24, loss = 0.69140625
c619-032: Epoch: 0, Step: 23, Rank: 19, loss = 0.69140625
c621-141: Epoch: 0, Step: 23, Rank: 38, loss = 0.69140625
c622-021: Epoch: 0, Step: 23, Rank: 46, loss = 0.69140625
c613-142: Epoch: 0, Step: 23, Rank: 9, loss = 0.69140625
c622-042: Epoch: 0, Step: 23, Rank: 51, loss = 0.69140625
c619-022: Epoch: 0, Step: 23, Rank: 17, loss = 0.69140625
c622-091: Epoch: 0, Step: 23, Rank: 60, loss = 0.69140625
c621-142: Epoch: 0, Step: 23, Rank: 39, loss = 0.69140625
c613-102: Epoch: 0, Step: 23, Rank: 1, loss = 0.69140625
c621-062: Epoch: 0, Step: 23, Rank: 23, loss = 0.69140625
c619-011: Epoch: 0, Step: 23, Rank: 14, loss = 0.69140625
c621-052: Epoch: 0, Step: 23, Rank: 21, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c619-001: Epoch: 0, Step: 24, Rank: 12, loss = 0.69140625
c619-031: Epoch: 0, Step: 24, Rank: 18, loss = 0.69140625
c619-002: Epoch: 0, Step: 24, Rank: 13, loss = 0.69140625
c613-151: Epoch: 0, Step: 24, Rank: 10, loss = 0.69140625
c619-021: Epoch: 0, Step: 24, Rank: 16, loss = 0.69140625
c622-002: Epoch: 0, Step: 24, Rank: 43, loss = 0.69140625
c622-092: Epoch: 0, Step: 24, Rank: 61, loss = 0.69140625
c619-032: Epoch: 0, Step: 24, Rank: 19, loss = 0.69140625
c613-152: Epoch: 0, Step: 24, Rank: 11, loss = 0.69140625
c622-062: Epoch: 0, Step: 24, Rank: 55, loss = 0.69140625
c622-081: Epoch: 0, Step: 24, Rank: 58, loss = 0.69140625
c613-141: Epoch: 0, Step: 24, Rank: 8, loss = 0.69140625
c613-111: Epoch: 0, Step: 24, Rank: 2, loss = 0.69140625
c613-132: Epoch: 0, Step: 24, Rank: 7, loss = 0.69140625
c613-101: Epoch: 0, Step: 24, Rank: 0, loss = 0.69140625
c622-012: Epoch: 0, Step: 24, Rank: 45, loss = 0.69140625
c613-142: Epoch: 0, Step: 24, Rank: 9, loss = 0.69140625
c619-022: Epoch: 0, Step: 24, Rank: 17, loss = 0.69140625
c622-061: Epoch: 0, Step: 24, Rank: 54, loss = 0.69140625
c621-151: Epoch: 0, Step: 24, Rank: 40, loss = 0.69140625
c619-011: Epoch: 0, Step: 24, Rank: 14, loss = 0.69140625
c619-012: Epoch: 0, Step: 24, Rank: 15, loss = 0.69140625
c621-132: Epoch: 0, Step: 24, Rank: 37, loss = 0.69140625
c613-121: Epoch: 0, Step: 24, Rank: 4, loss = 0.69140625
c622-001: Epoch: 0, Step: 24, Rank: 42, loss = 0.69140625
c613-131: Epoch: 0, Step: 24, Rank: 6, loss = 0.69140625
c613-112: Epoch: 0, Step: 24, Rank: 3, loss = 0.69140625
c622-041: Epoch: 0, Step: 24, Rank: 50, loss = 0.69140625
c613-122: Epoch: 0, Step: 24, Rank: 5, loss = 0.69140625
c622-072: Epoch: 0, Step: 24, Rank: 57, loss = 0.69140625
c622-031: Epoch: 0, Step: 24, Rank: 48, loss = 0.69140625
c622-071: Epoch: 0, Step: 24, Rank: 56, loss = 0.69140625
c622-052: Epoch: 0, Step: 24, Rank: 53, loss = 0.69140625
c621-142: Epoch: 0, Step: 24, Rank: 39, loss = 0.69140625
c622-021: Epoch: 0, Step: 24, Rank: 46, loss = 0.69140625
c622-082: Epoch: 0, Step: 24, Rank: 59, loss = 0.69140625
c622-022: Epoch: 0, Step: 24, Rank: 47, loss = 0.69140625
c622-102: Epoch: 0, Step: 24, Rank: 63, loss = 0.69140625
c613-102: Epoch: 0, Step: 24, Rank: 1, loss = 0.69140625
c621-111: Epoch: 0, Step: 24, Rank: 32, loss = 0.69140625
c619-041: Epoch: 0, Step: 24, Rank: 20, loss = 0.69140625
c621-141: Epoch: 0, Step: 24, Rank: 38, loss = 0.69140625
c622-011: Epoch: 0, Step: 24, Rank: 44, loss = 0.69140625
c621-131: Epoch: 0, Step: 24, Rank: 36, loss = 0.69140625
c621-152: Epoch: 0, Step: 24, Rank: 41, loss = 0.69140625
c622-101: Epoch: 0, Step: 24, Rank: 62, loss = 0.69140625
c621-091: Epoch: 0, Step: 24, Rank: 28, loss = 0.69140625
c622-091: Epoch: 0, Step: 24, Rank: 60, loss = 0.69140625
c621-122: Epoch: 0, Step: 24, Rank: 35, loss = 0.69140625
c621-052: Epoch: 0, Step: 24, Rank: 21, loss = 0.69140625
c622-051: Epoch: 0, Step: 24, Rank: 52, loss = 0.69140625
c621-121: Epoch: 0, Step: 24, Rank: 34, loss = 0.69140625
c621-112: Epoch: 0, Step: 24, Rank: 33, loss = 0.69140625
c621-061: Epoch: 0, Step: 24, Rank: 22, loss = 0.69140625
c621-081: Epoch: 0, Step: 24, Rank: 26, loss = 0.69140625
c621-082: Epoch: 0, Step: 24, Rank: 27, loss = 0.69140625
c621-062: Epoch: 0, Step: 24, Rank: 23, loss = 0.69140625
c622-032: Epoch: 0, Step: 24, Rank: 49, loss = 0.69140625
c621-072: Epoch: 0, Step: 24, Rank: 25, loss = 0.69140625
c622-042: Epoch: 0, Step: 24, Rank: 51, loss = 0.69140625
c621-102: Epoch: 0, Step: 24, Rank: 31, loss = 0.69140625
c621-101: Epoch: 0, Step: 24, Rank: 30, loss = 0.69140625
c621-092: Epoch: 0, Step: 24, Rank: 29, loss = 0.69140625
c621-071: Epoch: 0, Step: 24, Rank: 24, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.11s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.11s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 25, Rank: 16, loss = 0.69140625
c622-081: Epoch: 0, Step: 25, Rank: 58, loss = 0.69140625
c619-001: Epoch: 0, Step: 25, Rank: 12, loss = 0.69140625
c613-151: Epoch: 0, Step: 25, Rank: 10, loss = 0.69140625
c622-002: Epoch: 0, Step: 25, Rank: 43, loss = 0.69140625
c619-002: Epoch: 0, Step: 25, Rank: 13, loss = 0.69140625
c613-101: Epoch: 0, Step: 25, Rank: 0, loss = 0.69140625
c619-031: Epoch: 0, Step: 25, Rank: 18, loss = 0.69140625
c613-121: Epoch: 0, Step: 25, Rank: 4, loss = 0.69140625
c613-141: Epoch: 0, Step: 25, Rank: 8, loss = 0.69140625
c622-052: Epoch: 0, Step: 25, Rank: 53, loss = 0.69140625
c622-101: Epoch: 0, Step: 25, Rank: 62, loss = 0.69140625
c613-132: Epoch: 0, Step: 25, Rank: 7, loss = 0.69140625
c621-081: Epoch: 0, Step: 25, Rank: 26, loss = 0.69140625
c622-092: Epoch: 0, Step: 25, Rank: 61, loss = 0.69140625
c621-151: Epoch: 0, Step: 25, Rank: 40, loss = 0.69140625
c621-142: Epoch: 0, Step: 25, Rank: 39, loss = 0.69140625
c613-112: Epoch: 0, Step: 25, Rank: 3, loss = 0.69140625
c613-152: Epoch: 0, Step: 25, Rank: 11, loss = 0.69140625
c622-012: Epoch: 0, Step: 25, Rank: 45, loss = 0.69140625
c622-062: Epoch: 0, Step: 25, Rank: 55, loss = 0.69140625
c621-132: Epoch: 0, Step: 25, Rank: 37, loss = 0.69140625
c619-012: Epoch: 0, Step: 25, Rank: 15, loss = 0.69140625
c619-032: Epoch: 0, Step: 25, Rank: 19, loss = 0.69140625
c622-071: Epoch: 0, Step: 25, Rank: 56, loss = 0.69140625
c613-122: Epoch: 0, Step: 25, Rank: 5, loss = 0.69140625
c622-051: Epoch: 0, Step: 25, Rank: 52, loss = 0.69140625
c613-111: Epoch: 0, Step: 25, Rank: 2, loss = 0.69140625
c621-131: Epoch: 0, Step: 25, Rank: 36, loss = 0.69140625
c621-111: Epoch: 0, Step: 25, Rank: 32, loss = 0.69140625
c613-131: Epoch: 0, Step: 25, Rank: 6, loss = 0.69140625
c621-121: Epoch: 0, Step: 25, Rank: 34, loss = 0.69140625
c621-152: Epoch: 0, Step: 25, Rank: 41, loss = 0.69140625
c622-011: Epoch: 0, Step: 25, Rank: 44, loss = 0.69140625
c613-142: Epoch: 0, Step: 25, Rank: 9, loss = 0.69140625
c619-011: Epoch: 0, Step: 25, Rank: 14, loss = 0.69140625
c622-072: Epoch: 0, Step: 25, Rank: 57, loss = 0.69140625
c622-061: Epoch: 0, Step: 25, Rank: 54, loss = 0.69140625
c622-082: Epoch: 0, Step: 25, Rank: 59, loss = 0.69140625
c619-022: Epoch: 0, Step: 25, Rank: 17, loss = 0.69140625
c619-041: Epoch: 0, Step: 25, Rank: 20, loss = 0.69140625
c622-031: Epoch: 0, Step: 25, Rank: 48, loss = 0.69140625
c622-041: Epoch: 0, Step: 25, Rank: 50, loss = 0.69140625
c621-101: Epoch: 0, Step: 25, Rank: 30, loss = 0.69140625
c622-102: Epoch: 0, Step: 25, Rank: 63, loss = 0.69140625
c621-092: Epoch: 0, Step: 25, Rank: 29, loss = 0.69140625
c622-001: Epoch: 0, Step: 25, Rank: 42, loss = 0.69140625
c621-082: Epoch: 0, Step: 25, Rank: 27, loss = 0.69140625
c621-061: Epoch: 0, Step: 25, Rank: 22, loss = 0.69140625
c621-112: Epoch: 0, Step: 25, Rank: 33, loss = 0.69140625
c621-062: Epoch: 0, Step: 25, Rank: 23, loss = 0.69140625
c613-102: Epoch: 0, Step: 25, Rank: 1, loss = 0.69140625
c622-091: Epoch: 0, Step: 25, Rank: 60, loss = 0.69140625
c621-141: Epoch: 0, Step: 25, Rank: 38, loss = 0.69140625
c621-091: Epoch: 0, Step: 25, Rank: 28, loss = 0.69140625
c621-122: Epoch: 0, Step: 25, Rank: 35, loss = 0.69140625
c621-102: Epoch: 0, Step: 25, Rank: 31, loss = 0.69140625
c621-071: Epoch: 0, Step: 25, Rank: 24, loss = 0.69140625
c622-022: Epoch: 0, Step: 25, Rank: 47, loss = 0.69140625
c622-042: Epoch: 0, Step: 25, Rank: 51, loss = 0.69140625
c621-072: Epoch: 0, Step: 25, Rank: 25, loss = 0.69140625
c622-021: Epoch: 0, Step: 25, Rank: 46, loss = 0.69140625
c622-032: Epoch: 0, Step: 25, Rank: 49, loss = 0.69140625
c621-052: Epoch: 0, Step: 25, Rank: 21, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 26, Rank: 43, loss = 0.69140625
c621-132: Epoch: 0, Step: 26, Rank: 37, loss = 0.69140625
c619-021: Epoch: 0, Step: 26, Rank: 16, loss = 0.69140625
c613-101: Epoch: 0, Step: 26, Rank: 0, loss = 0.69140625
c622-052: Epoch: 0, Step: 26, Rank: 53, loss = 0.69140625
c622-092: Epoch: 0, Step: 26, Rank: 61, loss = 0.69140625
c622-081: Epoch: 0, Step: 26, Rank: 58, loss = 0.69140625
c622-071: Epoch: 0, Step: 26, Rank: 56, loss = 0.69140625
c613-121: Epoch: 0, Step: 26, Rank: 4, loss = 0.69140625
c621-081: Epoch: 0, Step: 26, Rank: 26, loss = 0.69140625
c621-121: Epoch: 0, Step: 26, Rank: 34, loss = 0.69140625
c613-132: Epoch: 0, Step: 26, Rank: 7, loss = 0.69140625
c622-001: Epoch: 0, Step: 26, Rank: 42, loss = 0.69140625
c622-012: Epoch: 0, Step: 26, Rank: 45, loss = 0.69140625
c613-112: Epoch: 0, Step: 26, Rank: 3, loss = 0.69140625
c622-062: Epoch: 0, Step: 26, Rank: 55, loss = 0.69140625
c621-111: Epoch: 0, Step: 26, Rank: 32, loss = 0.69140625
c621-112: Epoch: 0, Step: 26, Rank: 33, loss = 0.69140625
c613-111: Epoch: 0, Step: 26, Rank: 2, loss = 0.69140625
c613-131: Epoch: 0, Step: 26, Rank: 6, loss = 0.69140625
c621-131: Epoch: 0, Step: 26, Rank: 36, loss = 0.69140625
c619-001: Epoch: 0, Step: 26, Rank: 12, loss = 0.69140625
c621-102: Epoch: 0, Step: 26, Rank: 31, loss = 0.69140625
c622-032: Epoch: 0, Step: 26, Rank: 49, loss = 0.69140625
c621-091: Epoch: 0, Step: 26, Rank: 28, loss = 0.69140625
c613-141: Epoch: 0, Step: 26, Rank: 8, loss = 0.69140625
c621-151: Epoch: 0, Step: 26, Rank: 40, loss = 0.69140625
c613-142: Epoch: 0, Step: 26, Rank: 9, loss = 0.69140625
c622-101: Epoch: 0, Step: 26, Rank: 62, loss = 0.69140625
c622-061: Epoch: 0, Step: 26, Rank: 54, loss = 0.69140625
c622-031: Epoch: 0, Step: 26, Rank: 48, loss = 0.69140625
c619-002: Epoch: 0, Step: 26, Rank: 13, loss = 0.69140625
c621-072: Epoch: 0, Step: 26, Rank: 25, loss = 0.69140625
c621-152: Epoch: 0, Step: 26, Rank: 41, loss = 0.69140625
c621-142: Epoch: 0, Step: 26, Rank: 39, loss = 0.69140625
c622-072: Epoch: 0, Step: 26, Rank: 57, loss = 0.69140625
c613-122: Epoch: 0, Step: 26, Rank: 5, loss = 0.69140625
c622-051: Epoch: 0, Step: 26, Rank: 52, loss = 0.69140625
c622-011: Epoch: 0, Step: 26, Rank: 44, loss = 0.69140625
c619-041: Epoch: 0, Step: 26, Rank: 20, loss = 0.69140625
c622-041: Epoch: 0, Step: 26, Rank: 50, loss = 0.69140625
c621-101: Epoch: 0, Step: 26, Rank: 30, loss = 0.69140625
c621-071: Epoch: 0, Step: 26, Rank: 24, loss = 0.69140625
c622-082: Epoch: 0, Step: 26, Rank: 59, loss = 0.69140625
c613-152: Epoch: 0, Step: 26, Rank: 11, loss = 0.69140625
c621-052: Epoch: 0, Step: 26, Rank: 21, loss = 0.69140625
c622-042: Epoch: 0, Step: 26, Rank: 51, loss = 0.69140625
c621-062: Epoch: 0, Step: 26, Rank: 23, loss = 0.69140625
c621-122: Epoch: 0, Step: 26, Rank: 35, loss = 0.69140625
c613-102: Epoch: 0, Step: 26, Rank: 1, loss = 0.69140625
c621-092: Epoch: 0, Step: 26, Rank: 29, loss = 0.69140625
c619-011: Epoch: 0, Step: 26, Rank: 14, loss = 0.69140625
c619-022: Epoch: 0, Step: 26, Rank: 17, loss = 0.69140625
c621-141: Epoch: 0, Step: 26, Rank: 38, loss = 0.69140625
c621-061: Epoch: 0, Step: 26, Rank: 22, loss = 0.69140625
c619-031: Epoch: 0, Step: 26, Rank: 18, loss = 0.69140625
c622-021: Epoch: 0, Step: 26, Rank: 46, loss = 0.69140625
c619-012: Epoch: 0, Step: 26, Rank: 15, loss = 0.69140625
c622-102: Epoch: 0, Step: 26, Rank: 63, loss = 0.69140625
c619-032: Epoch: 0, Step: 26, Rank: 19, loss = 0.69140625
c621-082: Epoch: 0, Step: 26, Rank: 27, loss = 0.69140625
c613-151: Epoch: 0, Step: 26, Rank: 10, loss = 0.69140625
c622-091: Epoch: 0, Step: 26, Rank: 60, loss = 0.69140625
c622-022: Epoch: 0, Step: 26, Rank: 47, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.98s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.98s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 27, Rank: 0, loss = 0.69140625
c621-111: Epoch: 0, Step: 27, Rank: 32, loss = 0.69140625
c622-052: Epoch: 0, Step: 27, Rank: 53, loss = 0.69140625
c613-132: Epoch: 0, Step: 27, Rank: 7, loss = 0.69140625
c621-081: Epoch: 0, Step: 27, Rank: 26, loss = 0.69140625
c622-081: Epoch: 0, Step: 27, Rank: 58, loss = 0.69140625
c621-082: Epoch: 0, Step: 27, Rank: 27, loss = 0.69140625
c622-002: Epoch: 0, Step: 27, Rank: 43, loss = 0.69140625
c619-021: Epoch: 0, Step: 27, Rank: 16, loss = 0.69140625
c613-111: Epoch: 0, Step: 27, Rank: 2, loss = 0.69140625
c613-151: Epoch: 0, Step: 27, Rank: 10, loss = 0.69140625
c621-072: Epoch: 0, Step: 27, Rank: 25, loss = 0.69140625
c621-132: Epoch: 0, Step: 27, Rank: 37, loss = 0.69140625
c622-012: Epoch: 0, Step: 27, Rank: 45, loss = 0.69140625
c619-002: Epoch: 0, Step: 27, Rank: 13, loss = 0.69140625
c619-031: Epoch: 0, Step: 27, Rank: 18, loss = 0.69140625
c622-062: Epoch: 0, Step: 27, Rank: 55, loss = 0.69140625
c619-001: Epoch: 0, Step: 27, Rank: 12, loss = 0.69140625
c622-041: Epoch: 0, Step: 27, Rank: 50, loss = 0.69140625
c622-011: Epoch: 0, Step: 27, Rank: 44, loss = 0.69140625
c622-032: Epoch: 0, Step: 27, Rank: 49, loss = 0.69140625
c619-041: Epoch: 0, Step: 27, Rank: 20, loss = 0.69140625
c622-042: Epoch: 0, Step: 27, Rank: 51, loss = 0.69140625
c621-142: Epoch: 0, Step: 27, Rank: 39, loss = 0.69140625
c621-151: Epoch: 0, Step: 27, Rank: 40, loss = 0.69140625
c619-032: Epoch: 0, Step: 27, Rank: 19, loss = 0.69140625
c621-101: Epoch: 0, Step: 27, Rank: 30, loss = 0.69140625
c621-092: Epoch: 0, Step: 27, Rank: 29, loss = 0.69140625
c622-031: Epoch: 0, Step: 27, Rank: 48, loss = 0.69140625
c613-102: Epoch: 0, Step: 27, Rank: 1, loss = 0.69140625
c621-102: Epoch: 0, Step: 27, Rank: 31, loss = 0.69140625
c613-121: Epoch: 0, Step: 27, Rank: 4, loss = 0.69140625
c613-131: Epoch: 0, Step: 27, Rank: 6, loss = 0.69140625
c622-051: Epoch: 0, Step: 27, Rank: 52, loss = 0.69140625
c621-091: Epoch: 0, Step: 27, Rank: 28, loss = 0.69140625
c621-121: Epoch: 0, Step: 27, Rank: 34, loss = 0.69140625
c621-052: Epoch: 0, Step: 27, Rank: 21, loss = 0.69140625
c619-022: Epoch: 0, Step: 27, Rank: 17, loss = 0.69140625
c621-152: Epoch: 0, Step: 27, Rank: 41, loss = 0.69140625
c622-022: Epoch: 0, Step: 27, Rank: 47, loss = 0.69140625
c613-112: Epoch: 0, Step: 27, Rank: 3, loss = 0.69140625
c621-071: Epoch: 0, Step: 27, Rank: 24, loss = 0.69140625
c622-001: Epoch: 0, Step: 27, Rank: 42, loss = 0.69140625
c613-142: Epoch: 0, Step: 27, Rank: 9, loss = 0.69140625
c619-012: Epoch: 0, Step: 27, Rank: 15, loss = 0.69140625
c621-062: Epoch: 0, Step: 27, Rank: 23, loss = 0.69140625
c619-011: Epoch: 0, Step: 27, Rank: 14, loss = 0.69140625
c622-061: Epoch: 0, Step: 27, Rank: 54, loss = 0.69140625
c622-092: Epoch: 0, Step: 27, Rank: 61, loss = 0.69140625
c622-101: Epoch: 0, Step: 27, Rank: 62, loss = 0.69140625
c622-071: Epoch: 0, Step: 27, Rank: 56, loss = 0.69140625
c613-141: Epoch: 0, Step: 27, Rank: 8, loss = 0.69140625
c621-061: Epoch: 0, Step: 27, Rank: 22, loss = 0.69140625
c613-122: Epoch: 0, Step: 27, Rank: 5, loss = 0.69140625
c622-072: Epoch: 0, Step: 27, Rank: 57, loss = 0.69140625
c622-102: Epoch: 0, Step: 27, Rank: 63, loss = 0.69140625
c622-082: Epoch: 0, Step: 27, Rank: 59, loss = 0.69140625
c613-152: Epoch: 0, Step: 27, Rank: 11, loss = 0.69140625
c621-141: Epoch: 0, Step: 27, Rank: 38, loss = 0.69140625
c621-131: Epoch: 0, Step: 27, Rank: 36, loss = 0.69140625
c622-021: Epoch: 0, Step: 27, Rank: 46, loss = 0.69140625
c621-112: Epoch: 0, Step: 27, Rank: 33, loss = 0.69140625
c621-122: Epoch: 0, Step: 27, Rank: 35, loss = 0.69140625
c622-091: Epoch: 0, Step: 27, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.755859375 | max allocated: 11957.2314453125 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 28, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 28, Rank: 43, loss = 0.69140625
c613-132: Epoch: 0, Step: 28, Rank: 7, loss = 0.69140625
c619-001: Epoch: 0, Step: 28, Rank: 12, loss = 0.69140625
c619-021: Epoch: 0, Step: 28, Rank: 16, loss = 0.69140625
c613-151: Epoch: 0, Step: 28, Rank: 10, loss = 0.69140625
c622-081: Epoch: 0, Step: 28, Rank: 58, loss = 0.69140625
c613-131: Epoch: 0, Step: 28, Rank: 6, loss = 0.69140625
c613-141: Epoch: 0, Step: 28, Rank: 8, loss = 0.69140625
c613-142: Epoch: 0, Step: 28, Rank: 9, loss = 0.69140625
c619-002: Epoch: 0, Step: 28, Rank: 13, loss = 0.69140625
c619-031: Epoch: 0, Step: 28, Rank: 18, loss = 0.69140625
c613-152: Epoch: 0, Step: 28, Rank: 11, loss = 0.69140625
c613-111: Epoch: 0, Step: 28, Rank: 2, loss = 0.69140625
c621-111: Epoch: 0, Step: 28, Rank: 32, loss = 0.69140625
c621-132: Epoch: 0, Step: 28, Rank: 37, loss = 0.69140625
c619-011: Epoch: 0, Step: 28, Rank: 14, loss = 0.69140625
c621-061: Epoch: 0, Step: 28, Rank: 22, loss = 0.69140625
c619-012: Epoch: 0, Step: 28, Rank: 15, loss = 0.69140625
c622-102: Epoch: 0, Step: 28, Rank: 63, loss = 0.69140625
c622-092: Epoch: 0, Step: 28, Rank: 61, loss = 0.69140625
c622-052: Epoch: 0, Step: 28, Rank: 53, loss = 0.69140625
c621-062: Epoch: 0, Step: 28, Rank: 23, loss = 0.69140625
c622-071: Epoch: 0, Step: 28, Rank: 56, loss = 0.69140625
c622-101: Epoch: 0, Step: 28, Rank: 62, loss = 0.69140625
c613-122: Epoch: 0, Step: 28, Rank: 5, loss = 0.69140625
c622-001: Epoch: 0, Step: 28, Rank: 42, loss = 0.69140625
c613-102: Epoch: 0, Step: 28, Rank: 1, loss = 0.69140625
c621-052: Epoch: 0, Step: 28, Rank: 21, loss = 0.69140625
c621-121: Epoch: 0, Step: 28, Rank: 34, loss = 0.69140625
c622-051: Epoch: 0, Step: 28, Rank: 52, loss = 0.69140625
c619-032: Epoch: 0, Step: 28, Rank: 19, loss = 0.69140625
c622-012: Epoch: 0, Step: 28, Rank: 45, loss = 0.69140625
c619-041: Epoch: 0, Step: 28, Rank: 20, loss = 0.69140625
c621-151: Epoch: 0, Step: 28, Rank: 40, loss = 0.69140625
c621-142: Epoch: 0, Step: 28, Rank: 39, loss = 0.69140625
c621-131: Epoch: 0, Step: 28, Rank: 36, loss = 0.69140625
c621-141: Epoch: 0, Step: 28, Rank: 38, loss = 0.69140625
c621-112: Epoch: 0, Step: 28, Rank: 33, loss = 0.69140625
c622-041: Epoch: 0, Step: 28, Rank: 50, loss = 0.69140625
c622-062: Epoch: 0, Step: 28, Rank: 55, loss = 0.69140625
c619-022: Epoch: 0, Step: 28, Rank: 17, loss = 0.69140625
c622-011: Epoch: 0, Step: 28, Rank: 44, loss = 0.69140625
c622-082: Epoch: 0, Step: 28, Rank: 59, loss = 0.69140625
c613-121: Epoch: 0, Step: 28, Rank: 4, loss = 0.69140625
c621-071: Epoch: 0, Step: 28, Rank: 24, loss = 0.69140625
c622-031: Epoch: 0, Step: 28, Rank: 48, loss = 0.69140625
c621-152: Epoch: 0, Step: 28, Rank: 41, loss = 0.69140625
c613-112: Epoch: 0, Step: 28, Rank: 3, loss = 0.69140625
c621-081: Epoch: 0, Step: 28, Rank: 26, loss = 0.69140625
c621-092: Epoch: 0, Step: 28, Rank: 29, loss = 0.69140625
c622-061: Epoch: 0, Step: 28, Rank: 54, loss = 0.69140625
c622-032: Epoch: 0, Step: 28, Rank: 49, loss = 0.69140625
c621-122: Epoch: 0, Step: 28, Rank: 35, loss = 0.69140625
c622-072: Epoch: 0, Step: 28, Rank: 57, loss = 0.69140625
c622-042: Epoch: 0, Step: 28, Rank: 51, loss = 0.69140625
c621-082: Epoch: 0, Step: 28, Rank: 27, loss = 0.69140625
c622-022: Epoch: 0, Step: 28, Rank: 47, loss = 0.69140625
c622-021: Epoch: 0, Step: 28, Rank: 46, loss = 0.69140625
c621-101: Epoch: 0, Step: 28, Rank: 30, loss = 0.69140625
c621-072: Epoch: 0, Step: 28, Rank: 25, loss = 0.69140625
c621-102: Epoch: 0, Step: 28, Rank: 31, loss = 0.69140625
c622-091: Epoch: 0, Step: 28, Rank: 60, loss = 0.69140625
c621-091: Epoch: 0, Step: 28, Rank: 28, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 29, Rank: 32, loss = 0.69140625
c622-081: Epoch: 0, Step: 29, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 29, Rank: 53, loss = 0.69140625
c613-101: Epoch: 0, Step: 29, Rank: 0, loss = 0.69140625
c621-091: Epoch: 0, Step: 29, Rank: 28, loss = 0.69140625
c621-151: Epoch: 0, Step: 29, Rank: 40, loss = 0.69140625
c622-051: Epoch: 0, Step: 29, Rank: 52, loss = 0.69140625
c621-081: Epoch: 0, Step: 29, Rank: 26, loss = 0.69140625
c619-021: Epoch: 0, Step: 29, Rank: 16, loss = 0.69140625
c622-002: Epoch: 0, Step: 29, Rank: 43, loss = 0.69140625
c613-132: Epoch: 0, Step: 29, Rank: 7, loss = 0.69140625
c613-111: Epoch: 0, Step: 29, Rank: 2, loss = 0.69140625
c619-022: Epoch: 0, Step: 29, Rank: 17, loss = 0.69140625
c619-031: Epoch: 0, Step: 29, Rank: 18, loss = 0.69140625
c622-061: Epoch: 0, Step: 29, Rank: 54, loss = 0.69140625
c613-112: Epoch: 0, Step: 29, Rank: 3, loss = 0.69140625
c621-132: Epoch: 0, Step: 29, Rank: 37, loss = 0.69140625
c613-122: Epoch: 0, Step: 29, Rank: 5, loss = 0.69140625
c613-121: Epoch: 0, Step: 29, Rank: 4, loss = 0.69140625
c621-082: Epoch: 0, Step: 29, Rank: 27, loss = 0.69140625
c622-041: Epoch: 0, Step: 29, Rank: 50, loss = 0.69140625
c621-122: Epoch: 0, Step: 29, Rank: 35, loss = 0.69140625
c621-072: Epoch: 0, Step: 29, Rank: 25, loss = 0.69140625
c622-102: Epoch: 0, Step: 29, Rank: 63, loss = 0.69140625
c622-012: Epoch: 0, Step: 29, Rank: 45, loss = 0.69140625
c622-071: Epoch: 0, Step: 29, Rank: 56, loss = 0.69140625
c619-002: Epoch: 0, Step: 29, Rank: 13, loss = 0.69140625
c621-142: Epoch: 0, Step: 29, Rank: 39, loss = 0.69140625
c621-152: Epoch: 0, Step: 29, Rank: 41, loss = 0.69140625
c619-041: Epoch: 0, Step: 29, Rank: 20, loss = 0.69140625
c621-141: Epoch: 0, Step: 29, Rank: 38, loss = 0.69140625
c613-151: Epoch: 0, Step: 29, Rank: 10, loss = 0.69140625
c613-141: Epoch: 0, Step: 29, Rank: 8, loss = 0.69140625
c622-011: Epoch: 0, Step: 29, Rank: 44, loss = 0.69140625
c621-121: Epoch: 0, Step: 29, Rank: 34, loss = 0.69140625
c619-001: Epoch: 0, Step: 29, Rank: 12, loss = 0.69140625
c621-092: Epoch: 0, Step: 29, Rank: 29, loss = 0.69140625
c613-131: Epoch: 0, Step: 29, Rank: 6, loss = 0.69140625
c622-031: Epoch: 0, Step: 29, Rank: 48, loss = 0.69140625
c619-032: Epoch: 0, Step: 29, Rank: 19, loss = 0.69140625
c622-072: Epoch: 0, Step: 29, Rank: 57, loss = 0.69140625
c621-112: Epoch: 0, Step: 29, Rank: 33, loss = 0.69140625
c621-101: Epoch: 0, Step: 29, Rank: 30, loss = 0.69140625
c622-022: Epoch: 0, Step: 29, Rank: 47, loss = 0.69140625
c622-092: Epoch: 0, Step: 29, Rank: 61, loss = 0.69140625
c619-012: Epoch: 0, Step: 29, Rank: 15, loss = 0.69140625
c621-052: Epoch: 0, Step: 29, Rank: 21, loss = 0.69140625
c613-102: Epoch: 0, Step: 29, Rank: 1, loss = 0.69140625
c621-062: Epoch: 0, Step: 29, Rank: 23, loss = 0.69140625
c613-142: Epoch: 0, Step: 29, Rank: 9, loss = 0.69140625
c622-091: Epoch: 0, Step: 29, Rank: 60, loss = 0.69140625
c622-082: Epoch: 0, Step: 29, Rank: 59, loss = 0.69140625
c622-001: Epoch: 0, Step: 29, Rank: 42, loss = 0.69140625
c622-062: Epoch: 0, Step: 29, Rank: 55, loss = 0.69140625
c621-102: Epoch: 0, Step: 29, Rank: 31, loss = 0.69140625
c622-032: Epoch: 0, Step: 29, Rank: 49, loss = 0.69140625
c622-101: Epoch: 0, Step: 29, Rank: 62, loss = 0.69140625
c619-011: Epoch: 0, Step: 29, Rank: 14, loss = 0.69140625
c621-131: Epoch: 0, Step: 29, Rank: 36, loss = 0.69140625
c613-152: Epoch: 0, Step: 29, Rank: 11, loss = 0.69140625
c621-071: Epoch: 0, Step: 29, Rank: 24, loss = 0.69140625
c622-042: Epoch: 0, Step: 29, Rank: 51, loss = 0.69140625
c621-061: Epoch: 0, Step: 29, Rank: 22, loss = 0.69140625
c622-021: Epoch: 0, Step: 29, Rank: 46, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-062: Epoch: 0, Step: 30, Rank: 55, loss = 0.69140625
c621-111: Epoch: 0, Step: 30, Rank: 32, loss = 0.69140625
c622-052: Epoch: 0, Step: 30, Rank: 53, loss = 0.69140625
c619-002: Epoch: 0, Step: 30, Rank: 13, loss = 0.69140625
c622-002: Epoch: 0, Step: 30, Rank: 43, loss = 0.69140625
c622-101: Epoch: 0, Step: 30, Rank: 62, loss = 0.69140625
c613-101: Epoch: 0, Step: 30, Rank: 0, loss = 0.69140625
c613-151: Epoch: 0, Step: 30, Rank: 10, loss = 0.69140625
c622-061: Epoch: 0, Step: 30, Rank: 54, loss = 0.69140625
c622-102: Epoch: 0, Step: 30, Rank: 63, loss = 0.69140625
c621-131: Epoch: 0, Step: 30, Rank: 36, loss = 0.69140625
c613-111: Epoch: 0, Step: 30, Rank: 2, loss = 0.69140625
c622-092: Epoch: 0, Step: 30, Rank: 61, loss = 0.69140625
c622-071: Epoch: 0, Step: 30, Rank: 56, loss = 0.69140625
c622-012: Epoch: 0, Step: 30, Rank: 45, loss = 0.69140625
c621-081: Epoch: 0, Step: 30, Rank: 26, loss = 0.69140625
c619-041: Epoch: 0, Step: 30, Rank: 20, loss = 0.69140625
c621-112: Epoch: 0, Step: 30, Rank: 33, loss = 0.69140625
c619-001: Epoch: 0, Step: 30, Rank: 12, loss = 0.69140625
c621-091: Epoch: 0, Step: 30, Rank: 28, loss = 0.69140625
c613-132: Epoch: 0, Step: 30, Rank: 7, loss = 0.69140625
c619-021: Epoch: 0, Step: 30, Rank: 16, loss = 0.69140625
c622-081: Epoch: 0, Step: 30, Rank: 58, loss = 0.69140625
c622-001: Epoch: 0, Step: 30, Rank: 42, loss = 0.69140625
c621-052: Epoch: 0, Step: 30, Rank: 21, loss = 0.69140625
c621-132: Epoch: 0, Step: 30, Rank: 37, loss = 0.69140625
c621-121: Epoch: 0, Step: 30, Rank: 34, loss = 0.69140625
c613-152: Epoch: 0, Step: 30, Rank: 11, loss = 0.69140625
c622-022: Epoch: 0, Step: 30, Rank: 47, loss = 0.69140625
c622-091: Epoch: 0, Step: 30, Rank: 60, loss = 0.69140625
c613-102: Epoch: 0, Step: 30, Rank: 1, loss = 0.69140625
c622-041: Epoch: 0, Step: 30, Rank: 50, loss = 0.69140625
c622-011: Epoch: 0, Step: 30, Rank: 44, loss = 0.69140625
c619-022: Epoch: 0, Step: 30, Rank: 17, loss = 0.69140625
c613-121: Epoch: 0, Step: 30, Rank: 4, loss = 0.69140625
c619-032: Epoch: 0, Step: 30, Rank: 19, loss = 0.69140625
c621-101: Epoch: 0, Step: 30, Rank: 30, loss = 0.69140625
c619-031: Epoch: 0, Step: 30, Rank: 18, loss = 0.69140625
c621-082: Epoch: 0, Step: 30, Rank: 27, loss = 0.69140625
c622-051: Epoch: 0, Step: 30, Rank: 52, loss = 0.69140625
c621-142: Epoch: 0, Step: 30, Rank: 39, loss = 0.69140625
c622-082: Epoch: 0, Step: 30, Rank: 59, loss = 0.69140625
c621-151: Epoch: 0, Step: 30, Rank: 40, loss = 0.69140625
c619-011: Epoch: 0, Step: 30, Rank: 14, loss = 0.69140625
c621-122: Epoch: 0, Step: 30, Rank: 35, loss = 0.69140625
c621-102: Epoch: 0, Step: 30, Rank: 31, loss = 0.69140625
c621-062: Epoch: 0, Step: 30, Rank: 23, loss = 0.69140625
c622-031: Epoch: 0, Step: 30, Rank: 48, loss = 0.69140625
c621-092: Epoch: 0, Step: 30, Rank: 29, loss = 0.69140625
c613-141: Epoch: 0, Step: 30, Rank: 8, loss = 0.69140625
c619-012: Epoch: 0, Step: 30, Rank: 15, loss = 0.69140625
c622-021: Epoch: 0, Step: 30, Rank: 46, loss = 0.69140625
c622-072: Epoch: 0, Step: 30, Rank: 57, loss = 0.69140625
c622-042: Epoch: 0, Step: 30, Rank: 51, loss = 0.69140625
c622-032: Epoch: 0, Step: 30, Rank: 49, loss = 0.69140625
c621-152: Epoch: 0, Step: 30, Rank: 41, loss = 0.69140625
c613-131: Epoch: 0, Step: 30, Rank: 6, loss = 0.69140625
c621-141: Epoch: 0, Step: 30, Rank: 38, loss = 0.69140625
c613-112: Epoch: 0, Step: 30, Rank: 3, loss = 0.69140625
c621-061: Epoch: 0, Step: 30, Rank: 22, loss = 0.69140625
c621-072: Epoch: 0, Step: 30, Rank: 25, loss = 0.69140625
c613-142: Epoch: 0, Step: 30, Rank: 9, loss = 0.69140625
c621-071: Epoch: 0, Step: 30, Rank: 24, loss = 0.69140625
c613-122: Epoch: 0, Step: 30, Rank: 5, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 31, Rank: 43, loss = 0.69140625
c622-052: Epoch: 0, Step: 31, Rank: 53, loss = 0.69140625
c619-021: Epoch: 0, Step: 31, Rank: 16, loss = 0.69140625
c622-012: Epoch: 0, Step: 31, Rank: 45, loss = 0.69140625
c621-142: Epoch: 0, Step: 31, Rank: 39, loss = 0.69140625
c621-131: Epoch: 0, Step: 31, Rank: 36, loss = 0.69140625
c622-001: Epoch: 0, Step: 31, Rank: 42, loss = 0.69140625
c619-001: Epoch: 0, Step: 31, Rank: 12, loss = 0.69140625
c622-041: Epoch: 0, Step: 31, Rank: 50, loss = 0.69140625
c622-031: Epoch: 0, Step: 31, Rank: 48, loss = 0.69140625
c621-081: Epoch: 0, Step: 31, Rank: 26, loss = 0.69140625
c621-111: Epoch: 0, Step: 31, Rank: 32, loss = 0.69140625
c619-031: Epoch: 0, Step: 31, Rank: 18, loss = 0.69140625
c621-132: Epoch: 0, Step: 31, Rank: 37, loss = 0.69140625
c621-151: Epoch: 0, Step: 31, Rank: 40, loss = 0.69140625
c622-011: Epoch: 0, Step: 31, Rank: 44, loss = 0.69140625
c621-082: Epoch: 0, Step: 31, Rank: 27, loss = 0.69140625
c622-022: Epoch: 0, Step: 31, Rank: 47, loss = 0.69140625
c621-072: Epoch: 0, Step: 31, Rank: 25, loss = 0.69140625
c621-121: Epoch: 0, Step: 31, Rank: 34, loss = 0.69140625
c613-101: Epoch: 0, Step: 31, Rank: 0, loss = 0.69140625
c619-002: Epoch: 0, Step: 31, Rank: 13, loss = 0.69140625
c622-071: Epoch: 0, Step: 31, Rank: 56, loss = 0.69140625
c621-112: Epoch: 0, Step: 31, Rank: 33, loss = 0.69140625
c622-042: Epoch: 0, Step: 31, Rank: 51, loss = 0.69140625
c621-071: Epoch: 0, Step: 31, Rank: 24, loss = 0.69140625
c622-021: Epoch: 0, Step: 31, Rank: 46, loss = 0.69140625
c613-142: Epoch: 0, Step: 31, Rank: 9, loss = 0.69140625
c613-152: Epoch: 0, Step: 31, Rank: 11, loss = 0.69140625
c622-032: Epoch: 0, Step: 31, Rank: 49, loss = 0.69140625
c621-141: Epoch: 0, Step: 31, Rank: 38, loss = 0.69140625
c621-152: Epoch: 0, Step: 31, Rank: 41, loss = 0.69140625
c619-011: Epoch: 0, Step: 31, Rank: 14, loss = 0.69140625
c621-091: Epoch: 0, Step: 31, Rank: 28, loss = 0.69140625
c622-081: Epoch: 0, Step: 31, Rank: 58, loss = 0.69140625
c621-102: Epoch: 0, Step: 31, Rank: 31, loss = 0.69140625
c621-061: Epoch: 0, Step: 31, Rank: 22, loss = 0.69140625
c621-092: Epoch: 0, Step: 31, Rank: 29, loss = 0.69140625
c619-032: Epoch: 0, Step: 31, Rank: 19, loss = 0.69140625
c622-051: Epoch: 0, Step: 31, Rank: 52, loss = 0.69140625
c613-151: Epoch: 0, Step: 31, Rank: 10, loss = 0.69140625
c621-101: Epoch: 0, Step: 31, Rank: 30, loss = 0.69140625
c619-012: Epoch: 0, Step: 31, Rank: 15, loss = 0.69140625
c622-061: Epoch: 0, Step: 31, Rank: 54, loss = 0.69140625
c613-132: Epoch: 0, Step: 31, Rank: 7, loss = 0.69140625
c619-022: Epoch: 0, Step: 31, Rank: 17, loss = 0.69140625
c613-141: Epoch: 0, Step: 31, Rank: 8, loss = 0.69140625
c621-052: Epoch: 0, Step: 31, Rank: 21, loss = 0.69140625
c613-112: Epoch: 0, Step: 31, Rank: 3, loss = 0.69140625
c621-122: Epoch: 0, Step: 31, Rank: 35, loss = 0.69140625
c621-062: Epoch: 0, Step: 31, Rank: 23, loss = 0.69140625
c622-101: Epoch: 0, Step: 31, Rank: 62, loss = 0.69140625
c613-121: Epoch: 0, Step: 31, Rank: 4, loss = 0.69140625
c622-102: Epoch: 0, Step: 31, Rank: 63, loss = 0.69140625
c613-111: Epoch: 0, Step: 31, Rank: 2, loss = 0.69140625
c619-041: Epoch: 0, Step: 31, Rank: 20, loss = 0.69140625
c622-072: Epoch: 0, Step: 31, Rank: 57, loss = 0.69140625
c622-062: Epoch: 0, Step: 31, Rank: 55, loss = 0.69140625
c613-131: Epoch: 0, Step: 31, Rank: 6, loss = 0.69140625
c622-092: Epoch: 0, Step: 31, Rank: 61, loss = 0.69140625
c613-102: Epoch: 0, Step: 31, Rank: 1, loss = 0.69140625
c613-122: Epoch: 0, Step: 31, Rank: 5, loss = 0.69140625
c622-082: Epoch: 0, Step: 31, Rank: 59, loss = 0.69140625
c622-091: Epoch: 0, Step: 31, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24560546875 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 32, Rank: 0, loss = 0.6875
c619-031: Epoch: 0, Step: 32, Rank: 18, loss = 0.69921875
c619-021: Epoch: 0, Step: 32, Rank: 16, loss = 0.70703125
c621-052: Epoch: 0, Step: 32, Rank: 21, loss = 0.71875
c621-121: Epoch: 0, Step: 32, Rank: 34, loss = 0.69921875
c619-022: Epoch: 0, Step: 32, Rank: 17, loss = 0.69921875
c619-032: Epoch: 0, Step: 32, Rank: 19, loss = 0.69140625
c622-052: Epoch: 0, Step: 32, Rank: 53, loss = 0.6875
c621-111: Epoch: 0, Step: 32, Rank: 32, loss = 0.6953125
c622-071: Epoch: 0, Step: 32, Rank: 56, loss = 0.6875
c619-012: Epoch: 0, Step: 32, Rank: 15, loss = 0.6796875
c621-132: Epoch: 0, Step: 32, Rank: 37, loss = 0.66796875
c621-142: Epoch: 0, Step: 32, Rank: 39, loss = 0.6796875
c621-112: Epoch: 0, Step: 32, Rank: 33, loss = 0.69140625
c622-062: Epoch: 0, Step: 32, Rank: 55, loss = 0.74609375
c621-061: Epoch: 0, Step: 32, Rank: 22, loss = 0.6796875
c621-072: Epoch: 0, Step: 32, Rank: 25, loss = 0.6875
c619-041: Epoch: 0, Step: 32, Rank: 20, loss = 0.69921875
c621-141: Epoch: 0, Step: 32, Rank: 38, loss = 0.6796875
c622-081: Epoch: 0, Step: 32, Rank: 58, loss = 0.7109375
c621-131: Epoch: 0, Step: 32, Rank: 36, loss = 0.6796875
c619-002: Epoch: 0, Step: 32, Rank: 13, loss = 0.66796875
c622-002: Epoch: 0, Step: 32, Rank: 43, loss = 0.71875
c621-062: Epoch: 0, Step: 32, Rank: 23, loss = 0.70703125
c613-111: Epoch: 0, Step: 32, Rank: 2, loss = 0.66015625
c621-081: Epoch: 0, Step: 32, Rank: 26, loss = 0.765625
c622-101: Epoch: 0, Step: 32, Rank: 62, loss = 0.6796875
c613-151: Epoch: 0, Step: 32, Rank: 10, loss = 0.67578125
c613-131: Epoch: 0, Step: 32, Rank: 6, loss = 0.69140625
c622-041: Epoch: 0, Step: 32, Rank: 50, loss = 0.66796875
c621-151: Epoch: 0, Step: 32, Rank: 40, loss = 0.69140625
c619-011: Epoch: 0, Step: 32, Rank: 14, loss = 0.66796875
c613-102: Epoch: 0, Step: 32, Rank: 1, loss = 0.69140625
c622-031: Epoch: 0, Step: 32, Rank: 48, loss = 0.67578125
c621-122: Epoch: 0, Step: 32, Rank: 35, loss = 0.70703125
c622-051: Epoch: 0, Step: 32, Rank: 52, loss = 0.66796875
c621-101: Epoch: 0, Step: 32, Rank: 30, loss = 0.75
c613-122: Epoch: 0, Step: 32, Rank: 5, loss = 0.6796875
c622-061: Epoch: 0, Step: 32, Rank: 54, loss = 0.6875
c622-032: Epoch: 0, Step: 32, Rank: 49, loss = 0.69140625
c613-132: Epoch: 0, Step: 32, Rank: 7, loss = 0.6640625
c622-011: Epoch: 0, Step: 32, Rank: 44, loss = 0.64453125
c622-042: Epoch: 0, Step: 32, Rank: 51, loss = 0.6796875
c613-121: Epoch: 0, Step: 32, Rank: 4, loss = 0.69140625
c622-072: Epoch: 0, Step: 32, Rank: 57, loss = 0.64453125
c621-102: Epoch: 0, Step: 32, Rank: 31, loss = 0.64453125
c621-092: Epoch: 0, Step: 32, Rank: 29, loss = 0.74609375
c622-012: Epoch: 0, Step: 32, Rank: 45, loss = 0.6796875
c613-112: Epoch: 0, Step: 32, Rank: 3, loss = 1.0390625
c613-152: Epoch: 0, Step: 32, Rank: 11, loss = 0.71875
c621-091: Epoch: 0, Step: 32, Rank: 28, loss = 0.66796875
c622-102: Epoch: 0, Step: 32, Rank: 63, loss = 0.72265625
c621-152: Epoch: 0, Step: 32, Rank: 41, loss = 0.70703125
c622-021: Epoch: 0, Step: 32, Rank: 46, loss = 0.69921875
c621-071: Epoch: 0, Step: 32, Rank: 24, loss = 0.64453125
c613-141: Epoch: 0, Step: 32, Rank: 8, loss = 0.70703125
c613-142: Epoch: 0, Step: 32, Rank: 9, loss = 0.66796875
c622-092: Epoch: 0, Step: 32, Rank: 61, loss = 0.69921875
c622-022: Epoch: 0, Step: 32, Rank: 47, loss = 0.59765625
c619-001: Epoch: 0, Step: 32, Rank: 12, loss = 0.6796875
c622-082: Epoch: 0, Step: 32, Rank: 59, loss = 0.65625
c622-091: Epoch: 0, Step: 32, Rank: 60, loss = 0.71875
c622-001: Epoch: 0, Step: 32, Rank: 42, loss = 0.69921875
c621-082: Epoch: 0, Step: 32, Rank: 27, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c621-072: Epoch: 0, Step: 33, Rank: 25, loss = 0.69140625
c621-091: Epoch: 0, Step: 33, Rank: 28, loss = 0.71875
c621-081: Epoch: 0, Step: 33, Rank: 26, loss = 0.66796875
c613-101: Epoch: 0, Step: 33, Rank: 0, loss = 0.6796875
c622-002: Epoch: 0, Step: 33, Rank: 43, loss = 0.69140625
c621-132: Epoch: 0, Step: 33, Rank: 37, loss = 0.76953125
c621-082: Epoch: 0, Step: 33, Rank: 27, loss = 0.7578125
c622-012: Epoch: 0, Step: 33, Rank: 45, loss = 0.69921875
c621-121: Epoch: 0, Step: 33, Rank: 34, loss = 0.71875
c621-061: Epoch: 0, Step: 33, Rank: 22, loss = 0.69140625
c621-111: Epoch: 0, Step: 33, Rank: 32, loss = 0.6796875
c619-021: Epoch: 0, Step: 33, Rank: 16, loss = 0.66796875
c621-142: Epoch: 0, Step: 33, Rank: 39, loss = 0.703125
c622-062: Epoch: 0, Step: 33, Rank: 55, loss = 0.71875
c621-122: Epoch: 0, Step: 33, Rank: 35, loss = 0.69140625
c621-112: Epoch: 0, Step: 33, Rank: 33, loss = 0.64453125
c621-131: Epoch: 0, Step: 33, Rank: 36, loss = 0.64453125
c621-071: Epoch: 0, Step: 33, Rank: 24, loss = 0.69140625
c622-052: Epoch: 0, Step: 33, Rank: 53, loss = 0.6875
c622-001: Epoch: 0, Step: 33, Rank: 42, loss = 0.68359375
c621-152: Epoch: 0, Step: 33, Rank: 41, loss = 0.71875
c613-152: Epoch: 0, Step: 33, Rank: 11, loss = 0.69140625
c622-101: Epoch: 0, Step: 33, Rank: 62, loss = 0.66796875
c622-061: Epoch: 0, Step: 33, Rank: 54, loss = 0.703125
c622-041: Epoch: 0, Step: 33, Rank: 50, loss = 0.67578125
c619-031: Epoch: 0, Step: 33, Rank: 18, loss = 0.71875
c621-092: Epoch: 0, Step: 33, Rank: 29, loss = 0.70703125
c619-001: Epoch: 0, Step: 33, Rank: 12, loss = 0.62109375
c621-062: Epoch: 0, Step: 33, Rank: 23, loss = 0.69140625
c622-042: Epoch: 0, Step: 33, Rank: 51, loss = 0.6875
c622-011: Epoch: 0, Step: 33, Rank: 44, loss = 0.69140625
c621-102: Epoch: 0, Step: 33, Rank: 31, loss = 0.66796875
c613-111: Epoch: 0, Step: 33, Rank: 2, loss = 0.71484375
c621-141: Epoch: 0, Step: 33, Rank: 38, loss = 0.6875
c613-131: Epoch: 0, Step: 33, Rank: 6, loss = 0.69140625
c621-151: Epoch: 0, Step: 33, Rank: 40, loss = 0.69140625
c613-102: Epoch: 0, Step: 33, Rank: 1, loss = 0.6875
c622-102: Epoch: 0, Step: 33, Rank: 63, loss = 0.69921875
c619-012: Epoch: 0, Step: 33, Rank: 15, loss = 0.71875
c621-101: Epoch: 0, Step: 33, Rank: 30, loss = 0.6875
c622-031: Epoch: 0, Step: 33, Rank: 48, loss = 0.69140625
c622-081: Epoch: 0, Step: 33, Rank: 58, loss = 0.69140625
c621-052: Epoch: 0, Step: 33, Rank: 21, loss = 0.69140625
c613-112: Epoch: 0, Step: 33, Rank: 3, loss = 0.66796875
c613-132: Epoch: 0, Step: 33, Rank: 7, loss = 0.70703125
c622-021: Epoch: 0, Step: 33, Rank: 46, loss = 0.64453125
c613-151: Epoch: 0, Step: 33, Rank: 10, loss = 0.70703125
c619-002: Epoch: 0, Step: 33, Rank: 13, loss = 0.6796875
c619-022: Epoch: 0, Step: 33, Rank: 17, loss = 0.64453125
c619-041: Epoch: 0, Step: 33, Rank: 20, loss = 0.69140625
c622-092: Epoch: 0, Step: 33, Rank: 61, loss = 0.6328125
c613-122: Epoch: 0, Step: 33, Rank: 5, loss = 0.68359375
c622-051: Epoch: 0, Step: 33, Rank: 52, loss = 0.70703125
c622-071: Epoch: 0, Step: 33, Rank: 56, loss = 0.69921875
c619-032: Epoch: 0, Step: 33, Rank: 19, loss = 0.70703125
c622-082: Epoch: 0, Step: 33, Rank: 59, loss = 0.69140625
c613-141: Epoch: 0, Step: 33, Rank: 8, loss = 0.65625
c622-032: Epoch: 0, Step: 33, Rank: 49, loss = 0.69140625
c613-121: Epoch: 0, Step: 33, Rank: 4, loss = 0.69140625
c619-011: Epoch: 0, Step: 33, Rank: 14, loss = 0.69140625
c613-142: Epoch: 0, Step: 33, Rank: 9, loss = 0.6640625
c622-022: Epoch: 0, Step: 33, Rank: 47, loss = 0.69921875
c622-072: Epoch: 0, Step: 33, Rank: 57, loss = 0.6875
c622-091: Epoch: 0, Step: 33, Rank: 60, loss = 0.65625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 34, Rank: 43, loss = 0.69140625
c613-101: Epoch: 0, Step: 34, Rank: 0, loss = 0.62109375
c619-011: Epoch: 0, Step: 34, Rank: 14, loss = 0.671875
c619-002: Epoch: 0, Step: 34, Rank: 13, loss = 0.70703125
c622-081: Epoch: 0, Step: 34, Rank: 58, loss = 0.68359375
c619-001: Epoch: 0, Step: 34, Rank: 12, loss = 0.64453125
c621-091: Epoch: 0, Step: 34, Rank: 28, loss = 0.6953125
c621-111: Epoch: 0, Step: 34, Rank: 32, loss = 0.68359375
c619-021: Epoch: 0, Step: 34, Rank: 16, loss = 0.671875
c621-081: Epoch: 0, Step: 34, Rank: 26, loss = 0.69140625
c622-092: Epoch: 0, Step: 34, Rank: 61, loss = 0.59375
c622-052: Epoch: 0, Step: 34, Rank: 53, loss = 0.64453125
c622-001: Epoch: 0, Step: 34, Rank: 42, loss = 0.65625
c622-012: Epoch: 0, Step: 34, Rank: 45, loss = 0.6953125
c613-111: Epoch: 0, Step: 34, Rank: 2, loss = 0.71875
c621-151: Epoch: 0, Step: 34, Rank: 40, loss = 0.6328125
c619-031: Epoch: 0, Step: 34, Rank: 18, loss = 0.69140625
c613-151: Epoch: 0, Step: 34, Rank: 10, loss = 0.71875
c619-041: Epoch: 0, Step: 34, Rank: 20, loss = 0.69140625
c621-072: Epoch: 0, Step: 34, Rank: 25, loss = 0.74609375
c613-132: Epoch: 0, Step: 34, Rank: 7, loss = 0.66015625
c619-012: Epoch: 0, Step: 34, Rank: 15, loss = 0.6796875
c621-052: Epoch: 0, Step: 34, Rank: 21, loss = 0.69140625
c613-152: Epoch: 0, Step: 34, Rank: 11, loss = 0.69140625
c622-102: Epoch: 0, Step: 34, Rank: 63, loss = 0.796875
c621-082: Epoch: 0, Step: 34, Rank: 27, loss = 0.6796875
c621-101: Epoch: 0, Step: 34, Rank: 30, loss = 0.68359375
c622-011: Epoch: 0, Step: 34, Rank: 44, loss = 0.71875
c622-101: Epoch: 0, Step: 34, Rank: 62, loss = 0.6875
c621-142: Epoch: 0, Step: 34, Rank: 39, loss = 0.66796875
c613-131: Epoch: 0, Step: 34, Rank: 6, loss = 0.70703125
c621-132: Epoch: 0, Step: 34, Rank: 37, loss = 0.65234375
c621-112: Epoch: 0, Step: 34, Rank: 33, loss = 0.69140625
c613-141: Epoch: 0, Step: 34, Rank: 8, loss = 0.71875
c621-121: Epoch: 0, Step: 34, Rank: 34, loss = 0.69140625
c621-092: Epoch: 0, Step: 34, Rank: 29, loss = 0.71875
c613-122: Epoch: 0, Step: 34, Rank: 5, loss = 0.69140625
c613-102: Epoch: 0, Step: 34, Rank: 1, loss = 0.66796875
c622-071: Epoch: 0, Step: 34, Rank: 56, loss = 0.66796875
c622-062: Epoch: 0, Step: 34, Rank: 55, loss = 0.69140625
c622-042: Epoch: 0, Step: 34, Rank: 51, loss = 0.79296875
c613-121: Epoch: 0, Step: 34, Rank: 4, loss = 0.796875
c622-022: Epoch: 0, Step: 34, Rank: 47, loss = 0.6953125
c622-031: Epoch: 0, Step: 34, Rank: 48, loss = 0.6875
c619-032: Epoch: 0, Step: 34, Rank: 19, loss = 0.69140625
c621-061: Epoch: 0, Step: 34, Rank: 22, loss = 0.66796875
c619-022: Epoch: 0, Step: 34, Rank: 17, loss = 0.69140625
c622-032: Epoch: 0, Step: 34, Rank: 49, loss = 0.69140625
c622-091: Epoch: 0, Step: 34, Rank: 60, loss = 0.70703125
c622-061: Epoch: 0, Step: 34, Rank: 54, loss = 0.6796875
c621-152: Epoch: 0, Step: 34, Rank: 41, loss = 0.6796875
c621-071: Epoch: 0, Step: 34, Rank: 24, loss = 0.6796875
c621-131: Epoch: 0, Step: 34, Rank: 36, loss = 0.796875
c622-051: Epoch: 0, Step: 34, Rank: 52, loss = 0.71875
c622-021: Epoch: 0, Step: 34, Rank: 46, loss = 0.69140625
c613-112: Epoch: 0, Step: 34, Rank: 3, loss = 0.8046875
c621-141: Epoch: 0, Step: 34, Rank: 38, loss = 0.69140625
c622-041: Epoch: 0, Step: 34, Rank: 50, loss = 0.68359375
c613-142: Epoch: 0, Step: 34, Rank: 9, loss = 0.67578125
c622-082: Epoch: 0, Step: 34, Rank: 59, loss = 0.6953125
c621-102: Epoch: 0, Step: 34, Rank: 31, loss = 0.66796875
c621-122: Epoch: 0, Step: 34, Rank: 35, loss = 0.69140625
c622-072: Epoch: 0, Step: 34, Rank: 57, loss = 0.70703125
c621-062: Epoch: 0, Step: 34, Rank: 23, loss = 0.66796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 35, Rank: 58, loss = 0.71875
c613-101: Epoch: 0, Step: 35, Rank: 0, loss = 0.69140625
c619-021: Epoch: 0, Step: 35, Rank: 16, loss = 0.69140625
c622-052: Epoch: 0, Step: 35, Rank: 53, loss = 0.6875
c622-012: Epoch: 0, Step: 35, Rank: 45, loss = 0.70703125
c622-002: Epoch: 0, Step: 35, Rank: 43, loss = 0.6328125
c613-142: Epoch: 0, Step: 35, Rank: 9, loss = 0.70703125
c622-082: Epoch: 0, Step: 35, Rank: 59, loss = 0.71875
c622-101: Epoch: 0, Step: 35, Rank: 62, loss = 0.6796875
c619-002: Epoch: 0, Step: 35, Rank: 13, loss = 0.67578125
c613-121: Epoch: 0, Step: 35, Rank: 4, loss = 0.6796875
c619-001: Epoch: 0, Step: 35, Rank: 12, loss = 0.69140625
c613-111: Epoch: 0, Step: 35, Rank: 2, loss = 0.69921875
c621-152: Epoch: 0, Step: 35, Rank: 41, loss = 0.70703125
c622-021: Epoch: 0, Step: 35, Rank: 46, loss = 0.69140625
c622-001: Epoch: 0, Step: 35, Rank: 42, loss = 0.69140625
c622-102: Epoch: 0, Step: 35, Rank: 63, loss = 0.70703125
c622-011: Epoch: 0, Step: 35, Rank: 44, loss = 0.70703125
c621-132: Epoch: 0, Step: 35, Rank: 37, loss = 0.7109375
c613-151: Epoch: 0, Step: 35, Rank: 10, loss = 0.703125
c622-032: Epoch: 0, Step: 35, Rank: 49, loss = 0.67578125
c619-012: Epoch: 0, Step: 35, Rank: 15, loss = 0.74609375
c621-151: Epoch: 0, Step: 35, Rank: 40, loss = 0.6953125
c621-142: Epoch: 0, Step: 35, Rank: 39, loss = 0.66015625
c621-121: Epoch: 0, Step: 35, Rank: 34, loss = 0.65625
c622-031: Epoch: 0, Step: 35, Rank: 48, loss = 0.68359375
c613-152: Epoch: 0, Step: 35, Rank: 11, loss = 0.70703125
c621-052: Epoch: 0, Step: 35, Rank: 21, loss = 0.74609375
c613-102: Epoch: 0, Step: 35, Rank: 1, loss = 0.69140625
c613-122: Epoch: 0, Step: 35, Rank: 5, loss = 0.69140625
c613-131: Epoch: 0, Step: 35, Rank: 6, loss = 0.65625
c622-042: Epoch: 0, Step: 35, Rank: 51, loss = 0.69140625
c622-022: Epoch: 0, Step: 35, Rank: 47, loss = 0.74609375
c619-032: Epoch: 0, Step: 35, Rank: 19, loss = 0.62109375
c613-112: Epoch: 0, Step: 35, Rank: 3, loss = 0.69921875
c619-022: Epoch: 0, Step: 35, Rank: 17, loss = 0.74609375
c613-132: Epoch: 0, Step: 35, Rank: 7, loss = 0.64453125
c621-131: Epoch: 0, Step: 35, Rank: 36, loss = 0.6953125
c622-092: Epoch: 0, Step: 35, Rank: 61, loss = 0.65625
c619-011: Epoch: 0, Step: 35, Rank: 14, loss = 0.70703125
c622-041: Epoch: 0, Step: 35, Rank: 50, loss = 0.6640625
c622-061: Epoch: 0, Step: 35, Rank: 54, loss = 0.69140625
c622-051: Epoch: 0, Step: 35, Rank: 52, loss = 0.51171875
c621-081: Epoch: 0, Step: 35, Rank: 26, loss = 0.6875
c619-031: Epoch: 0, Step: 35, Rank: 18, loss = 0.76953125
c621-072: Epoch: 0, Step: 35, Rank: 25, loss = 0.69140625
c622-091: Epoch: 0, Step: 35, Rank: 60, loss = 0.70703125
c621-141: Epoch: 0, Step: 35, Rank: 38, loss = 0.6796875
c621-062: Epoch: 0, Step: 35, Rank: 23, loss = 0.71875
c619-041: Epoch: 0, Step: 35, Rank: 20, loss = 0.70703125
c621-111: Epoch: 0, Step: 35, Rank: 32, loss = 0.76953125
c621-101: Epoch: 0, Step: 35, Rank: 30, loss = 0.71875
c622-072: Epoch: 0, Step: 35, Rank: 57, loss = 0.6640625
c621-112: Epoch: 0, Step: 35, Rank: 33, loss = 0.70703125
c621-082: Epoch: 0, Step: 35, Rank: 27, loss = 0.7109375
c621-061: Epoch: 0, Step: 35, Rank: 22, loss = 0.6328125
c621-122: Epoch: 0, Step: 35, Rank: 35, loss = 0.5859375
c621-092: Epoch: 0, Step: 35, Rank: 29, loss = 0.69921875
c621-091: Epoch: 0, Step: 35, Rank: 28, loss = 0.65625
c622-071: Epoch: 0, Step: 35, Rank: 56, loss = 0.69140625
c613-141: Epoch: 0, Step: 35, Rank: 8, loss = 0.70703125
c621-071: Epoch: 0, Step: 35, Rank: 24, loss = 0.69140625
c622-062: Epoch: 0, Step: 35, Rank: 55, loss = 0.67578125
c621-102: Epoch: 0, Step: 35, Rank: 31, loss = 0.703125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 36, Rank: 0, loss = 0.6796875
c622-081: Epoch: 0, Step: 36, Rank: 58, loss = 0.69140625
c621-081: Epoch: 0, Step: 36, Rank: 26, loss = 0.65625
c613-111: Epoch: 0, Step: 36, Rank: 2, loss = 0.71875
c621-072: Epoch: 0, Step: 36, Rank: 25, loss = 0.69140625
c621-091: Epoch: 0, Step: 36, Rank: 28, loss = 0.6796875
c622-071: Epoch: 0, Step: 36, Rank: 56, loss = 0.73828125
c622-102: Epoch: 0, Step: 36, Rank: 63, loss = 0.69140625
c621-131: Epoch: 0, Step: 36, Rank: 36, loss = 0.703125
c619-001: Epoch: 0, Step: 36, Rank: 12, loss = 0.796875
c622-002: Epoch: 0, Step: 36, Rank: 43, loss = 0.71875
c619-021: Epoch: 0, Step: 36, Rank: 16, loss = 0.69140625
c622-092: Epoch: 0, Step: 36, Rank: 61, loss = 0.69140625
c621-151: Epoch: 0, Step: 36, Rank: 40, loss = 0.71875
c622-001: Epoch: 0, Step: 36, Rank: 42, loss = 0.6953125
c622-101: Epoch: 0, Step: 36, Rank: 62, loss = 0.69140625
c613-132: Epoch: 0, Step: 36, Rank: 7, loss = 0.6796875
c621-082: Epoch: 0, Step: 36, Rank: 27, loss = 0.69140625
c622-012: Epoch: 0, Step: 36, Rank: 45, loss = 0.66796875
c621-142: Epoch: 0, Step: 36, Rank: 39, loss = 0.796875
c622-011: Epoch: 0, Step: 36, Rank: 44, loss = 0.69140625
c613-102: Epoch: 0, Step: 36, Rank: 1, loss = 0.71875
c613-131: Epoch: 0, Step: 36, Rank: 6, loss = 0.69921875
c619-031: Epoch: 0, Step: 36, Rank: 18, loss = 0.69140625
c619-002: Epoch: 0, Step: 36, Rank: 13, loss = 0.6875
c622-031: Epoch: 0, Step: 36, Rank: 48, loss = 0.70703125
c613-152: Epoch: 0, Step: 36, Rank: 11, loss = 0.73046875
c619-012: Epoch: 0, Step: 36, Rank: 15, loss = 0.69140625
c619-032: Epoch: 0, Step: 36, Rank: 19, loss = 0.671875
c621-132: Epoch: 0, Step: 36, Rank: 37, loss = 0.69140625
c613-122: Epoch: 0, Step: 36, Rank: 5, loss = 0.7109375
c621-121: Epoch: 0, Step: 36, Rank: 34, loss = 0.71875
c621-111: Epoch: 0, Step: 36, Rank: 32, loss = 0.60546875
c622-032: Epoch: 0, Step: 36, Rank: 49, loss = 0.6875
c622-042: Epoch: 0, Step: 36, Rank: 51, loss = 0.69140625
c613-151: Epoch: 0, Step: 36, Rank: 10, loss = 0.67578125
c622-061: Epoch: 0, Step: 36, Rank: 54, loss = 0.74609375
c619-022: Epoch: 0, Step: 36, Rank: 17, loss = 0.6796875
c613-121: Epoch: 0, Step: 36, Rank: 4, loss = 0.69140625
c622-051: Epoch: 0, Step: 36, Rank: 52, loss = 0.71875
c621-122: Epoch: 0, Step: 36, Rank: 35, loss = 0.70703125
c621-112: Epoch: 0, Step: 36, Rank: 33, loss = 0.69140625
c621-092: Epoch: 0, Step: 36, Rank: 29, loss = 0.6796875
c621-101: Epoch: 0, Step: 36, Rank: 30, loss = 0.64453125
c621-052: Epoch: 0, Step: 36, Rank: 21, loss = 0.6796875
c622-072: Epoch: 0, Step: 36, Rank: 57, loss = 0.71875
c622-022: Epoch: 0, Step: 36, Rank: 47, loss = 0.74609375
c619-011: Epoch: 0, Step: 36, Rank: 14, loss = 0.6796875
c621-152: Epoch: 0, Step: 36, Rank: 41, loss = 0.69140625
c621-071: Epoch: 0, Step: 36, Rank: 24, loss = 0.6328125
c622-021: Epoch: 0, Step: 36, Rank: 46, loss = 0.6796875
c613-141: Epoch: 0, Step: 36, Rank: 8, loss = 0.69140625
c621-061: Epoch: 0, Step: 36, Rank: 22, loss = 0.66796875
c622-062: Epoch: 0, Step: 36, Rank: 55, loss = 0.6953125
c613-142: Epoch: 0, Step: 36, Rank: 9, loss = 0.69140625
c619-041: Epoch: 0, Step: 36, Rank: 20, loss = 0.6796875
c622-041: Epoch: 0, Step: 36, Rank: 50, loss = 0.6875
c622-082: Epoch: 0, Step: 36, Rank: 59, loss = 0.70703125
c613-112: Epoch: 0, Step: 36, Rank: 3, loss = 0.64453125
c621-062: Epoch: 0, Step: 36, Rank: 23, loss = 0.64453125
c621-102: Epoch: 0, Step: 36, Rank: 31, loss = 0.6875
c621-141: Epoch: 0, Step: 36, Rank: 38, loss = 0.74609375
c622-091: Epoch: 0, Step: 36, Rank: 60, loss = 0.70703125
c622-052: Epoch: 0, Step: 36, Rank: 53, loss = 0.66796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 37, Rank: 0, loss = 0.69140625
c621-111: Epoch: 0, Step: 37, Rank: 32, loss = 0.69140625
c622-002: Epoch: 0, Step: 37, Rank: 43, loss = 0.69140625
c622-101: Epoch: 0, Step: 37, Rank: 62, loss = 0.6796875
c622-102: Epoch: 0, Step: 37, Rank: 63, loss = 0.6875
c613-111: Epoch: 0, Step: 37, Rank: 2, loss = 0.671875
c621-091: Epoch: 0, Step: 37, Rank: 28, loss = 0.69921875
c619-021: Epoch: 0, Step: 37, Rank: 16, loss = 0.65625
c621-101: Epoch: 0, Step: 37, Rank: 30, loss = 0.69140625
c621-151: Epoch: 0, Step: 37, Rank: 40, loss = 0.71875
c621-132: Epoch: 0, Step: 37, Rank: 37, loss = 0.69140625
c622-012: Epoch: 0, Step: 37, Rank: 45, loss = 0.67578125
c613-112: Epoch: 0, Step: 37, Rank: 3, loss = 0.71875
c622-071: Epoch: 0, Step: 37, Rank: 56, loss = 0.65625
c622-092: Epoch: 0, Step: 37, Rank: 61, loss = 0.66796875
c613-122: Epoch: 0, Step: 37, Rank: 5, loss = 0.69140625
c621-081: Epoch: 0, Step: 37, Rank: 26, loss = 0.6875
c621-131: Epoch: 0, Step: 37, Rank: 36, loss = 0.66796875
c621-092: Epoch: 0, Step: 37, Rank: 29, loss = 0.70703125
c621-152: Epoch: 0, Step: 37, Rank: 41, loss = 0.6796875
c622-052: Epoch: 0, Step: 37, Rank: 53, loss = 0.69140625
c622-001: Epoch: 0, Step: 37, Rank: 42, loss = 0.6875
c621-121: Epoch: 0, Step: 37, Rank: 34, loss = 0.66796875
c621-112: Epoch: 0, Step: 37, Rank: 33, loss = 0.6953125
c613-131: Epoch: 0, Step: 37, Rank: 6, loss = 0.69140625
c621-142: Epoch: 0, Step: 37, Rank: 39, loss = 0.70703125
c613-121: Epoch: 0, Step: 37, Rank: 4, loss = 0.71875
c613-102: Epoch: 0, Step: 37, Rank: 1, loss = 0.66796875
c621-052: Epoch: 0, Step: 37, Rank: 21, loss = 0.69140625
c621-072: Epoch: 0, Step: 37, Rank: 25, loss = 0.7109375
c622-011: Epoch: 0, Step: 37, Rank: 44, loss = 0.69140625
c621-141: Epoch: 0, Step: 37, Rank: 38, loss = 0.6796875
c622-031: Epoch: 0, Step: 37, Rank: 48, loss = 0.6953125
c619-012: Epoch: 0, Step: 37, Rank: 15, loss = 0.69140625
c621-082: Epoch: 0, Step: 37, Rank: 27, loss = 0.64453125
c619-032: Epoch: 0, Step: 37, Rank: 19, loss = 0.6796875
c622-051: Epoch: 0, Step: 37, Rank: 52, loss = 0.70703125
c622-072: Epoch: 0, Step: 37, Rank: 57, loss = 0.69140625
c622-032: Epoch: 0, Step: 37, Rank: 49, loss = 0.71484375
c619-001: Epoch: 0, Step: 37, Rank: 12, loss = 0.70703125
c621-102: Epoch: 0, Step: 37, Rank: 31, loss = 0.6953125
c619-031: Epoch: 0, Step: 37, Rank: 18, loss = 0.70703125
c619-002: Epoch: 0, Step: 37, Rank: 13, loss = 0.796875
c622-041: Epoch: 0, Step: 37, Rank: 50, loss = 0.69140625
c613-151: Epoch: 0, Step: 37, Rank: 10, loss = 0.70703125
c613-152: Epoch: 0, Step: 37, Rank: 11, loss = 0.6640625
c622-081: Epoch: 0, Step: 37, Rank: 58, loss = 0.66796875
c622-021: Epoch: 0, Step: 37, Rank: 46, loss = 0.7265625
c622-061: Epoch: 0, Step: 37, Rank: 54, loss = 0.69140625
c622-062: Epoch: 0, Step: 37, Rank: 55, loss = 0.67578125
c621-071: Epoch: 0, Step: 37, Rank: 24, loss = 0.75
c619-011: Epoch: 0, Step: 37, Rank: 14, loss = 0.7109375
c622-091: Epoch: 0, Step: 37, Rank: 60, loss = 0.6796875
c619-022: Epoch: 0, Step: 37, Rank: 17, loss = 0.65625
c621-122: Epoch: 0, Step: 37, Rank: 35, loss = 0.70703125
c621-062: Epoch: 0, Step: 37, Rank: 23, loss = 0.68359375
c622-022: Epoch: 0, Step: 37, Rank: 47, loss = 0.71875
c613-142: Epoch: 0, Step: 37, Rank: 9, loss = 0.75
c613-132: Epoch: 0, Step: 37, Rank: 7, loss = 0.70703125
c622-042: Epoch: 0, Step: 37, Rank: 51, loss = 0.66796875
c619-041: Epoch: 0, Step: 37, Rank: 20, loss = 0.73046875
c621-061: Epoch: 0, Step: 37, Rank: 22, loss = 0.67578125
c613-141: Epoch: 0, Step: 37, Rank: 8, loss = 0.66796875
c622-082: Epoch: 0, Step: 37, Rank: 59, loss = 0.71875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 38, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 38, Rank: 43, loss = 0.70703125
c622-081: Epoch: 0, Step: 38, Rank: 58, loss = 0.7109375
c621-081: Epoch: 0, Step: 38, Rank: 26, loss = 0.69140625
c622-052: Epoch: 0, Step: 38, Rank: 53, loss = 0.7109375
c613-131: Epoch: 0, Step: 38, Rank: 6, loss = 0.71875
c621-072: Epoch: 0, Step: 38, Rank: 25, loss = 0.6875
c621-151: Epoch: 0, Step: 38, Rank: 40, loss = 0.65625
c619-021: Epoch: 0, Step: 38, Rank: 16, loss = 0.6640625
c621-091: Epoch: 0, Step: 38, Rank: 28, loss = 0.6484375
c619-002: Epoch: 0, Step: 38, Rank: 13, loss = 0.69140625
c621-082: Epoch: 0, Step: 38, Rank: 27, loss = 0.6796875
c622-092: Epoch: 0, Step: 38, Rank: 61, loss = 0.69140625
c613-112: Epoch: 0, Step: 38, Rank: 3, loss = 0.69140625
c621-111: Epoch: 0, Step: 38, Rank: 32, loss = 0.65625
c621-132: Epoch: 0, Step: 38, Rank: 37, loss = 0.6875
c622-012: Epoch: 0, Step: 38, Rank: 45, loss = 0.71875
c621-121: Epoch: 0, Step: 38, Rank: 34, loss = 0.73046875
c613-142: Epoch: 0, Step: 38, Rank: 9, loss = 0.67578125
c621-122: Epoch: 0, Step: 38, Rank: 35, loss = 0.69140625
c613-102: Epoch: 0, Step: 38, Rank: 1, loss = 0.69140625
c621-131: Epoch: 0, Step: 38, Rank: 36, loss = 0.6796875
c613-151: Epoch: 0, Step: 38, Rank: 10, loss = 0.74609375
c621-052: Epoch: 0, Step: 38, Rank: 21, loss = 0.6796875
c621-152: Epoch: 0, Step: 38, Rank: 41, loss = 0.6875
c621-112: Epoch: 0, Step: 38, Rank: 33, loss = 0.6640625
c613-111: Epoch: 0, Step: 38, Rank: 2, loss = 0.69140625
c613-121: Epoch: 0, Step: 38, Rank: 4, loss = 0.6875
c621-101: Epoch: 0, Step: 38, Rank: 30, loss = 0.69140625
c621-142: Epoch: 0, Step: 38, Rank: 39, loss = 0.64453125
c622-001: Epoch: 0, Step: 38, Rank: 42, loss = 0.74609375
c619-031: Epoch: 0, Step: 38, Rank: 18, loss = 0.6875
c621-141: Epoch: 0, Step: 38, Rank: 38, loss = 0.69921875
c619-001: Epoch: 0, Step: 38, Rank: 12, loss = 0.66796875
c619-032: Epoch: 0, Step: 38, Rank: 19, loss = 0.64453125
c613-122: Epoch: 0, Step: 38, Rank: 5, loss = 0.69140625
c622-071: Epoch: 0, Step: 38, Rank: 56, loss = 0.70703125
c622-032: Epoch: 0, Step: 38, Rank: 49, loss = 0.6640625
c621-071: Epoch: 0, Step: 38, Rank: 24, loss = 0.6796875
c622-101: Epoch: 0, Step: 38, Rank: 62, loss = 0.69140625
c622-061: Epoch: 0, Step: 38, Rank: 54, loss = 0.74609375
c622-031: Epoch: 0, Step: 38, Rank: 48, loss = 0.70703125
c622-082: Epoch: 0, Step: 38, Rank: 59, loss = 0.6796875
c622-062: Epoch: 0, Step: 38, Rank: 55, loss = 0.5859375
c621-061: Epoch: 0, Step: 38, Rank: 22, loss = 0.6796875
c619-041: Epoch: 0, Step: 38, Rank: 20, loss = 0.62109375
c619-022: Epoch: 0, Step: 38, Rank: 17, loss = 0.69140625
c622-102: Epoch: 0, Step: 38, Rank: 63, loss = 0.59765625
c621-102: Epoch: 0, Step: 38, Rank: 31, loss = 0.65625
c622-042: Epoch: 0, Step: 38, Rank: 51, loss = 0.69140625
c613-141: Epoch: 0, Step: 38, Rank: 8, loss = 0.71875
c622-091: Epoch: 0, Step: 38, Rank: 60, loss = 0.72265625
c622-011: Epoch: 0, Step: 38, Rank: 44, loss = 0.73046875
c613-132: Epoch: 0, Step: 38, Rank: 7, loss = 0.6796875
c619-011: Epoch: 0, Step: 38, Rank: 14, loss = 0.71875
c621-092: Epoch: 0, Step: 38, Rank: 29, loss = 0.71875
c621-062: Epoch: 0, Step: 38, Rank: 23, loss = 0.71875
c622-022: Epoch: 0, Step: 38, Rank: 47, loss = 0.66796875
c622-072: Epoch: 0, Step: 38, Rank: 57, loss = 0.76953125
c622-041: Epoch: 0, Step: 38, Rank: 50, loss = 0.69140625
c619-012: Epoch: 0, Step: 38, Rank: 15, loss = 0.69140625
c622-051: Epoch: 0, Step: 38, Rank: 52, loss = 0.70703125
c622-021: Epoch: 0, Step: 38, Rank: 46, loss = 0.66796875
c613-152: Epoch: 0, Step: 38, Rank: 11, loss = 0.70703125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c619-032: Epoch: 0, Step: 39, Rank: 19, loss = 0.74609375
c619-002: Epoch: 0, Step: 39, Rank: 13, loss = 0.67578125
c613-101: Epoch: 0, Step: 39, Rank: 0, loss = 0.69140625
c619-021: Epoch: 0, Step: 39, Rank: 16, loss = 0.65625
c619-001: Epoch: 0, Step: 39, Rank: 12, loss = 0.66796875
c619-031: Epoch: 0, Step: 39, Rank: 18, loss = 0.69140625
c619-041: Epoch: 0, Step: 39, Rank: 20, loss = 0.66796875
c613-141: Epoch: 0, Step: 39, Rank: 8, loss = 0.70703125
c613-132: Epoch: 0, Step: 39, Rank: 7, loss = 0.69140625
c622-002: Epoch: 0, Step: 39, Rank: 43, loss = 0.69140625
c613-151: Epoch: 0, Step: 39, Rank: 10, loss = 0.6796875
c621-111: Epoch: 0, Step: 39, Rank: 32, loss = 0.64453125
c613-131: Epoch: 0, Step: 39, Rank: 6, loss = 0.71875
c621-131: Epoch: 0, Step: 39, Rank: 36, loss = 0.69140625
c621-081: Epoch: 0, Step: 39, Rank: 26, loss = 0.64453125
c622-071: Epoch: 0, Step: 39, Rank: 56, loss = 0.67578125
c621-072: Epoch: 0, Step: 39, Rank: 25, loss = 0.69140625
c613-122: Epoch: 0, Step: 39, Rank: 5, loss = 0.59765625
c613-152: Epoch: 0, Step: 39, Rank: 11, loss = 0.66796875
c622-101: Epoch: 0, Step: 39, Rank: 62, loss = 0.70703125
c622-001: Epoch: 0, Step: 39, Rank: 42, loss = 0.69140625
c622-052: Epoch: 0, Step: 39, Rank: 53, loss = 0.64453125
c622-012: Epoch: 0, Step: 39, Rank: 45, loss = 0.69140625
c622-081: Epoch: 0, Step: 39, Rank: 58, loss = 0.6796875
c621-062: Epoch: 0, Step: 39, Rank: 23, loss = 0.69140625
c619-012: Epoch: 0, Step: 39, Rank: 15, loss = 0.6875
c621-151: Epoch: 0, Step: 39, Rank: 40, loss = 0.69140625
c619-022: Epoch: 0, Step: 39, Rank: 17, loss = 0.6953125
c621-132: Epoch: 0, Step: 39, Rank: 37, loss = 0.64453125
c621-091: Epoch: 0, Step: 39, Rank: 28, loss = 0.71484375
c621-121: Epoch: 0, Step: 39, Rank: 34, loss = 0.71875
c621-092: Epoch: 0, Step: 39, Rank: 29, loss = 0.6875
c621-052: Epoch: 0, Step: 39, Rank: 21, loss = 0.5546875
c613-111: Epoch: 0, Step: 39, Rank: 2, loss = 0.69140625
c621-082: Epoch: 0, Step: 39, Rank: 27, loss = 0.69140625
c622-061: Epoch: 0, Step: 39, Rank: 54, loss = 0.65625
c613-121: Epoch: 0, Step: 39, Rank: 4, loss = 0.6953125
c621-142: Epoch: 0, Step: 39, Rank: 39, loss = 0.74609375
c621-101: Epoch: 0, Step: 39, Rank: 30, loss = 0.734375
c621-112: Epoch: 0, Step: 39, Rank: 33, loss = 0.6875
c622-062: Epoch: 0, Step: 39, Rank: 55, loss = 0.6875
c613-102: Epoch: 0, Step: 39, Rank: 1, loss = 0.9140625
c619-011: Epoch: 0, Step: 39, Rank: 14, loss = 0.71875
c622-011: Epoch: 0, Step: 39, Rank: 44, loss = 0.69140625
c622-022: Epoch: 0, Step: 39, Rank: 47, loss = 0.7265625
c622-051: Epoch: 0, Step: 39, Rank: 52, loss = 0.71484375
c622-072: Epoch: 0, Step: 39, Rank: 57, loss = 0.6875
c621-141: Epoch: 0, Step: 39, Rank: 38, loss = 0.69140625
c622-102: Epoch: 0, Step: 39, Rank: 63, loss = 0.6875
c621-102: Epoch: 0, Step: 39, Rank: 31, loss = 0.70703125
c622-032: Epoch: 0, Step: 39, Rank: 49, loss = 0.6796875
c622-082: Epoch: 0, Step: 39, Rank: 59, loss = 0.69140625
c622-091: Epoch: 0, Step: 39, Rank: 60, loss = 0.609375
c622-031: Epoch: 0, Step: 39, Rank: 48, loss = 0.66796875
c621-122: Epoch: 0, Step: 39, Rank: 35, loss = 0.63671875
c622-092: Epoch: 0, Step: 39, Rank: 61, loss = 0.69140625
c621-152: Epoch: 0, Step: 39, Rank: 41, loss = 0.66796875
c622-042: Epoch: 0, Step: 39, Rank: 51, loss = 0.67578125
c622-021: Epoch: 0, Step: 39, Rank: 46, loss = 0.69140625
c613-112: Epoch: 0, Step: 39, Rank: 3, loss = 0.6796875
c622-041: Epoch: 0, Step: 39, Rank: 50, loss = 0.67578125
c621-061: Epoch: 0, Step: 39, Rank: 22, loss = 0.69140625
c621-071: Epoch: 0, Step: 39, Rank: 24, loss = 0.64453125
c613-142: Epoch: 0, Step: 39, Rank: 9, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c621-142: Epoch: 0, Step: 40, Rank: 39, loss = 0.6875
c622-002: Epoch: 0, Step: 40, Rank: 43, loss = 0.66796875
c622-052: Epoch: 0, Step: 40, Rank: 53, loss = 0.69140625
c622-081: Epoch: 0, Step: 40, Rank: 58, loss = 0.66796875
c619-021: Epoch: 0, Step: 40, Rank: 16, loss = 0.74609375
c622-092: Epoch: 0, Step: 40, Rank: 61, loss = 0.70703125
c622-061: Epoch: 0, Step: 40, Rank: 54, loss = 0.64453125
c621-132: Epoch: 0, Step: 40, Rank: 37, loss = 0.7109375
c619-002: Epoch: 0, Step: 40, Rank: 13, loss = 0.69140625
c621-151: Epoch: 0, Step: 40, Rank: 40, loss = 0.69140625
c622-022: Epoch: 0, Step: 40, Rank: 47, loss = 0.69140625
c622-082: Epoch: 0, Step: 40, Rank: 59, loss = 0.6796875
c622-012: Epoch: 0, Step: 40, Rank: 45, loss = 0.6640625
c613-101: Epoch: 0, Step: 40, Rank: 0, loss = 0.63671875
c622-051: Epoch: 0, Step: 40, Rank: 52, loss = 0.62109375
c622-021: Epoch: 0, Step: 40, Rank: 46, loss = 0.69140625
c622-001: Epoch: 0, Step: 40, Rank: 42, loss = 0.6875
c619-001: Epoch: 0, Step: 40, Rank: 12, loss = 0.67578125
c619-022: Epoch: 0, Step: 40, Rank: 17, loss = 0.640625
c622-041: Epoch: 0, Step: 40, Rank: 50, loss = 0.69140625
c621-081: Epoch: 0, Step: 40, Rank: 26, loss = 0.74609375
c622-042: Epoch: 0, Step: 40, Rank: 51, loss = 0.69140625
c622-011: Epoch: 0, Step: 40, Rank: 44, loss = 0.6953125
c622-071: Epoch: 0, Step: 40, Rank: 56, loss = 0.69140625
c621-141: Epoch: 0, Step: 40, Rank: 38, loss = 0.6796875
c622-091: Epoch: 0, Step: 40, Rank: 60, loss = 0.6796875
c621-122: Epoch: 0, Step: 40, Rank: 35, loss = 0.74609375
c621-121: Epoch: 0, Step: 40, Rank: 34, loss = 0.6796875
c621-111: Epoch: 0, Step: 40, Rank: 32, loss = 0.69140625
c621-091: Epoch: 0, Step: 40, Rank: 28, loss = 0.71875
c621-152: Epoch: 0, Step: 40, Rank: 41, loss = 0.7890625
c621-131: Epoch: 0, Step: 40, Rank: 36, loss = 0.6875
c622-032: Epoch: 0, Step: 40, Rank: 49, loss = 0.78515625
c622-031: Epoch: 0, Step: 40, Rank: 48, loss = 0.69140625
c619-012: Epoch: 0, Step: 40, Rank: 15, loss = 0.6875
c613-152: Epoch: 0, Step: 40, Rank: 11, loss = 0.796875
c621-112: Epoch: 0, Step: 40, Rank: 33, loss = 0.70703125
c613-151: Epoch: 0, Step: 40, Rank: 10, loss = 0.51171875
c622-062: Epoch: 0, Step: 40, Rank: 55, loss = 0.66796875
c613-121: Epoch: 0, Step: 40, Rank: 4, loss = 0.58984375
c619-041: Epoch: 0, Step: 40, Rank: 20, loss = 0.64453125
c621-092: Epoch: 0, Step: 40, Rank: 29, loss = 0.70703125
c613-111: Epoch: 0, Step: 40, Rank: 2, loss = 0.69140625
c619-031: Epoch: 0, Step: 40, Rank: 18, loss = 0.70703125
c619-011: Epoch: 0, Step: 40, Rank: 14, loss = 0.66796875
c621-072: Epoch: 0, Step: 40, Rank: 25, loss = 0.6875
c621-082: Epoch: 0, Step: 40, Rank: 27, loss = 0.70703125
c622-101: Epoch: 0, Step: 40, Rank: 62, loss = 0.69140625
c621-101: Epoch: 0, Step: 40, Rank: 30, loss = 0.6796875
c613-122: Epoch: 0, Step: 40, Rank: 5, loss = 0.6875
c621-052: Epoch: 0, Step: 40, Rank: 21, loss = 0.671875
c613-112: Epoch: 0, Step: 40, Rank: 3, loss = 0.70703125
c621-071: Epoch: 0, Step: 40, Rank: 24, loss = 0.66796875
c621-061: Epoch: 0, Step: 40, Rank: 22, loss = 0.63671875
c619-032: Epoch: 0, Step: 40, Rank: 19, loss = 0.70703125
c622-072: Epoch: 0, Step: 40, Rank: 57, loss = 0.68359375
c613-132: Epoch: 0, Step: 40, Rank: 7, loss = 0.73046875
c613-102: Epoch: 0, Step: 40, Rank: 1, loss = 0.69140625
c621-062: Epoch: 0, Step: 40, Rank: 23, loss = 0.74609375
c621-102: Epoch: 0, Step: 40, Rank: 31, loss = 0.7109375
c622-102: Epoch: 0, Step: 40, Rank: 63, loss = 0.67578125
c613-142: Epoch: 0, Step: 40, Rank: 9, loss = 0.71875
c613-141: Epoch: 0, Step: 40, Rank: 8, loss = 0.71484375
c613-131: Epoch: 0, Step: 40, Rank: 6, loss = 0.70703125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 41, Rank: 32, loss = 0.6796875
c613-101: Epoch: 0, Step: 41, Rank: 0, loss = 0.71875
c619-021: Epoch: 0, Step: 41, Rank: 16, loss = 0.71875
c613-141: Epoch: 0, Step: 41, Rank: 8, loss = 0.67578125
c622-002: Epoch: 0, Step: 41, Rank: 43, loss = 0.68359375
c622-081: Epoch: 0, Step: 41, Rank: 58, loss = 0.65625
c622-101: Epoch: 0, Step: 41, Rank: 62, loss = 0.75
c619-002: Epoch: 0, Step: 41, Rank: 13, loss = 0.64453125
c613-131: Epoch: 0, Step: 41, Rank: 6, loss = 0.69140625
c613-132: Epoch: 0, Step: 41, Rank: 7, loss = 0.6640625
c613-151: Epoch: 0, Step: 41, Rank: 10, loss = 0.6484375
c619-001: Epoch: 0, Step: 41, Rank: 12, loss = 0.6796875
c619-031: Epoch: 0, Step: 41, Rank: 18, loss = 0.734375
c619-012: Epoch: 0, Step: 41, Rank: 15, loss = 0.76953125
c613-142: Epoch: 0, Step: 41, Rank: 9, loss = 0.7109375
c619-011: Epoch: 0, Step: 41, Rank: 14, loss = 0.6796875
c622-092: Epoch: 0, Step: 41, Rank: 61, loss = 0.6796875
c622-012: Epoch: 0, Step: 41, Rank: 45, loss = 0.74609375
c621-151: Epoch: 0, Step: 41, Rank: 40, loss = 0.69140625
c613-111: Epoch: 0, Step: 41, Rank: 2, loss = 0.69921875
c621-081: Epoch: 0, Step: 41, Rank: 26, loss = 0.69140625
c613-121: Epoch: 0, Step: 41, Rank: 4, loss = 0.73046875
c622-001: Epoch: 0, Step: 41, Rank: 42, loss = 0.71875
c621-091: Epoch: 0, Step: 41, Rank: 28, loss = 0.70703125
c622-071: Epoch: 0, Step: 41, Rank: 56, loss = 0.64453125
c613-152: Epoch: 0, Step: 41, Rank: 11, loss = 0.69140625
c621-121: Epoch: 0, Step: 41, Rank: 34, loss = 0.69140625
c622-011: Epoch: 0, Step: 41, Rank: 44, loss = 0.69140625
c621-132: Epoch: 0, Step: 41, Rank: 37, loss = 0.6875
c621-092: Epoch: 0, Step: 41, Rank: 29, loss = 0.796875
c621-131: Epoch: 0, Step: 41, Rank: 36, loss = 0.71875
c621-052: Epoch: 0, Step: 41, Rank: 21, loss = 0.6796875
c621-072: Epoch: 0, Step: 41, Rank: 25, loss = 0.6875
c613-112: Epoch: 0, Step: 41, Rank: 3, loss = 0.69140625
c621-082: Epoch: 0, Step: 41, Rank: 27, loss = 0.68359375
c621-122: Epoch: 0, Step: 41, Rank: 35, loss = 0.66796875
c621-101: Epoch: 0, Step: 41, Rank: 30, loss = 0.67578125
c622-042: Epoch: 0, Step: 41, Rank: 51, loss = 0.7109375
c622-032: Epoch: 0, Step: 41, Rank: 49, loss = 0.6796875
c622-062: Epoch: 0, Step: 41, Rank: 55, loss = 0.66796875
c621-141: Epoch: 0, Step: 41, Rank: 38, loss = 0.6640625
c613-122: Epoch: 0, Step: 41, Rank: 5, loss = 0.69140625
c622-102: Epoch: 0, Step: 41, Rank: 63, loss = 0.66796875
c621-112: Epoch: 0, Step: 41, Rank: 33, loss = 0.59375
c621-142: Epoch: 0, Step: 41, Rank: 39, loss = 0.65625
c621-102: Epoch: 0, Step: 41, Rank: 31, loss = 0.69140625
c621-062: Epoch: 0, Step: 41, Rank: 23, loss = 0.64453125
c619-022: Epoch: 0, Step: 41, Rank: 17, loss = 0.68359375
c619-041: Epoch: 0, Step: 41, Rank: 20, loss = 0.59765625
c622-072: Epoch: 0, Step: 41, Rank: 57, loss = 0.6875
c622-091: Epoch: 0, Step: 41, Rank: 60, loss = 0.68359375
c613-102: Epoch: 0, Step: 41, Rank: 1, loss = 0.69140625
c622-022: Epoch: 0, Step: 41, Rank: 47, loss = 0.6796875
c622-061: Epoch: 0, Step: 41, Rank: 54, loss = 0.66796875
c621-061: Epoch: 0, Step: 41, Rank: 22, loss = 0.6640625
c622-041: Epoch: 0, Step: 41, Rank: 50, loss = 0.7578125
c622-051: Epoch: 0, Step: 41, Rank: 52, loss = 0.65625
c622-082: Epoch: 0, Step: 41, Rank: 59, loss = 0.71875
c619-032: Epoch: 0, Step: 41, Rank: 19, loss = 0.71875
c621-152: Epoch: 0, Step: 41, Rank: 41, loss = 0.7109375
c622-031: Epoch: 0, Step: 41, Rank: 48, loss = 0.68359375
c621-071: Epoch: 0, Step: 41, Rank: 24, loss = 0.69140625
c622-021: Epoch: 0, Step: 41, Rank: 46, loss = 0.69140625
c622-052: Epoch: 0, Step: 41, Rank: 53, loss = 0.6953125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 42, Rank: 26, loss = 0.6953125
c619-002: Epoch: 0, Step: 42, Rank: 13, loss = 0.6875
c621-142: Epoch: 0, Step: 42, Rank: 39, loss = 0.6796875
c622-052: Epoch: 0, Step: 42, Rank: 53, loss = 0.69140625
c622-002: Epoch: 0, Step: 42, Rank: 43, loss = 0.67578125
c619-021: Epoch: 0, Step: 42, Rank: 16, loss = 0.67578125
c619-001: Epoch: 0, Step: 42, Rank: 12, loss = 0.62109375
c622-012: Epoch: 0, Step: 42, Rank: 45, loss = 0.6953125
c619-022: Epoch: 0, Step: 42, Rank: 17, loss = 0.69140625
c621-151: Epoch: 0, Step: 42, Rank: 40, loss = 0.59765625
c622-001: Epoch: 0, Step: 42, Rank: 42, loss = 0.69921875
c613-101: Epoch: 0, Step: 42, Rank: 0, loss = 0.64453125
c621-072: Epoch: 0, Step: 42, Rank: 25, loss = 0.69140625
c621-121: Epoch: 0, Step: 42, Rank: 34, loss = 0.69921875
c621-131: Epoch: 0, Step: 42, Rank: 36, loss = 0.66796875
c613-121: Epoch: 0, Step: 42, Rank: 4, loss = 0.69140625
c619-011: Epoch: 0, Step: 42, Rank: 14, loss = 0.6953125
c613-111: Epoch: 0, Step: 42, Rank: 2, loss = 0.59765625
c622-011: Epoch: 0, Step: 42, Rank: 44, loss = 0.6796875
c613-152: Epoch: 0, Step: 42, Rank: 11, loss = 0.69921875
c622-051: Epoch: 0, Step: 42, Rank: 52, loss = 0.6796875
c613-122: Epoch: 0, Step: 42, Rank: 5, loss = 0.71484375
c619-012: Epoch: 0, Step: 42, Rank: 15, loss = 0.70703125
c613-132: Epoch: 0, Step: 42, Rank: 7, loss = 0.76953125
c622-081: Epoch: 0, Step: 42, Rank: 58, loss = 0.63671875
c621-122: Epoch: 0, Step: 42, Rank: 35, loss = 0.69140625
c621-111: Epoch: 0, Step: 42, Rank: 32, loss = 0.71875
c621-132: Epoch: 0, Step: 42, Rank: 37, loss = 0.69140625
c621-052: Epoch: 0, Step: 42, Rank: 21, loss = 0.70703125
c619-041: Epoch: 0, Step: 42, Rank: 20, loss = 0.69140625
c622-041: Epoch: 0, Step: 42, Rank: 50, loss = 0.6875
c621-091: Epoch: 0, Step: 42, Rank: 28, loss = 0.6640625
c621-112: Epoch: 0, Step: 42, Rank: 33, loss = 0.703125
c622-102: Epoch: 0, Step: 42, Rank: 63, loss = 0.60546875
c621-141: Epoch: 0, Step: 42, Rank: 38, loss = 0.69140625
c622-042: Epoch: 0, Step: 42, Rank: 51, loss = 0.671875
c622-061: Epoch: 0, Step: 42, Rank: 54, loss = 0.68359375
c622-071: Epoch: 0, Step: 42, Rank: 56, loss = 0.6796875
c619-031: Epoch: 0, Step: 42, Rank: 18, loss = 0.71484375
c613-131: Epoch: 0, Step: 42, Rank: 6, loss = 0.703125
c621-082: Epoch: 0, Step: 42, Rank: 27, loss = 0.71875
c621-061: Epoch: 0, Step: 42, Rank: 22, loss = 0.69140625
c622-062: Epoch: 0, Step: 42, Rank: 55, loss = 0.69140625
c613-151: Epoch: 0, Step: 42, Rank: 10, loss = 0.70703125
c613-112: Epoch: 0, Step: 42, Rank: 3, loss = 0.7109375
c622-032: Epoch: 0, Step: 42, Rank: 49, loss = 0.69140625
c613-142: Epoch: 0, Step: 42, Rank: 9, loss = 0.65625
c622-022: Epoch: 0, Step: 42, Rank: 47, loss = 0.6484375
c621-062: Epoch: 0, Step: 42, Rank: 23, loss = 0.63671875
c619-032: Epoch: 0, Step: 42, Rank: 19, loss = 0.6640625
c621-102: Epoch: 0, Step: 42, Rank: 31, loss = 0.69140625
c613-102: Epoch: 0, Step: 42, Rank: 1, loss = 0.71875
c622-082: Epoch: 0, Step: 42, Rank: 59, loss = 0.73828125
c622-101: Epoch: 0, Step: 42, Rank: 62, loss = 0.74609375
c622-072: Epoch: 0, Step: 42, Rank: 57, loss = 0.6796875
c621-152: Epoch: 0, Step: 42, Rank: 41, loss = 0.69921875
c621-092: Epoch: 0, Step: 42, Rank: 29, loss = 0.64453125
c622-031: Epoch: 0, Step: 42, Rank: 48, loss = 0.65625
c613-141: Epoch: 0, Step: 42, Rank: 8, loss = 0.66796875
c621-101: Epoch: 0, Step: 42, Rank: 30, loss = 0.66796875
c622-092: Epoch: 0, Step: 42, Rank: 61, loss = 0.76953125
c621-071: Epoch: 0, Step: 42, Rank: 24, loss = 0.57421875
c622-021: Epoch: 0, Step: 42, Rank: 46, loss = 0.6640625
c622-091: Epoch: 0, Step: 42, Rank: 60, loss = 0.71875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-031: Epoch: 0, Step: 43, Rank: 18, loss = 0.6875
c621-131: Epoch: 0, Step: 43, Rank: 36, loss = 0.69921875
c619-041: Epoch: 0, Step: 43, Rank: 20, loss = 0.66796875
c619-021: Epoch: 0, Step: 43, Rank: 16, loss = 0.6875
c613-101: Epoch: 0, Step: 43, Rank: 0, loss = 0.71875
c622-002: Epoch: 0, Step: 43, Rank: 43, loss = 0.796875
c621-122: Epoch: 0, Step: 43, Rank: 35, loss = 0.765625
c621-121: Epoch: 0, Step: 43, Rank: 34, loss = 0.69140625
c619-032: Epoch: 0, Step: 43, Rank: 19, loss = 0.6875
c622-001: Epoch: 0, Step: 43, Rank: 42, loss = 0.6796875
c621-091: Epoch: 0, Step: 43, Rank: 28, loss = 0.6484375
c621-111: Epoch: 0, Step: 43, Rank: 32, loss = 0.62109375
c622-081: Epoch: 0, Step: 43, Rank: 58, loss = 0.69921875
c622-052: Epoch: 0, Step: 43, Rank: 53, loss = 0.6875
c621-052: Epoch: 0, Step: 43, Rank: 21, loss = 0.7109375
c621-082: Epoch: 0, Step: 43, Rank: 27, loss = 0.6640625
c619-001: Epoch: 0, Step: 43, Rank: 12, loss = 0.80859375
c613-102: Epoch: 0, Step: 43, Rank: 1, loss = 0.64453125
c621-112: Epoch: 0, Step: 43, Rank: 33, loss = 0.64453125
c622-011: Epoch: 0, Step: 43, Rank: 44, loss = 0.6796875
c622-012: Epoch: 0, Step: 43, Rank: 45, loss = 0.69921875
c621-072: Epoch: 0, Step: 43, Rank: 25, loss = 0.59765625
c622-071: Epoch: 0, Step: 43, Rank: 56, loss = 0.6875
c621-151: Epoch: 0, Step: 43, Rank: 40, loss = 0.64453125
c613-141: Epoch: 0, Step: 43, Rank: 8, loss = 0.68359375
c621-101: Epoch: 0, Step: 43, Rank: 30, loss = 0.71484375
c622-051: Epoch: 0, Step: 43, Rank: 52, loss = 0.69140625
c622-062: Epoch: 0, Step: 43, Rank: 55, loss = 0.76953125
c621-152: Epoch: 0, Step: 43, Rank: 41, loss = 0.74609375
c613-121: Epoch: 0, Step: 43, Rank: 4, loss = 0.6796875
c621-081: Epoch: 0, Step: 43, Rank: 26, loss = 0.66796875
c613-111: Epoch: 0, Step: 43, Rank: 2, loss = 0.69140625
c613-151: Epoch: 0, Step: 43, Rank: 10, loss = 0.66796875
c621-142: Epoch: 0, Step: 43, Rank: 39, loss = 0.69140625
c619-022: Epoch: 0, Step: 43, Rank: 17, loss = 0.66796875
c613-132: Epoch: 0, Step: 43, Rank: 7, loss = 0.64453125
c622-101: Epoch: 0, Step: 43, Rank: 62, loss = 0.625
c613-122: Epoch: 0, Step: 43, Rank: 5, loss = 0.69140625
c621-132: Epoch: 0, Step: 43, Rank: 37, loss = 0.6953125
c621-062: Epoch: 0, Step: 43, Rank: 23, loss = 0.67578125
c613-152: Epoch: 0, Step: 43, Rank: 11, loss = 0.66796875
c619-012: Epoch: 0, Step: 43, Rank: 15, loss = 0.67578125
c622-102: Epoch: 0, Step: 43, Rank: 63, loss = 0.71875
c621-071: Epoch: 0, Step: 43, Rank: 24, loss = 0.73046875
c622-092: Epoch: 0, Step: 43, Rank: 61, loss = 0.69921875
c613-112: Epoch: 0, Step: 43, Rank: 3, loss = 0.66796875
c622-061: Epoch: 0, Step: 43, Rank: 54, loss = 0.6953125
c619-002: Epoch: 0, Step: 43, Rank: 13, loss = 0.74609375
c622-021: Epoch: 0, Step: 43, Rank: 46, loss = 0.6875
c621-102: Epoch: 0, Step: 43, Rank: 31, loss = 0.69140625
c622-022: Epoch: 0, Step: 43, Rank: 47, loss = 0.69140625
c622-042: Epoch: 0, Step: 43, Rank: 51, loss = 0.6875
c621-061: Epoch: 0, Step: 43, Rank: 22, loss = 0.69140625
c622-072: Epoch: 0, Step: 43, Rank: 57, loss = 0.7109375
c621-092: Epoch: 0, Step: 43, Rank: 29, loss = 0.6875
c622-031: Epoch: 0, Step: 43, Rank: 48, loss = 0.69140625
c622-082: Epoch: 0, Step: 43, Rank: 59, loss = 0.69921875
c619-011: Epoch: 0, Step: 43, Rank: 14, loss = 0.68359375
c622-032: Epoch: 0, Step: 43, Rank: 49, loss = 0.71875
c613-142: Epoch: 0, Step: 43, Rank: 9, loss = 0.64453125
c622-041: Epoch: 0, Step: 43, Rank: 50, loss = 0.71875
c613-131: Epoch: 0, Step: 43, Rank: 6, loss = 0.6796875
c621-141: Epoch: 0, Step: 43, Rank: 38, loss = 0.64453125
c622-091: Epoch: 0, Step: 43, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 44, Rank: 43, loss = 0.69921875
c622-012: Epoch: 0, Step: 44, Rank: 45, loss = 0.70703125
c621-151: Epoch: 0, Step: 44, Rank: 40, loss = 0.7109375
c619-031: Epoch: 0, Step: 44, Rank: 18, loss = 0.6875
c621-072: Epoch: 0, Step: 44, Rank: 25, loss = 0.64453125
c622-001: Epoch: 0, Step: 44, Rank: 42, loss = 0.69140625
c621-091: Epoch: 0, Step: 44, Rank: 28, loss = 0.64453125
c621-121: Epoch: 0, Step: 44, Rank: 34, loss = 0.6796875
c621-142: Epoch: 0, Step: 44, Rank: 39, loss = 0.64453125
c621-101: Epoch: 0, Step: 44, Rank: 30, loss = 0.51171875
c619-041: Epoch: 0, Step: 44, Rank: 20, loss = 0.7421875
c621-152: Epoch: 0, Step: 44, Rank: 41, loss = 0.6796875
c622-011: Epoch: 0, Step: 44, Rank: 44, loss = 0.69140625
c619-021: Epoch: 0, Step: 44, Rank: 16, loss = 0.69140625
c619-001: Epoch: 0, Step: 44, Rank: 12, loss = 0.64453125
c619-002: Epoch: 0, Step: 44, Rank: 13, loss = 0.64453125
c622-042: Epoch: 0, Step: 44, Rank: 51, loss = 0.69140625
c613-101: Epoch: 0, Step: 44, Rank: 0, loss = 0.6640625
c622-052: Epoch: 0, Step: 44, Rank: 53, loss = 0.6875
c621-111: Epoch: 0, Step: 44, Rank: 32, loss = 0.71875
c621-131: Epoch: 0, Step: 44, Rank: 36, loss = 0.6875
c622-041: Epoch: 0, Step: 44, Rank: 50, loss = 0.69140625
c621-081: Epoch: 0, Step: 44, Rank: 26, loss = 0.71484375
c613-132: Epoch: 0, Step: 44, Rank: 7, loss = 0.69140625
c613-122: Epoch: 0, Step: 44, Rank: 5, loss = 0.68359375
c613-141: Epoch: 0, Step: 44, Rank: 8, loss = 0.67578125
c621-132: Epoch: 0, Step: 44, Rank: 37, loss = 0.66796875
c621-102: Epoch: 0, Step: 44, Rank: 31, loss = 0.69140625
c613-111: Epoch: 0, Step: 44, Rank: 2, loss = 0.70703125
c622-032: Epoch: 0, Step: 44, Rank: 49, loss = 0.69140625
c621-082: Epoch: 0, Step: 44, Rank: 27, loss = 0.671875
c613-112: Epoch: 0, Step: 44, Rank: 3, loss = 0.69140625
c613-151: Epoch: 0, Step: 44, Rank: 10, loss = 0.69140625
c613-121: Epoch: 0, Step: 44, Rank: 4, loss = 0.6953125
c613-102: Epoch: 0, Step: 44, Rank: 1, loss = 0.6328125
c622-061: Epoch: 0, Step: 44, Rank: 54, loss = 0.65625
c619-032: Epoch: 0, Step: 44, Rank: 19, loss = 0.75
c621-061: Epoch: 0, Step: 44, Rank: 22, loss = 0.6796875
c622-022: Epoch: 0, Step: 44, Rank: 47, loss = 0.69140625
c621-141: Epoch: 0, Step: 44, Rank: 38, loss = 0.69140625
c613-131: Epoch: 0, Step: 44, Rank: 6, loss = 0.71484375
c622-101: Epoch: 0, Step: 44, Rank: 62, loss = 0.67578125
c621-122: Epoch: 0, Step: 44, Rank: 35, loss = 0.74609375
c622-092: Epoch: 0, Step: 44, Rank: 61, loss = 0.64453125
c621-092: Epoch: 0, Step: 44, Rank: 29, loss = 0.69140625
c622-031: Epoch: 0, Step: 44, Rank: 48, loss = 0.64453125
c622-021: Epoch: 0, Step: 44, Rank: 46, loss = 0.70703125
c613-152: Epoch: 0, Step: 44, Rank: 11, loss = 0.69921875
c621-052: Epoch: 0, Step: 44, Rank: 21, loss = 0.69140625
c621-062: Epoch: 0, Step: 44, Rank: 23, loss = 0.6796875
c621-071: Epoch: 0, Step: 44, Rank: 24, loss = 0.65625
c622-051: Epoch: 0, Step: 44, Rank: 52, loss = 0.68359375
c613-142: Epoch: 0, Step: 44, Rank: 9, loss = 0.6796875
c619-022: Epoch: 0, Step: 44, Rank: 17, loss = 0.67578125
c622-081: Epoch: 0, Step: 44, Rank: 58, loss = 0.6796875
c619-012: Epoch: 0, Step: 44, Rank: 15, loss = 0.796875
c619-011: Epoch: 0, Step: 44, Rank: 14, loss = 0.6875
c621-112: Epoch: 0, Step: 44, Rank: 33, loss = 0.6875
c622-102: Epoch: 0, Step: 44, Rank: 63, loss = 0.67578125
c622-062: Epoch: 0, Step: 44, Rank: 55, loss = 0.69140625
c622-072: Epoch: 0, Step: 44, Rank: 57, loss = 0.6875
c622-082: Epoch: 0, Step: 44, Rank: 59, loss = 0.67578125
c622-071: Epoch: 0, Step: 44, Rank: 56, loss = 0.68359375
c622-091: Epoch: 0, Step: 44, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 45, Rank: 0, loss = 0.69140625
c622-081: Epoch: 0, Step: 45, Rank: 58, loss = 0.671875
c621-081: Epoch: 0, Step: 45, Rank: 26, loss = 0.74609375
c613-111: Epoch: 0, Step: 45, Rank: 2, loss = 0.70703125
c622-002: Epoch: 0, Step: 45, Rank: 43, loss = 0.6875
c619-021: Epoch: 0, Step: 45, Rank: 16, loss = 0.67578125
c622-101: Epoch: 0, Step: 45, Rank: 62, loss = 0.67578125
c621-091: Epoch: 0, Step: 45, Rank: 28, loss = 0.51171875
c622-082: Epoch: 0, Step: 45, Rank: 59, loss = 0.63671875
c619-001: Epoch: 0, Step: 45, Rank: 12, loss = 0.65625
c619-002: Epoch: 0, Step: 45, Rank: 13, loss = 0.64453125
c613-122: Epoch: 0, Step: 45, Rank: 5, loss = 0.6796875
c622-062: Epoch: 0, Step: 45, Rank: 55, loss = 0.6953125
c613-131: Epoch: 0, Step: 45, Rank: 6, loss = 0.72265625
c619-012: Epoch: 0, Step: 45, Rank: 15, loss = 0.66796875
c621-132: Epoch: 0, Step: 45, Rank: 37, loss = 0.69140625
c622-052: Epoch: 0, Step: 45, Rank: 53, loss = 0.72265625
c621-151: Epoch: 0, Step: 45, Rank: 40, loss = 0.73828125
c622-012: Epoch: 0, Step: 45, Rank: 45, loss = 0.71875
c613-151: Epoch: 0, Step: 45, Rank: 10, loss = 0.7265625
c622-011: Epoch: 0, Step: 45, Rank: 44, loss = 0.59765625
c621-111: Epoch: 0, Step: 45, Rank: 32, loss = 0.69140625
c622-092: Epoch: 0, Step: 45, Rank: 61, loss = 0.7109375
c613-102: Epoch: 0, Step: 45, Rank: 1, loss = 0.69140625
c613-142: Epoch: 0, Step: 45, Rank: 9, loss = 0.76953125
c622-071: Epoch: 0, Step: 45, Rank: 56, loss = 0.6875
c619-031: Epoch: 0, Step: 45, Rank: 18, loss = 0.64453125
c621-142: Epoch: 0, Step: 45, Rank: 39, loss = 0.64453125
c622-072: Epoch: 0, Step: 45, Rank: 57, loss = 0.69140625
c622-001: Epoch: 0, Step: 45, Rank: 42, loss = 0.68359375
c613-152: Epoch: 0, Step: 45, Rank: 11, loss = 0.7109375
c621-062: Epoch: 0, Step: 45, Rank: 23, loss = 0.71875
c621-101: Epoch: 0, Step: 45, Rank: 30, loss = 0.703125
c613-141: Epoch: 0, Step: 45, Rank: 8, loss = 0.69140625
c621-072: Epoch: 0, Step: 45, Rank: 25, loss = 0.70703125
c613-132: Epoch: 0, Step: 45, Rank: 7, loss = 0.64453125
c621-052: Epoch: 0, Step: 45, Rank: 21, loss = 0.6328125
c613-121: Epoch: 0, Step: 45, Rank: 4, loss = 0.69140625
c613-112: Epoch: 0, Step: 45, Rank: 3, loss = 0.69921875
c622-061: Epoch: 0, Step: 45, Rank: 54, loss = 0.69140625
c619-032: Epoch: 0, Step: 45, Rank: 19, loss = 0.69140625
c619-011: Epoch: 0, Step: 45, Rank: 14, loss = 0.71875
c621-131: Epoch: 0, Step: 45, Rank: 36, loss = 0.69140625
c621-122: Epoch: 0, Step: 45, Rank: 35, loss = 0.69140625
c621-121: Epoch: 0, Step: 45, Rank: 34, loss = 0.6875
c621-141: Epoch: 0, Step: 45, Rank: 38, loss = 0.6796875
c621-082: Epoch: 0, Step: 45, Rank: 27, loss = 0.6796875
c622-091: Epoch: 0, Step: 45, Rank: 60, loss = 0.69921875
c622-032: Epoch: 0, Step: 45, Rank: 49, loss = 0.66796875
c619-041: Epoch: 0, Step: 45, Rank: 20, loss = 0.71875
c622-022: Epoch: 0, Step: 45, Rank: 47, loss = 0.69140625
c622-041: Epoch: 0, Step: 45, Rank: 50, loss = 0.69140625
c622-051: Epoch: 0, Step: 45, Rank: 52, loss = 0.66796875
c619-022: Epoch: 0, Step: 45, Rank: 17, loss = 0.64453125
c622-031: Epoch: 0, Step: 45, Rank: 48, loss = 0.69140625
c621-071: Epoch: 0, Step: 45, Rank: 24, loss = 0.69140625
c621-061: Epoch: 0, Step: 45, Rank: 22, loss = 0.64453125
c621-152: Epoch: 0, Step: 45, Rank: 41, loss = 0.69140625
c621-112: Epoch: 0, Step: 45, Rank: 33, loss = 0.69140625
c622-021: Epoch: 0, Step: 45, Rank: 46, loss = 0.69140625
c622-042: Epoch: 0, Step: 45, Rank: 51, loss = 0.69921875
c622-102: Epoch: 0, Step: 45, Rank: 63, loss = 0.796875
c621-102: Epoch: 0, Step: 45, Rank: 31, loss = 0.7109375
c621-092: Epoch: 0, Step: 45, Rank: 29, loss = 0.71875
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.7548828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 46, Rank: 43, loss = 0.62109375
c621-082: Epoch: 0, Step: 46, Rank: 27, loss = 0.6875
c621-091: Epoch: 0, Step: 46, Rank: 28, loss = 0.69140625
c621-132: Epoch: 0, Step: 46, Rank: 37, loss = 0.74609375
c622-012: Epoch: 0, Step: 46, Rank: 45, loss = 0.64453125
c622-052: Epoch: 0, Step: 46, Rank: 53, loss = 0.67578125
c621-131: Epoch: 0, Step: 46, Rank: 36, loss = 0.69140625
c613-151: Epoch: 0, Step: 46, Rank: 10, loss = 0.6796875
c622-032: Epoch: 0, Step: 46, Rank: 49, loss = 0.59765625
c622-001: Epoch: 0, Step: 46, Rank: 42, loss = 0.66796875
c613-101: Epoch: 0, Step: 46, Rank: 0, loss = 0.69140625
c613-152: Epoch: 0, Step: 46, Rank: 11, loss = 0.69140625
c619-021: Epoch: 0, Step: 46, Rank: 16, loss = 0.70703125
c621-072: Epoch: 0, Step: 46, Rank: 25, loss = 0.66796875
c621-081: Epoch: 0, Step: 46, Rank: 26, loss = 0.72265625
c619-002: Epoch: 0, Step: 46, Rank: 13, loss = 0.6796875
c619-031: Epoch: 0, Step: 46, Rank: 18, loss = 0.66796875
c621-111: Epoch: 0, Step: 46, Rank: 32, loss = 0.69921875
c621-141: Epoch: 0, Step: 46, Rank: 38, loss = 0.64453125
c619-022: Epoch: 0, Step: 46, Rank: 17, loss = 0.69140625
c621-142: Epoch: 0, Step: 46, Rank: 39, loss = 0.69921875
c621-061: Epoch: 0, Step: 46, Rank: 22, loss = 0.70703125
c621-151: Epoch: 0, Step: 46, Rank: 40, loss = 0.67578125
c613-141: Epoch: 0, Step: 46, Rank: 8, loss = 0.6796875
c621-112: Epoch: 0, Step: 46, Rank: 33, loss = 0.70703125
c621-121: Epoch: 0, Step: 46, Rank: 34, loss = 0.59765625
c622-051: Epoch: 0, Step: 46, Rank: 52, loss = 0.69140625
c621-052: Epoch: 0, Step: 46, Rank: 21, loss = 0.69140625
c613-132: Epoch: 0, Step: 46, Rank: 7, loss = 0.69140625
c613-131: Epoch: 0, Step: 46, Rank: 6, loss = 0.66796875
c622-092: Epoch: 0, Step: 46, Rank: 61, loss = 0.796875
c621-152: Epoch: 0, Step: 46, Rank: 41, loss = 0.68359375
c613-111: Epoch: 0, Step: 46, Rank: 2, loss = 0.66796875
c622-041: Epoch: 0, Step: 46, Rank: 50, loss = 0.69140625
c622-061: Epoch: 0, Step: 46, Rank: 54, loss = 0.69140625
c613-142: Epoch: 0, Step: 46, Rank: 9, loss = 0.67578125
c622-011: Epoch: 0, Step: 46, Rank: 44, loss = 0.69140625
c619-032: Epoch: 0, Step: 46, Rank: 19, loss = 0.71875
c619-041: Epoch: 0, Step: 46, Rank: 20, loss = 0.62109375
c619-011: Epoch: 0, Step: 46, Rank: 14, loss = 0.66796875
c621-122: Epoch: 0, Step: 46, Rank: 35, loss = 0.640625
c622-101: Epoch: 0, Step: 46, Rank: 62, loss = 0.69140625
c619-012: Epoch: 0, Step: 46, Rank: 15, loss = 0.6875
c613-122: Epoch: 0, Step: 46, Rank: 5, loss = 0.6796875
c619-001: Epoch: 0, Step: 46, Rank: 12, loss = 0.74609375
c622-081: Epoch: 0, Step: 46, Rank: 58, loss = 0.6796875
c622-031: Epoch: 0, Step: 46, Rank: 48, loss = 0.69140625
c622-042: Epoch: 0, Step: 46, Rank: 51, loss = 0.64453125
c613-102: Epoch: 0, Step: 46, Rank: 1, loss = 0.64453125
c621-071: Epoch: 0, Step: 46, Rank: 24, loss = 0.671875
c622-022: Epoch: 0, Step: 46, Rank: 47, loss = 0.66796875
c621-092: Epoch: 0, Step: 46, Rank: 29, loss = 0.74609375
c621-101: Epoch: 0, Step: 46, Rank: 30, loss = 0.79296875
c622-021: Epoch: 0, Step: 46, Rank: 46, loss = 0.66796875
c613-121: Epoch: 0, Step: 46, Rank: 4, loss = 0.69140625
c621-062: Epoch: 0, Step: 46, Rank: 23, loss = 0.59765625
c613-112: Epoch: 0, Step: 46, Rank: 3, loss = 0.69140625
c621-102: Epoch: 0, Step: 46, Rank: 31, loss = 0.64453125
c622-082: Epoch: 0, Step: 46, Rank: 59, loss = 0.64453125
c622-072: Epoch: 0, Step: 46, Rank: 57, loss = 0.69140625
c622-102: Epoch: 0, Step: 46, Rank: 63, loss = 0.69140625
c622-091: Epoch: 0, Step: 46, Rank: 60, loss = 0.7109375
c622-071: Epoch: 0, Step: 46, Rank: 56, loss = 0.6796875
c622-062: Epoch: 0, Step: 46, Rank: 55, loss = 0.6796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 47, Rank: 0, loss = 0.703125
c622-002: Epoch: 0, Step: 47, Rank: 43, loss = 0.66796875
c622-092: Epoch: 0, Step: 47, Rank: 61, loss = 0.6796875
c622-081: Epoch: 0, Step: 47, Rank: 58, loss = 0.69921875
c622-101: Epoch: 0, Step: 47, Rank: 62, loss = 0.69140625
c622-062: Epoch: 0, Step: 47, Rank: 55, loss = 0.69140625
c622-071: Epoch: 0, Step: 47, Rank: 56, loss = 0.69140625
c622-052: Epoch: 0, Step: 47, Rank: 53, loss = 0.69140625
c622-012: Epoch: 0, Step: 47, Rank: 45, loss = 0.71875
c621-111: Epoch: 0, Step: 47, Rank: 32, loss = 0.70703125
c613-151: Epoch: 0, Step: 47, Rank: 10, loss = 0.640625
c613-122: Epoch: 0, Step: 47, Rank: 5, loss = 0.70703125
c622-051: Epoch: 0, Step: 47, Rank: 52, loss = 0.6875
c621-072: Epoch: 0, Step: 47, Rank: 25, loss = 0.69140625
c621-082: Epoch: 0, Step: 47, Rank: 27, loss = 0.68359375
c621-132: Epoch: 0, Step: 47, Rank: 37, loss = 0.69921875
c613-142: Epoch: 0, Step: 47, Rank: 9, loss = 0.71875
c613-132: Epoch: 0, Step: 47, Rank: 7, loss = 0.66796875
c621-081: Epoch: 0, Step: 47, Rank: 26, loss = 0.69140625
c613-111: Epoch: 0, Step: 47, Rank: 2, loss = 0.69140625
c619-022: Epoch: 0, Step: 47, Rank: 17, loss = 0.6796875
c622-061: Epoch: 0, Step: 47, Rank: 54, loss = 0.671875
c619-001: Epoch: 0, Step: 47, Rank: 12, loss = 0.65234375
c619-002: Epoch: 0, Step: 47, Rank: 13, loss = 0.66796875
c619-031: Epoch: 0, Step: 47, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 47, Rank: 16, loss = 0.69140625
c622-072: Epoch: 0, Step: 47, Rank: 57, loss = 0.70703125
c621-142: Epoch: 0, Step: 47, Rank: 39, loss = 0.71875
c621-091: Epoch: 0, Step: 47, Rank: 28, loss = 0.69140625
c622-001: Epoch: 0, Step: 47, Rank: 42, loss = 0.69140625
c613-131: Epoch: 0, Step: 47, Rank: 6, loss = 0.70703125
c619-012: Epoch: 0, Step: 47, Rank: 15, loss = 0.74609375
c622-032: Epoch: 0, Step: 47, Rank: 49, loss = 0.6953125
c622-102: Epoch: 0, Step: 47, Rank: 63, loss = 0.6328125
c622-091: Epoch: 0, Step: 47, Rank: 60, loss = 0.69140625
c619-041: Epoch: 0, Step: 47, Rank: 20, loss = 0.70703125
c613-152: Epoch: 0, Step: 47, Rank: 11, loss = 0.69140625
c622-082: Epoch: 0, Step: 47, Rank: 59, loss = 0.6328125
c613-141: Epoch: 0, Step: 47, Rank: 8, loss = 0.64453125
c613-102: Epoch: 0, Step: 47, Rank: 1, loss = 0.69140625
c621-062: Epoch: 0, Step: 47, Rank: 23, loss = 0.6796875
c621-151: Epoch: 0, Step: 47, Rank: 40, loss = 0.69140625
c619-032: Epoch: 0, Step: 47, Rank: 19, loss = 0.63671875
c621-102: Epoch: 0, Step: 47, Rank: 31, loss = 0.67578125
c613-121: Epoch: 0, Step: 47, Rank: 4, loss = 0.703125
c621-061: Epoch: 0, Step: 47, Rank: 22, loss = 0.70703125
c619-011: Epoch: 0, Step: 47, Rank: 14, loss = 0.64453125
c621-052: Epoch: 0, Step: 47, Rank: 21, loss = 0.74609375
c621-101: Epoch: 0, Step: 47, Rank: 30, loss = 0.70703125
c622-011: Epoch: 0, Step: 47, Rank: 44, loss = 0.6640625
c621-152: Epoch: 0, Step: 47, Rank: 41, loss = 0.6796875
c621-121: Epoch: 0, Step: 47, Rank: 34, loss = 0.74609375
c621-131: Epoch: 0, Step: 47, Rank: 36, loss = 0.62109375
c613-112: Epoch: 0, Step: 47, Rank: 3, loss = 0.71875
c622-041: Epoch: 0, Step: 47, Rank: 50, loss = 0.59375
c621-071: Epoch: 0, Step: 47, Rank: 24, loss = 0.70703125
c622-042: Epoch: 0, Step: 47, Rank: 51, loss = 0.71875
c621-141: Epoch: 0, Step: 47, Rank: 38, loss = 0.71875
c622-031: Epoch: 0, Step: 47, Rank: 48, loss = 0.69140625
c622-021: Epoch: 0, Step: 47, Rank: 46, loss = 0.6796875
c621-112: Epoch: 0, Step: 47, Rank: 33, loss = 0.69140625
c622-022: Epoch: 0, Step: 47, Rank: 47, loss = 0.6796875
c621-092: Epoch: 0, Step: 47, Rank: 29, loss = 0.796875
c621-122: Epoch: 0, Step: 47, Rank: 35, loss = 0.6328125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.12s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.12s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 48, Rank: 0, loss = 0.64453125
c622-002: Epoch: 0, Step: 48, Rank: 43, loss = 0.64453125
c619-021: Epoch: 0, Step: 48, Rank: 16, loss = 0.640625
c622-012: Epoch: 0, Step: 48, Rank: 45, loss = 0.65625
c622-001: Epoch: 0, Step: 48, Rank: 42, loss = 0.6328125
c621-081: Epoch: 0, Step: 48, Rank: 26, loss = 0.6015625
c619-002: Epoch: 0, Step: 48, Rank: 13, loss = 0.67578125
c622-092: Epoch: 0, Step: 48, Rank: 61, loss = 0.5546875
c622-102: Epoch: 0, Step: 48, Rank: 63, loss = 0.6796875
c613-151: Epoch: 0, Step: 48, Rank: 10, loss = 0.67578125
c613-132: Epoch: 0, Step: 48, Rank: 7, loss = 0.69140625
c613-142: Epoch: 0, Step: 48, Rank: 9, loss = 0.66015625
c622-071: Epoch: 0, Step: 48, Rank: 56, loss = 0.64453125
c621-091: Epoch: 0, Step: 48, Rank: 28, loss = 0.7578125
c621-122: Epoch: 0, Step: 48, Rank: 35, loss = 0.65234375
c622-062: Epoch: 0, Step: 48, Rank: 55, loss = 0.6796875
c622-081: Epoch: 0, Step: 48, Rank: 58, loss = 0.67578125
c622-011: Epoch: 0, Step: 48, Rank: 44, loss = 0.474609375
c619-001: Epoch: 0, Step: 48, Rank: 12, loss = 0.63671875
c622-101: Epoch: 0, Step: 48, Rank: 62, loss = 0.65625
c621-072: Epoch: 0, Step: 48, Rank: 25, loss = 0.58203125
c621-132: Epoch: 0, Step: 48, Rank: 37, loss = 0.59765625
c621-121: Epoch: 0, Step: 48, Rank: 34, loss = 0.66796875
c621-131: Epoch: 0, Step: 48, Rank: 36, loss = 0.6640625
c621-101: Epoch: 0, Step: 48, Rank: 30, loss = 0.69140625
c621-142: Epoch: 0, Step: 48, Rank: 39, loss = 0.6328125
c621-111: Epoch: 0, Step: 48, Rank: 32, loss = 0.65625
c613-121: Epoch: 0, Step: 48, Rank: 4, loss = 0.69140625
c621-151: Epoch: 0, Step: 48, Rank: 40, loss = 0.64453125
c621-061: Epoch: 0, Step: 48, Rank: 22, loss = 0.64453125
c622-091: Epoch: 0, Step: 48, Rank: 60, loss = 0.57421875
c622-052: Epoch: 0, Step: 48, Rank: 53, loss = 0.671875
c613-141: Epoch: 0, Step: 48, Rank: 8, loss = 0.5546875
c613-111: Epoch: 0, Step: 48, Rank: 2, loss = 0.65625
c619-032: Epoch: 0, Step: 48, Rank: 19, loss = 0.46875
c621-141: Epoch: 0, Step: 48, Rank: 38, loss = 0.64453125
c622-032: Epoch: 0, Step: 48, Rank: 49, loss = 0.6640625
c621-052: Epoch: 0, Step: 48, Rank: 21, loss = 0.59765625
c619-041: Epoch: 0, Step: 48, Rank: 20, loss = 0.71875
c621-062: Epoch: 0, Step: 48, Rank: 23, loss = 0.64453125
c619-031: Epoch: 0, Step: 48, Rank: 18, loss = 0.69921875
c622-041: Epoch: 0, Step: 48, Rank: 50, loss = 0.66796875
c622-082: Epoch: 0, Step: 48, Rank: 59, loss = 0.59765625
c619-012: Epoch: 0, Step: 48, Rank: 15, loss = 0.65234375
c621-112: Epoch: 0, Step: 48, Rank: 33, loss = 0.625
c619-022: Epoch: 0, Step: 48, Rank: 17, loss = 0.69140625
c613-112: Epoch: 0, Step: 48, Rank: 3, loss = 0.6796875
c622-051: Epoch: 0, Step: 48, Rank: 52, loss = 0.66796875
c613-152: Epoch: 0, Step: 48, Rank: 11, loss = 0.6015625
c621-082: Epoch: 0, Step: 48, Rank: 27, loss = 0.6484375
c621-152: Epoch: 0, Step: 48, Rank: 41, loss = 0.69140625
c621-092: Epoch: 0, Step: 48, Rank: 29, loss = 0.65625
c613-131: Epoch: 0, Step: 48, Rank: 6, loss = 0.65625
c621-102: Epoch: 0, Step: 48, Rank: 31, loss = 0.6328125
c622-021: Epoch: 0, Step: 48, Rank: 46, loss = 0.5390625
c621-071: Epoch: 0, Step: 48, Rank: 24, loss = 0.6484375
c613-122: Epoch: 0, Step: 48, Rank: 5, loss = 0.67578125
c622-031: Epoch: 0, Step: 48, Rank: 48, loss = 0.640625
c622-022: Epoch: 0, Step: 48, Rank: 47, loss = 0.59765625
c622-072: Epoch: 0, Step: 48, Rank: 57, loss = 0.6640625
c619-011: Epoch: 0, Step: 48, Rank: 14, loss = 0.66796875
c613-102: Epoch: 0, Step: 48, Rank: 1, loss = 0.64453125
c622-061: Epoch: 0, Step: 48, Rank: 54, loss = 0.59375
c622-042: Epoch: 0, Step: 48, Rank: 51, loss = 0.59765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 49, Rank: 43, loss = 0.66796875
c621-081: Epoch: 0, Step: 49, Rank: 26, loss = 0.671875
c621-131: Epoch: 0, Step: 49, Rank: 36, loss = 0.447265625
c621-132: Epoch: 0, Step: 49, Rank: 37, loss = 0.69140625
c622-052: Epoch: 0, Step: 49, Rank: 53, loss = 0.59765625
c621-092: Epoch: 0, Step: 49, Rank: 29, loss = 0.5546875
c621-082: Epoch: 0, Step: 49, Rank: 27, loss = 0.62109375
c622-012: Epoch: 0, Step: 49, Rank: 45, loss = 0.6484375
c621-151: Epoch: 0, Step: 49, Rank: 40, loss = 0.66796875
c619-041: Epoch: 0, Step: 49, Rank: 20, loss = 0.66015625
c621-091: Epoch: 0, Step: 49, Rank: 28, loss = 0.59765625
c622-001: Epoch: 0, Step: 49, Rank: 42, loss = 0.69140625
c622-011: Epoch: 0, Step: 49, Rank: 44, loss = 0.59765625
c621-052: Epoch: 0, Step: 49, Rank: 21, loss = 0.62109375
c621-101: Epoch: 0, Step: 49, Rank: 30, loss = 0.69140625
c619-002: Epoch: 0, Step: 49, Rank: 13, loss = 0.66796875
c619-001: Epoch: 0, Step: 49, Rank: 12, loss = 0.625
c619-031: Epoch: 0, Step: 49, Rank: 18, loss = 0.64453125
c621-111: Epoch: 0, Step: 49, Rank: 32, loss = 0.69140625
c622-061: Epoch: 0, Step: 49, Rank: 54, loss = 0.6328125
c621-122: Epoch: 0, Step: 49, Rank: 35, loss = 0.69140625
c621-152: Epoch: 0, Step: 49, Rank: 41, loss = 0.57421875
c619-032: Epoch: 0, Step: 49, Rank: 19, loss = 0.5859375
c621-121: Epoch: 0, Step: 49, Rank: 34, loss = 0.64453125
c613-101: Epoch: 0, Step: 49, Rank: 0, loss = 0.64453125
c619-021: Epoch: 0, Step: 49, Rank: 16, loss = 0.64453125
c621-061: Epoch: 0, Step: 49, Rank: 22, loss = 0.62109375
c622-051: Epoch: 0, Step: 49, Rank: 52, loss = 0.66796875
c621-062: Epoch: 0, Step: 49, Rank: 23, loss = 0.63671875
c613-152: Epoch: 0, Step: 49, Rank: 11, loss = 0.53515625
c621-072: Epoch: 0, Step: 49, Rank: 25, loss = 0.66796875
c613-151: Epoch: 0, Step: 49, Rank: 10, loss = 0.494140625
c621-142: Epoch: 0, Step: 49, Rank: 39, loss = 0.69140625
c619-022: Epoch: 0, Step: 49, Rank: 17, loss = 0.6171875
c621-112: Epoch: 0, Step: 49, Rank: 33, loss = 0.64453125
c622-042: Epoch: 0, Step: 49, Rank: 51, loss = 0.59765625
c621-141: Epoch: 0, Step: 49, Rank: 38, loss = 0.66796875
c622-101: Epoch: 0, Step: 49, Rank: 62, loss = 0.59765625
c613-111: Epoch: 0, Step: 49, Rank: 2, loss = 0.65625
c621-071: Epoch: 0, Step: 49, Rank: 24, loss = 0.76953125
c622-081: Epoch: 0, Step: 49, Rank: 58, loss = 0.5546875
c622-032: Epoch: 0, Step: 49, Rank: 49, loss = 0.60546875
c622-021: Epoch: 0, Step: 49, Rank: 46, loss = 0.6640625
c613-131: Epoch: 0, Step: 49, Rank: 6, loss = 0.64453125
c621-102: Epoch: 0, Step: 49, Rank: 31, loss = 0.66796875
c622-022: Epoch: 0, Step: 49, Rank: 47, loss = 0.62109375
c613-132: Epoch: 0, Step: 49, Rank: 7, loss = 0.64453125
c622-031: Epoch: 0, Step: 49, Rank: 48, loss = 0.6875
c613-121: Epoch: 0, Step: 49, Rank: 4, loss = 0.66796875
c622-092: Epoch: 0, Step: 49, Rank: 61, loss = 0.515625
c622-102: Epoch: 0, Step: 49, Rank: 63, loss = 0.64453125
c613-112: Epoch: 0, Step: 49, Rank: 3, loss = 0.671875
c613-122: Epoch: 0, Step: 49, Rank: 5, loss = 0.64453125
c622-071: Epoch: 0, Step: 49, Rank: 56, loss = 0.64453125
c619-011: Epoch: 0, Step: 49, Rank: 14, loss = 0.65625
c622-041: Epoch: 0, Step: 49, Rank: 50, loss = 0.64453125
c619-012: Epoch: 0, Step: 49, Rank: 15, loss = 0.6640625
c622-062: Epoch: 0, Step: 49, Rank: 55, loss = 0.64453125
c613-102: Epoch: 0, Step: 49, Rank: 1, loss = 0.64453125
c613-141: Epoch: 0, Step: 49, Rank: 8, loss = 0.59765625
c622-072: Epoch: 0, Step: 49, Rank: 57, loss = 0.6953125
c613-142: Epoch: 0, Step: 49, Rank: 9, loss = 0.59765625
c622-091: Epoch: 0, Step: 49, Rank: 60, loss = 0.67578125
c622-082: Epoch: 0, Step: 49, Rank: 59, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 50, Rank: 16, loss = 0.625
c621-111: Epoch: 0, Step: 50, Rank: 32, loss = 0.64453125
c622-002: Epoch: 0, Step: 50, Rank: 43, loss = 0.64453125
c619-001: Epoch: 0, Step: 50, Rank: 12, loss = 0.51171875
c619-002: Epoch: 0, Step: 50, Rank: 13, loss = 0.71875
c619-022: Epoch: 0, Step: 50, Rank: 17, loss = 0.625
c621-132: Epoch: 0, Step: 50, Rank: 37, loss = 0.64453125
c622-012: Epoch: 0, Step: 50, Rank: 45, loss = 0.65625
c613-101: Epoch: 0, Step: 50, Rank: 0, loss = 0.62109375
c613-132: Epoch: 0, Step: 50, Rank: 7, loss = 0.66015625
c613-152: Epoch: 0, Step: 50, Rank: 11, loss = 0.64453125
c622-001: Epoch: 0, Step: 50, Rank: 42, loss = 0.6640625
c613-122: Epoch: 0, Step: 50, Rank: 5, loss = 0.68359375
c621-151: Epoch: 0, Step: 50, Rank: 40, loss = 0.220703125
c622-081: Epoch: 0, Step: 50, Rank: 58, loss = 0.69140625
c621-121: Epoch: 0, Step: 50, Rank: 34, loss = 0.6328125
c619-012: Epoch: 0, Step: 50, Rank: 15, loss = 0.67578125
c621-142: Epoch: 0, Step: 50, Rank: 39, loss = 0.5546875
c621-112: Epoch: 0, Step: 50, Rank: 33, loss = 0.67578125
c622-032: Epoch: 0, Step: 50, Rank: 49, loss = 0.51171875
c621-072: Epoch: 0, Step: 50, Rank: 25, loss = 0.66796875
c622-052: Epoch: 0, Step: 50, Rank: 53, loss = 0.6640625
c621-101: Epoch: 0, Step: 50, Rank: 30, loss = 0.69140625
c621-152: Epoch: 0, Step: 50, Rank: 41, loss = 0.66796875
c622-101: Epoch: 0, Step: 50, Rank: 62, loss = 0.6328125
c621-082: Epoch: 0, Step: 50, Rank: 27, loss = 0.7109375
c613-151: Epoch: 0, Step: 50, Rank: 10, loss = 0.65234375
c613-121: Epoch: 0, Step: 50, Rank: 4, loss = 0.64453125
c621-061: Epoch: 0, Step: 50, Rank: 22, loss = 0.64453125
c621-122: Epoch: 0, Step: 50, Rank: 35, loss = 0.70703125
c613-111: Epoch: 0, Step: 50, Rank: 2, loss = 0.66015625
c621-081: Epoch: 0, Step: 50, Rank: 26, loss = 0.66796875
c619-031: Epoch: 0, Step: 50, Rank: 18, loss = 0.64453125
c613-131: Epoch: 0, Step: 50, Rank: 6, loss = 0.66796875
c622-041: Epoch: 0, Step: 50, Rank: 50, loss = 0.7109375
c622-022: Epoch: 0, Step: 50, Rank: 47, loss = 0.640625
c619-011: Epoch: 0, Step: 50, Rank: 14, loss = 0.69140625
c619-032: Epoch: 0, Step: 50, Rank: 19, loss = 0.69921875
c622-031: Epoch: 0, Step: 50, Rank: 48, loss = 0.69140625
c621-102: Epoch: 0, Step: 50, Rank: 31, loss = 0.62109375
c621-071: Epoch: 0, Step: 50, Rank: 24, loss = 0.66015625
c622-071: Epoch: 0, Step: 50, Rank: 56, loss = 0.64453125
c621-131: Epoch: 0, Step: 50, Rank: 36, loss = 0.66796875
c621-141: Epoch: 0, Step: 50, Rank: 38, loss = 0.5859375
c613-102: Epoch: 0, Step: 50, Rank: 1, loss = 0.6328125
c621-062: Epoch: 0, Step: 50, Rank: 23, loss = 0.5546875
c622-011: Epoch: 0, Step: 50, Rank: 44, loss = 0.474609375
c622-061: Epoch: 0, Step: 50, Rank: 54, loss = 0.65234375
c622-102: Epoch: 0, Step: 50, Rank: 63, loss = 0.64453125
c622-072: Epoch: 0, Step: 50, Rank: 57, loss = 0.65625
c622-091: Epoch: 0, Step: 50, Rank: 60, loss = 0.6640625
c621-052: Epoch: 0, Step: 50, Rank: 21, loss = 0.6796875
c613-112: Epoch: 0, Step: 50, Rank: 3, loss = 0.69921875
c621-091: Epoch: 0, Step: 50, Rank: 28, loss = 0.66796875
c613-142: Epoch: 0, Step: 50, Rank: 9, loss = 0.55078125
c619-041: Epoch: 0, Step: 50, Rank: 20, loss = 0.64453125
c613-141: Epoch: 0, Step: 50, Rank: 8, loss = 0.6640625
c622-042: Epoch: 0, Step: 50, Rank: 51, loss = 0.62109375
c622-051: Epoch: 0, Step: 50, Rank: 52, loss = 0.64453125
c621-092: Epoch: 0, Step: 50, Rank: 29, loss = 0.5546875
c622-092: Epoch: 0, Step: 50, Rank: 61, loss = 0.6953125
c622-021: Epoch: 0, Step: 50, Rank: 46, loss = 0.66796875
c622-082: Epoch: 0, Step: 50, Rank: 59, loss = 0.59375
c622-062: Epoch: 0, Step: 50, Rank: 55, loss = 0.65234375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 51, Rank: 16, loss = 0.73828125
c619-031: Epoch: 0, Step: 51, Rank: 18, loss = 0.64453125
c613-101: Epoch: 0, Step: 51, Rank: 0, loss = 0.6796875
c613-111: Epoch: 0, Step: 51, Rank: 2, loss = 0.609375
c619-012: Epoch: 0, Step: 51, Rank: 15, loss = 0.67578125
c613-121: Epoch: 0, Step: 51, Rank: 4, loss = 0.671875
c622-081: Epoch: 0, Step: 51, Rank: 58, loss = 0.66796875
c619-001: Epoch: 0, Step: 51, Rank: 12, loss = 0.69140625
c613-131: Epoch: 0, Step: 51, Rank: 6, loss = 0.62109375
c622-092: Epoch: 0, Step: 51, Rank: 61, loss = 0.65625
c613-102: Epoch: 0, Step: 51, Rank: 1, loss = 0.625
c622-002: Epoch: 0, Step: 51, Rank: 43, loss = 0.65625
c619-002: Epoch: 0, Step: 51, Rank: 13, loss = 0.59375
c613-112: Epoch: 0, Step: 51, Rank: 3, loss = 0.625
c622-091: Epoch: 0, Step: 51, Rank: 60, loss = 0.78515625
c613-152: Epoch: 0, Step: 51, Rank: 11, loss = 0.6796875
c622-101: Epoch: 0, Step: 51, Rank: 62, loss = 0.62890625
c622-052: Epoch: 0, Step: 51, Rank: 53, loss = 0.6796875
c613-122: Epoch: 0, Step: 51, Rank: 5, loss = 0.71875
c622-012: Epoch: 0, Step: 51, Rank: 45, loss = 0.625
c619-022: Epoch: 0, Step: 51, Rank: 17, loss = 0.65234375
c613-151: Epoch: 0, Step: 51, Rank: 10, loss = 0.6796875
c621-081: Epoch: 0, Step: 51, Rank: 26, loss = 0.6953125
c619-011: Epoch: 0, Step: 51, Rank: 14, loss = 0.65234375
c621-111: Epoch: 0, Step: 51, Rank: 32, loss = 0.5546875
c622-102: Epoch: 0, Step: 51, Rank: 63, loss = 0.64453125
c621-091: Epoch: 0, Step: 51, Rank: 28, loss = 0.6640625
c622-082: Epoch: 0, Step: 51, Rank: 59, loss = 0.66796875
c621-131: Epoch: 0, Step: 51, Rank: 36, loss = 0.69140625
c613-132: Epoch: 0, Step: 51, Rank: 7, loss = 0.64453125
c613-142: Epoch: 0, Step: 51, Rank: 9, loss = 0.66796875
c613-141: Epoch: 0, Step: 51, Rank: 8, loss = 0.65234375
c621-112: Epoch: 0, Step: 51, Rank: 33, loss = 0.66796875
c621-151: Epoch: 0, Step: 51, Rank: 40, loss = 0.65625
c621-072: Epoch: 0, Step: 51, Rank: 25, loss = 0.69140625
c621-132: Epoch: 0, Step: 51, Rank: 37, loss = 0.62109375
c619-032: Epoch: 0, Step: 51, Rank: 19, loss = 0.6640625
c622-032: Epoch: 0, Step: 51, Rank: 49, loss = 0.63671875
c622-031: Epoch: 0, Step: 51, Rank: 48, loss = 0.64453125
c621-121: Epoch: 0, Step: 51, Rank: 34, loss = 0.474609375
c622-061: Epoch: 0, Step: 51, Rank: 54, loss = 0.66796875
c622-042: Epoch: 0, Step: 51, Rank: 51, loss = 0.63671875
c622-072: Epoch: 0, Step: 51, Rank: 57, loss = 0.66796875
c621-122: Epoch: 0, Step: 51, Rank: 35, loss = 0.69140625
c622-051: Epoch: 0, Step: 51, Rank: 52, loss = 0.59375
c621-092: Epoch: 0, Step: 51, Rank: 29, loss = 0.59765625
c622-021: Epoch: 0, Step: 51, Rank: 46, loss = 0.6328125
c621-082: Epoch: 0, Step: 51, Rank: 27, loss = 0.6875
c622-022: Epoch: 0, Step: 51, Rank: 47, loss = 0.6328125
c622-041: Epoch: 0, Step: 51, Rank: 50, loss = 0.67578125
c622-001: Epoch: 0, Step: 51, Rank: 42, loss = 0.59765625
c622-011: Epoch: 0, Step: 51, Rank: 44, loss = 0.69140625
c621-101: Epoch: 0, Step: 51, Rank: 30, loss = 0.65625
c621-061: Epoch: 0, Step: 51, Rank: 22, loss = 0.6875
c622-071: Epoch: 0, Step: 51, Rank: 56, loss = 0.609375
c622-062: Epoch: 0, Step: 51, Rank: 55, loss = 0.60546875
c621-052: Epoch: 0, Step: 51, Rank: 21, loss = 0.66796875
c621-152: Epoch: 0, Step: 51, Rank: 41, loss = 0.64453125
c621-102: Epoch: 0, Step: 51, Rank: 31, loss = 0.66796875
c621-142: Epoch: 0, Step: 51, Rank: 39, loss = 0.69140625
c621-141: Epoch: 0, Step: 51, Rank: 38, loss = 0.69140625
c619-041: Epoch: 0, Step: 51, Rank: 20, loss = 0.625
c621-071: Epoch: 0, Step: 51, Rank: 24, loss = 0.59765625
c621-062: Epoch: 0, Step: 51, Rank: 23, loss = 0.65234375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 52, Rank: 43, loss = 0.6640625
c622-001: Epoch: 0, Step: 52, Rank: 42, loss = 0.68359375
c622-012: Epoch: 0, Step: 52, Rank: 45, loss = 0.66796875
c621-152: Epoch: 0, Step: 52, Rank: 41, loss = 0.6796875
c622-011: Epoch: 0, Step: 52, Rank: 44, loss = 0.59375
c621-131: Epoch: 0, Step: 52, Rank: 36, loss = 0.65625
c622-032: Epoch: 0, Step: 52, Rank: 49, loss = 0.69921875
c621-151: Epoch: 0, Step: 52, Rank: 40, loss = 0.65625
c621-142: Epoch: 0, Step: 52, Rank: 39, loss = 0.69921875
c622-022: Epoch: 0, Step: 52, Rank: 47, loss = 0.66796875
c622-021: Epoch: 0, Step: 52, Rank: 46, loss = 0.62109375
c621-132: Epoch: 0, Step: 52, Rank: 37, loss = 0.6328125
c622-081: Epoch: 0, Step: 52, Rank: 58, loss = 0.6796875
c622-051: Epoch: 0, Step: 52, Rank: 52, loss = 0.66796875
c622-031: Epoch: 0, Step: 52, Rank: 48, loss = 0.6328125
c621-111: Epoch: 0, Step: 52, Rank: 32, loss = 0.65625
c622-052: Epoch: 0, Step: 52, Rank: 53, loss = 0.66796875
c622-041: Epoch: 0, Step: 52, Rank: 50, loss = 0.64453125
c621-121: Epoch: 0, Step: 52, Rank: 34, loss = 0.5546875
c621-141: Epoch: 0, Step: 52, Rank: 38, loss = 0.65234375
c622-061: Epoch: 0, Step: 52, Rank: 54, loss = 0.65625
c622-042: Epoch: 0, Step: 52, Rank: 51, loss = 0.61328125
c619-031: Epoch: 0, Step: 52, Rank: 18, loss = 0.62109375
c622-071: Epoch: 0, Step: 52, Rank: 56, loss = 0.62109375
c613-101: Epoch: 0, Step: 52, Rank: 0, loss = 0.65625
c622-072: Epoch: 0, Step: 52, Rank: 57, loss = 0.69140625
c619-021: Epoch: 0, Step: 52, Rank: 16, loss = 0.66796875
c621-112: Epoch: 0, Step: 52, Rank: 33, loss = 0.62109375
c621-091: Epoch: 0, Step: 52, Rank: 28, loss = 0.65234375
c619-002: Epoch: 0, Step: 52, Rank: 13, loss = 0.66796875
c622-092: Epoch: 0, Step: 52, Rank: 61, loss = 0.69140625
c619-001: Epoch: 0, Step: 52, Rank: 12, loss = 0.59765625
c621-122: Epoch: 0, Step: 52, Rank: 35, loss = 0.6640625
c622-101: Epoch: 0, Step: 52, Rank: 62, loss = 0.67578125
c622-062: Epoch: 0, Step: 52, Rank: 55, loss = 0.67578125
c613-132: Epoch: 0, Step: 52, Rank: 7, loss = 0.66796875
c613-131: Epoch: 0, Step: 52, Rank: 6, loss = 0.6640625
c619-032: Epoch: 0, Step: 52, Rank: 19, loss = 0.51953125
c613-121: Epoch: 0, Step: 52, Rank: 4, loss = 0.5546875
c619-022: Epoch: 0, Step: 52, Rank: 17, loss = 0.671875
c613-152: Epoch: 0, Step: 52, Rank: 11, loss = 0.66796875
c613-102: Epoch: 0, Step: 52, Rank: 1, loss = 0.64453125
c619-012: Epoch: 0, Step: 52, Rank: 15, loss = 0.57421875
c621-052: Epoch: 0, Step: 52, Rank: 21, loss = 0.74609375
c621-102: Epoch: 0, Step: 52, Rank: 31, loss = 0.515625
c622-082: Epoch: 0, Step: 52, Rank: 59, loss = 0.6640625
c621-081: Epoch: 0, Step: 52, Rank: 26, loss = 0.70703125
c613-111: Epoch: 0, Step: 52, Rank: 2, loss = 0.69140625
c613-112: Epoch: 0, Step: 52, Rank: 3, loss = 0.62109375
c621-092: Epoch: 0, Step: 52, Rank: 29, loss = 0.66796875
c621-101: Epoch: 0, Step: 52, Rank: 30, loss = 0.609375
c619-041: Epoch: 0, Step: 52, Rank: 20, loss = 0.65234375
c619-011: Epoch: 0, Step: 52, Rank: 14, loss = 0.69140625
c621-082: Epoch: 0, Step: 52, Rank: 27, loss = 0.67578125
c613-142: Epoch: 0, Step: 52, Rank: 9, loss = 0.64453125
c613-122: Epoch: 0, Step: 52, Rank: 5, loss = 0.6640625
c613-141: Epoch: 0, Step: 52, Rank: 8, loss = 0.64453125
c622-091: Epoch: 0, Step: 52, Rank: 60, loss = 0.69140625
c622-102: Epoch: 0, Step: 52, Rank: 63, loss = 0.6171875
c621-072: Epoch: 0, Step: 52, Rank: 25, loss = 0.6328125
c613-151: Epoch: 0, Step: 52, Rank: 10, loss = 0.67578125
c621-062: Epoch: 0, Step: 52, Rank: 23, loss = 0.62109375
c621-061: Epoch: 0, Step: 52, Rank: 22, loss = 0.67578125
c621-071: Epoch: 0, Step: 52, Rank: 24, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 53, Rank: 43, loss = 0.64453125
c621-072: Epoch: 0, Step: 53, Rank: 25, loss = 0.5546875
c619-002: Epoch: 0, Step: 53, Rank: 13, loss = 0.6640625
c621-112: Epoch: 0, Step: 53, Rank: 33, loss = 0.625
c621-121: Epoch: 0, Step: 53, Rank: 34, loss = 0.66796875
c621-081: Epoch: 0, Step: 53, Rank: 26, loss = 0.6640625
c621-131: Epoch: 0, Step: 53, Rank: 36, loss = 0.57421875
c621-151: Epoch: 0, Step: 53, Rank: 40, loss = 0.59765625
c621-091: Epoch: 0, Step: 53, Rank: 28, loss = 0.62109375
c622-001: Epoch: 0, Step: 53, Rank: 42, loss = 0.66796875
c613-132: Epoch: 0, Step: 53, Rank: 7, loss = 0.62109375
c613-101: Epoch: 0, Step: 53, Rank: 0, loss = 0.69140625
c621-101: Epoch: 0, Step: 53, Rank: 30, loss = 0.64453125
c613-152: Epoch: 0, Step: 53, Rank: 11, loss = 0.69140625
c619-031: Epoch: 0, Step: 53, Rank: 18, loss = 0.6796875
c621-071: Epoch: 0, Step: 53, Rank: 24, loss = 0.51171875
c621-111: Epoch: 0, Step: 53, Rank: 32, loss = 0.64453125
c619-021: Epoch: 0, Step: 53, Rank: 16, loss = 0.66796875
c613-131: Epoch: 0, Step: 53, Rank: 6, loss = 0.66796875
c621-132: Epoch: 0, Step: 53, Rank: 37, loss = 0.65625
c621-102: Epoch: 0, Step: 53, Rank: 31, loss = 0.57421875
c613-151: Epoch: 0, Step: 53, Rank: 10, loss = 0.69140625
c613-142: Epoch: 0, Step: 53, Rank: 9, loss = 0.69140625
c613-121: Epoch: 0, Step: 53, Rank: 4, loss = 0.67578125
c621-141: Epoch: 0, Step: 53, Rank: 38, loss = 0.59765625
c621-082: Epoch: 0, Step: 53, Rank: 27, loss = 0.64453125
c619-012: Epoch: 0, Step: 53, Rank: 15, loss = 0.64453125
c621-061: Epoch: 0, Step: 53, Rank: 22, loss = 0.64453125
c621-052: Epoch: 0, Step: 53, Rank: 21, loss = 0.69140625
c621-142: Epoch: 0, Step: 53, Rank: 39, loss = 0.66796875
c613-111: Epoch: 0, Step: 53, Rank: 2, loss = 0.65234375
c622-081: Epoch: 0, Step: 53, Rank: 58, loss = 0.69140625
c622-101: Epoch: 0, Step: 53, Rank: 62, loss = 0.66796875
c619-022: Epoch: 0, Step: 53, Rank: 17, loss = 0.474609375
c619-041: Epoch: 0, Step: 53, Rank: 20, loss = 0.64453125
c621-152: Epoch: 0, Step: 53, Rank: 41, loss = 0.62109375
c619-032: Epoch: 0, Step: 53, Rank: 19, loss = 0.70703125
c622-092: Epoch: 0, Step: 53, Rank: 61, loss = 0.64453125
c613-141: Epoch: 0, Step: 53, Rank: 8, loss = 0.69140625
c621-062: Epoch: 0, Step: 53, Rank: 23, loss = 0.59765625
c619-001: Epoch: 0, Step: 53, Rank: 12, loss = 0.64453125
c619-011: Epoch: 0, Step: 53, Rank: 14, loss = 0.69140625
c622-102: Epoch: 0, Step: 53, Rank: 63, loss = 0.64453125
c621-122: Epoch: 0, Step: 53, Rank: 35, loss = 0.6796875
c622-052: Epoch: 0, Step: 53, Rank: 53, loss = 0.62109375
c613-112: Epoch: 0, Step: 53, Rank: 3, loss = 0.63671875
c622-012: Epoch: 0, Step: 53, Rank: 45, loss = 0.66796875
c621-092: Epoch: 0, Step: 53, Rank: 29, loss = 0.65234375
c613-122: Epoch: 0, Step: 53, Rank: 5, loss = 0.57421875
c622-041: Epoch: 0, Step: 53, Rank: 50, loss = 0.62109375
c622-011: Epoch: 0, Step: 53, Rank: 44, loss = 0.68359375
c613-102: Epoch: 0, Step: 53, Rank: 1, loss = 0.671875
c622-071: Epoch: 0, Step: 53, Rank: 56, loss = 0.69140625
c622-062: Epoch: 0, Step: 53, Rank: 55, loss = 0.66796875
c622-072: Epoch: 0, Step: 53, Rank: 57, loss = 0.64453125
c622-042: Epoch: 0, Step: 53, Rank: 51, loss = 0.625
c622-032: Epoch: 0, Step: 53, Rank: 49, loss = 0.65234375
c622-031: Epoch: 0, Step: 53, Rank: 48, loss = 0.67578125
c622-051: Epoch: 0, Step: 53, Rank: 52, loss = 0.609375
c622-022: Epoch: 0, Step: 53, Rank: 47, loss = 0.64453125
c622-091: Epoch: 0, Step: 53, Rank: 60, loss = 0.6796875
c622-082: Epoch: 0, Step: 53, Rank: 59, loss = 0.64453125
c622-021: Epoch: 0, Step: 53, Rank: 46, loss = 0.6640625
c622-061: Epoch: 0, Step: 53, Rank: 54, loss = 0.59765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 54, Rank: 58, loss = 0.66796875
c622-002: Epoch: 0, Step: 54, Rank: 43, loss = 0.69921875
c621-081: Epoch: 0, Step: 54, Rank: 26, loss = 0.58984375
c613-101: Epoch: 0, Step: 54, Rank: 0, loss = 0.66796875
c613-132: Epoch: 0, Step: 54, Rank: 7, loss = 0.6796875
c619-002: Epoch: 0, Step: 54, Rank: 13, loss = 0.64453125
c621-111: Epoch: 0, Step: 54, Rank: 32, loss = 0.640625
c621-072: Epoch: 0, Step: 54, Rank: 25, loss = 0.66796875
c619-021: Epoch: 0, Step: 54, Rank: 16, loss = 0.6015625
c622-061: Epoch: 0, Step: 54, Rank: 54, loss = 0.6328125
c621-151: Epoch: 0, Step: 54, Rank: 40, loss = 0.5859375
c622-012: Epoch: 0, Step: 54, Rank: 45, loss = 0.6796875
c622-062: Epoch: 0, Step: 54, Rank: 55, loss = 0.59765625
c619-001: Epoch: 0, Step: 54, Rank: 12, loss = 0.62109375
c622-001: Epoch: 0, Step: 54, Rank: 42, loss = 0.66796875
c622-071: Epoch: 0, Step: 54, Rank: 56, loss = 0.63671875
c621-132: Epoch: 0, Step: 54, Rank: 37, loss = 0.6796875
c613-111: Epoch: 0, Step: 54, Rank: 2, loss = 0.6796875
c619-041: Epoch: 0, Step: 54, Rank: 20, loss = 0.59765625
c622-052: Epoch: 0, Step: 54, Rank: 53, loss = 0.6171875
c613-152: Epoch: 0, Step: 54, Rank: 11, loss = 0.59765625
c621-101: Epoch: 0, Step: 54, Rank: 30, loss = 0.6328125
c622-082: Epoch: 0, Step: 54, Rank: 59, loss = 0.51171875
c613-151: Epoch: 0, Step: 54, Rank: 10, loss = 0.6640625
c613-131: Epoch: 0, Step: 54, Rank: 6, loss = 0.56640625
c621-102: Epoch: 0, Step: 54, Rank: 31, loss = 0.609375
c622-091: Epoch: 0, Step: 54, Rank: 60, loss = 0.70703125
c622-072: Epoch: 0, Step: 54, Rank: 57, loss = 0.45703125
c621-082: Epoch: 0, Step: 54, Rank: 27, loss = 0.66796875
c621-091: Epoch: 0, Step: 54, Rank: 28, loss = 0.65625
c619-012: Epoch: 0, Step: 54, Rank: 15, loss = 0.65625
c622-092: Epoch: 0, Step: 54, Rank: 61, loss = 0.65625
c622-101: Epoch: 0, Step: 54, Rank: 62, loss = 0.69140625
c621-141: Epoch: 0, Step: 54, Rank: 38, loss = 0.69140625
c621-052: Epoch: 0, Step: 54, Rank: 21, loss = 0.64453125
c621-142: Epoch: 0, Step: 54, Rank: 39, loss = 0.5546875
c621-061: Epoch: 0, Step: 54, Rank: 22, loss = 0.6640625
c621-152: Epoch: 0, Step: 54, Rank: 41, loss = 0.6796875
c622-011: Epoch: 0, Step: 54, Rank: 44, loss = 0.62109375
c622-032: Epoch: 0, Step: 54, Rank: 49, loss = 0.59765625
c613-122: Epoch: 0, Step: 54, Rank: 5, loss = 0.59765625
c621-131: Epoch: 0, Step: 54, Rank: 36, loss = 0.6796875
c613-121: Epoch: 0, Step: 54, Rank: 4, loss = 0.65625
c622-022: Epoch: 0, Step: 54, Rank: 47, loss = 0.64453125
c619-031: Epoch: 0, Step: 54, Rank: 18, loss = 0.57421875
c621-112: Epoch: 0, Step: 54, Rank: 33, loss = 0.64453125
c621-121: Epoch: 0, Step: 54, Rank: 34, loss = 0.69140625
c621-062: Epoch: 0, Step: 54, Rank: 23, loss = 0.64453125
c622-051: Epoch: 0, Step: 54, Rank: 52, loss = 0.58984375
c622-042: Epoch: 0, Step: 54, Rank: 51, loss = 0.66796875
c613-142: Epoch: 0, Step: 54, Rank: 9, loss = 0.71875
c622-102: Epoch: 0, Step: 54, Rank: 63, loss = 0.64453125
c613-112: Epoch: 0, Step: 54, Rank: 3, loss = 0.67578125
c619-022: Epoch: 0, Step: 54, Rank: 17, loss = 0.69140625
c621-122: Epoch: 0, Step: 54, Rank: 35, loss = 0.6875
c613-141: Epoch: 0, Step: 54, Rank: 8, loss = 0.51171875
c622-041: Epoch: 0, Step: 54, Rank: 50, loss = 0.66796875
c619-011: Epoch: 0, Step: 54, Rank: 14, loss = 0.64453125
c613-102: Epoch: 0, Step: 54, Rank: 1, loss = 0.5546875
c619-032: Epoch: 0, Step: 54, Rank: 19, loss = 0.64453125
c621-071: Epoch: 0, Step: 54, Rank: 24, loss = 0.65625
c622-031: Epoch: 0, Step: 54, Rank: 48, loss = 0.68359375
c621-092: Epoch: 0, Step: 54, Rank: 29, loss = 0.65234375
c622-021: Epoch: 0, Step: 54, Rank: 46, loss = 0.6640625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 55, Rank: 16, loss = 0.69921875
c622-002: Epoch: 0, Step: 55, Rank: 43, loss = 0.69140625
c621-111: Epoch: 0, Step: 55, Rank: 32, loss = 0.59375
c621-081: Epoch: 0, Step: 55, Rank: 26, loss = 0.6171875
c621-131: Epoch: 0, Step: 55, Rank: 36, loss = 0.64453125
c619-022: Epoch: 0, Step: 55, Rank: 17, loss = 0.5546875
c621-101: Epoch: 0, Step: 55, Rank: 30, loss = 0.6328125
c622-012: Epoch: 0, Step: 55, Rank: 45, loss = 0.640625
c613-101: Epoch: 0, Step: 55, Rank: 0, loss = 0.71875
c621-072: Epoch: 0, Step: 55, Rank: 25, loss = 0.59765625
c621-082: Epoch: 0, Step: 55, Rank: 27, loss = 0.59765625
c622-032: Epoch: 0, Step: 55, Rank: 49, loss = 0.67578125
c622-052: Epoch: 0, Step: 55, Rank: 53, loss = 0.65625
c621-112: Epoch: 0, Step: 55, Rank: 33, loss = 0.59765625
c619-032: Epoch: 0, Step: 55, Rank: 19, loss = 0.71875
c621-052: Epoch: 0, Step: 55, Rank: 21, loss = 0.6328125
c621-121: Epoch: 0, Step: 55, Rank: 34, loss = 0.6171875
c619-002: Epoch: 0, Step: 55, Rank: 13, loss = 0.65625
c619-041: Epoch: 0, Step: 55, Rank: 20, loss = 0.671875
c621-132: Epoch: 0, Step: 55, Rank: 37, loss = 0.6640625
c621-122: Epoch: 0, Step: 55, Rank: 35, loss = 0.55859375
c619-031: Epoch: 0, Step: 55, Rank: 18, loss = 0.69140625
c621-152: Epoch: 0, Step: 55, Rank: 41, loss = 0.65234375
c621-151: Epoch: 0, Step: 55, Rank: 40, loss = 0.51171875
c621-102: Epoch: 0, Step: 55, Rank: 31, loss = 0.59375
c622-001: Epoch: 0, Step: 55, Rank: 42, loss = 0.671875
c621-091: Epoch: 0, Step: 55, Rank: 28, loss = 0.5546875
c621-071: Epoch: 0, Step: 55, Rank: 24, loss = 0.70703125
c621-092: Epoch: 0, Step: 55, Rank: 29, loss = 0.66796875
c622-071: Epoch: 0, Step: 55, Rank: 56, loss = 0.6953125
c622-061: Epoch: 0, Step: 55, Rank: 54, loss = 0.51171875
c621-061: Epoch: 0, Step: 55, Rank: 22, loss = 0.62109375
c619-001: Epoch: 0, Step: 55, Rank: 12, loss = 0.6796875
c621-142: Epoch: 0, Step: 55, Rank: 39, loss = 0.65625
c622-011: Epoch: 0, Step: 55, Rank: 44, loss = 0.66796875
c622-051: Epoch: 0, Step: 55, Rank: 52, loss = 0.62109375
c622-092: Epoch: 0, Step: 55, Rank: 61, loss = 0.671875
c621-141: Epoch: 0, Step: 55, Rank: 38, loss = 0.65625
c613-111: Epoch: 0, Step: 55, Rank: 2, loss = 0.6328125
c622-062: Epoch: 0, Step: 55, Rank: 55, loss = 0.66796875
c613-152: Epoch: 0, Step: 55, Rank: 11, loss = 0.64453125
c619-011: Epoch: 0, Step: 55, Rank: 14, loss = 0.60546875
c621-062: Epoch: 0, Step: 55, Rank: 23, loss = 0.65625
c619-012: Epoch: 0, Step: 55, Rank: 15, loss = 0.6484375
c622-031: Epoch: 0, Step: 55, Rank: 48, loss = 0.69140625
c622-081: Epoch: 0, Step: 55, Rank: 58, loss = 0.6015625
c613-112: Epoch: 0, Step: 55, Rank: 3, loss = 0.62109375
c622-101: Epoch: 0, Step: 55, Rank: 62, loss = 0.63671875
c622-041: Epoch: 0, Step: 55, Rank: 50, loss = 0.62109375
c622-072: Epoch: 0, Step: 55, Rank: 57, loss = 0.65625
c613-142: Epoch: 0, Step: 55, Rank: 9, loss = 0.6875
c613-121: Epoch: 0, Step: 55, Rank: 4, loss = 0.59765625
c613-151: Epoch: 0, Step: 55, Rank: 10, loss = 0.71875
c622-082: Epoch: 0, Step: 55, Rank: 59, loss = 0.66796875
c622-022: Epoch: 0, Step: 55, Rank: 47, loss = 0.69140625
c613-132: Epoch: 0, Step: 55, Rank: 7, loss = 0.59765625
c622-042: Epoch: 0, Step: 55, Rank: 51, loss = 0.474609375
c613-102: Epoch: 0, Step: 55, Rank: 1, loss = 0.66796875
c622-021: Epoch: 0, Step: 55, Rank: 46, loss = 0.65625
c622-102: Epoch: 0, Step: 55, Rank: 63, loss = 0.66015625
c613-141: Epoch: 0, Step: 55, Rank: 8, loss = 0.66796875
c613-131: Epoch: 0, Step: 55, Rank: 6, loss = 0.65625
c622-091: Epoch: 0, Step: 55, Rank: 60, loss = 0.474609375
c613-122: Epoch: 0, Step: 55, Rank: 5, loss = 0.67578125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 56, Rank: 16, loss = 0.65234375
c613-101: Epoch: 0, Step: 56, Rank: 0, loss = 0.65625
c619-022: Epoch: 0, Step: 56, Rank: 17, loss = 0.64453125
c621-072: Epoch: 0, Step: 56, Rank: 25, loss = 0.69140625
c622-002: Epoch: 0, Step: 56, Rank: 43, loss = 0.62109375
c622-052: Epoch: 0, Step: 56, Rank: 53, loss = 0.64453125
c622-101: Epoch: 0, Step: 56, Rank: 62, loss = 0.65625
c621-061: Epoch: 0, Step: 56, Rank: 22, loss = 0.71875
c622-012: Epoch: 0, Step: 56, Rank: 45, loss = 0.69140625
c621-131: Epoch: 0, Step: 56, Rank: 36, loss = 0.65625
c621-091: Epoch: 0, Step: 56, Rank: 28, loss = 0.69140625
c621-111: Epoch: 0, Step: 56, Rank: 32, loss = 0.6875
c621-081: Epoch: 0, Step: 56, Rank: 26, loss = 0.64453125
c619-031: Epoch: 0, Step: 56, Rank: 18, loss = 0.66796875
c621-092: Epoch: 0, Step: 56, Rank: 29, loss = 0.67578125
c621-101: Epoch: 0, Step: 56, Rank: 30, loss = 0.6953125
c619-032: Epoch: 0, Step: 56, Rank: 19, loss = 0.66796875
c613-132: Epoch: 0, Step: 56, Rank: 7, loss = 0.66796875
c613-111: Epoch: 0, Step: 56, Rank: 2, loss = 0.69921875
c619-012: Epoch: 0, Step: 56, Rank: 15, loss = 0.66796875
c622-092: Epoch: 0, Step: 56, Rank: 61, loss = 0.64453125
c613-131: Epoch: 0, Step: 56, Rank: 6, loss = 0.66015625
c622-081: Epoch: 0, Step: 56, Rank: 58, loss = 0.66796875
c613-112: Epoch: 0, Step: 56, Rank: 3, loss = 0.53515625
c622-032: Epoch: 0, Step: 56, Rank: 49, loss = 0.69921875
c621-112: Epoch: 0, Step: 56, Rank: 33, loss = 0.51171875
c622-041: Epoch: 0, Step: 56, Rank: 50, loss = 0.66796875
c621-052: Epoch: 0, Step: 56, Rank: 21, loss = 0.5546875
c613-121: Epoch: 0, Step: 56, Rank: 4, loss = 0.66796875
c613-151: Epoch: 0, Step: 56, Rank: 10, loss = 0.65625
c613-142: Epoch: 0, Step: 56, Rank: 9, loss = 0.69140625
c621-082: Epoch: 0, Step: 56, Rank: 27, loss = 0.69140625
c622-001: Epoch: 0, Step: 56, Rank: 42, loss = 0.64453125
c621-142: Epoch: 0, Step: 56, Rank: 39, loss = 0.66796875
c613-102: Epoch: 0, Step: 56, Rank: 1, loss = 0.6796875
c621-151: Epoch: 0, Step: 56, Rank: 40, loss = 0.59375
c621-121: Epoch: 0, Step: 56, Rank: 34, loss = 0.64453125
c621-062: Epoch: 0, Step: 56, Rank: 23, loss = 0.65625
c619-011: Epoch: 0, Step: 56, Rank: 14, loss = 0.55078125
c622-051: Epoch: 0, Step: 56, Rank: 52, loss = 0.69140625
c619-041: Epoch: 0, Step: 56, Rank: 20, loss = 0.66796875
c622-091: Epoch: 0, Step: 56, Rank: 60, loss = 0.69140625
c613-122: Epoch: 0, Step: 56, Rank: 5, loss = 0.640625
c622-072: Epoch: 0, Step: 56, Rank: 57, loss = 0.69140625
c622-061: Epoch: 0, Step: 56, Rank: 54, loss = 0.66796875
c619-002: Epoch: 0, Step: 56, Rank: 13, loss = 0.54296875
c622-071: Epoch: 0, Step: 56, Rank: 56, loss = 0.6640625
c622-102: Epoch: 0, Step: 56, Rank: 63, loss = 0.59765625
c619-001: Epoch: 0, Step: 56, Rank: 12, loss = 0.67578125
c622-031: Epoch: 0, Step: 56, Rank: 48, loss = 0.66796875
c622-011: Epoch: 0, Step: 56, Rank: 44, loss = 0.65625
c622-022: Epoch: 0, Step: 56, Rank: 47, loss = 0.64453125
c622-062: Epoch: 0, Step: 56, Rank: 55, loss = 0.65625
c621-122: Epoch: 0, Step: 56, Rank: 35, loss = 0.69140625
c613-141: Epoch: 0, Step: 56, Rank: 8, loss = 0.6015625
c622-082: Epoch: 0, Step: 56, Rank: 59, loss = 0.69140625
c622-021: Epoch: 0, Step: 56, Rank: 46, loss = 0.65625
c621-071: Epoch: 0, Step: 56, Rank: 24, loss = 0.65625
c621-141: Epoch: 0, Step: 56, Rank: 38, loss = 0.66796875
c621-152: Epoch: 0, Step: 56, Rank: 41, loss = 0.62109375
c621-102: Epoch: 0, Step: 56, Rank: 31, loss = 0.69921875
c622-042: Epoch: 0, Step: 56, Rank: 51, loss = 0.67578125
c613-152: Epoch: 0, Step: 56, Rank: 11, loss = 0.640625
c621-132: Epoch: 0, Step: 56, Rank: 37, loss = 0.6796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 57, Rank: 43, loss = 0.625
c622-052: Epoch: 0, Step: 57, Rank: 53, loss = 0.6328125
c613-101: Epoch: 0, Step: 57, Rank: 0, loss = 0.62109375
c619-002: Epoch: 0, Step: 57, Rank: 13, loss = 0.59765625
c622-081: Epoch: 0, Step: 57, Rank: 58, loss = 0.65625
c622-032: Epoch: 0, Step: 57, Rank: 49, loss = 0.73046875
c621-111: Epoch: 0, Step: 57, Rank: 32, loss = 0.65625
c622-051: Epoch: 0, Step: 57, Rank: 52, loss = 0.671875
c619-021: Epoch: 0, Step: 57, Rank: 16, loss = 0.66796875
c622-101: Epoch: 0, Step: 57, Rank: 62, loss = 0.59765625
c622-012: Epoch: 0, Step: 57, Rank: 45, loss = 0.6640625
c622-001: Epoch: 0, Step: 57, Rank: 42, loss = 0.6796875
c622-042: Epoch: 0, Step: 57, Rank: 51, loss = 0.73828125
c622-082: Epoch: 0, Step: 57, Rank: 59, loss = 0.66796875
c613-132: Epoch: 0, Step: 57, Rank: 7, loss = 0.65234375
c613-111: Epoch: 0, Step: 57, Rank: 2, loss = 0.65234375
c613-152: Epoch: 0, Step: 57, Rank: 11, loss = 0.64453125
c619-031: Epoch: 0, Step: 57, Rank: 18, loss = 0.6875
c622-022: Epoch: 0, Step: 57, Rank: 47, loss = 0.66796875
c622-092: Epoch: 0, Step: 57, Rank: 61, loss = 0.6171875
c619-001: Epoch: 0, Step: 57, Rank: 12, loss = 0.65625
c621-131: Epoch: 0, Step: 57, Rank: 36, loss = 0.59765625
c622-041: Epoch: 0, Step: 57, Rank: 50, loss = 0.6640625
c621-151: Epoch: 0, Step: 57, Rank: 40, loss = 0.59765625
c622-011: Epoch: 0, Step: 57, Rank: 44, loss = 0.66796875
c622-102: Epoch: 0, Step: 57, Rank: 63, loss = 0.671875
c622-071: Epoch: 0, Step: 57, Rank: 56, loss = 0.67578125
c622-061: Epoch: 0, Step: 57, Rank: 54, loss = 0.609375
c622-072: Epoch: 0, Step: 57, Rank: 57, loss = 0.65625
c619-022: Epoch: 0, Step: 57, Rank: 17, loss = 0.671875
c621-112: Epoch: 0, Step: 57, Rank: 33, loss = 0.6875
c622-031: Epoch: 0, Step: 57, Rank: 48, loss = 0.66796875
c622-091: Epoch: 0, Step: 57, Rank: 60, loss = 0.57421875
c621-142: Epoch: 0, Step: 57, Rank: 39, loss = 0.6796875
c621-141: Epoch: 0, Step: 57, Rank: 38, loss = 0.59765625
c619-032: Epoch: 0, Step: 57, Rank: 19, loss = 0.69140625
c621-121: Epoch: 0, Step: 57, Rank: 34, loss = 0.62109375
c622-062: Epoch: 0, Step: 57, Rank: 55, loss = 0.6015625
c613-102: Epoch: 0, Step: 57, Rank: 1, loss = 0.64453125
c613-141: Epoch: 0, Step: 57, Rank: 8, loss = 0.64453125
c621-091: Epoch: 0, Step: 57, Rank: 28, loss = 0.64453125
c613-131: Epoch: 0, Step: 57, Rank: 6, loss = 0.63671875
c613-122: Epoch: 0, Step: 57, Rank: 5, loss = 0.6640625
c621-152: Epoch: 0, Step: 57, Rank: 41, loss = 0.7578125
c613-112: Epoch: 0, Step: 57, Rank: 3, loss = 0.6796875
c621-082: Epoch: 0, Step: 57, Rank: 27, loss = 0.6875
c619-012: Epoch: 0, Step: 57, Rank: 15, loss = 0.64453125
c621-122: Epoch: 0, Step: 57, Rank: 35, loss = 0.6640625
c613-151: Epoch: 0, Step: 57, Rank: 10, loss = 0.59765625
c619-011: Epoch: 0, Step: 57, Rank: 14, loss = 0.66796875
c621-101: Epoch: 0, Step: 57, Rank: 30, loss = 0.59765625
c621-072: Epoch: 0, Step: 57, Rank: 25, loss = 0.5859375
c621-081: Epoch: 0, Step: 57, Rank: 26, loss = 0.6796875
c613-121: Epoch: 0, Step: 57, Rank: 4, loss = 0.6796875
c613-142: Epoch: 0, Step: 57, Rank: 9, loss = 0.5546875
c621-052: Epoch: 0, Step: 57, Rank: 21, loss = 0.69140625
c621-061: Epoch: 0, Step: 57, Rank: 22, loss = 0.66796875
c621-062: Epoch: 0, Step: 57, Rank: 23, loss = 0.64453125
c621-102: Epoch: 0, Step: 57, Rank: 31, loss = 0.69140625
c622-021: Epoch: 0, Step: 57, Rank: 46, loss = 0.6796875
c621-132: Epoch: 0, Step: 57, Rank: 37, loss = 0.62109375
c621-071: Epoch: 0, Step: 57, Rank: 24, loss = 0.69140625
c619-041: Epoch: 0, Step: 57, Rank: 20, loss = 0.64453125
c621-092: Epoch: 0, Step: 57, Rank: 29, loss = 0.59765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75390625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c621-131: Epoch: 0, Step: 58, Rank: 36, loss = 0.6953125
c621-122: Epoch: 0, Step: 58, Rank: 35, loss = 0.69140625
c621-132: Epoch: 0, Step: 58, Rank: 37, loss = 0.6796875
c621-121: Epoch: 0, Step: 58, Rank: 34, loss = 0.671875
c621-112: Epoch: 0, Step: 58, Rank: 33, loss = 0.59765625
c621-111: Epoch: 0, Step: 58, Rank: 32, loss = 0.6328125
c621-101: Epoch: 0, Step: 58, Rank: 30, loss = 0.66796875
c621-102: Epoch: 0, Step: 58, Rank: 31, loss = 0.66796875
c621-141: Epoch: 0, Step: 58, Rank: 38, loss = 0.64453125
c621-092: Epoch: 0, Step: 58, Rank: 29, loss = 0.69140625
c621-082: Epoch: 0, Step: 58, Rank: 27, loss = 0.65625
c621-091: Epoch: 0, Step: 58, Rank: 28, loss = 0.65234375
c621-081: Epoch: 0, Step: 58, Rank: 26, loss = 0.65234375
c621-072: Epoch: 0, Step: 58, Rank: 25, loss = 0.69140625
c621-142: Epoch: 0, Step: 58, Rank: 39, loss = 0.6640625
c621-062: Epoch: 0, Step: 58, Rank: 23, loss = 0.64453125
c621-071: Epoch: 0, Step: 58, Rank: 24, loss = 0.60546875
c621-061: Epoch: 0, Step: 58, Rank: 22, loss = 0.5546875
c621-052: Epoch: 0, Step: 58, Rank: 21, loss = 0.64453125
c619-041: Epoch: 0, Step: 58, Rank: 20, loss = 0.64453125
c621-151: Epoch: 0, Step: 58, Rank: 40, loss = 0.69140625
c619-032: Epoch: 0, Step: 58, Rank: 19, loss = 0.62890625
c619-031: Epoch: 0, Step: 58, Rank: 18, loss = 0.67578125
c619-021: Epoch: 0, Step: 58, Rank: 16, loss = 0.65234375
c619-022: Epoch: 0, Step: 58, Rank: 17, loss = 0.625
c619-012: Epoch: 0, Step: 58, Rank: 15, loss = 0.59765625
c621-152: Epoch: 0, Step: 58, Rank: 41, loss = 0.6640625
c619-001: Epoch: 0, Step: 58, Rank: 12, loss = 0.69140625
c619-011: Epoch: 0, Step: 58, Rank: 14, loss = 0.70703125
c619-002: Epoch: 0, Step: 58, Rank: 13, loss = 0.6640625
c613-151: Epoch: 0, Step: 58, Rank: 10, loss = 0.68359375
c613-152: Epoch: 0, Step: 58, Rank: 11, loss = 0.69140625
c613-132: Epoch: 0, Step: 58, Rank: 7, loss = 0.59765625
c613-142: Epoch: 0, Step: 58, Rank: 9, loss = 0.62109375
c622-001: Epoch: 0, Step: 58, Rank: 42, loss = 0.65625
c613-141: Epoch: 0, Step: 58, Rank: 8, loss = 0.68359375
c613-131: Epoch: 0, Step: 58, Rank: 6, loss = 0.6640625
c613-122: Epoch: 0, Step: 58, Rank: 5, loss = 0.66796875
c613-121: Epoch: 0, Step: 58, Rank: 4, loss = 0.73046875
c613-111: Epoch: 0, Step: 58, Rank: 2, loss = 0.66796875
c613-112: Epoch: 0, Step: 58, Rank: 3, loss = 0.59765625
c622-002: Epoch: 0, Step: 58, Rank: 43, loss = 0.62109375
c613-101: Epoch: 0, Step: 58, Rank: 0, loss = 0.68359375
c613-102: Epoch: 0, Step: 58, Rank: 1, loss = 0.71875
c622-102: Epoch: 0, Step: 58, Rank: 63, loss = 0.64453125
c622-101: Epoch: 0, Step: 58, Rank: 62, loss = 0.6328125
c622-092: Epoch: 0, Step: 58, Rank: 61, loss = 0.6328125
c622-082: Epoch: 0, Step: 58, Rank: 59, loss = 0.65625
c622-081: Epoch: 0, Step: 58, Rank: 58, loss = 0.65234375
c622-091: Epoch: 0, Step: 58, Rank: 60, loss = 0.6640625
c622-062: Epoch: 0, Step: 58, Rank: 55, loss = 0.6796875
c622-072: Epoch: 0, Step: 58, Rank: 57, loss = 0.62109375
c622-071: Epoch: 0, Step: 58, Rank: 56, loss = 0.59765625
c622-052: Epoch: 0, Step: 58, Rank: 53, loss = 0.64453125
c622-011: Epoch: 0, Step: 58, Rank: 44, loss = 0.67578125
c622-061: Epoch: 0, Step: 58, Rank: 54, loss = 0.64453125
c622-012: Epoch: 0, Step: 58, Rank: 45, loss = 0.63671875
c622-051: Epoch: 0, Step: 58, Rank: 52, loss = 0.51171875
c622-022: Epoch: 0, Step: 58, Rank: 47, loss = 0.64453125
c622-021: Epoch: 0, Step: 58, Rank: 46, loss = 0.62109375
c622-041: Epoch: 0, Step: 58, Rank: 50, loss = 0.66796875
c622-032: Epoch: 0, Step: 58, Rank: 49, loss = 0.67578125
c622-031: Epoch: 0, Step: 58, Rank: 48, loss = 0.671875
c622-042: Epoch: 0, Step: 58, Rank: 51, loss = 0.67578125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 59, Rank: 53, loss = 0.65625
c622-061: Epoch: 0, Step: 59, Rank: 54, loss = 0.6328125
c622-101: Epoch: 0, Step: 59, Rank: 62, loss = 0.64453125
c622-081: Epoch: 0, Step: 59, Rank: 58, loss = 0.69140625
c622-071: Epoch: 0, Step: 59, Rank: 56, loss = 0.66796875
c613-101: Epoch: 0, Step: 59, Rank: 0, loss = 0.65625
c622-092: Epoch: 0, Step: 59, Rank: 61, loss = 0.63671875
c622-082: Epoch: 0, Step: 59, Rank: 59, loss = 0.609375
c622-062: Epoch: 0, Step: 59, Rank: 55, loss = 0.64453125
c613-111: Epoch: 0, Step: 59, Rank: 2, loss = 0.85546875
c622-072: Epoch: 0, Step: 59, Rank: 57, loss = 0.52734375
c622-041: Epoch: 0, Step: 59, Rank: 50, loss = 0.64453125
c622-051: Epoch: 0, Step: 59, Rank: 52, loss = 0.68359375
c613-112: Epoch: 0, Step: 59, Rank: 3, loss = 0.57421875
c622-102: Epoch: 0, Step: 59, Rank: 63, loss = 0.6328125
c622-091: Epoch: 0, Step: 59, Rank: 60, loss = 0.6640625
c613-121: Epoch: 0, Step: 59, Rank: 4, loss = 0.65625
c622-032: Epoch: 0, Step: 59, Rank: 49, loss = 0.65625
c622-042: Epoch: 0, Step: 59, Rank: 51, loss = 0.6484375
c613-122: Epoch: 0, Step: 59, Rank: 5, loss = 0.66796875
c613-102: Epoch: 0, Step: 59, Rank: 1, loss = 0.6796875
c622-031: Epoch: 0, Step: 59, Rank: 48, loss = 0.62109375
c622-022: Epoch: 0, Step: 59, Rank: 47, loss = 0.66796875
c622-002: Epoch: 0, Step: 59, Rank: 43, loss = 0.70703125
c622-012: Epoch: 0, Step: 59, Rank: 45, loss = 0.6796875
c622-021: Epoch: 0, Step: 59, Rank: 46, loss = 0.66796875
c613-131: Epoch: 0, Step: 59, Rank: 6, loss = 0.50390625
c622-011: Epoch: 0, Step: 59, Rank: 44, loss = 0.66796875
c622-001: Epoch: 0, Step: 59, Rank: 42, loss = 0.69921875
c621-151: Epoch: 0, Step: 59, Rank: 40, loss = 0.69140625
c621-152: Epoch: 0, Step: 59, Rank: 41, loss = 0.53515625
c613-132: Epoch: 0, Step: 59, Rank: 7, loss = 0.66796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c621-142: Epoch: 0, Step: 59, Rank: 39, loss = 0.640625
c621-132: Epoch: 0, Step: 59, Rank: 37, loss = 0.6640625
c619-021: Epoch: 0, Step: 59, Rank: 16, loss = 0.59765625
c621-141: Epoch: 0, Step: 59, Rank: 38, loss = 0.671875
c621-131: Epoch: 0, Step: 59, Rank: 36, loss = 0.64453125
c621-112: Epoch: 0, Step: 59, Rank: 33, loss = 0.65234375
c613-152: Epoch: 0, Step: 59, Rank: 11, loss = 0.64453125
c621-111: Epoch: 0, Step: 59, Rank: 32, loss = 0.65625
c619-001: Epoch: 0, Step: 59, Rank: 12, loss = 0.6796875
c613-151: Epoch: 0, Step: 59, Rank: 10, loss = 0.6640625
c621-092: Epoch: 0, Step: 59, Rank: 29, loss = 0.6328125
c621-081: Epoch: 0, Step: 59, Rank: 26, loss = 0.671875
c619-002: Epoch: 0, Step: 59, Rank: 13, loss = 0.67578125
c621-082: Epoch: 0, Step: 59, Rank: 27, loss = 0.66796875
c621-101: Epoch: 0, Step: 59, Rank: 30, loss = 0.65625
c621-121: Epoch: 0, Step: 59, Rank: 34, loss = 0.6640625
c619-012: Epoch: 0, Step: 59, Rank: 15, loss = 0.69140625
c621-091: Epoch: 0, Step: 59, Rank: 28, loss = 0.70703125
c621-072: Epoch: 0, Step: 59, Rank: 25, loss = 0.6015625
c621-122: Epoch: 0, Step: 59, Rank: 35, loss = 0.58203125
c613-141: Epoch: 0, Step: 59, Rank: 8, loss = 0.65234375
c619-031: Epoch: 0, Step: 59, Rank: 18, loss = 0.6796875
c619-011: Epoch: 0, Step: 59, Rank: 14, loss = 0.64453125
c621-102: Epoch: 0, Step: 59, Rank: 31, loss = 0.58203125
c619-022: Epoch: 0, Step: 59, Rank: 17, loss = 0.62109375
c613-142: Epoch: 0, Step: 59, Rank: 9, loss = 0.609375
c621-071: Epoch: 0, Step: 59, Rank: 24, loss = 0.66796875
c621-062: Epoch: 0, Step: 59, Rank: 23, loss = 0.6796875
c621-061: Epoch: 0, Step: 59, Rank: 22, loss = 0.69140625
c621-052: Epoch: 0, Step: 59, Rank: 21, loss = 0.6640625
c619-032: Epoch: 0, Step: 59, Rank: 19, loss = 0.671875
c619-041: Epoch: 0, Step: 59, Rank: 20, loss = 0.66796875
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 60, Rank: 43, loss = 0.62109375
c621-091: Epoch: 0, Step: 60, Rank: 28, loss = 0.6796875
c622-001: Epoch: 0, Step: 60, Rank: 42, loss = 0.66796875
c622-052: Epoch: 0, Step: 60, Rank: 53, loss = 0.67578125
c621-082: Epoch: 0, Step: 60, Rank: 27, loss = 0.74609375
c619-021: Epoch: 0, Step: 60, Rank: 16, loss = 0.66796875
c621-151: Epoch: 0, Step: 60, Rank: 40, loss = 0.66796875
c622-012: Epoch: 0, Step: 60, Rank: 45, loss = 0.5703125
c621-092: Epoch: 0, Step: 60, Rank: 29, loss = 0.62890625
c619-001: Epoch: 0, Step: 60, Rank: 12, loss = 0.6875
c613-132: Epoch: 0, Step: 60, Rank: 7, loss = 0.66796875
c621-101: Epoch: 0, Step: 60, Rank: 30, loss = 0.64453125
c621-121: Epoch: 0, Step: 60, Rank: 34, loss = 0.4765625
c622-011: Epoch: 0, Step: 60, Rank: 44, loss = 0.6796875
c621-072: Epoch: 0, Step: 60, Rank: 25, loss = 0.37109375
c621-081: Epoch: 0, Step: 60, Rank: 26, loss = 0.37109375
c619-041: Epoch: 0, Step: 60, Rank: 20, loss = 0.64453125
c621-111: Epoch: 0, Step: 60, Rank: 32, loss = 0.6640625
c621-152: Epoch: 0, Step: 60, Rank: 41, loss = 0.66796875
c619-002: Epoch: 0, Step: 60, Rank: 13, loss = 0.6875
c613-152: Epoch: 0, Step: 60, Rank: 11, loss = 0.66796875
c613-131: Epoch: 0, Step: 60, Rank: 6, loss = 0.66796875
c613-101: Epoch: 0, Step: 60, Rank: 0, loss = 0.6484375
c619-031: Epoch: 0, Step: 60, Rank: 18, loss = 0.6328125
c622-032: Epoch: 0, Step: 60, Rank: 49, loss = 0.64453125
c613-141: Epoch: 0, Step: 60, Rank: 8, loss = 0.65625
c621-142: Epoch: 0, Step: 60, Rank: 39, loss = 0.59765625
c621-112: Epoch: 0, Step: 60, Rank: 33, loss = 0.66015625
c613-111: Epoch: 0, Step: 60, Rank: 2, loss = 0.69140625
c621-141: Epoch: 0, Step: 60, Rank: 38, loss = 0.65625
c621-132: Epoch: 0, Step: 60, Rank: 37, loss = 0.59765625
c622-041: Epoch: 0, Step: 60, Rank: 50, loss = 0.65625
c622-031: Epoch: 0, Step: 60, Rank: 48, loss = 0.6328125
c622-022: Epoch: 0, Step: 60, Rank: 47, loss = 0.62109375
c619-022: Epoch: 0, Step: 60, Rank: 17, loss = 0.6328125
c622-071: Epoch: 0, Step: 60, Rank: 56, loss = 0.67578125
c613-142: Epoch: 0, Step: 60, Rank: 9, loss = 0.66796875
c613-151: Epoch: 0, Step: 60, Rank: 10, loss = 0.6875
c621-131: Epoch: 0, Step: 60, Rank: 36, loss = 0.69140625
c621-061: Epoch: 0, Step: 60, Rank: 22, loss = 0.65625
c619-012: Epoch: 0, Step: 60, Rank: 15, loss = 0.6640625
c622-061: Epoch: 0, Step: 60, Rank: 54, loss = 0.65625
c621-122: Epoch: 0, Step: 60, Rank: 35, loss = 0.64453125
c619-011: Epoch: 0, Step: 60, Rank: 14, loss = 0.66796875
c613-122: Epoch: 0, Step: 60, Rank: 5, loss = 0.5234375
c619-032: Epoch: 0, Step: 60, Rank: 19, loss = 0.64453125
c621-102: Epoch: 0, Step: 60, Rank: 31, loss = 0.69140625
c622-042: Epoch: 0, Step: 60, Rank: 51, loss = 0.67578125
c613-121: Epoch: 0, Step: 60, Rank: 4, loss = 0.64453125
c621-071: Epoch: 0, Step: 60, Rank: 24, loss = 0.65625
c613-102: Epoch: 0, Step: 60, Rank: 1, loss = 0.66796875
c622-081: Epoch: 0, Step: 60, Rank: 58, loss = 0.66015625
c613-112: Epoch: 0, Step: 60, Rank: 3, loss = 0.7265625
c622-092: Epoch: 0, Step: 60, Rank: 61, loss = 0.5546875
c621-052: Epoch: 0, Step: 60, Rank: 21, loss = 0.66015625
c622-101: Epoch: 0, Step: 60, Rank: 62, loss = 0.68359375
c622-102: Epoch: 0, Step: 60, Rank: 63, loss = 0.65625
c622-062: Epoch: 0, Step: 60, Rank: 55, loss = 0.64453125
c622-021: Epoch: 0, Step: 60, Rank: 46, loss = 0.5859375
c621-062: Epoch: 0, Step: 60, Rank: 23, loss = 0.6640625
c622-051: Epoch: 0, Step: 60, Rank: 52, loss = 0.66796875
c622-091: Epoch: 0, Step: 60, Rank: 60, loss = 0.65625
c622-082: Epoch: 0, Step: 60, Rank: 59, loss = 0.6875
c622-072: Epoch: 0, Step: 60, Rank: 57, loss = 0.57421875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 61, Rank: 32, loss = 0.671875
c621-081: Epoch: 0, Step: 61, Rank: 26, loss = 0.609375
c622-002: Epoch: 0, Step: 61, Rank: 43, loss = 0.64453125
c621-112: Epoch: 0, Step: 61, Rank: 33, loss = 0.6796875
c621-131: Epoch: 0, Step: 61, Rank: 36, loss = 0.71875
c621-121: Epoch: 0, Step: 61, Rank: 34, loss = 0.69140625
c619-031: Epoch: 0, Step: 61, Rank: 18, loss = 0.69140625
c622-001: Epoch: 0, Step: 61, Rank: 42, loss = 0.69140625
c621-072: Epoch: 0, Step: 61, Rank: 25, loss = 0.59765625
c622-052: Epoch: 0, Step: 61, Rank: 53, loss = 0.61328125
c621-132: Epoch: 0, Step: 61, Rank: 37, loss = 0.65625
c621-082: Epoch: 0, Step: 61, Rank: 27, loss = 0.64453125
c621-091: Epoch: 0, Step: 61, Rank: 28, loss = 0.69140625
c621-061: Epoch: 0, Step: 61, Rank: 22, loss = 0.59765625
c622-041: Epoch: 0, Step: 61, Rank: 50, loss = 0.64453125
c621-151: Epoch: 0, Step: 61, Rank: 40, loss = 0.64453125
c619-021: Epoch: 0, Step: 61, Rank: 16, loss = 0.6796875
c622-032: Epoch: 0, Step: 61, Rank: 49, loss = 0.65625
c621-152: Epoch: 0, Step: 61, Rank: 41, loss = 0.65234375
c622-081: Epoch: 0, Step: 61, Rank: 58, loss = 0.66796875
c621-142: Epoch: 0, Step: 61, Rank: 39, loss = 0.6875
c621-071: Epoch: 0, Step: 61, Rank: 24, loss = 0.62109375
c621-102: Epoch: 0, Step: 61, Rank: 31, loss = 0.59765625
c622-042: Epoch: 0, Step: 61, Rank: 51, loss = 0.6796875
c613-151: Epoch: 0, Step: 61, Rank: 10, loss = 0.671875
c621-122: Epoch: 0, Step: 61, Rank: 35, loss = 0.53515625
c619-032: Epoch: 0, Step: 61, Rank: 19, loss = 0.69140625
c621-141: Epoch: 0, Step: 61, Rank: 38, loss = 0.64453125
c619-041: Epoch: 0, Step: 61, Rank: 20, loss = 0.68359375
c622-061: Epoch: 0, Step: 61, Rank: 54, loss = 0.65625
c622-051: Epoch: 0, Step: 61, Rank: 52, loss = 0.57421875
c621-062: Epoch: 0, Step: 61, Rank: 23, loss = 0.66796875
c613-152: Epoch: 0, Step: 61, Rank: 11, loss = 0.37109375
c613-101: Epoch: 0, Step: 61, Rank: 0, loss = 0.60546875
c622-012: Epoch: 0, Step: 61, Rank: 45, loss = 0.66796875
c621-092: Epoch: 0, Step: 61, Rank: 29, loss = 0.64453125
c619-022: Epoch: 0, Step: 61, Rank: 17, loss = 0.66796875
c619-001: Epoch: 0, Step: 61, Rank: 12, loss = 0.62109375
c622-011: Epoch: 0, Step: 61, Rank: 44, loss = 0.51171875
c621-052: Epoch: 0, Step: 61, Rank: 21, loss = 0.64453125
c622-031: Epoch: 0, Step: 61, Rank: 48, loss = 0.59765625
c622-082: Epoch: 0, Step: 61, Rank: 59, loss = 0.6328125
c621-101: Epoch: 0, Step: 61, Rank: 30, loss = 0.6015625
c622-022: Epoch: 0, Step: 61, Rank: 47, loss = 0.62109375
c613-121: Epoch: 0, Step: 61, Rank: 4, loss = 0.59765625
c622-021: Epoch: 0, Step: 61, Rank: 46, loss = 0.65625
c622-072: Epoch: 0, Step: 61, Rank: 57, loss = 0.51171875
c613-132: Epoch: 0, Step: 61, Rank: 7, loss = 0.64453125
c622-071: Epoch: 0, Step: 61, Rank: 56, loss = 0.65625
c613-142: Epoch: 0, Step: 61, Rank: 9, loss = 0.69140625
c619-012: Epoch: 0, Step: 61, Rank: 15, loss = 0.65625
c619-011: Epoch: 0, Step: 61, Rank: 14, loss = 0.65234375
c613-111: Epoch: 0, Step: 61, Rank: 2, loss = 0.64453125
c622-101: Epoch: 0, Step: 61, Rank: 62, loss = 0.6796875
c619-002: Epoch: 0, Step: 61, Rank: 13, loss = 0.609375
c622-091: Epoch: 0, Step: 61, Rank: 60, loss = 0.60546875
c622-062: Epoch: 0, Step: 61, Rank: 55, loss = 0.65625
c613-102: Epoch: 0, Step: 61, Rank: 1, loss = 0.59765625
c613-112: Epoch: 0, Step: 61, Rank: 3, loss = 0.64453125
c613-141: Epoch: 0, Step: 61, Rank: 8, loss = 0.69140625
c622-092: Epoch: 0, Step: 61, Rank: 61, loss = 0.66796875
c613-131: Epoch: 0, Step: 61, Rank: 6, loss = 0.6796875
c622-102: Epoch: 0, Step: 61, Rank: 63, loss = 0.6953125
c613-122: Epoch: 0, Step: 61, Rank: 5, loss = 0.66796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24658203125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 62, Rank: 53, loss = 0.62109375
c622-041: Epoch: 0, Step: 62, Rank: 50, loss = 0.66796875
c622-032: Epoch: 0, Step: 62, Rank: 49, loss = 0.6796875
c622-002: Epoch: 0, Step: 62, Rank: 43, loss = 0.66796875
c622-051: Epoch: 0, Step: 62, Rank: 52, loss = 0.6640625
c622-012: Epoch: 0, Step: 62, Rank: 45, loss = 0.65625
c622-021: Epoch: 0, Step: 62, Rank: 46, loss = 0.69921875
c622-001: Epoch: 0, Step: 62, Rank: 42, loss = 0.66796875
c622-031: Epoch: 0, Step: 62, Rank: 48, loss = 0.69140625
c622-042: Epoch: 0, Step: 62, Rank: 51, loss = 0.59765625
c622-022: Epoch: 0, Step: 62, Rank: 47, loss = 0.6796875
c622-081: Epoch: 0, Step: 62, Rank: 58, loss = 0.66796875
c622-061: Epoch: 0, Step: 62, Rank: 54, loss = 0.62109375
c622-092: Epoch: 0, Step: 62, Rank: 61, loss = 0.62109375
c622-082: Epoch: 0, Step: 62, Rank: 59, loss = 0.6796875
c622-011: Epoch: 0, Step: 62, Rank: 44, loss = 0.6640625
c622-071: Epoch: 0, Step: 62, Rank: 56, loss = 0.63671875
c621-151: Epoch: 0, Step: 62, Rank: 40, loss = 0.66796875
c621-152: Epoch: 0, Step: 62, Rank: 41, loss = 0.51171875
c622-091: Epoch: 0, Step: 62, Rank: 60, loss = 0.6796875
c622-072: Epoch: 0, Step: 62, Rank: 57, loss = 0.65234375
c621-142: Epoch: 0, Step: 62, Rank: 39, loss = 0.66796875
c621-132: Epoch: 0, Step: 62, Rank: 37, loss = 0.64453125
c622-062: Epoch: 0, Step: 62, Rank: 55, loss = 0.6640625
c622-101: Epoch: 0, Step: 62, Rank: 62, loss = 0.671875
c621-141: Epoch: 0, Step: 62, Rank: 38, loss = 0.66796875
c621-131: Epoch: 0, Step: 62, Rank: 36, loss = 0.64453125
c621-111: Epoch: 0, Step: 62, Rank: 32, loss = 0.671875
c621-121: Epoch: 0, Step: 62, Rank: 34, loss = 0.57421875
c621-122: Epoch: 0, Step: 62, Rank: 35, loss = 0.66796875
c621-112: Epoch: 0, Step: 62, Rank: 33, loss = 0.62109375
c621-092: Epoch: 0, Step: 62, Rank: 29, loss = 0.6640625
c621-102: Epoch: 0, Step: 62, Rank: 31, loss = 0.6796875
c621-101: Epoch: 0, Step: 62, Rank: 30, loss = 0.51171875
c621-082: Epoch: 0, Step: 62, Rank: 27, loss = 0.65625
c613-101: Epoch: 0, Step: 62, Rank: 0, loss = 0.37109375
c621-091: Epoch: 0, Step: 62, Rank: 28, loss = 0.66796875
c622-102: Epoch: 0, Step: 62, Rank: 63, loss = 0.59765625
c621-081: Epoch: 0, Step: 62, Rank: 26, loss = 0.68359375
c621-072: Epoch: 0, Step: 62, Rank: 25, loss = 0.67578125
c621-071: Epoch: 0, Step: 62, Rank: 24, loss = 0.57421875
c621-061: Epoch: 0, Step: 62, Rank: 22, loss = 0.64453125
c621-062: Epoch: 0, Step: 62, Rank: 23, loss = 0.66015625
c613-102: Epoch: 0, Step: 62, Rank: 1, loss = 0.64453125
c621-052: Epoch: 0, Step: 62, Rank: 21, loss = 0.6796875
c613-111: Epoch: 0, Step: 62, Rank: 2, loss = 0.62109375
c619-041: Epoch: 0, Step: 62, Rank: 20, loss = 0.6328125
c619-021: Epoch: 0, Step: 62, Rank: 16, loss = 0.63671875
c619-032: Epoch: 0, Step: 62, Rank: 19, loss = 0.55859375
c619-031: Epoch: 0, Step: 62, Rank: 18, loss = 0.69140625
c619-022: Epoch: 0, Step: 62, Rank: 17, loss = 0.69140625
c619-012: Epoch: 0, Step: 62, Rank: 15, loss = 0.609375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c619-002: Epoch: 0, Step: 62, Rank: 13, loss = 0.57421875
c619-011: Epoch: 0, Step: 62, Rank: 14, loss = 0.64453125
c619-001: Epoch: 0, Step: 62, Rank: 12, loss = 0.65625
c613-152: Epoch: 0, Step: 62, Rank: 11, loss = 0.6640625
c613-142: Epoch: 0, Step: 62, Rank: 9, loss = 0.69921875
c613-112: Epoch: 0, Step: 62, Rank: 3, loss = 0.609375
c613-121: Epoch: 0, Step: 62, Rank: 4, loss = 0.64453125
c613-132: Epoch: 0, Step: 62, Rank: 7, loss = 0.70703125
c613-151: Epoch: 0, Step: 62, Rank: 10, loss = 0.65625
c613-141: Epoch: 0, Step: 62, Rank: 8, loss = 0.66015625
c613-131: Epoch: 0, Step: 62, Rank: 6, loss = 0.64453125
c613-122: Epoch: 0, Step: 62, Rank: 5, loss = 0.671875
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 63, Rank: 0, loss = 0.3671875
c622-002: Epoch: 0, Step: 63, Rank: 43, loss = 0.59765625
c622-081: Epoch: 0, Step: 63, Rank: 58, loss = 0.66796875
c622-052: Epoch: 0, Step: 63, Rank: 53, loss = 0.66796875
c621-111: Epoch: 0, Step: 63, Rank: 32, loss = 0.69140625
c613-112: Epoch: 0, Step: 63, Rank: 3, loss = 0.65234375
c621-132: Epoch: 0, Step: 63, Rank: 37, loss = 0.65234375
c622-061: Epoch: 0, Step: 63, Rank: 54, loss = 0.6640625
c621-151: Epoch: 0, Step: 63, Rank: 40, loss = 0.64453125
c613-111: Epoch: 0, Step: 63, Rank: 2, loss = 0.66796875
c622-012: Epoch: 0, Step: 63, Rank: 45, loss = 0.6640625
c613-132: Epoch: 0, Step: 63, Rank: 7, loss = 0.50390625
c621-081: Epoch: 0, Step: 63, Rank: 26, loss = 0.64453125
c621-152: Epoch: 0, Step: 63, Rank: 41, loss = 0.6640625
c613-142: Epoch: 0, Step: 63, Rank: 9, loss = 0.69140625
c621-131: Epoch: 0, Step: 63, Rank: 36, loss = 0.64453125
c622-001: Epoch: 0, Step: 63, Rank: 42, loss = 0.69921875
c622-071: Epoch: 0, Step: 63, Rank: 56, loss = 0.6640625
c622-092: Epoch: 0, Step: 63, Rank: 61, loss = 0.67578125
c613-131: Epoch: 0, Step: 63, Rank: 6, loss = 0.64453125
c621-142: Epoch: 0, Step: 63, Rank: 39, loss = 0.6328125
c613-141: Epoch: 0, Step: 63, Rank: 8, loss = 0.65625
c613-121: Epoch: 0, Step: 63, Rank: 4, loss = 0.65625
c621-072: Epoch: 0, Step: 63, Rank: 25, loss = 0.64453125
c621-121: Epoch: 0, Step: 63, Rank: 34, loss = 0.69140625
c622-051: Epoch: 0, Step: 63, Rank: 52, loss = 0.62109375
c622-101: Epoch: 0, Step: 63, Rank: 62, loss = 0.671875
c622-032: Epoch: 0, Step: 63, Rank: 49, loss = 0.62109375
c613-152: Epoch: 0, Step: 63, Rank: 11, loss = 0.640625
c613-102: Epoch: 0, Step: 63, Rank: 1, loss = 0.62109375
c622-041: Epoch: 0, Step: 63, Rank: 50, loss = 0.5859375
c622-022: Epoch: 0, Step: 63, Rank: 47, loss = 0.5234375
c619-021: Epoch: 0, Step: 63, Rank: 16, loss = 0.69140625
c613-151: Epoch: 0, Step: 63, Rank: 10, loss = 0.66796875
c622-042: Epoch: 0, Step: 63, Rank: 51, loss = 0.51171875
c622-082: Epoch: 0, Step: 63, Rank: 59, loss = 0.57421875
c621-102: Epoch: 0, Step: 63, Rank: 31, loss = 0.62109375
c619-041: Epoch: 0, Step: 63, Rank: 20, loss = 0.65625
c621-112: Epoch: 0, Step: 63, Rank: 33, loss = 0.62109375
c619-031: Epoch: 0, Step: 63, Rank: 18, loss = 0.69140625
c619-002: Epoch: 0, Step: 63, Rank: 13, loss = 0.5546875
c622-072: Epoch: 0, Step: 63, Rank: 57, loss = 0.51171875
c621-082: Epoch: 0, Step: 63, Rank: 27, loss = 0.65625
c622-031: Epoch: 0, Step: 63, Rank: 48, loss = 0.65625
c619-001: Epoch: 0, Step: 63, Rank: 12, loss = 0.65625
c622-062: Epoch: 0, Step: 63, Rank: 55, loss = 0.68359375
c622-102: Epoch: 0, Step: 63, Rank: 63, loss = 0.64453125
c621-122: Epoch: 0, Step: 63, Rank: 35, loss = 0.64453125
c613-122: Epoch: 0, Step: 63, Rank: 5, loss = 0.64453125
c621-141: Epoch: 0, Step: 63, Rank: 38, loss = 0.65234375
c622-091: Epoch: 0, Step: 63, Rank: 60, loss = 0.6640625
c619-011: Epoch: 0, Step: 63, Rank: 14, loss = 0.59765625
c622-011: Epoch: 0, Step: 63, Rank: 44, loss = 0.64453125
c621-101: Epoch: 0, Step: 63, Rank: 30, loss = 0.65234375
c619-032: Epoch: 0, Step: 63, Rank: 19, loss = 0.65234375
c621-092: Epoch: 0, Step: 63, Rank: 29, loss = 0.6328125
c621-091: Epoch: 0, Step: 63, Rank: 28, loss = 0.64453125
c622-021: Epoch: 0, Step: 63, Rank: 46, loss = 0.64453125
c621-071: Epoch: 0, Step: 63, Rank: 24, loss = 0.6796875
c619-022: Epoch: 0, Step: 63, Rank: 17, loss = 0.64453125
c621-062: Epoch: 0, Step: 63, Rank: 23, loss = 0.69140625
c621-052: Epoch: 0, Step: 63, Rank: 21, loss = 0.64453125
c621-061: Epoch: 0, Step: 63, Rank: 22, loss = 0.62109375
c619-012: Epoch: 0, Step: 63, Rank: 15, loss = 0.59765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24755859375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.15s, TFLOPs: 0.88, Samples/sec: 0.46, Time/seq 2.15s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 64, Rank: 0, loss = 0.69140625
c619-002: Epoch: 0, Step: 64, Rank: 13, loss = 0.59375
c619-021: Epoch: 0, Step: 64, Rank: 16, loss = 0.494140625
c613-142: Epoch: 0, Step: 64, Rank: 9, loss = 0.18359375
c622-052: Epoch: 0, Step: 64, Rank: 53, loss = 0.474609375
c621-081: Epoch: 0, Step: 64, Rank: 26, loss = 0.5546875
c619-022: Epoch: 0, Step: 64, Rank: 17, loss = 0.57421875
c622-081: Epoch: 0, Step: 64, Rank: 58, loss = 0.69140625
c622-071: Epoch: 0, Step: 64, Rank: 56, loss = 0.46484375
c613-152: Epoch: 0, Step: 64, Rank: 11, loss = 0.69140625
c613-151: Epoch: 0, Step: 64, Rank: 10, loss = 0.625
c622-012: Epoch: 0, Step: 64, Rank: 45, loss = 0.419921875
c621-091: Epoch: 0, Step: 64, Rank: 28, loss = 0.69140625
c622-002: Epoch: 0, Step: 64, Rank: 43, loss = 0.59765625
c619-011: Epoch: 0, Step: 64, Rank: 14, loss = 0.51171875
c622-101: Epoch: 0, Step: 64, Rank: 62, loss = 0.4375
c622-092: Epoch: 0, Step: 64, Rank: 61, loss = 0.59375
c622-032: Epoch: 0, Step: 64, Rank: 49, loss = 0.56640625
c621-132: Epoch: 0, Step: 64, Rank: 37, loss = 0.5703125
c621-112: Epoch: 0, Step: 64, Rank: 33, loss = 0.56640625
c621-111: Epoch: 0, Step: 64, Rank: 32, loss = 0.65234375
c621-122: Epoch: 0, Step: 64, Rank: 35, loss = 0.5390625
c621-101: Epoch: 0, Step: 64, Rank: 30, loss = 0.59765625
c622-001: Epoch: 0, Step: 64, Rank: 42, loss = 0.51171875
c621-131: Epoch: 0, Step: 64, Rank: 36, loss = 0.56640625
c621-151: Epoch: 0, Step: 64, Rank: 40, loss = 0.58203125
c619-001: Epoch: 0, Step: 64, Rank: 12, loss = 0.5
c622-042: Epoch: 0, Step: 64, Rank: 51, loss = 0.56640625
c613-132: Epoch: 0, Step: 64, Rank: 7, loss = 0.609375
c613-112: Epoch: 0, Step: 64, Rank: 3, loss = 0.6328125
c613-111: Epoch: 0, Step: 64, Rank: 2, loss = 0.609375
c622-061: Epoch: 0, Step: 64, Rank: 54, loss = 0.5546875
c619-031: Epoch: 0, Step: 64, Rank: 18, loss = 0.53515625
c613-131: Epoch: 0, Step: 64, Rank: 6, loss = 0.5546875
c622-102: Epoch: 0, Step: 64, Rank: 63, loss = 0.18359375
c613-102: Epoch: 0, Step: 64, Rank: 1, loss = 0.609375
c619-041: Epoch: 0, Step: 64, Rank: 20, loss = 0.59375
c621-072: Epoch: 0, Step: 64, Rank: 25, loss = 0.5625
c622-082: Epoch: 0, Step: 64, Rank: 59, loss = 0.490234375
c622-051: Epoch: 0, Step: 64, Rank: 52, loss = 0.57421875
c613-121: Epoch: 0, Step: 64, Rank: 4, loss = 0.5234375
c621-082: Epoch: 0, Step: 64, Rank: 27, loss = 0.470703125
c622-041: Epoch: 0, Step: 64, Rank: 50, loss = 0.4375
c619-012: Epoch: 0, Step: 64, Rank: 15, loss = 0.51171875
c621-121: Epoch: 0, Step: 64, Rank: 34, loss = 0.126953125
c621-142: Epoch: 0, Step: 64, Rank: 39, loss = 0.474609375
c621-141: Epoch: 0, Step: 64, Rank: 38, loss = 0.26171875
c621-071: Epoch: 0, Step: 64, Rank: 24, loss = 0.447265625
c622-072: Epoch: 0, Step: 64, Rank: 57, loss = 0.201171875
c621-052: Epoch: 0, Step: 64, Rank: 21, loss = 0.455078125
c622-062: Epoch: 0, Step: 64, Rank: 55, loss = 0.62109375
c613-122: Epoch: 0, Step: 64, Rank: 5, loss = 0.341796875
c621-152: Epoch: 0, Step: 64, Rank: 41, loss = 0.58203125
c622-091: Epoch: 0, Step: 64, Rank: 60, loss = 0.6328125
c622-022: Epoch: 0, Step: 64, Rank: 47, loss = 0.53515625
c619-032: Epoch: 0, Step: 64, Rank: 19, loss = 0.6328125
c622-011: Epoch: 0, Step: 64, Rank: 44, loss = 0.36328125
c621-102: Epoch: 0, Step: 64, Rank: 31, loss = 0.35546875
c621-062: Epoch: 0, Step: 64, Rank: 23, loss = 0.5546875
c622-031: Epoch: 0, Step: 64, Rank: 48, loss = 0.57421875
c621-061: Epoch: 0, Step: 64, Rank: 22, loss = 0.455078125
c621-092: Epoch: 0, Step: 64, Rank: 29, loss = 0.54296875
c622-021: Epoch: 0, Step: 64, Rank: 46, loss = 0.306640625
c613-141: Epoch: 0, Step: 64, Rank: 8, loss = 0.5546875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2421875 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 65, Rank: 0, loss = 0.625
c622-002: Epoch: 0, Step: 65, Rank: 43, loss = 0.56640625
c619-021: Epoch: 0, Step: 65, Rank: 16, loss = 0.57421875
c622-052: Epoch: 0, Step: 65, Rank: 53, loss = 0.57421875
c622-062: Epoch: 0, Step: 65, Rank: 55, loss = 0.61328125
c622-081: Epoch: 0, Step: 65, Rank: 58, loss = 0.62109375
c613-111: Epoch: 0, Step: 65, Rank: 2, loss = 0.6328125
c619-022: Epoch: 0, Step: 65, Rank: 17, loss = 0.4765625
c622-071: Epoch: 0, Step: 65, Rank: 56, loss = 0.62109375
c622-012: Epoch: 0, Step: 65, Rank: 45, loss = 0.53125
c622-061: Epoch: 0, Step: 65, Rank: 54, loss = 0.494140625
c619-001: Epoch: 0, Step: 65, Rank: 12, loss = 0.5859375
c622-101: Epoch: 0, Step: 65, Rank: 62, loss = 0.54296875
c619-031: Epoch: 0, Step: 65, Rank: 18, loss = 0.4375
c622-011: Epoch: 0, Step: 65, Rank: 44, loss = 0.5546875
c622-041: Epoch: 0, Step: 65, Rank: 50, loss = 0.57421875
c622-102: Epoch: 0, Step: 65, Rank: 63, loss = 0.64453125
c621-111: Epoch: 0, Step: 65, Rank: 32, loss = 0.5859375
c622-032: Epoch: 0, Step: 65, Rank: 49, loss = 0.69140625
c621-061: Epoch: 0, Step: 65, Rank: 22, loss = 0.56640625
c613-112: Epoch: 0, Step: 65, Rank: 3, loss = 0.59765625
c622-001: Epoch: 0, Step: 65, Rank: 42, loss = 0.38671875
c619-041: Epoch: 0, Step: 65, Rank: 20, loss = 0.26171875
c613-152: Epoch: 0, Step: 65, Rank: 11, loss = 0.5390625
c619-032: Epoch: 0, Step: 65, Rank: 19, loss = 0.3046875
c619-002: Epoch: 0, Step: 65, Rank: 13, loss = 0.26171875
c622-092: Epoch: 0, Step: 65, Rank: 61, loss = 0.48828125
c621-052: Epoch: 0, Step: 65, Rank: 21, loss = 0.64453125
c622-082: Epoch: 0, Step: 65, Rank: 59, loss = 0.6328125
c613-121: Epoch: 0, Step: 65, Rank: 4, loss = 0.53515625
c622-022: Epoch: 0, Step: 65, Rank: 47, loss = 0.64453125
c613-122: Epoch: 0, Step: 65, Rank: 5, loss = 0.5703125
c619-011: Epoch: 0, Step: 65, Rank: 14, loss = 0.51171875
c621-151: Epoch: 0, Step: 65, Rank: 40, loss = 0.54296875
c613-132: Epoch: 0, Step: 65, Rank: 7, loss = 0.55859375
c621-132: Epoch: 0, Step: 65, Rank: 37, loss = 0.6171875
c622-031: Epoch: 0, Step: 65, Rank: 48, loss = 0.5234375
c621-112: Epoch: 0, Step: 65, Rank: 33, loss = 0.51171875
c622-051: Epoch: 0, Step: 65, Rank: 52, loss = 0.431640625
c619-012: Epoch: 0, Step: 65, Rank: 15, loss = 0.65234375
c621-131: Epoch: 0, Step: 65, Rank: 36, loss = 0.447265625
c622-072: Epoch: 0, Step: 65, Rank: 57, loss = 0.50390625
c613-131: Epoch: 0, Step: 65, Rank: 6, loss = 0.5390625
c621-152: Epoch: 0, Step: 65, Rank: 41, loss = 0.494140625
c621-121: Epoch: 0, Step: 65, Rank: 34, loss = 0.51953125
c613-151: Epoch: 0, Step: 65, Rank: 10, loss = 0.5078125
c621-081: Epoch: 0, Step: 65, Rank: 26, loss = 0.38671875
c621-062: Epoch: 0, Step: 65, Rank: 23, loss = 0.69140625
c621-122: Epoch: 0, Step: 65, Rank: 35, loss = 0.69140625
c621-102: Epoch: 0, Step: 65, Rank: 31, loss = 0.26171875
c621-091: Epoch: 0, Step: 65, Rank: 28, loss = 0.69140625
c613-141: Epoch: 0, Step: 65, Rank: 8, loss = 0.51171875
c622-042: Epoch: 0, Step: 65, Rank: 51, loss = 0.43359375
c621-101: Epoch: 0, Step: 65, Rank: 30, loss = 0.5703125
c621-141: Epoch: 0, Step: 65, Rank: 38, loss = 0.5859375
c622-091: Epoch: 0, Step: 65, Rank: 60, loss = 0.404296875
c613-142: Epoch: 0, Step: 65, Rank: 9, loss = 0.47265625
c621-082: Epoch: 0, Step: 65, Rank: 27, loss = 0.51171875
c621-072: Epoch: 0, Step: 65, Rank: 25, loss = 0.6015625
c621-142: Epoch: 0, Step: 65, Rank: 39, loss = 0.5625
c621-092: Epoch: 0, Step: 65, Rank: 29, loss = 0.53515625
c613-102: Epoch: 0, Step: 65, Rank: 1, loss = 0.5234375
c622-021: Epoch: 0, Step: 65, Rank: 46, loss = 0.62109375
c621-071: Epoch: 0, Step: 65, Rank: 24, loss = 0.59375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 66, Rank: 0, loss = 0.53515625
c622-101: Epoch: 0, Step: 66, Rank: 62, loss = 0.64453125
c622-081: Epoch: 0, Step: 66, Rank: 58, loss = 0.57421875
c621-081: Epoch: 0, Step: 66, Rank: 26, loss = 0.51953125
c621-072: Epoch: 0, Step: 66, Rank: 25, loss = 0.474609375
c622-092: Epoch: 0, Step: 66, Rank: 61, loss = 0.494140625
c613-111: Epoch: 0, Step: 66, Rank: 2, loss = 0.57421875
c622-052: Epoch: 0, Step: 66, Rank: 53, loss = 0.51171875
c622-032: Epoch: 0, Step: 66, Rank: 49, loss = 0.54296875
c621-111: Epoch: 0, Step: 66, Rank: 32, loss = 0.494140625
c622-041: Epoch: 0, Step: 66, Rank: 50, loss = 0.6640625
c622-002: Epoch: 0, Step: 66, Rank: 43, loss = 0.56640625
c622-102: Epoch: 0, Step: 66, Rank: 63, loss = 0.59765625
c619-021: Epoch: 0, Step: 66, Rank: 16, loss = 0.55859375
c613-121: Epoch: 0, Step: 66, Rank: 4, loss = 0.455078125
c619-001: Epoch: 0, Step: 66, Rank: 12, loss = 0.3671875
c621-151: Epoch: 0, Step: 66, Rank: 40, loss = 0.6171875
c622-062: Epoch: 0, Step: 66, Rank: 55, loss = 0.5390625
c621-091: Epoch: 0, Step: 66, Rank: 28, loss = 0.58203125
c619-002: Epoch: 0, Step: 66, Rank: 13, loss = 0.53515625
c613-112: Epoch: 0, Step: 66, Rank: 3, loss = 0.53515625
c613-102: Epoch: 0, Step: 66, Rank: 1, loss = 0.51953125
c621-071: Epoch: 0, Step: 66, Rank: 24, loss = 0.56640625
c621-142: Epoch: 0, Step: 66, Rank: 39, loss = 0.54296875
c622-001: Epoch: 0, Step: 66, Rank: 42, loss = 0.126953125
c619-011: Epoch: 0, Step: 66, Rank: 14, loss = 0.62109375
c622-031: Epoch: 0, Step: 66, Rank: 48, loss = 0.57421875
c613-122: Epoch: 0, Step: 66, Rank: 5, loss = 0.51171875
c619-041: Epoch: 0, Step: 66, Rank: 20, loss = 0.38671875
c622-071: Epoch: 0, Step: 66, Rank: 56, loss = 0.69140625
c622-012: Epoch: 0, Step: 66, Rank: 45, loss = 0.50390625
c621-052: Epoch: 0, Step: 66, Rank: 21, loss = 0.48828125
c613-131: Epoch: 0, Step: 66, Rank: 6, loss = 0.53515625
c619-031: Epoch: 0, Step: 66, Rank: 18, loss = 0.5390625
c619-032: Epoch: 0, Step: 66, Rank: 19, loss = 0.5546875
c622-042: Epoch: 0, Step: 66, Rank: 51, loss = 0.6171875
c621-101: Epoch: 0, Step: 66, Rank: 30, loss = 0.57421875
c621-061: Epoch: 0, Step: 66, Rank: 22, loss = 0.51171875
c613-142: Epoch: 0, Step: 66, Rank: 9, loss = 0.5078125
c621-082: Epoch: 0, Step: 66, Rank: 27, loss = 0.69140625
c613-152: Epoch: 0, Step: 66, Rank: 11, loss = 0.5546875
c613-151: Epoch: 0, Step: 66, Rank: 10, loss = 0.474609375
c622-091: Epoch: 0, Step: 66, Rank: 60, loss = 0.474609375
c613-132: Epoch: 0, Step: 66, Rank: 7, loss = 0.53515625
c622-051: Epoch: 0, Step: 66, Rank: 52, loss = 0.59765625
c621-121: Epoch: 0, Step: 66, Rank: 34, loss = 0.6328125
c621-122: Epoch: 0, Step: 66, Rank: 35, loss = 0.494140625
c613-141: Epoch: 0, Step: 66, Rank: 8, loss = 0.57421875
c621-141: Epoch: 0, Step: 66, Rank: 38, loss = 0.5546875
c622-061: Epoch: 0, Step: 66, Rank: 54, loss = 0.431640625
c622-022: Epoch: 0, Step: 66, Rank: 47, loss = 0.51171875
c621-132: Epoch: 0, Step: 66, Rank: 37, loss = 0.18359375
c622-072: Epoch: 0, Step: 66, Rank: 57, loss = 0.251953125
c621-112: Epoch: 0, Step: 66, Rank: 33, loss = 0.609375
c621-102: Epoch: 0, Step: 66, Rank: 31, loss = 0.57421875
c622-082: Epoch: 0, Step: 66, Rank: 59, loss = 0.51171875
c619-012: Epoch: 0, Step: 66, Rank: 15, loss = 0.625
c621-062: Epoch: 0, Step: 66, Rank: 23, loss = 0.57421875
c621-152: Epoch: 0, Step: 66, Rank: 41, loss = 0.5546875
c619-022: Epoch: 0, Step: 66, Rank: 17, loss = 0.50390625
c621-092: Epoch: 0, Step: 66, Rank: 29, loss = 0.3671875
c622-021: Epoch: 0, Step: 66, Rank: 46, loss = 0.4375
c621-131: Epoch: 0, Step: 66, Rank: 36, loss = 0.54296875
c622-011: Epoch: 0, Step: 66, Rank: 44, loss = 0.12890625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-001: Epoch: 0, Step: 67, Rank: 12, loss = 0.53515625
c619-021: Epoch: 0, Step: 67, Rank: 16, loss = 0.484375
c621-091: Epoch: 0, Step: 67, Rank: 28, loss = 0.419921875
c622-002: Epoch: 0, Step: 67, Rank: 43, loss = 0.578125
c619-002: Epoch: 0, Step: 67, Rank: 13, loss = 0.5234375
c613-152: Epoch: 0, Step: 67, Rank: 11, loss = 0.263671875
c621-121: Epoch: 0, Step: 67, Rank: 34, loss = 0.57421875
c621-052: Epoch: 0, Step: 67, Rank: 21, loss = 0.57421875
c619-041: Epoch: 0, Step: 67, Rank: 20, loss = 0.59765625
c619-031: Epoch: 0, Step: 67, Rank: 18, loss = 0.56640625
c621-061: Epoch: 0, Step: 67, Rank: 22, loss = 0.1640625
c621-082: Epoch: 0, Step: 67, Rank: 27, loss = 0.6171875
c621-081: Epoch: 0, Step: 67, Rank: 26, loss = 0.61328125
c622-012: Epoch: 0, Step: 67, Rank: 45, loss = 0.455078125
c621-131: Epoch: 0, Step: 67, Rank: 36, loss = 0.419921875
c613-101: Epoch: 0, Step: 67, Rank: 0, loss = 0.419921875
c621-132: Epoch: 0, Step: 67, Rank: 37, loss = 0.30859375
c621-111: Epoch: 0, Step: 67, Rank: 32, loss = 0.4375
c621-072: Epoch: 0, Step: 67, Rank: 25, loss = 0.59765625
c619-022: Epoch: 0, Step: 67, Rank: 17, loss = 0.5859375
c621-101: Epoch: 0, Step: 67, Rank: 30, loss = 0.5859375
c621-122: Epoch: 0, Step: 67, Rank: 35, loss = 0.62109375
c622-052: Epoch: 0, Step: 67, Rank: 53, loss = 0.51171875
c621-112: Epoch: 0, Step: 67, Rank: 33, loss = 0.498046875
c613-151: Epoch: 0, Step: 67, Rank: 10, loss = 0.55859375
c619-032: Epoch: 0, Step: 67, Rank: 19, loss = 0.55078125
c622-001: Epoch: 0, Step: 67, Rank: 42, loss = 0.62109375
c621-102: Epoch: 0, Step: 67, Rank: 31, loss = 0.53515625
c622-011: Epoch: 0, Step: 67, Rank: 44, loss = 0.59765625
c622-101: Epoch: 0, Step: 67, Rank: 62, loss = 0.54296875
c621-142: Epoch: 0, Step: 67, Rank: 39, loss = 0.4375
c622-092: Epoch: 0, Step: 67, Rank: 61, loss = 0.53515625
c621-151: Epoch: 0, Step: 67, Rank: 40, loss = 0.5234375
c622-022: Epoch: 0, Step: 67, Rank: 47, loss = 0.63671875
c622-032: Epoch: 0, Step: 67, Rank: 49, loss = 0.62109375
c621-062: Epoch: 0, Step: 67, Rank: 23, loss = 0.48828125
c621-152: Epoch: 0, Step: 67, Rank: 41, loss = 0.59765625
c622-081: Epoch: 0, Step: 67, Rank: 58, loss = 0.53515625
c621-071: Epoch: 0, Step: 67, Rank: 24, loss = 0.52734375
c621-141: Epoch: 0, Step: 67, Rank: 38, loss = 0.54296875
c613-111: Epoch: 0, Step: 67, Rank: 2, loss = 0.5625
c613-142: Epoch: 0, Step: 67, Rank: 9, loss = 0.58203125
c622-051: Epoch: 0, Step: 67, Rank: 52, loss = 0.5546875
c622-102: Epoch: 0, Step: 67, Rank: 63, loss = 0.53515625
c613-112: Epoch: 0, Step: 67, Rank: 3, loss = 0.5078125
c613-132: Epoch: 0, Step: 67, Rank: 7, loss = 0.609375
c619-012: Epoch: 0, Step: 67, Rank: 15, loss = 0.3125
c621-092: Epoch: 0, Step: 67, Rank: 29, loss = 0.56640625
c622-021: Epoch: 0, Step: 67, Rank: 46, loss = 0.494140625
c619-011: Epoch: 0, Step: 67, Rank: 14, loss = 0.46875
c613-122: Epoch: 0, Step: 67, Rank: 5, loss = 0.51171875
c622-041: Epoch: 0, Step: 67, Rank: 50, loss = 0.62109375
c613-141: Epoch: 0, Step: 67, Rank: 8, loss = 0.62109375
c622-031: Epoch: 0, Step: 67, Rank: 48, loss = 0.5546875
c613-121: Epoch: 0, Step: 67, Rank: 4, loss = 0.515625
c613-131: Epoch: 0, Step: 67, Rank: 6, loss = 0.59765625
c613-102: Epoch: 0, Step: 67, Rank: 1, loss = 0.69140625
c622-072: Epoch: 0, Step: 67, Rank: 57, loss = 0.51171875
c622-082: Epoch: 0, Step: 67, Rank: 59, loss = 0.5390625
c622-042: Epoch: 0, Step: 67, Rank: 51, loss = 0.54296875
c622-061: Epoch: 0, Step: 67, Rank: 54, loss = 0.62109375
c622-091: Epoch: 0, Step: 67, Rank: 60, loss = 0.56640625
c622-071: Epoch: 0, Step: 67, Rank: 56, loss = 0.69140625
c622-062: Epoch: 0, Step: 67, Rank: 55, loss = 0.625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 68, Rank: 0, loss = 0.57421875
c622-081: Epoch: 0, Step: 68, Rank: 58, loss = 0.6171875
c622-002: Epoch: 0, Step: 68, Rank: 43, loss = 0.62109375
c622-062: Epoch: 0, Step: 68, Rank: 55, loss = 0.419921875
c613-132: Epoch: 0, Step: 68, Rank: 7, loss = 0.69140625
c622-032: Epoch: 0, Step: 68, Rank: 49, loss = 0.5234375
c622-012: Epoch: 0, Step: 68, Rank: 45, loss = 0.63671875
c621-072: Epoch: 0, Step: 68, Rank: 25, loss = 0.5546875
c621-111: Epoch: 0, Step: 68, Rank: 32, loss = 0.55078125
c622-052: Epoch: 0, Step: 68, Rank: 53, loss = 0.18359375
c619-002: Epoch: 0, Step: 68, Rank: 13, loss = 0.51171875
c621-091: Epoch: 0, Step: 68, Rank: 28, loss = 0.54296875
c621-082: Epoch: 0, Step: 68, Rank: 27, loss = 0.625
c613-142: Epoch: 0, Step: 68, Rank: 9, loss = 0.53515625
c622-071: Epoch: 0, Step: 68, Rank: 56, loss = 0.55859375
c619-032: Epoch: 0, Step: 68, Rank: 19, loss = 0.58203125
c621-132: Epoch: 0, Step: 68, Rank: 37, loss = 0.55859375
c613-141: Epoch: 0, Step: 68, Rank: 8, loss = 0.08642578125
c622-001: Epoch: 0, Step: 68, Rank: 42, loss = 0.69140625
c613-152: Epoch: 0, Step: 68, Rank: 11, loss = 0.54296875
c619-021: Epoch: 0, Step: 68, Rank: 16, loss = 0.35546875
c613-111: Epoch: 0, Step: 68, Rank: 2, loss = 0.16015625
c621-081: Epoch: 0, Step: 68, Rank: 26, loss = 0.5859375
c622-061: Epoch: 0, Step: 68, Rank: 54, loss = 0.51171875
c621-151: Epoch: 0, Step: 68, Rank: 40, loss = 0.455078125
c613-112: Epoch: 0, Step: 68, Rank: 3, loss = 0.6328125
c619-001: Epoch: 0, Step: 68, Rank: 12, loss = 0.6171875
c621-131: Epoch: 0, Step: 68, Rank: 36, loss = 0.62109375
c622-092: Epoch: 0, Step: 68, Rank: 61, loss = 0.287109375
c619-011: Epoch: 0, Step: 68, Rank: 14, loss = 0.53515625
c621-101: Epoch: 0, Step: 68, Rank: 30, loss = 0.51171875
c622-102: Epoch: 0, Step: 68, Rank: 63, loss = 0.5390625
c613-131: Epoch: 0, Step: 68, Rank: 6, loss = 0.60546875
c621-121: Epoch: 0, Step: 68, Rank: 34, loss = 0.6171875
c619-031: Epoch: 0, Step: 68, Rank: 18, loss = 0.60546875
c622-011: Epoch: 0, Step: 68, Rank: 44, loss = 0.51171875
c619-041: Epoch: 0, Step: 68, Rank: 20, loss = 0.494140625
c621-061: Epoch: 0, Step: 68, Rank: 22, loss = 0.53515625
c619-022: Epoch: 0, Step: 68, Rank: 17, loss = 0.56640625
c613-122: Epoch: 0, Step: 68, Rank: 5, loss = 0.69140625
c622-031: Epoch: 0, Step: 68, Rank: 48, loss = 0.69140625
c613-121: Epoch: 0, Step: 68, Rank: 4, loss = 0.498046875
c622-072: Epoch: 0, Step: 68, Rank: 57, loss = 0.5859375
c621-142: Epoch: 0, Step: 68, Rank: 39, loss = 0.62890625
c622-082: Epoch: 0, Step: 68, Rank: 59, loss = 0.251953125
c622-041: Epoch: 0, Step: 68, Rank: 50, loss = 0.64453125
c622-022: Epoch: 0, Step: 68, Rank: 47, loss = 0.65625
c622-051: Epoch: 0, Step: 68, Rank: 52, loss = 0.322265625
c622-101: Epoch: 0, Step: 68, Rank: 62, loss = 0.474609375
c621-052: Epoch: 0, Step: 68, Rank: 21, loss = 0.5859375
c621-152: Epoch: 0, Step: 68, Rank: 41, loss = 0.53515625
c621-071: Epoch: 0, Step: 68, Rank: 24, loss = 0.5546875
c613-102: Epoch: 0, Step: 68, Rank: 1, loss = 0.57421875
c621-102: Epoch: 0, Step: 68, Rank: 31, loss = 0.69140625
c621-112: Epoch: 0, Step: 68, Rank: 33, loss = 0.287109375
c621-122: Epoch: 0, Step: 68, Rank: 35, loss = 0.6328125
c622-042: Epoch: 0, Step: 68, Rank: 51, loss = 0.455078125
c621-141: Epoch: 0, Step: 68, Rank: 38, loss = 0.57421875
c619-012: Epoch: 0, Step: 68, Rank: 15, loss = 0.59375
c621-062: Epoch: 0, Step: 68, Rank: 23, loss = 0.5859375
c622-091: Epoch: 0, Step: 68, Rank: 60, loss = 0.546875
c622-021: Epoch: 0, Step: 68, Rank: 46, loss = 0.54296875
c621-092: Epoch: 0, Step: 68, Rank: 29, loss = 0.498046875
c613-151: Epoch: 0, Step: 68, Rank: 10, loss = 0.443359375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 69, Rank: 16, loss = 0.494140625
c622-052: Epoch: 0, Step: 69, Rank: 53, loss = 0.5703125
c613-101: Epoch: 0, Step: 69, Rank: 0, loss = 0.57421875
c619-002: Epoch: 0, Step: 69, Rank: 13, loss = 0.55859375
c622-002: Epoch: 0, Step: 69, Rank: 43, loss = 0.4375
c621-081: Epoch: 0, Step: 69, Rank: 26, loss = 0.57421875
c621-111: Epoch: 0, Step: 69, Rank: 32, loss = 0.609375
c621-101: Epoch: 0, Step: 69, Rank: 30, loss = 0.5859375
c621-091: Epoch: 0, Step: 69, Rank: 28, loss = 0.3125
c622-071: Epoch: 0, Step: 69, Rank: 56, loss = 0.5234375
c619-031: Epoch: 0, Step: 69, Rank: 18, loss = 0.5859375
c622-041: Epoch: 0, Step: 69, Rank: 50, loss = 0.57421875
c622-001: Epoch: 0, Step: 69, Rank: 42, loss = 0.6328125
c619-022: Epoch: 0, Step: 69, Rank: 17, loss = 0.54296875
c621-072: Epoch: 0, Step: 69, Rank: 25, loss = 0.59765625
c621-061: Epoch: 0, Step: 69, Rank: 22, loss = 0.55859375
c622-051: Epoch: 0, Step: 69, Rank: 52, loss = 0.59765625
c622-012: Epoch: 0, Step: 69, Rank: 45, loss = 0.56640625
c613-122: Epoch: 0, Step: 69, Rank: 5, loss = 0.59765625
c621-092: Epoch: 0, Step: 69, Rank: 29, loss = 0.5546875
c621-052: Epoch: 0, Step: 69, Rank: 21, loss = 0.58203125
c621-102: Epoch: 0, Step: 69, Rank: 31, loss = 0.54296875
c621-121: Epoch: 0, Step: 69, Rank: 34, loss = 0.37890625
c621-082: Epoch: 0, Step: 69, Rank: 27, loss = 0.53515625
c619-001: Epoch: 0, Step: 69, Rank: 12, loss = 0.6328125
c621-071: Epoch: 0, Step: 69, Rank: 24, loss = 0.6328125
c622-101: Epoch: 0, Step: 69, Rank: 62, loss = 0.474609375
c622-061: Epoch: 0, Step: 69, Rank: 54, loss = 0.62109375
c613-131: Epoch: 0, Step: 69, Rank: 6, loss = 0.26953125
c619-041: Epoch: 0, Step: 69, Rank: 20, loss = 0.328125
c622-032: Epoch: 0, Step: 69, Rank: 49, loss = 0.423828125
c613-152: Epoch: 0, Step: 69, Rank: 11, loss = 0.62109375
c622-011: Epoch: 0, Step: 69, Rank: 44, loss = 0.56640625
c613-132: Epoch: 0, Step: 69, Rank: 7, loss = 0.59375
c621-151: Epoch: 0, Step: 69, Rank: 40, loss = 0.55078125
c622-062: Epoch: 0, Step: 69, Rank: 55, loss = 0.5859375
c613-142: Epoch: 0, Step: 69, Rank: 9, loss = 0.56640625
c613-111: Epoch: 0, Step: 69, Rank: 2, loss = 0.6015625
c621-062: Epoch: 0, Step: 69, Rank: 23, loss = 0.60546875
c613-141: Epoch: 0, Step: 69, Rank: 8, loss = 0.333984375
c621-112: Epoch: 0, Step: 69, Rank: 33, loss = 0.6328125
c613-121: Epoch: 0, Step: 69, Rank: 4, loss = 0.57421875
c619-012: Epoch: 0, Step: 69, Rank: 15, loss = 0.341796875
c621-122: Epoch: 0, Step: 69, Rank: 35, loss = 0.6015625
c621-132: Epoch: 0, Step: 69, Rank: 37, loss = 0.69140625
c622-022: Epoch: 0, Step: 69, Rank: 47, loss = 0.53125
c613-112: Epoch: 0, Step: 69, Rank: 3, loss = 0.69140625
c619-011: Epoch: 0, Step: 69, Rank: 14, loss = 0.71875
c621-152: Epoch: 0, Step: 69, Rank: 41, loss = 0.51171875
c622-031: Epoch: 0, Step: 69, Rank: 48, loss = 0.53515625
c619-032: Epoch: 0, Step: 69, Rank: 19, loss = 0.4375
c622-092: Epoch: 0, Step: 69, Rank: 61, loss = 0.271484375
c622-102: Epoch: 0, Step: 69, Rank: 63, loss = 0.58984375
c622-082: Epoch: 0, Step: 69, Rank: 59, loss = 0.37109375
c621-142: Epoch: 0, Step: 69, Rank: 39, loss = 0.59765625
c622-081: Epoch: 0, Step: 69, Rank: 58, loss = 0.427734375
c621-131: Epoch: 0, Step: 69, Rank: 36, loss = 0.38671875
c621-141: Epoch: 0, Step: 69, Rank: 38, loss = 0.4375
c613-102: Epoch: 0, Step: 69, Rank: 1, loss = 0.55859375
c622-072: Epoch: 0, Step: 69, Rank: 57, loss = 0.37109375
c622-042: Epoch: 0, Step: 69, Rank: 51, loss = 0.5859375
c622-021: Epoch: 0, Step: 69, Rank: 46, loss = 0.4375
c613-151: Epoch: 0, Step: 69, Rank: 10, loss = 0.341796875
c622-091: Epoch: 0, Step: 69, Rank: 60, loss = 0.455078125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 70, Rank: 0, loss = 0.5234375
c622-092: Epoch: 0, Step: 70, Rank: 61, loss = 0.58203125
c622-101: Epoch: 0, Step: 70, Rank: 62, loss = 0.57421875
c619-002: Epoch: 0, Step: 70, Rank: 13, loss = 0.4375
c622-082: Epoch: 0, Step: 70, Rank: 59, loss = 0.69140625
c613-132: Epoch: 0, Step: 70, Rank: 7, loss = 0.26171875
c613-131: Epoch: 0, Step: 70, Rank: 6, loss = 0.59765625
c613-152: Epoch: 0, Step: 70, Rank: 11, loss = 0.5546875
c613-151: Epoch: 0, Step: 70, Rank: 10, loss = 0.5625
c622-081: Epoch: 0, Step: 70, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 70, Rank: 53, loss = 0.5546875
c613-111: Epoch: 0, Step: 70, Rank: 2, loss = 0.5546875
c622-012: Epoch: 0, Step: 70, Rank: 45, loss = 0.484375
c622-002: Epoch: 0, Step: 70, Rank: 43, loss = 0.5234375
c619-021: Epoch: 0, Step: 70, Rank: 16, loss = 0.220703125
c613-121: Epoch: 0, Step: 70, Rank: 4, loss = 0.546875
c621-151: Epoch: 0, Step: 70, Rank: 40, loss = 0.494140625
c613-141: Epoch: 0, Step: 70, Rank: 8, loss = 0.484375
c621-131: Epoch: 0, Step: 70, Rank: 36, loss = 0.5703125
c622-001: Epoch: 0, Step: 70, Rank: 42, loss = 0.419921875
c619-001: Epoch: 0, Step: 70, Rank: 12, loss = 0.49609375
c621-121: Epoch: 0, Step: 70, Rank: 34, loss = 0.5703125
c622-061: Epoch: 0, Step: 70, Rank: 54, loss = 0.515625
c621-111: Epoch: 0, Step: 70, Rank: 32, loss = 0.56640625
c619-022: Epoch: 0, Step: 70, Rank: 17, loss = 0.6328125
c619-031: Epoch: 0, Step: 70, Rank: 18, loss = 0.271484375
c622-091: Epoch: 0, Step: 70, Rank: 60, loss = 0.5078125
c621-122: Epoch: 0, Step: 70, Rank: 35, loss = 0.609375
c622-071: Epoch: 0, Step: 70, Rank: 56, loss = 0.2578125
c622-072: Epoch: 0, Step: 70, Rank: 57, loss = 0.55078125
c613-142: Epoch: 0, Step: 70, Rank: 9, loss = 0.58203125
c621-142: Epoch: 0, Step: 70, Rank: 39, loss = 0.18359375
c621-072: Epoch: 0, Step: 70, Rank: 25, loss = 0.38671875
c619-041: Epoch: 0, Step: 70, Rank: 20, loss = 0.474609375
c622-051: Epoch: 0, Step: 70, Rank: 52, loss = 0.62109375
c622-041: Epoch: 0, Step: 70, Rank: 50, loss = 0.5234375
c622-011: Epoch: 0, Step: 70, Rank: 44, loss = 0.34765625
c622-062: Epoch: 0, Step: 70, Rank: 55, loss = 0.5859375
c621-081: Epoch: 0, Step: 70, Rank: 26, loss = 0.50390625
c622-032: Epoch: 0, Step: 70, Rank: 49, loss = 0.2412109375
c621-152: Epoch: 0, Step: 70, Rank: 41, loss = 0.6171875
c622-022: Epoch: 0, Step: 70, Rank: 47, loss = 0.55859375
c613-102: Epoch: 0, Step: 70, Rank: 1, loss = 0.59375
c619-011: Epoch: 0, Step: 70, Rank: 14, loss = 0.609375
c613-122: Epoch: 0, Step: 70, Rank: 5, loss = 0.30078125
c621-141: Epoch: 0, Step: 70, Rank: 38, loss = 0.51171875
c621-052: Epoch: 0, Step: 70, Rank: 21, loss = 0.60546875
c622-031: Epoch: 0, Step: 70, Rank: 48, loss = 0.59765625
c621-082: Epoch: 0, Step: 70, Rank: 27, loss = 0.50390625
c621-112: Epoch: 0, Step: 70, Rank: 33, loss = 0.59375
c613-112: Epoch: 0, Step: 70, Rank: 3, loss = 0.51171875
c622-102: Epoch: 0, Step: 70, Rank: 63, loss = 0.51953125
c621-101: Epoch: 0, Step: 70, Rank: 30, loss = 0.55859375
c621-071: Epoch: 0, Step: 70, Rank: 24, loss = 0.37109375
c619-032: Epoch: 0, Step: 70, Rank: 19, loss = 0.56640625
c621-102: Epoch: 0, Step: 70, Rank: 31, loss = 0.64453125
c622-042: Epoch: 0, Step: 70, Rank: 51, loss = 0.435546875
c621-091: Epoch: 0, Step: 70, Rank: 28, loss = 0.59375
c619-012: Epoch: 0, Step: 70, Rank: 15, loss = 0.69140625
c621-062: Epoch: 0, Step: 70, Rank: 23, loss = 0.5703125
c622-021: Epoch: 0, Step: 70, Rank: 46, loss = 0.62109375
c621-092: Epoch: 0, Step: 70, Rank: 29, loss = 0.474609375
c621-061: Epoch: 0, Step: 70, Rank: 22, loss = 0.59375
c621-132: Epoch: 0, Step: 70, Rank: 37, loss = 0.51171875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 71, Rank: 58, loss = 0.64453125
c622-082: Epoch: 0, Step: 71, Rank: 59, loss = 0.55859375
c622-091: Epoch: 0, Step: 71, Rank: 60, loss = 0.30859375
c613-101: Epoch: 0, Step: 71, Rank: 0, loss = 0.58203125
c622-071: Epoch: 0, Step: 71, Rank: 56, loss = 0.56640625
c622-072: Epoch: 0, Step: 71, Rank: 57, loss = 0.56640625
c622-101: Epoch: 0, Step: 71, Rank: 62, loss = 0.404296875
c622-092: Epoch: 0, Step: 71, Rank: 61, loss = 0.60546875
c622-062: Epoch: 0, Step: 71, Rank: 55, loss = 0.474609375
c622-052: Epoch: 0, Step: 71, Rank: 53, loss = 0.37109375
c622-061: Epoch: 0, Step: 71, Rank: 54, loss = 0.51171875
c622-002: Epoch: 0, Step: 71, Rank: 43, loss = 0.5859375
c613-111: Epoch: 0, Step: 71, Rank: 2, loss = 0.6328125
c622-102: Epoch: 0, Step: 71, Rank: 63, loss = 0.57421875
c622-051: Epoch: 0, Step: 71, Rank: 52, loss = 0.51171875
c622-041: Epoch: 0, Step: 71, Rank: 50, loss = 0.55078125
c613-112: Epoch: 0, Step: 71, Rank: 3, loss = 0.5546875
c622-042: Epoch: 0, Step: 71, Rank: 51, loss = 0.53515625
c613-102: Epoch: 0, Step: 71, Rank: 1, loss = 0.474609375
c622-022: Epoch: 0, Step: 71, Rank: 47, loss = 0.5859375
c613-121: Epoch: 0, Step: 71, Rank: 4, loss = 0.1826171875
c621-132: Epoch: 0, Step: 71, Rank: 37, loss = 0.578125
c622-031: Epoch: 0, Step: 71, Rank: 48, loss = 0.494140625
c622-001: Epoch: 0, Step: 71, Rank: 42, loss = 0.625
c622-012: Epoch: 0, Step: 71, Rank: 45, loss = 0.16015625
c613-151: Epoch: 0, Step: 71, Rank: 10, loss = 0.474609375
c621-111: Epoch: 0, Step: 71, Rank: 32, loss = 0.419921875
c622-032: Epoch: 0, Step: 71, Rank: 49, loss = 0.6328125
c622-021: Epoch: 0, Step: 71, Rank: 46, loss = 0.69140625
c613-131: Epoch: 0, Step: 71, Rank: 6, loss = 0.59765625
c621-151: Epoch: 0, Step: 71, Rank: 40, loss = 0.57421875
c613-132: Epoch: 0, Step: 71, Rank: 7, loss = 0.53515625
c621-141: Epoch: 0, Step: 71, Rank: 38, loss = 0.53515625
c619-001: Epoch: 0, Step: 71, Rank: 12, loss = 0.484375
c621-152: Epoch: 0, Step: 71, Rank: 41, loss = 0.18359375
c613-152: Epoch: 0, Step: 71, Rank: 11, loss = 0.404296875
c621-091: Epoch: 0, Step: 71, Rank: 28, loss = 0.53515625
c613-142: Epoch: 0, Step: 71, Rank: 9, loss = 0.58984375
c622-011: Epoch: 0, Step: 71, Rank: 44, loss = 0.55859375
c621-081: Epoch: 0, Step: 71, Rank: 26, loss = 0.5390625
c621-131: Epoch: 0, Step: 71, Rank: 36, loss = 0.56640625
c613-122: Epoch: 0, Step: 71, Rank: 5, loss = 0.447265625
c621-121: Epoch: 0, Step: 71, Rank: 34, loss = 0.56640625
c619-021: Epoch: 0, Step: 71, Rank: 16, loss = 0.5546875
c621-142: Epoch: 0, Step: 71, Rank: 39, loss = 0.494140625
c621-122: Epoch: 0, Step: 71, Rank: 35, loss = 0.5625
c613-141: Epoch: 0, Step: 71, Rank: 8, loss = 0.474609375
c621-082: Epoch: 0, Step: 71, Rank: 27, loss = 0.3125
c619-011: Epoch: 0, Step: 71, Rank: 14, loss = 0.419921875
c621-112: Epoch: 0, Step: 71, Rank: 33, loss = 0.59765625
c619-012: Epoch: 0, Step: 71, Rank: 15, loss = 0.55859375
c619-041: Epoch: 0, Step: 71, Rank: 20, loss = 0.59375
c621-092: Epoch: 0, Step: 71, Rank: 29, loss = 0.5546875
c621-072: Epoch: 0, Step: 71, Rank: 25, loss = 0.3125
c619-002: Epoch: 0, Step: 71, Rank: 13, loss = 0.69140625
c619-031: Epoch: 0, Step: 71, Rank: 18, loss = 0.5390625
c621-101: Epoch: 0, Step: 71, Rank: 30, loss = 0.5625
c619-022: Epoch: 0, Step: 71, Rank: 17, loss = 0.59765625
c621-071: Epoch: 0, Step: 71, Rank: 24, loss = 0.404296875
c621-052: Epoch: 0, Step: 71, Rank: 21, loss = 0.4375
c621-061: Epoch: 0, Step: 71, Rank: 22, loss = 0.57421875
c621-062: Epoch: 0, Step: 71, Rank: 23, loss = 0.52734375
c619-032: Epoch: 0, Step: 71, Rank: 19, loss = 0.59765625
c621-102: Epoch: 0, Step: 71, Rank: 31, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 72, Rank: 43, loss = 0.640625
c622-001: Epoch: 0, Step: 72, Rank: 42, loss = 0.5703125
c621-091: Epoch: 0, Step: 72, Rank: 28, loss = 0.35546875
c621-151: Epoch: 0, Step: 72, Rank: 40, loss = 0.26171875
c613-101: Epoch: 0, Step: 72, Rank: 0, loss = 0.30859375
c619-021: Epoch: 0, Step: 72, Rank: 16, loss = 0.54296875
c622-052: Epoch: 0, Step: 72, Rank: 53, loss = 0.59765625
c621-081: Epoch: 0, Step: 72, Rank: 26, loss = 0.455078125
c622-041: Epoch: 0, Step: 72, Rank: 50, loss = 0.625
c622-012: Epoch: 0, Step: 72, Rank: 45, loss = 0.53515625
c621-111: Epoch: 0, Step: 72, Rank: 32, loss = 0.5546875
c619-031: Epoch: 0, Step: 72, Rank: 18, loss = 0.255859375
c621-132: Epoch: 0, Step: 72, Rank: 37, loss = 0.474609375
c621-131: Epoch: 0, Step: 72, Rank: 36, loss = 0.59765625
c621-072: Epoch: 0, Step: 72, Rank: 25, loss = 0.5703125
c621-092: Epoch: 0, Step: 72, Rank: 29, loss = 0.56640625
c621-152: Epoch: 0, Step: 72, Rank: 41, loss = 0.6171875
c622-081: Epoch: 0, Step: 72, Rank: 58, loss = 0.52734375
c622-011: Epoch: 0, Step: 72, Rank: 44, loss = 0.5546875
c621-101: Epoch: 0, Step: 72, Rank: 30, loss = 0.625
c621-122: Epoch: 0, Step: 72, Rank: 35, loss = 0.39453125
c621-112: Epoch: 0, Step: 72, Rank: 33, loss = 0.69140625
c621-052: Epoch: 0, Step: 72, Rank: 21, loss = 0.4375
c622-051: Epoch: 0, Step: 72, Rank: 52, loss = 0.35546875
c619-041: Epoch: 0, Step: 72, Rank: 20, loss = 0.56640625
c613-141: Epoch: 0, Step: 72, Rank: 8, loss = 0.51953125
c619-002: Epoch: 0, Step: 72, Rank: 13, loss = 0.51171875
c621-082: Epoch: 0, Step: 72, Rank: 27, loss = 0.58203125
c613-132: Epoch: 0, Step: 72, Rank: 7, loss = 0.51171875
c622-082: Epoch: 0, Step: 72, Rank: 59, loss = 0.58203125
c622-042: Epoch: 0, Step: 72, Rank: 51, loss = 0.494140625
c621-142: Epoch: 0, Step: 72, Rank: 39, loss = 0.53515625
c613-131: Epoch: 0, Step: 72, Rank: 6, loss = 0.69140625
c621-071: Epoch: 0, Step: 72, Rank: 24, loss = 0.609375
c613-152: Epoch: 0, Step: 72, Rank: 11, loss = 0.61328125
c622-061: Epoch: 0, Step: 72, Rank: 54, loss = 0.51171875
c613-121: Epoch: 0, Step: 72, Rank: 4, loss = 0.455078125
c622-101: Epoch: 0, Step: 72, Rank: 62, loss = 0.51171875
c621-061: Epoch: 0, Step: 72, Rank: 22, loss = 0.1767578125
c621-121: Epoch: 0, Step: 72, Rank: 34, loss = 0.51171875
c613-111: Epoch: 0, Step: 72, Rank: 2, loss = 0.5859375
c622-031: Epoch: 0, Step: 72, Rank: 48, loss = 0.54296875
c619-022: Epoch: 0, Step: 72, Rank: 17, loss = 0.55859375
c621-102: Epoch: 0, Step: 72, Rank: 31, loss = 0.6640625
c622-071: Epoch: 0, Step: 72, Rank: 56, loss = 0.5546875
c622-062: Epoch: 0, Step: 72, Rank: 55, loss = 0.5546875
c622-022: Epoch: 0, Step: 72, Rank: 47, loss = 0.50390625
c619-001: Epoch: 0, Step: 72, Rank: 12, loss = 0.62890625
c622-092: Epoch: 0, Step: 72, Rank: 61, loss = 0.57421875
c621-141: Epoch: 0, Step: 72, Rank: 38, loss = 0.51171875
c613-112: Epoch: 0, Step: 72, Rank: 3, loss = 0.53515625
c619-032: Epoch: 0, Step: 72, Rank: 19, loss = 0.5546875
c621-062: Epoch: 0, Step: 72, Rank: 23, loss = 0.56640625
c613-151: Epoch: 0, Step: 72, Rank: 10, loss = 0.62109375
c613-102: Epoch: 0, Step: 72, Rank: 1, loss = 0.404296875
c622-102: Epoch: 0, Step: 72, Rank: 63, loss = 0.53515625
c613-122: Epoch: 0, Step: 72, Rank: 5, loss = 0.59765625
c613-142: Epoch: 0, Step: 72, Rank: 9, loss = 0.71484375
c622-091: Epoch: 0, Step: 72, Rank: 60, loss = 0.56640625
c619-011: Epoch: 0, Step: 72, Rank: 14, loss = 0.5859375
c622-072: Epoch: 0, Step: 72, Rank: 57, loss = 0.5703125
c619-012: Epoch: 0, Step: 72, Rank: 15, loss = 0.5546875
c622-021: Epoch: 0, Step: 72, Rank: 46, loss = 0.5859375
c622-032: Epoch: 0, Step: 72, Rank: 49, loss = 0.6796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2451171875 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 73, Rank: 0, loss = 0.08740234375
c622-002: Epoch: 0, Step: 73, Rank: 43, loss = 0.5234375
c613-111: Epoch: 0, Step: 73, Rank: 2, loss = 0.625
c619-002: Epoch: 0, Step: 73, Rank: 13, loss = 0.52734375
c621-111: Epoch: 0, Step: 73, Rank: 32, loss = 0.64453125
c621-072: Epoch: 0, Step: 73, Rank: 25, loss = 0.51171875
c622-052: Epoch: 0, Step: 73, Rank: 53, loss = 0.3125
c622-081: Epoch: 0, Step: 73, Rank: 58, loss = 0.62109375
c621-132: Epoch: 0, Step: 73, Rank: 37, loss = 0.484375
c622-101: Epoch: 0, Step: 73, Rank: 62, loss = 0.69140625
c621-151: Epoch: 0, Step: 73, Rank: 40, loss = 0.54296875
c619-001: Epoch: 0, Step: 73, Rank: 12, loss = 0.55078125
c613-142: Epoch: 0, Step: 73, Rank: 9, loss = 0.50390625
c621-091: Epoch: 0, Step: 73, Rank: 28, loss = 0.474609375
c621-081: Epoch: 0, Step: 73, Rank: 26, loss = 0.59765625
c613-152: Epoch: 0, Step: 73, Rank: 11, loss = 0.35546875
c619-021: Epoch: 0, Step: 73, Rank: 16, loss = 0.44140625
c613-102: Epoch: 0, Step: 73, Rank: 1, loss = 0.30078125
c621-082: Epoch: 0, Step: 73, Rank: 27, loss = 0.38671875
c622-012: Epoch: 0, Step: 73, Rank: 45, loss = 0.55078125
c622-001: Epoch: 0, Step: 73, Rank: 42, loss = 0.57421875
c619-041: Epoch: 0, Step: 73, Rank: 20, loss = 0.53515625
c613-132: Epoch: 0, Step: 73, Rank: 7, loss = 0.65234375
c621-052: Epoch: 0, Step: 73, Rank: 21, loss = 0.54296875
c621-101: Epoch: 0, Step: 73, Rank: 30, loss = 0.62890625
c622-092: Epoch: 0, Step: 73, Rank: 61, loss = 0.59765625
c622-071: Epoch: 0, Step: 73, Rank: 56, loss = 0.5859375
c622-061: Epoch: 0, Step: 73, Rank: 54, loss = 0.51171875
c613-131: Epoch: 0, Step: 73, Rank: 6, loss = 0.62109375
c613-112: Epoch: 0, Step: 73, Rank: 3, loss = 0.6328125
c613-121: Epoch: 0, Step: 73, Rank: 4, loss = 0.59765625
c621-061: Epoch: 0, Step: 73, Rank: 22, loss = 0.61328125
c621-121: Epoch: 0, Step: 73, Rank: 34, loss = 0.56640625
c622-102: Epoch: 0, Step: 73, Rank: 63, loss = 0.46484375
c622-011: Epoch: 0, Step: 73, Rank: 44, loss = 0.5859375
c619-011: Epoch: 0, Step: 73, Rank: 14, loss = 0.57421875
c622-032: Epoch: 0, Step: 73, Rank: 49, loss = 0.55078125
c619-022: Epoch: 0, Step: 73, Rank: 17, loss = 0.69140625
c619-032: Epoch: 0, Step: 73, Rank: 19, loss = 0.201171875
c621-141: Epoch: 0, Step: 73, Rank: 38, loss = 0.494140625
c622-082: Epoch: 0, Step: 73, Rank: 59, loss = 0.60546875
c621-131: Epoch: 0, Step: 73, Rank: 36, loss = 0.51171875
c619-031: Epoch: 0, Step: 73, Rank: 18, loss = 0.59375
c622-062: Epoch: 0, Step: 73, Rank: 55, loss = 0.546875
c622-041: Epoch: 0, Step: 73, Rank: 50, loss = 0.51171875
c621-112: Epoch: 0, Step: 73, Rank: 33, loss = 0.5546875
c613-122: Epoch: 0, Step: 73, Rank: 5, loss = 0.404296875
c622-072: Epoch: 0, Step: 73, Rank: 57, loss = 0.484375
c621-062: Epoch: 0, Step: 73, Rank: 23, loss = 0.59375
c621-142: Epoch: 0, Step: 73, Rank: 39, loss = 0.54296875
c621-071: Epoch: 0, Step: 73, Rank: 24, loss = 0.5546875
c622-051: Epoch: 0, Step: 73, Rank: 52, loss = 0.6171875
c621-152: Epoch: 0, Step: 73, Rank: 41, loss = 0.59375
c621-102: Epoch: 0, Step: 73, Rank: 31, loss = 0.61328125
c622-022: Epoch: 0, Step: 73, Rank: 47, loss = 0.57421875
c613-141: Epoch: 0, Step: 73, Rank: 8, loss = 0.64453125
c619-012: Epoch: 0, Step: 73, Rank: 15, loss = 0.59765625
c621-122: Epoch: 0, Step: 73, Rank: 35, loss = 0.62109375
c622-021: Epoch: 0, Step: 73, Rank: 46, loss = 0.251953125
c622-042: Epoch: 0, Step: 73, Rank: 51, loss = 0.51171875
c622-091: Epoch: 0, Step: 73, Rank: 60, loss = 0.5703125
c622-031: Epoch: 0, Step: 73, Rank: 48, loss = 0.64453125
c621-092: Epoch: 0, Step: 73, Rank: 29, loss = 0.57421875
c613-151: Epoch: 0, Step: 73, Rank: 10, loss = 0.5859375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 74, Rank: 43, loss = 0.51953125
c621-091: Epoch: 0, Step: 74, Rank: 28, loss = 0.69140625
c621-111: Epoch: 0, Step: 74, Rank: 32, loss = 0.1298828125
c621-102: Epoch: 0, Step: 74, Rank: 31, loss = 0.5546875
c621-081: Epoch: 0, Step: 74, Rank: 26, loss = 0.412109375
c619-031: Epoch: 0, Step: 74, Rank: 18, loss = 0.5546875
c621-112: Epoch: 0, Step: 74, Rank: 33, loss = 0.51171875
c621-132: Epoch: 0, Step: 74, Rank: 37, loss = 0.37109375
c619-021: Epoch: 0, Step: 74, Rank: 16, loss = 0.46875
c621-101: Epoch: 0, Step: 74, Rank: 30, loss = 0.54296875
c622-012: Epoch: 0, Step: 74, Rank: 45, loss = 0.5546875
c621-082: Epoch: 0, Step: 74, Rank: 27, loss = 0.62109375
c621-092: Epoch: 0, Step: 74, Rank: 29, loss = 0.55078125
c621-131: Epoch: 0, Step: 74, Rank: 36, loss = 0.5234375
c619-022: Epoch: 0, Step: 74, Rank: 17, loss = 0.56640625
c613-101: Epoch: 0, Step: 74, Rank: 0, loss = 0.455078125
c621-122: Epoch: 0, Step: 74, Rank: 35, loss = 0.404296875
c622-001: Epoch: 0, Step: 74, Rank: 42, loss = 0.546875
c613-121: Epoch: 0, Step: 74, Rank: 4, loss = 0.28515625
c621-121: Epoch: 0, Step: 74, Rank: 34, loss = 0.60546875
c621-151: Epoch: 0, Step: 74, Rank: 40, loss = 0.60546875
c621-142: Epoch: 0, Step: 74, Rank: 39, loss = 0.58203125
c622-052: Epoch: 0, Step: 74, Rank: 53, loss = 0.404296875
c622-081: Epoch: 0, Step: 74, Rank: 58, loss = 0.69140625
c619-001: Epoch: 0, Step: 74, Rank: 12, loss = 0.57421875
c622-011: Epoch: 0, Step: 74, Rank: 44, loss = 0.5625
c619-002: Epoch: 0, Step: 74, Rank: 13, loss = 0.5625
c621-072: Epoch: 0, Step: 74, Rank: 25, loss = 0.6328125
c613-131: Epoch: 0, Step: 74, Rank: 6, loss = 0.1298828125
c613-151: Epoch: 0, Step: 74, Rank: 10, loss = 0.57421875
c621-152: Epoch: 0, Step: 74, Rank: 41, loss = 0.5234375
c619-032: Epoch: 0, Step: 74, Rank: 19, loss = 0.51171875
c621-141: Epoch: 0, Step: 74, Rank: 38, loss = 0.56640625
c613-132: Epoch: 0, Step: 74, Rank: 7, loss = 0.56640625
c622-032: Epoch: 0, Step: 74, Rank: 49, loss = 0.1826171875
c613-111: Epoch: 0, Step: 74, Rank: 2, loss = 0.474609375
c613-152: Epoch: 0, Step: 74, Rank: 11, loss = 0.62109375
c622-061: Epoch: 0, Step: 74, Rank: 54, loss = 0.51171875
c613-142: Epoch: 0, Step: 74, Rank: 9, loss = 0.56640625
c622-022: Epoch: 0, Step: 74, Rank: 47, loss = 0.58203125
c613-141: Epoch: 0, Step: 74, Rank: 8, loss = 0.56640625
c619-012: Epoch: 0, Step: 74, Rank: 15, loss = 0.5546875
c622-031: Epoch: 0, Step: 74, Rank: 48, loss = 0.53125
c622-092: Epoch: 0, Step: 74, Rank: 61, loss = 0.59765625
c622-062: Epoch: 0, Step: 74, Rank: 55, loss = 0.57421875
c622-041: Epoch: 0, Step: 74, Rank: 50, loss = 0.220703125
c622-071: Epoch: 0, Step: 74, Rank: 56, loss = 0.1669921875
c619-011: Epoch: 0, Step: 74, Rank: 14, loss = 0.69140625
c622-072: Epoch: 0, Step: 74, Rank: 57, loss = 0.57421875
c613-102: Epoch: 0, Step: 74, Rank: 1, loss = 0.56640625
c622-101: Epoch: 0, Step: 74, Rank: 62, loss = 0.51171875
c613-112: Epoch: 0, Step: 74, Rank: 3, loss = 0.37109375
c622-051: Epoch: 0, Step: 74, Rank: 52, loss = 0.59765625
c621-071: Epoch: 0, Step: 74, Rank: 24, loss = 0.5546875
c622-042: Epoch: 0, Step: 74, Rank: 51, loss = 0.3125
c622-021: Epoch: 0, Step: 74, Rank: 46, loss = 0.5859375
c613-122: Epoch: 0, Step: 74, Rank: 5, loss = 0.5859375
c619-041: Epoch: 0, Step: 74, Rank: 20, loss = 0.51171875
c622-091: Epoch: 0, Step: 74, Rank: 60, loss = 0.52734375
c621-052: Epoch: 0, Step: 74, Rank: 21, loss = 0.55078125
c621-062: Epoch: 0, Step: 74, Rank: 23, loss = 0.57421875
c622-102: Epoch: 0, Step: 74, Rank: 63, loss = 0.484375
c621-061: Epoch: 0, Step: 74, Rank: 22, loss = 0.455078125
c622-082: Epoch: 0, Step: 74, Rank: 59, loss = 0.6171875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 75, Rank: 16, loss = 0.5546875
c621-081: Epoch: 0, Step: 75, Rank: 26, loss = 0.474609375
c621-082: Epoch: 0, Step: 75, Rank: 27, loss = 0.61328125
c622-081: Epoch: 0, Step: 75, Rank: 58, loss = 0.62109375
c619-032: Epoch: 0, Step: 75, Rank: 19, loss = 0.5546875
c619-002: Epoch: 0, Step: 75, Rank: 13, loss = 0.69140625
c621-052: Epoch: 0, Step: 75, Rank: 21, loss = 0.47265625
c619-041: Epoch: 0, Step: 75, Rank: 20, loss = 0.474609375
c619-001: Epoch: 0, Step: 75, Rank: 12, loss = 0.59765625
c622-052: Epoch: 0, Step: 75, Rank: 53, loss = 0.52734375
c613-151: Epoch: 0, Step: 75, Rank: 10, loss = 0.455078125
c621-072: Epoch: 0, Step: 75, Rank: 25, loss = 0.54296875
c613-152: Epoch: 0, Step: 75, Rank: 11, loss = 0.26171875
c622-002: Epoch: 0, Step: 75, Rank: 43, loss = 0.6171875
c621-061: Epoch: 0, Step: 75, Rank: 22, loss = 0.5546875
c622-012: Epoch: 0, Step: 75, Rank: 45, loss = 0.53515625
c613-132: Epoch: 0, Step: 75, Rank: 7, loss = 0.306640625
c622-001: Epoch: 0, Step: 75, Rank: 42, loss = 0.51171875
c619-031: Epoch: 0, Step: 75, Rank: 18, loss = 0.5078125
c613-101: Epoch: 0, Step: 75, Rank: 0, loss = 0.59765625
c621-111: Epoch: 0, Step: 75, Rank: 32, loss = 0.63671875
c621-101: Epoch: 0, Step: 75, Rank: 30, loss = 0.4375
c621-121: Epoch: 0, Step: 75, Rank: 34, loss = 0.333984375
c622-051: Epoch: 0, Step: 75, Rank: 52, loss = 0.69140625
c622-032: Epoch: 0, Step: 75, Rank: 49, loss = 0.3125
c621-151: Epoch: 0, Step: 75, Rank: 40, loss = 0.4375
c619-022: Epoch: 0, Step: 75, Rank: 17, loss = 0.5546875
c622-011: Epoch: 0, Step: 75, Rank: 44, loss = 0.5546875
c613-122: Epoch: 0, Step: 75, Rank: 5, loss = 0.4375
c622-061: Epoch: 0, Step: 75, Rank: 54, loss = 0.56640625
c621-112: Epoch: 0, Step: 75, Rank: 33, loss = 0.62109375
c613-142: Epoch: 0, Step: 75, Rank: 9, loss = 0.341796875
c621-131: Epoch: 0, Step: 75, Rank: 36, loss = 0.62890625
c613-141: Epoch: 0, Step: 75, Rank: 8, loss = 0.51171875
c621-091: Epoch: 0, Step: 75, Rank: 28, loss = 0.51953125
c621-062: Epoch: 0, Step: 75, Rank: 23, loss = 0.54296875
c621-071: Epoch: 0, Step: 75, Rank: 24, loss = 0.69140625
c619-011: Epoch: 0, Step: 75, Rank: 14, loss = 0.455078125
c622-022: Epoch: 0, Step: 75, Rank: 47, loss = 0.62109375
c622-041: Epoch: 0, Step: 75, Rank: 50, loss = 0.59765625
c621-132: Epoch: 0, Step: 75, Rank: 37, loss = 0.59765625
c622-102: Epoch: 0, Step: 75, Rank: 63, loss = 0.69140625
c619-012: Epoch: 0, Step: 75, Rank: 15, loss = 0.6328125
c621-142: Epoch: 0, Step: 75, Rank: 39, loss = 0.328125
c613-131: Epoch: 0, Step: 75, Rank: 6, loss = 0.56640625
c622-071: Epoch: 0, Step: 75, Rank: 56, loss = 0.69140625
c622-062: Epoch: 0, Step: 75, Rank: 55, loss = 0.59765625
c613-102: Epoch: 0, Step: 75, Rank: 1, loss = 0.56640625
c622-031: Epoch: 0, Step: 75, Rank: 48, loss = 0.494140625
c613-112: Epoch: 0, Step: 75, Rank: 3, loss = 0.6171875
c621-152: Epoch: 0, Step: 75, Rank: 41, loss = 0.341796875
c621-102: Epoch: 0, Step: 75, Rank: 31, loss = 0.53515625
c622-082: Epoch: 0, Step: 75, Rank: 59, loss = 0.35546875
c622-042: Epoch: 0, Step: 75, Rank: 51, loss = 0.51171875
c622-072: Epoch: 0, Step: 75, Rank: 57, loss = 0.5546875
c621-141: Epoch: 0, Step: 75, Rank: 38, loss = 0.51171875
c613-121: Epoch: 0, Step: 75, Rank: 4, loss = 0.6171875
c621-122: Epoch: 0, Step: 75, Rank: 35, loss = 0.64453125
c622-091: Epoch: 0, Step: 75, Rank: 60, loss = 0.55078125
c622-092: Epoch: 0, Step: 75, Rank: 61, loss = 0.55078125
c622-021: Epoch: 0, Step: 75, Rank: 46, loss = 0.53515625
c621-092: Epoch: 0, Step: 75, Rank: 29, loss = 0.6328125
c613-111: Epoch: 0, Step: 75, Rank: 2, loss = 0.6171875
c622-101: Epoch: 0, Step: 75, Rank: 62, loss = 0.44140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 76, Rank: 0, loss = 0.55078125
c619-021: Epoch: 0, Step: 76, Rank: 16, loss = 0.5546875
c622-081: Epoch: 0, Step: 76, Rank: 58, loss = 0.43359375
c622-002: Epoch: 0, Step: 76, Rank: 43, loss = 0.50390625
c622-052: Epoch: 0, Step: 76, Rank: 53, loss = 0.404296875
c613-151: Epoch: 0, Step: 76, Rank: 10, loss = 0.5703125
c613-141: Epoch: 0, Step: 76, Rank: 8, loss = 0.26171875
c613-152: Epoch: 0, Step: 76, Rank: 11, loss = 0.5546875
c622-071: Epoch: 0, Step: 76, Rank: 56, loss = 0.69140625
c613-131: Epoch: 0, Step: 76, Rank: 6, loss = 0.404296875
c622-092: Epoch: 0, Step: 76, Rank: 61, loss = 0.59765625
c622-102: Epoch: 0, Step: 76, Rank: 63, loss = 0.16015625
c619-001: Epoch: 0, Step: 76, Rank: 12, loss = 0.59375
c622-012: Epoch: 0, Step: 76, Rank: 45, loss = 0.61328125
c619-031: Epoch: 0, Step: 76, Rank: 18, loss = 0.5859375
c619-041: Epoch: 0, Step: 76, Rank: 20, loss = 0.62890625
c613-132: Epoch: 0, Step: 76, Rank: 7, loss = 0.5703125
c622-082: Epoch: 0, Step: 76, Rank: 59, loss = 0.51171875
c613-111: Epoch: 0, Step: 76, Rank: 2, loss = 0.4375
c622-062: Epoch: 0, Step: 76, Rank: 55, loss = 0.57421875
c622-101: Epoch: 0, Step: 76, Rank: 62, loss = 0.60546875
c613-122: Epoch: 0, Step: 76, Rank: 5, loss = 0.275390625
c613-112: Epoch: 0, Step: 76, Rank: 3, loss = 0.578125
c619-002: Epoch: 0, Step: 76, Rank: 13, loss = 0.62109375
c622-072: Epoch: 0, Step: 76, Rank: 57, loss = 0.55859375
c622-051: Epoch: 0, Step: 76, Rank: 52, loss = 0.5859375
c613-102: Epoch: 0, Step: 76, Rank: 1, loss = 0.62109375
c622-061: Epoch: 0, Step: 76, Rank: 54, loss = 0.609375
c621-132: Epoch: 0, Step: 76, Rank: 37, loss = 0.53515625
c613-142: Epoch: 0, Step: 76, Rank: 9, loss = 0.2412109375
c622-032: Epoch: 0, Step: 76, Rank: 49, loss = 0.54296875
c622-041: Epoch: 0, Step: 76, Rank: 50, loss = 0.462890625
c622-091: Epoch: 0, Step: 76, Rank: 60, loss = 0.59765625
c621-072: Epoch: 0, Step: 76, Rank: 25, loss = 0.474609375
c619-032: Epoch: 0, Step: 76, Rank: 19, loss = 0.64453125
c622-001: Epoch: 0, Step: 76, Rank: 42, loss = 0.546875
c622-021: Epoch: 0, Step: 76, Rank: 46, loss = 0.50390625
c622-011: Epoch: 0, Step: 76, Rank: 44, loss = 0.37109375
c621-052: Epoch: 0, Step: 76, Rank: 21, loss = 0.400390625
c621-081: Epoch: 0, Step: 76, Rank: 26, loss = 0.60546875
c622-022: Epoch: 0, Step: 76, Rank: 47, loss = 0.57421875
c621-151: Epoch: 0, Step: 76, Rank: 40, loss = 0.419921875
c621-091: Epoch: 0, Step: 76, Rank: 28, loss = 0.474609375
c619-011: Epoch: 0, Step: 76, Rank: 14, loss = 0.5546875
c619-022: Epoch: 0, Step: 76, Rank: 17, loss = 0.6796875
c619-012: Epoch: 0, Step: 76, Rank: 15, loss = 0.5390625
c621-121: Epoch: 0, Step: 76, Rank: 34, loss = 0.46484375
c621-131: Epoch: 0, Step: 76, Rank: 36, loss = 0.341796875
c622-031: Epoch: 0, Step: 76, Rank: 48, loss = 0.5703125
c622-042: Epoch: 0, Step: 76, Rank: 51, loss = 0.57421875
c621-101: Epoch: 0, Step: 76, Rank: 30, loss = 0.6171875
c621-111: Epoch: 0, Step: 76, Rank: 32, loss = 0.59375
c621-152: Epoch: 0, Step: 76, Rank: 41, loss = 0.26171875
c621-141: Epoch: 0, Step: 76, Rank: 38, loss = 0.5390625
c621-082: Epoch: 0, Step: 76, Rank: 27, loss = 0.5546875
c621-112: Epoch: 0, Step: 76, Rank: 33, loss = 0.51953125
c621-071: Epoch: 0, Step: 76, Rank: 24, loss = 0.455078125
c621-142: Epoch: 0, Step: 76, Rank: 39, loss = 0.57421875
c613-121: Epoch: 0, Step: 76, Rank: 4, loss = 0.37890625
c621-122: Epoch: 0, Step: 76, Rank: 35, loss = 0.50390625
c621-061: Epoch: 0, Step: 76, Rank: 22, loss = 0.5546875
c621-102: Epoch: 0, Step: 76, Rank: 31, loss = 0.51171875
c621-092: Epoch: 0, Step: 76, Rank: 29, loss = 0.36328125
c621-062: Epoch: 0, Step: 76, Rank: 23, loss = 0.53515625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.13s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.13s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 77, Rank: 0, loss = 0.69140625
c622-081: Epoch: 0, Step: 77, Rank: 58, loss = 0.51171875
c621-081: Epoch: 0, Step: 77, Rank: 26, loss = 0.54296875
c621-072: Epoch: 0, Step: 77, Rank: 25, loss = 0.5390625
c622-002: Epoch: 0, Step: 77, Rank: 43, loss = 0.59375
c613-111: Epoch: 0, Step: 77, Rank: 2, loss = 0.474609375
c622-101: Epoch: 0, Step: 77, Rank: 62, loss = 0.419921875
c619-001: Epoch: 0, Step: 77, Rank: 12, loss = 0.64453125
c622-052: Epoch: 0, Step: 77, Rank: 53, loss = 0.57421875
c613-112: Epoch: 0, Step: 77, Rank: 3, loss = 0.5546875
c621-111: Epoch: 0, Step: 77, Rank: 32, loss = 0.474609375
c622-082: Epoch: 0, Step: 77, Rank: 59, loss = 0.53515625
c622-062: Epoch: 0, Step: 77, Rank: 55, loss = 0.62890625
c619-002: Epoch: 0, Step: 77, Rank: 13, loss = 0.59765625
c619-021: Epoch: 0, Step: 77, Rank: 16, loss = 0.53515625
c622-092: Epoch: 0, Step: 77, Rank: 61, loss = 0.59765625
c622-102: Epoch: 0, Step: 77, Rank: 63, loss = 0.494140625
c613-132: Epoch: 0, Step: 77, Rank: 7, loss = 0.51171875
c622-072: Epoch: 0, Step: 77, Rank: 57, loss = 0.287109375
c622-001: Epoch: 0, Step: 77, Rank: 42, loss = 0.55078125
c613-151: Epoch: 0, Step: 77, Rank: 10, loss = 0.59765625
c622-061: Epoch: 0, Step: 77, Rank: 54, loss = 0.5546875
c622-091: Epoch: 0, Step: 77, Rank: 60, loss = 0.69140625
c621-132: Epoch: 0, Step: 77, Rank: 37, loss = 0.1796875
c622-032: Epoch: 0, Step: 77, Rank: 49, loss = 0.5546875
c613-122: Epoch: 0, Step: 77, Rank: 5, loss = 0.5703125
c622-041: Epoch: 0, Step: 77, Rank: 50, loss = 0.56640625
c621-121: Epoch: 0, Step: 77, Rank: 34, loss = 0.6328125
c613-102: Epoch: 0, Step: 77, Rank: 1, loss = 0.18359375
c613-141: Epoch: 0, Step: 77, Rank: 8, loss = 0.474609375
c621-142: Epoch: 0, Step: 77, Rank: 39, loss = 0.5234375
c613-152: Epoch: 0, Step: 77, Rank: 11, loss = 0.275390625
c621-101: Epoch: 0, Step: 77, Rank: 30, loss = 0.57421875
c622-012: Epoch: 0, Step: 77, Rank: 45, loss = 0.59765625
c621-071: Epoch: 0, Step: 77, Rank: 24, loss = 0.37109375
c622-031: Epoch: 0, Step: 77, Rank: 48, loss = 0.51171875
c621-091: Epoch: 0, Step: 77, Rank: 28, loss = 0.37109375
c619-041: Epoch: 0, Step: 77, Rank: 20, loss = 0.59765625
c621-052: Epoch: 0, Step: 77, Rank: 21, loss = 0.64453125
c622-022: Epoch: 0, Step: 77, Rank: 47, loss = 0.4375
c621-131: Epoch: 0, Step: 77, Rank: 36, loss = 0.474609375
c613-131: Epoch: 0, Step: 77, Rank: 6, loss = 0.58984375
c613-142: Epoch: 0, Step: 77, Rank: 9, loss = 0.51171875
c622-011: Epoch: 0, Step: 77, Rank: 44, loss = 0.5625
c621-062: Epoch: 0, Step: 77, Rank: 23, loss = 0.5546875
c621-151: Epoch: 0, Step: 77, Rank: 40, loss = 0.56640625
c621-082: Epoch: 0, Step: 77, Rank: 27, loss = 0.6875
c619-011: Epoch: 0, Step: 77, Rank: 14, loss = 0.51953125
c619-012: Epoch: 0, Step: 77, Rank: 15, loss = 0.51953125
c621-112: Epoch: 0, Step: 77, Rank: 33, loss = 0.494140625
c621-152: Epoch: 0, Step: 77, Rank: 41, loss = 0.46875
c621-122: Epoch: 0, Step: 77, Rank: 35, loss = 0.37109375
c621-102: Epoch: 0, Step: 77, Rank: 31, loss = 0.59375
c622-042: Epoch: 0, Step: 77, Rank: 51, loss = 0.34765625
c613-121: Epoch: 0, Step: 77, Rank: 4, loss = 0.4375
c619-022: Epoch: 0, Step: 77, Rank: 17, loss = 0.5390625
c622-071: Epoch: 0, Step: 77, Rank: 56, loss = 0.5546875
c622-021: Epoch: 0, Step: 77, Rank: 46, loss = 0.37109375
c622-051: Epoch: 0, Step: 77, Rank: 52, loss = 0.6171875
c619-031: Epoch: 0, Step: 77, Rank: 18, loss = 0.51171875
c621-061: Epoch: 0, Step: 77, Rank: 22, loss = 0.62109375
c621-141: Epoch: 0, Step: 77, Rank: 38, loss = 0.59765625
c621-092: Epoch: 0, Step: 77, Rank: 29, loss = 0.55859375
c619-032: Epoch: 0, Step: 77, Rank: 19, loss = 0.57421875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 78, Rank: 0, loss = 0.5546875
c613-112: Epoch: 0, Step: 78, Rank: 3, loss = 0.55078125
c613-151: Epoch: 0, Step: 78, Rank: 10, loss = 0.52734375
c613-102: Epoch: 0, Step: 78, Rank: 1, loss = 0.59375
c622-081: Epoch: 0, Step: 78, Rank: 58, loss = 0.57421875
c613-132: Epoch: 0, Step: 78, Rank: 7, loss = 0.5234375
c613-111: Epoch: 0, Step: 78, Rank: 2, loss = 0.5546875
c613-142: Epoch: 0, Step: 78, Rank: 9, loss = 0.474609375
c613-152: Epoch: 0, Step: 78, Rank: 11, loss = 0.6171875
c622-071: Epoch: 0, Step: 78, Rank: 56, loss = 0.58984375
c619-021: Epoch: 0, Step: 78, Rank: 16, loss = 0.609375
c622-102: Epoch: 0, Step: 78, Rank: 63, loss = 0.46484375
c622-002: Epoch: 0, Step: 78, Rank: 43, loss = 0.5390625
c613-131: Epoch: 0, Step: 78, Rank: 6, loss = 0.53125
c613-121: Epoch: 0, Step: 78, Rank: 4, loss = 0.5390625
c613-122: Epoch: 0, Step: 78, Rank: 5, loss = 0.55859375
c619-032: Epoch: 0, Step: 78, Rank: 19, loss = 0.62109375
c619-001: Epoch: 0, Step: 78, Rank: 12, loss = 0.62109375
c613-141: Epoch: 0, Step: 78, Rank: 8, loss = 0.439453125
c622-072: Epoch: 0, Step: 78, Rank: 57, loss = 0.60546875
c622-052: Epoch: 0, Step: 78, Rank: 53, loss = 0.5859375
c622-062: Epoch: 0, Step: 78, Rank: 55, loss = 0.58203125
c619-031: Epoch: 0, Step: 78, Rank: 18, loss = 0.5546875
c622-092: Epoch: 0, Step: 78, Rank: 61, loss = 0.69140625
c619-022: Epoch: 0, Step: 78, Rank: 17, loss = 0.37109375
c619-002: Epoch: 0, Step: 78, Rank: 13, loss = 0.62109375
c622-061: Epoch: 0, Step: 78, Rank: 54, loss = 0.171875
c619-011: Epoch: 0, Step: 78, Rank: 14, loss = 0.64453125
c621-052: Epoch: 0, Step: 78, Rank: 21, loss = 0.3125
c622-101: Epoch: 0, Step: 78, Rank: 62, loss = 0.51171875
c622-051: Epoch: 0, Step: 78, Rank: 52, loss = 0.54296875
c622-082: Epoch: 0, Step: 78, Rank: 59, loss = 0.474609375
c622-091: Epoch: 0, Step: 78, Rank: 60, loss = 0.494140625
c622-012: Epoch: 0, Step: 78, Rank: 45, loss = 0.5703125
c619-012: Epoch: 0, Step: 78, Rank: 15, loss = 0.412109375
c622-001: Epoch: 0, Step: 78, Rank: 42, loss = 0.455078125
c621-061: Epoch: 0, Step: 78, Rank: 22, loss = 0.5859375
c621-111: Epoch: 0, Step: 78, Rank: 32, loss = 0.6171875
c619-041: Epoch: 0, Step: 78, Rank: 20, loss = 0.56640625
c621-062: Epoch: 0, Step: 78, Rank: 23, loss = 0.6328125
c621-121: Epoch: 0, Step: 78, Rank: 34, loss = 0.3125
c621-151: Epoch: 0, Step: 78, Rank: 40, loss = 0.54296875
c622-011: Epoch: 0, Step: 78, Rank: 44, loss = 0.56640625
c621-131: Epoch: 0, Step: 78, Rank: 36, loss = 0.625
c621-142: Epoch: 0, Step: 78, Rank: 39, loss = 0.5703125
c621-081: Epoch: 0, Step: 78, Rank: 26, loss = 0.59765625
c621-152: Epoch: 0, Step: 78, Rank: 41, loss = 0.6171875
c621-132: Epoch: 0, Step: 78, Rank: 37, loss = 0.59375
c621-122: Epoch: 0, Step: 78, Rank: 35, loss = 0.50390625
c621-091: Epoch: 0, Step: 78, Rank: 28, loss = 0.6328125
c621-082: Epoch: 0, Step: 78, Rank: 27, loss = 0.53515625
c622-031: Epoch: 0, Step: 78, Rank: 48, loss = 0.65625
c622-032: Epoch: 0, Step: 78, Rank: 49, loss = 0.5546875
c622-042: Epoch: 0, Step: 78, Rank: 51, loss = 0.62109375
c622-021: Epoch: 0, Step: 78, Rank: 46, loss = 0.69140625
c621-072: Epoch: 0, Step: 78, Rank: 25, loss = 0.59765625
c621-101: Epoch: 0, Step: 78, Rank: 30, loss = 0.51171875
c621-071: Epoch: 0, Step: 78, Rank: 24, loss = 0.490234375
c622-022: Epoch: 0, Step: 78, Rank: 47, loss = 0.62109375
c621-112: Epoch: 0, Step: 78, Rank: 33, loss = 0.56640625
c621-102: Epoch: 0, Step: 78, Rank: 31, loss = 0.455078125
c622-041: Epoch: 0, Step: 78, Rank: 50, loss = 0.43359375
c621-141: Epoch: 0, Step: 78, Rank: 38, loss = 0.51171875
c621-092: Epoch: 0, Step: 78, Rank: 29, loss = 0.53515625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 79, Rank: 43, loss = 0.54296875
c622-081: Epoch: 0, Step: 79, Rank: 58, loss = 0.59765625
c622-001: Epoch: 0, Step: 79, Rank: 42, loss = 0.50390625
c619-021: Epoch: 0, Step: 79, Rank: 16, loss = 0.451171875
c613-101: Epoch: 0, Step: 79, Rank: 0, loss = 0.5546875
c619-031: Epoch: 0, Step: 79, Rank: 18, loss = 0.62109375
c622-052: Epoch: 0, Step: 79, Rank: 53, loss = 0.54296875
c622-102: Epoch: 0, Step: 79, Rank: 63, loss = 0.57421875
c621-132: Epoch: 0, Step: 79, Rank: 37, loss = 0.404296875
c621-151: Epoch: 0, Step: 79, Rank: 40, loss = 0.51171875
c622-012: Epoch: 0, Step: 79, Rank: 45, loss = 0.37109375
c619-022: Epoch: 0, Step: 79, Rank: 17, loss = 0.5859375
c621-052: Epoch: 0, Step: 79, Rank: 21, loss = 0.3203125
c619-002: Epoch: 0, Step: 79, Rank: 13, loss = 0.2431640625
c622-101: Epoch: 0, Step: 79, Rank: 62, loss = 0.56640625
c621-152: Epoch: 0, Step: 79, Rank: 41, loss = 0.69140625
c619-041: Epoch: 0, Step: 79, Rank: 20, loss = 0.5546875
c622-032: Epoch: 0, Step: 79, Rank: 49, loss = 0.5546875
c622-082: Epoch: 0, Step: 79, Rank: 59, loss = 0.59375
c622-042: Epoch: 0, Step: 79, Rank: 51, loss = 0.6171875
c622-051: Epoch: 0, Step: 79, Rank: 52, loss = 0.50390625
c621-142: Epoch: 0, Step: 79, Rank: 39, loss = 0.5546875
c622-062: Epoch: 0, Step: 79, Rank: 55, loss = 0.58203125
c621-061: Epoch: 0, Step: 79, Rank: 22, loss = 0.494140625
c622-041: Epoch: 0, Step: 79, Rank: 50, loss = 0.494140625
c613-121: Epoch: 0, Step: 79, Rank: 4, loss = 0.484375
c622-061: Epoch: 0, Step: 79, Rank: 54, loss = 0.53515625
c621-081: Epoch: 0, Step: 79, Rank: 26, loss = 0.57421875
c613-142: Epoch: 0, Step: 79, Rank: 9, loss = 0.6015625
c613-152: Epoch: 0, Step: 79, Rank: 11, loss = 0.48828125
c613-131: Epoch: 0, Step: 79, Rank: 6, loss = 0.46875
c619-001: Epoch: 0, Step: 79, Rank: 12, loss = 0.494140625
c619-032: Epoch: 0, Step: 79, Rank: 19, loss = 0.57421875
c621-111: Epoch: 0, Step: 79, Rank: 32, loss = 0.5234375
c622-031: Epoch: 0, Step: 79, Rank: 48, loss = 0.6328125
c619-012: Epoch: 0, Step: 79, Rank: 15, loss = 0.5390625
c613-151: Epoch: 0, Step: 79, Rank: 10, loss = 0.59375
c622-072: Epoch: 0, Step: 79, Rank: 57, loss = 0.37109375
c622-021: Epoch: 0, Step: 79, Rank: 46, loss = 0.69140625
c622-011: Epoch: 0, Step: 79, Rank: 44, loss = 0.609375
c619-011: Epoch: 0, Step: 79, Rank: 14, loss = 0.5703125
c621-091: Epoch: 0, Step: 79, Rank: 28, loss = 0.474609375
c622-091: Epoch: 0, Step: 79, Rank: 60, loss = 0.625
c613-112: Epoch: 0, Step: 79, Rank: 3, loss = 0.5546875
c613-102: Epoch: 0, Step: 79, Rank: 1, loss = 0.6015625
c621-072: Epoch: 0, Step: 79, Rank: 25, loss = 0.5546875
c621-122: Epoch: 0, Step: 79, Rank: 35, loss = 0.57421875
c622-022: Epoch: 0, Step: 79, Rank: 47, loss = 0.53515625
c622-092: Epoch: 0, Step: 79, Rank: 61, loss = 0.37890625
c613-111: Epoch: 0, Step: 79, Rank: 2, loss = 0.55078125
c622-071: Epoch: 0, Step: 79, Rank: 56, loss = 0.51171875
c621-112: Epoch: 0, Step: 79, Rank: 33, loss = 0.53515625
c621-121: Epoch: 0, Step: 79, Rank: 34, loss = 0.5859375
c613-141: Epoch: 0, Step: 79, Rank: 8, loss = 0.37890625
c613-122: Epoch: 0, Step: 79, Rank: 5, loss = 0.578125
c613-132: Epoch: 0, Step: 79, Rank: 7, loss = 0.55859375
c621-131: Epoch: 0, Step: 79, Rank: 36, loss = 0.55859375
c621-141: Epoch: 0, Step: 79, Rank: 38, loss = 0.4375
c621-062: Epoch: 0, Step: 79, Rank: 23, loss = 0.4375
c621-102: Epoch: 0, Step: 79, Rank: 31, loss = 0.51171875
c621-071: Epoch: 0, Step: 79, Rank: 24, loss = 0.427734375
c621-092: Epoch: 0, Step: 79, Rank: 29, loss = 0.5859375
c621-101: Epoch: 0, Step: 79, Rank: 30, loss = 0.404296875
c621-082: Epoch: 0, Step: 79, Rank: 27, loss = 0.59765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.18s, TFLOPs: 0.87, Samples/sec: 0.46, Time/seq 2.18s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 80, Rank: 0, loss = 0.3515625
c621-111: Epoch: 0, Step: 80, Rank: 32, loss = 0.1572265625
c621-091: Epoch: 0, Step: 80, Rank: 28, loss = 0.29296875
c622-052: Epoch: 0, Step: 80, Rank: 53, loss = 0.13671875
c622-081: Epoch: 0, Step: 80, Rank: 58, loss = 0.46875
c621-072: Epoch: 0, Step: 80, Rank: 25, loss = 0.287109375
c622-002: Epoch: 0, Step: 80, Rank: 43, loss = 0.30859375
c621-052: Epoch: 0, Step: 80, Rank: 21, loss = 0.057373046875
c621-082: Epoch: 0, Step: 80, Rank: 27, loss = 0.69140625
c621-071: Epoch: 0, Step: 80, Rank: 24, loss = 0.69140625
c619-001: Epoch: 0, Step: 80, Rank: 12, loss = 0.423828125
c621-102: Epoch: 0, Step: 80, Rank: 31, loss = 0.46875
c619-002: Epoch: 0, Step: 80, Rank: 13, loss = 0.37109375
c622-082: Epoch: 0, Step: 80, Rank: 59, loss = 0.56640625
c621-092: Epoch: 0, Step: 80, Rank: 29, loss = 0.10498046875
c619-021: Epoch: 0, Step: 80, Rank: 16, loss = 0.35546875
c621-061: Epoch: 0, Step: 80, Rank: 22, loss = 0.00164794921875
c621-081: Epoch: 0, Step: 80, Rank: 26, loss = 0.455078125
c619-041: Epoch: 0, Step: 80, Rank: 20, loss = 0.3828125
c613-132: Epoch: 0, Step: 80, Rank: 7, loss = 0.404296875
c621-101: Epoch: 0, Step: 80, Rank: 30, loss = 0.69140625
c622-011: Epoch: 0, Step: 80, Rank: 44, loss = 0.2412109375
c619-031: Epoch: 0, Step: 80, Rank: 18, loss = 0.287109375
c621-062: Epoch: 0, Step: 80, Rank: 23, loss = 0.1796875
c621-112: Epoch: 0, Step: 80, Rank: 33, loss = 0.408203125
c613-142: Epoch: 0, Step: 80, Rank: 9, loss = 0.3515625
c613-151: Epoch: 0, Step: 80, Rank: 10, loss = 0.2412109375
c613-122: Epoch: 0, Step: 80, Rank: 5, loss = 0.341796875
c622-032: Epoch: 0, Step: 80, Rank: 49, loss = 0.400390625
c622-001: Epoch: 0, Step: 80, Rank: 42, loss = 0.2099609375
c622-012: Epoch: 0, Step: 80, Rank: 45, loss = 0.1396484375
c613-131: Epoch: 0, Step: 80, Rank: 6, loss = 0.38671875
c619-032: Epoch: 0, Step: 80, Rank: 19, loss = 0.2412109375
c621-152: Epoch: 0, Step: 80, Rank: 41, loss = 0.2412109375
c622-091: Epoch: 0, Step: 80, Rank: 60, loss = 0.18359375
c621-121: Epoch: 0, Step: 80, Rank: 34, loss = 0.2353515625
c619-011: Epoch: 0, Step: 80, Rank: 14, loss = 0.333984375
c613-152: Epoch: 0, Step: 80, Rank: 11, loss = 0.345703125
c622-041: Epoch: 0, Step: 80, Rank: 50, loss = 0.26171875
c621-151: Epoch: 0, Step: 80, Rank: 40, loss = 0.251953125
c622-101: Epoch: 0, Step: 80, Rank: 62, loss = 0.423828125
c621-131: Epoch: 0, Step: 80, Rank: 36, loss = 0.23046875
c622-022: Epoch: 0, Step: 80, Rank: 47, loss = 0.2255859375
c619-022: Epoch: 0, Step: 80, Rank: 17, loss = 0.28515625
c621-122: Epoch: 0, Step: 80, Rank: 35, loss = 0.37890625
c613-112: Epoch: 0, Step: 80, Rank: 3, loss = 0.341796875
c621-132: Epoch: 0, Step: 80, Rank: 37, loss = 0.359375
c622-021: Epoch: 0, Step: 80, Rank: 46, loss = 0.18359375
c613-141: Epoch: 0, Step: 80, Rank: 8, loss = 0.29296875
c622-051: Epoch: 0, Step: 80, Rank: 52, loss = 0.1796875
c621-142: Epoch: 0, Step: 80, Rank: 39, loss = 0.3125
c619-012: Epoch: 0, Step: 80, Rank: 15, loss = 0.482421875
c622-062: Epoch: 0, Step: 80, Rank: 55, loss = 0.37890625
c622-061: Epoch: 0, Step: 80, Rank: 54, loss = 0.69140625
c622-031: Epoch: 0, Step: 80, Rank: 48, loss = 0.333984375
c622-042: Epoch: 0, Step: 80, Rank: 51, loss = 0.6875
c621-141: Epoch: 0, Step: 80, Rank: 38, loss = 0.35546875
c613-102: Epoch: 0, Step: 80, Rank: 1, loss = 0.2353515625
c622-092: Epoch: 0, Step: 80, Rank: 61, loss = 0.3828125
c613-121: Epoch: 0, Step: 80, Rank: 4, loss = 0.29296875
c622-071: Epoch: 0, Step: 80, Rank: 56, loss = 0.345703125
c622-072: Epoch: 0, Step: 80, Rank: 57, loss = 0.458984375
c622-102: Epoch: 0, Step: 80, Rank: 63, loss = 0.0242919921875
c613-111: Epoch: 0, Step: 80, Rank: 2, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 81, Rank: 32, loss = 0.46484375
c622-002: Epoch: 0, Step: 81, Rank: 43, loss = 0.3671875
c622-052: Epoch: 0, Step: 81, Rank: 53, loss = 0.27734375
c622-012: Epoch: 0, Step: 81, Rank: 45, loss = 0.375
c619-021: Epoch: 0, Step: 81, Rank: 16, loss = 0.1396484375
c621-131: Epoch: 0, Step: 81, Rank: 36, loss = 0.69140625
c622-001: Epoch: 0, Step: 81, Rank: 42, loss = 0.004486083984375
c621-151: Epoch: 0, Step: 81, Rank: 40, loss = 0.30859375
c621-081: Epoch: 0, Step: 81, Rank: 26, loss = 0.482421875
c622-011: Epoch: 0, Step: 81, Rank: 44, loss = 0.37890625
c621-132: Epoch: 0, Step: 81, Rank: 37, loss = 0.328125
c613-101: Epoch: 0, Step: 81, Rank: 0, loss = 0.69140625
c621-142: Epoch: 0, Step: 81, Rank: 39, loss = 0.29296875
c622-101: Epoch: 0, Step: 81, Rank: 62, loss = 0.275390625
c621-121: Epoch: 0, Step: 81, Rank: 34, loss = 0.275390625
c622-032: Epoch: 0, Step: 81, Rank: 49, loss = 0.380859375
c619-001: Epoch: 0, Step: 81, Rank: 12, loss = 0.2099609375
c621-152: Epoch: 0, Step: 81, Rank: 41, loss = 0.2177734375
c622-031: Epoch: 0, Step: 81, Rank: 48, loss = 0.1396484375
c621-101: Epoch: 0, Step: 81, Rank: 30, loss = 0.458984375
c621-082: Epoch: 0, Step: 81, Rank: 27, loss = 0.400390625
c622-022: Epoch: 0, Step: 81, Rank: 47, loss = 0.39453125
c613-151: Epoch: 0, Step: 81, Rank: 10, loss = 0.048583984375
c621-122: Epoch: 0, Step: 81, Rank: 35, loss = 0.34765625
c619-002: Epoch: 0, Step: 81, Rank: 13, loss = 0.0262451171875
c622-021: Epoch: 0, Step: 81, Rank: 46, loss = 0.29296875
c622-092: Epoch: 0, Step: 81, Rank: 61, loss = 0.38671875
c622-102: Epoch: 0, Step: 81, Rank: 63, loss = 0.474609375
c621-102: Epoch: 0, Step: 81, Rank: 31, loss = 0.453125
c621-112: Epoch: 0, Step: 81, Rank: 33, loss = 0.38671875
c622-051: Epoch: 0, Step: 81, Rank: 52, loss = 0.01007080078125
c613-121: Epoch: 0, Step: 81, Rank: 4, loss = 0.5234375
c622-081: Epoch: 0, Step: 81, Rank: 58, loss = 0.419921875
c621-141: Epoch: 0, Step: 81, Rank: 38, loss = 0.2578125
c621-072: Epoch: 0, Step: 81, Rank: 25, loss = 0.384765625
c622-042: Epoch: 0, Step: 81, Rank: 51, loss = 0.416015625
c619-032: Epoch: 0, Step: 81, Rank: 19, loss = 0.37109375
c613-152: Epoch: 0, Step: 81, Rank: 11, loss = 0.419921875
c622-061: Epoch: 0, Step: 81, Rank: 54, loss = 0.039794921875
c622-041: Epoch: 0, Step: 81, Rank: 50, loss = 0.3125
c622-082: Epoch: 0, Step: 81, Rank: 59, loss = 0.404296875
c621-071: Epoch: 0, Step: 81, Rank: 24, loss = 0.3203125
c613-122: Epoch: 0, Step: 81, Rank: 5, loss = 0.35546875
c619-022: Epoch: 0, Step: 81, Rank: 17, loss = 0.212890625
c613-142: Epoch: 0, Step: 81, Rank: 9, loss = 0.1533203125
c621-062: Epoch: 0, Step: 81, Rank: 23, loss = 0.28515625
c619-041: Epoch: 0, Step: 81, Rank: 20, loss = 0.287109375
c613-112: Epoch: 0, Step: 81, Rank: 3, loss = 0.69140625
c613-141: Epoch: 0, Step: 81, Rank: 8, loss = 0.048583984375
c613-111: Epoch: 0, Step: 81, Rank: 2, loss = 0.3125
c613-132: Epoch: 0, Step: 81, Rank: 7, loss = 0.484375
c622-071: Epoch: 0, Step: 81, Rank: 56, loss = 0.5859375
c621-092: Epoch: 0, Step: 81, Rank: 29, loss = 0.036376953125
c619-031: Epoch: 0, Step: 81, Rank: 18, loss = 0.43359375
c613-131: Epoch: 0, Step: 81, Rank: 6, loss = 0.484375
c621-091: Epoch: 0, Step: 81, Rank: 28, loss = 0.047119140625
c622-062: Epoch: 0, Step: 81, Rank: 55, loss = 0.2353515625
c621-052: Epoch: 0, Step: 81, Rank: 21, loss = 0.345703125
c613-102: Epoch: 0, Step: 81, Rank: 1, loss = 0.69140625
c622-091: Epoch: 0, Step: 81, Rank: 60, loss = 0.2099609375
c619-011: Epoch: 0, Step: 81, Rank: 14, loss = 0.42578125
c621-061: Epoch: 0, Step: 81, Rank: 22, loss = 0.41015625
c619-012: Epoch: 0, Step: 81, Rank: 15, loss = 0.26953125
c622-072: Epoch: 0, Step: 81, Rank: 57, loss = 0.306640625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 82, Rank: 58, loss = 0.69140625
c622-062: Epoch: 0, Step: 82, Rank: 55, loss = 0.30078125
c621-081: Epoch: 0, Step: 82, Rank: 26, loss = 0.69140625
c619-001: Epoch: 0, Step: 82, Rank: 12, loss = 0.220703125
c613-101: Epoch: 0, Step: 82, Rank: 0, loss = 0.3203125
c622-082: Epoch: 0, Step: 82, Rank: 59, loss = 0.3125
c622-052: Epoch: 0, Step: 82, Rank: 53, loss = 0.2431640625
c621-072: Epoch: 0, Step: 82, Rank: 25, loss = 0.08642578125
c622-002: Epoch: 0, Step: 82, Rank: 43, loss = 0.1669921875
c613-151: Epoch: 0, Step: 82, Rank: 10, loss = 0.0322265625
c621-111: Epoch: 0, Step: 82, Rank: 32, loss = 0.26953125
c622-061: Epoch: 0, Step: 82, Rank: 54, loss = 0.298828125
c622-101: Epoch: 0, Step: 82, Rank: 62, loss = 0.00136566162109375
c613-111: Epoch: 0, Step: 82, Rank: 2, loss = 0.35546875
c619-002: Epoch: 0, Step: 82, Rank: 13, loss = 0.412109375
c621-132: Epoch: 0, Step: 82, Rank: 37, loss = 0.0791015625
c622-001: Epoch: 0, Step: 82, Rank: 42, loss = 0.1640625
c621-082: Epoch: 0, Step: 82, Rank: 27, loss = 0.37890625
c621-152: Epoch: 0, Step: 82, Rank: 41, loss = 0.404296875
c613-121: Epoch: 0, Step: 82, Rank: 4, loss = 0.412109375
c621-091: Epoch: 0, Step: 82, Rank: 28, loss = 0.275390625
c619-021: Epoch: 0, Step: 82, Rank: 16, loss = 0.5234375
c613-132: Epoch: 0, Step: 82, Rank: 7, loss = 0.12158203125
c619-032: Epoch: 0, Step: 82, Rank: 19, loss = 0.0021209716796875
c621-052: Epoch: 0, Step: 82, Rank: 21, loss = 0.7109375
c619-022: Epoch: 0, Step: 82, Rank: 17, loss = 0.1328125
c619-041: Epoch: 0, Step: 82, Rank: 20, loss = 0.37109375
c621-061: Epoch: 0, Step: 82, Rank: 22, loss = 0.328125
c619-011: Epoch: 0, Step: 82, Rank: 14, loss = 0.39453125
c621-131: Epoch: 0, Step: 82, Rank: 36, loss = 0.06005859375
c619-031: Epoch: 0, Step: 82, Rank: 18, loss = 0.37890625
c622-012: Epoch: 0, Step: 82, Rank: 45, loss = 0.333984375
c621-101: Epoch: 0, Step: 82, Rank: 30, loss = 0.328125
c613-152: Epoch: 0, Step: 82, Rank: 11, loss = 0.484375
c613-112: Epoch: 0, Step: 82, Rank: 3, loss = 0.69140625
c622-091: Epoch: 0, Step: 82, Rank: 60, loss = 0.396484375
c621-121: Epoch: 0, Step: 82, Rank: 34, loss = 0.3828125
c613-122: Epoch: 0, Step: 82, Rank: 5, loss = 0.396484375
c613-141: Epoch: 0, Step: 82, Rank: 8, loss = 0.271484375
c622-051: Epoch: 0, Step: 82, Rank: 52, loss = 0.1484375
c613-131: Epoch: 0, Step: 82, Rank: 6, loss = 0.28125
c621-151: Epoch: 0, Step: 82, Rank: 40, loss = 0.333984375
c622-102: Epoch: 0, Step: 82, Rank: 63, loss = 0.359375
c622-041: Epoch: 0, Step: 82, Rank: 50, loss = 0.384765625
c621-142: Epoch: 0, Step: 82, Rank: 39, loss = 0.002471923828125
c621-102: Epoch: 0, Step: 82, Rank: 31, loss = 0.02978515625
c622-011: Epoch: 0, Step: 82, Rank: 44, loss = 0.26171875
c621-071: Epoch: 0, Step: 82, Rank: 24, loss = 0.09033203125
c622-022: Epoch: 0, Step: 82, Rank: 47, loss = 0.36328125
c613-102: Epoch: 0, Step: 82, Rank: 1, loss = 0.404296875
c619-012: Epoch: 0, Step: 82, Rank: 15, loss = 0.259765625
c621-062: Epoch: 0, Step: 82, Rank: 23, loss = 0.201171875
c622-092: Epoch: 0, Step: 82, Rank: 61, loss = 0.302734375
c621-112: Epoch: 0, Step: 82, Rank: 33, loss = 0.29296875
c613-142: Epoch: 0, Step: 82, Rank: 9, loss = 0.18359375
c622-072: Epoch: 0, Step: 82, Rank: 57, loss = 0.23046875
c622-032: Epoch: 0, Step: 82, Rank: 49, loss = 0.404296875
c622-042: Epoch: 0, Step: 82, Rank: 51, loss = 0.46484375
c621-122: Epoch: 0, Step: 82, Rank: 35, loss = 0.38671875
c621-141: Epoch: 0, Step: 82, Rank: 38, loss = 0.36328125
c621-092: Epoch: 0, Step: 82, Rank: 29, loss = 0.2470703125
c622-021: Epoch: 0, Step: 82, Rank: 46, loss = 0.1796875
c622-071: Epoch: 0, Step: 82, Rank: 56, loss = 0.275390625
c622-031: Epoch: 0, Step: 82, Rank: 48, loss = 0.490234375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 83, Rank: 43, loss = 0.37109375
c622-041: Epoch: 0, Step: 83, Rank: 50, loss = 0.287109375
c621-111: Epoch: 0, Step: 83, Rank: 32, loss = 0.404296875
c622-051: Epoch: 0, Step: 83, Rank: 52, loss = 0.048583984375
c621-132: Epoch: 0, Step: 83, Rank: 37, loss = 0.35546875
c622-032: Epoch: 0, Step: 83, Rank: 49, loss = 0.18359375
c622-012: Epoch: 0, Step: 83, Rank: 45, loss = 0.059326171875
c613-101: Epoch: 0, Step: 83, Rank: 0, loss = 0.56640625
c622-031: Epoch: 0, Step: 83, Rank: 48, loss = 0.17578125
c622-001: Epoch: 0, Step: 83, Rank: 42, loss = 0.5078125
c621-142: Epoch: 0, Step: 83, Rank: 39, loss = 0.287109375
c621-151: Epoch: 0, Step: 83, Rank: 40, loss = 0.5234375
c619-021: Epoch: 0, Step: 83, Rank: 16, loss = 0.30078125
c622-101: Epoch: 0, Step: 83, Rank: 62, loss = 0.427734375
c622-081: Epoch: 0, Step: 83, Rank: 58, loss = 0.69140625
c622-052: Epoch: 0, Step: 83, Rank: 53, loss = 0.439453125
c621-131: Epoch: 0, Step: 83, Rank: 36, loss = 0.59765625
c622-022: Epoch: 0, Step: 83, Rank: 47, loss = 0.091796875
c621-112: Epoch: 0, Step: 83, Rank: 33, loss = 0.443359375
c622-011: Epoch: 0, Step: 83, Rank: 44, loss = 0.05078125
c621-152: Epoch: 0, Step: 83, Rank: 41, loss = 0.458984375
c622-062: Epoch: 0, Step: 83, Rank: 55, loss = 0.251953125
c622-061: Epoch: 0, Step: 83, Rank: 54, loss = 0.220703125
c621-081: Epoch: 0, Step: 83, Rank: 26, loss = 0.4375
c621-101: Epoch: 0, Step: 83, Rank: 30, loss = 0.10498046875
c621-122: Epoch: 0, Step: 83, Rank: 35, loss = 0.36328125
c622-082: Epoch: 0, Step: 83, Rank: 59, loss = 0.26171875
c621-102: Epoch: 0, Step: 83, Rank: 31, loss = 0.306640625
c621-091: Epoch: 0, Step: 83, Rank: 28, loss = 0.23046875
c622-042: Epoch: 0, Step: 83, Rank: 51, loss = 0.451171875
c622-092: Epoch: 0, Step: 83, Rank: 61, loss = 0.39453125
c613-142: Epoch: 0, Step: 83, Rank: 9, loss = 0.35546875
c621-141: Epoch: 0, Step: 83, Rank: 38, loss = 0.287109375
c621-121: Epoch: 0, Step: 83, Rank: 34, loss = 0.341796875
c621-072: Epoch: 0, Step: 83, Rank: 25, loss = 0.220703125
c622-102: Epoch: 0, Step: 83, Rank: 63, loss = 0.306640625
c619-002: Epoch: 0, Step: 83, Rank: 13, loss = 0.171875
c613-151: Epoch: 0, Step: 83, Rank: 10, loss = 0.126953125
c621-082: Epoch: 0, Step: 83, Rank: 27, loss = 0.427734375
c622-071: Epoch: 0, Step: 83, Rank: 56, loss = 0.35546875
c621-052: Epoch: 0, Step: 83, Rank: 21, loss = 0.443359375
c619-001: Epoch: 0, Step: 83, Rank: 12, loss = 0.419921875
c613-111: Epoch: 0, Step: 83, Rank: 2, loss = 0.00180816650390625
c622-021: Epoch: 0, Step: 83, Rank: 46, loss = 0.220703125
c622-091: Epoch: 0, Step: 83, Rank: 60, loss = 0.193359375
c619-041: Epoch: 0, Step: 83, Rank: 20, loss = 0.291015625
c619-031: Epoch: 0, Step: 83, Rank: 18, loss = 0.0986328125
c613-152: Epoch: 0, Step: 83, Rank: 11, loss = 0.10498046875
c619-011: Epoch: 0, Step: 83, Rank: 14, loss = 0.1767578125
c613-132: Epoch: 0, Step: 83, Rank: 7, loss = 0.162109375
c619-022: Epoch: 0, Step: 83, Rank: 17, loss = 0.287109375
c613-122: Epoch: 0, Step: 83, Rank: 5, loss = 0.37109375
c613-112: Epoch: 0, Step: 83, Rank: 3, loss = 0.330078125
c613-121: Epoch: 0, Step: 83, Rank: 4, loss = 0.10498046875
c613-131: Epoch: 0, Step: 83, Rank: 6, loss = 0.392578125
c622-072: Epoch: 0, Step: 83, Rank: 57, loss = 0.69140625
c613-102: Epoch: 0, Step: 83, Rank: 1, loss = 0.09033203125
c619-012: Epoch: 0, Step: 83, Rank: 15, loss = 0.34765625
c613-141: Epoch: 0, Step: 83, Rank: 8, loss = 0.416015625
c619-032: Epoch: 0, Step: 83, Rank: 19, loss = 0.53515625
c621-092: Epoch: 0, Step: 83, Rank: 29, loss = 0.37890625
c621-071: Epoch: 0, Step: 83, Rank: 24, loss = 0.404296875
c621-062: Epoch: 0, Step: 83, Rank: 23, loss = 0.390625
c621-061: Epoch: 0, Step: 83, Rank: 22, loss = 0.287109375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 84, Rank: 53, loss = 0.3828125
c619-021: Epoch: 0, Step: 84, Rank: 16, loss = 0.34765625
c619-002: Epoch: 0, Step: 84, Rank: 13, loss = 0.3828125
c622-061: Epoch: 0, Step: 84, Rank: 54, loss = 0.5078125
c613-132: Epoch: 0, Step: 84, Rank: 7, loss = 0.69140625
c613-101: Epoch: 0, Step: 84, Rank: 0, loss = 0.337890625
c622-012: Epoch: 0, Step: 84, Rank: 45, loss = 0.42578125
c619-001: Epoch: 0, Step: 84, Rank: 12, loss = 0.3828125
c622-051: Epoch: 0, Step: 84, Rank: 52, loss = 0.67578125
c613-112: Epoch: 0, Step: 84, Rank: 3, loss = 0.515625
c619-031: Epoch: 0, Step: 84, Rank: 18, loss = 0.31640625
c613-131: Epoch: 0, Step: 84, Rank: 6, loss = 0.384765625
c622-041: Epoch: 0, Step: 84, Rank: 50, loss = 0.458984375
c622-032: Epoch: 0, Step: 84, Rank: 49, loss = 0.18359375
c622-002: Epoch: 0, Step: 84, Rank: 43, loss = 0.328125
c613-122: Epoch: 0, Step: 84, Rank: 5, loss = 0.39453125
c613-111: Epoch: 0, Step: 84, Rank: 2, loss = 0.05322265625
c613-152: Epoch: 0, Step: 84, Rank: 11, loss = 0.37109375
c622-042: Epoch: 0, Step: 84, Rank: 51, loss = 0.35546875
c613-121: Epoch: 0, Step: 84, Rank: 4, loss = 0.35546875
c621-132: Epoch: 0, Step: 84, Rank: 37, loss = 0.34765625
c622-102: Epoch: 0, Step: 84, Rank: 63, loss = 0.29296875
c613-141: Epoch: 0, Step: 84, Rank: 8, loss = 0.353515625
c622-031: Epoch: 0, Step: 84, Rank: 48, loss = 0.022216796875
c622-022: Epoch: 0, Step: 84, Rank: 47, loss = 0.220703125
c613-102: Epoch: 0, Step: 84, Rank: 1, loss = 0.201171875
c613-142: Epoch: 0, Step: 84, Rank: 9, loss = 0.52734375
c621-091: Epoch: 0, Step: 84, Rank: 28, loss = 0.39453125
c621-081: Epoch: 0, Step: 84, Rank: 26, loss = 0.265625
c622-101: Epoch: 0, Step: 84, Rank: 62, loss = 0.39453125
c622-092: Epoch: 0, Step: 84, Rank: 61, loss = 0.30859375
c622-062: Epoch: 0, Step: 84, Rank: 55, loss = 0.341796875
c613-151: Epoch: 0, Step: 84, Rank: 10, loss = 0.158203125
c622-001: Epoch: 0, Step: 84, Rank: 42, loss = 0.384765625
c619-012: Epoch: 0, Step: 84, Rank: 15, loss = 0.2099609375
c622-072: Epoch: 0, Step: 84, Rank: 57, loss = 0.69140625
c621-122: Epoch: 0, Step: 84, Rank: 35, loss = 0.3515625
c621-111: Epoch: 0, Step: 84, Rank: 32, loss = 0.328125
c621-072: Epoch: 0, Step: 84, Rank: 25, loss = 0.412109375
c621-151: Epoch: 0, Step: 84, Rank: 40, loss = 0.027099609375
c619-011: Epoch: 0, Step: 84, Rank: 14, loss = 0.419921875
c622-021: Epoch: 0, Step: 84, Rank: 46, loss = 0.412109375
c621-082: Epoch: 0, Step: 84, Rank: 27, loss = 0.1865234375
c621-121: Epoch: 0, Step: 84, Rank: 34, loss = 0.69140625
c622-082: Epoch: 0, Step: 84, Rank: 59, loss = 0.306640625
c621-112: Epoch: 0, Step: 84, Rank: 33, loss = 0.419921875
c621-052: Epoch: 0, Step: 84, Rank: 21, loss = 0.3125
c622-071: Epoch: 0, Step: 84, Rank: 56, loss = 0.46484375
c619-041: Epoch: 0, Step: 84, Rank: 20, loss = 0.455078125
c621-142: Epoch: 0, Step: 84, Rank: 39, loss = 0.21484375
c622-081: Epoch: 0, Step: 84, Rank: 58, loss = 0.453125
c622-011: Epoch: 0, Step: 84, Rank: 44, loss = 0.419921875
c619-022: Epoch: 0, Step: 84, Rank: 17, loss = 0.388671875
c621-101: Epoch: 0, Step: 84, Rank: 30, loss = 0.30859375
c621-141: Epoch: 0, Step: 84, Rank: 38, loss = 0.470703125
c621-062: Epoch: 0, Step: 84, Rank: 23, loss = 0.29296875
c622-091: Epoch: 0, Step: 84, Rank: 60, loss = 0.52734375
c621-131: Epoch: 0, Step: 84, Rank: 36, loss = 0.16015625
c621-152: Epoch: 0, Step: 84, Rank: 41, loss = 0.337890625
c621-102: Epoch: 0, Step: 84, Rank: 31, loss = 0.328125
c621-061: Epoch: 0, Step: 84, Rank: 22, loss = 0.494140625
c621-071: Epoch: 0, Step: 84, Rank: 24, loss = 0.400390625
c621-092: Epoch: 0, Step: 84, Rank: 29, loss = 0.345703125
c619-032: Epoch: 0, Step: 84, Rank: 19, loss = 0.27734375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 85, Rank: 16, loss = 0.1669921875
c622-002: Epoch: 0, Step: 85, Rank: 43, loss = 0.287109375
c621-072: Epoch: 0, Step: 85, Rank: 25, loss = 0.32421875
c622-071: Epoch: 0, Step: 85, Rank: 56, loss = 0.08642578125
c613-101: Epoch: 0, Step: 85, Rank: 0, loss = 0.26171875
c621-132: Epoch: 0, Step: 85, Rank: 37, loss = 0.69140625
c619-031: Epoch: 0, Step: 85, Rank: 18, loss = 0.1328125
c622-101: Epoch: 0, Step: 85, Rank: 62, loss = 0.384765625
c622-062: Epoch: 0, Step: 85, Rank: 55, loss = 0.28125
c621-151: Epoch: 0, Step: 85, Rank: 40, loss = 0.2578125
c622-081: Epoch: 0, Step: 85, Rank: 58, loss = 0.484375
c622-052: Epoch: 0, Step: 85, Rank: 53, loss = 0.00299072265625
c621-082: Epoch: 0, Step: 85, Rank: 27, loss = 0.373046875
c613-151: Epoch: 0, Step: 85, Rank: 10, loss = 0.197265625
c621-052: Epoch: 0, Step: 85, Rank: 21, loss = 0.38671875
c621-081: Epoch: 0, Step: 85, Rank: 26, loss = 0.27734375
c619-001: Epoch: 0, Step: 85, Rank: 12, loss = 0.4375
c622-012: Epoch: 0, Step: 85, Rank: 45, loss = 0.2099609375
c619-022: Epoch: 0, Step: 85, Rank: 17, loss = 0.376953125
c613-152: Epoch: 0, Step: 85, Rank: 11, loss = 0.30078125
c622-082: Epoch: 0, Step: 85, Rank: 59, loss = 0.35546875
c622-061: Epoch: 0, Step: 85, Rank: 54, loss = 0.01373291015625
c621-091: Epoch: 0, Step: 85, Rank: 28, loss = 0.072265625
c622-001: Epoch: 0, Step: 85, Rank: 42, loss = 0.1494140625
c621-112: Epoch: 0, Step: 85, Rank: 33, loss = 0.4375
c619-041: Epoch: 0, Step: 85, Rank: 20, loss = 0.2412109375
c613-142: Epoch: 0, Step: 85, Rank: 9, loss = 0.059326171875
c622-092: Epoch: 0, Step: 85, Rank: 61, loss = 0.494140625
c621-142: Epoch: 0, Step: 85, Rank: 39, loss = 0.54296875
c621-122: Epoch: 0, Step: 85, Rank: 35, loss = 0.46484375
c621-061: Epoch: 0, Step: 85, Rank: 22, loss = 0.01416015625
c621-131: Epoch: 0, Step: 85, Rank: 36, loss = 0.000911712646484375
c613-132: Epoch: 0, Step: 85, Rank: 7, loss = 0.53515625
c619-002: Epoch: 0, Step: 85, Rank: 13, loss = 0.34765625
c621-121: Epoch: 0, Step: 85, Rank: 34, loss = 0.453125
c622-072: Epoch: 0, Step: 85, Rank: 57, loss = 0.341796875
c622-011: Epoch: 0, Step: 85, Rank: 44, loss = 0.38671875
c613-122: Epoch: 0, Step: 85, Rank: 5, loss = 0.1669921875
c621-062: Epoch: 0, Step: 85, Rank: 23, loss = 0.38671875
c621-101: Epoch: 0, Step: 85, Rank: 30, loss = 0.451171875
c622-091: Epoch: 0, Step: 85, Rank: 60, loss = 0.26171875
c619-012: Epoch: 0, Step: 85, Rank: 15, loss = 0.275390625
c622-051: Epoch: 0, Step: 85, Rank: 52, loss = 0.3671875
c621-111: Epoch: 0, Step: 85, Rank: 32, loss = 0.33984375
c622-041: Epoch: 0, Step: 85, Rank: 50, loss = 0.02978515625
c613-102: Epoch: 0, Step: 85, Rank: 1, loss = 0.3203125
c613-121: Epoch: 0, Step: 85, Rank: 4, loss = 0.29296875
c619-032: Epoch: 0, Step: 85, Rank: 19, loss = 0.1669921875
c613-141: Epoch: 0, Step: 85, Rank: 8, loss = 0.46875
c621-152: Epoch: 0, Step: 85, Rank: 41, loss = 0.00396728515625
c619-011: Epoch: 0, Step: 85, Rank: 14, loss = 0.39453125
c622-102: Epoch: 0, Step: 85, Rank: 63, loss = 0.3671875
c613-131: Epoch: 0, Step: 85, Rank: 6, loss = 0.201171875
c622-032: Epoch: 0, Step: 85, Rank: 49, loss = 0.474609375
c621-102: Epoch: 0, Step: 85, Rank: 31, loss = 0.419921875
c622-031: Epoch: 0, Step: 85, Rank: 48, loss = 0.38671875
c622-021: Epoch: 0, Step: 85, Rank: 46, loss = 0.390625
c613-111: Epoch: 0, Step: 85, Rank: 2, loss = 0.341796875
c613-112: Epoch: 0, Step: 85, Rank: 3, loss = 0.275390625
c621-092: Epoch: 0, Step: 85, Rank: 29, loss = 0.3125
c622-042: Epoch: 0, Step: 85, Rank: 51, loss = 0.32421875
c621-141: Epoch: 0, Step: 85, Rank: 38, loss = 0.37890625
c621-071: Epoch: 0, Step: 85, Rank: 24, loss = 0.57421875
c622-022: Epoch: 0, Step: 85, Rank: 47, loss = 0.36328125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 86, Rank: 58, loss = 0.17578125
c613-101: Epoch: 0, Step: 86, Rank: 0, loss = 0.201171875
c613-111: Epoch: 0, Step: 86, Rank: 2, loss = 0.10498046875
c622-001: Epoch: 0, Step: 86, Rank: 42, loss = 0.38671875
c621-151: Epoch: 0, Step: 86, Rank: 40, loss = 0.2255859375
c619-001: Epoch: 0, Step: 86, Rank: 12, loss = 0.333984375
c619-002: Epoch: 0, Step: 86, Rank: 13, loss = 0.201171875
c622-052: Epoch: 0, Step: 86, Rank: 53, loss = 0.369140625
c621-091: Epoch: 0, Step: 86, Rank: 28, loss = 0.3203125
c621-111: Epoch: 0, Step: 86, Rank: 32, loss = 0.451171875
c621-082: Epoch: 0, Step: 86, Rank: 27, loss = 0.46875
c619-021: Epoch: 0, Step: 86, Rank: 16, loss = 0.28515625
c622-092: Epoch: 0, Step: 86, Rank: 61, loss = 0.412109375
c621-081: Epoch: 0, Step: 86, Rank: 26, loss = 0.53515625
c621-132: Epoch: 0, Step: 86, Rank: 37, loss = 0.38671875
c613-142: Epoch: 0, Step: 86, Rank: 9, loss = 0.027587890625
c613-131: Epoch: 0, Step: 86, Rank: 6, loss = 0.462890625
c622-002: Epoch: 0, Step: 86, Rank: 43, loss = 0.333984375
c613-152: Epoch: 0, Step: 86, Rank: 11, loss = 0.388671875
c613-151: Epoch: 0, Step: 86, Rank: 10, loss = 0.474609375
c622-101: Epoch: 0, Step: 86, Rank: 62, loss = 0.35546875
c621-121: Epoch: 0, Step: 86, Rank: 34, loss = 0.08642578125
c622-062: Epoch: 0, Step: 86, Rank: 55, loss = 0.296875
c621-052: Epoch: 0, Step: 86, Rank: 21, loss = 0.259765625
c621-072: Epoch: 0, Step: 86, Rank: 25, loss = 0.2412109375
c622-051: Epoch: 0, Step: 86, Rank: 52, loss = 0.46484375
c613-121: Epoch: 0, Step: 86, Rank: 4, loss = 0.201171875
c613-112: Epoch: 0, Step: 86, Rank: 3, loss = 0.2470703125
c621-061: Epoch: 0, Step: 86, Rank: 22, loss = 0.416015625
c621-152: Epoch: 0, Step: 86, Rank: 41, loss = 0.34765625
c622-012: Epoch: 0, Step: 86, Rank: 45, loss = 0.36328125
c613-102: Epoch: 0, Step: 86, Rank: 1, loss = 0.46875
c622-102: Epoch: 0, Step: 86, Rank: 63, loss = 0.69140625
c621-142: Epoch: 0, Step: 86, Rank: 39, loss = 0.06298828125
c613-122: Epoch: 0, Step: 86, Rank: 5, loss = 0.259765625
c621-131: Epoch: 0, Step: 86, Rank: 36, loss = 0.1796875
c621-101: Epoch: 0, Step: 86, Rank: 30, loss = 0.39453125
c613-132: Epoch: 0, Step: 86, Rank: 7, loss = 0.373046875
c622-072: Epoch: 0, Step: 86, Rank: 57, loss = 0.26171875
c622-032: Epoch: 0, Step: 86, Rank: 49, loss = 0.412109375
c621-112: Epoch: 0, Step: 86, Rank: 33, loss = 0.37890625
c619-022: Epoch: 0, Step: 86, Rank: 17, loss = 0.37109375
c619-031: Epoch: 0, Step: 86, Rank: 18, loss = 0.376953125
c619-032: Epoch: 0, Step: 86, Rank: 19, loss = 0.30078125
c621-122: Epoch: 0, Step: 86, Rank: 35, loss = 0.54296875
c622-042: Epoch: 0, Step: 86, Rank: 51, loss = 0.32421875
c619-012: Epoch: 0, Step: 86, Rank: 15, loss = 0.384765625
c619-041: Epoch: 0, Step: 86, Rank: 20, loss = 0.494140625
c621-102: Epoch: 0, Step: 86, Rank: 31, loss = 0.4375
c621-071: Epoch: 0, Step: 86, Rank: 24, loss = 0.41796875
c622-082: Epoch: 0, Step: 86, Rank: 59, loss = 0.359375
c622-031: Epoch: 0, Step: 86, Rank: 48, loss = 0.466796875
c622-061: Epoch: 0, Step: 86, Rank: 54, loss = 0.423828125
c622-022: Epoch: 0, Step: 86, Rank: 47, loss = 0.36328125
c622-041: Epoch: 0, Step: 86, Rank: 50, loss = 0.404296875
c622-091: Epoch: 0, Step: 86, Rank: 60, loss = 0.220703125
c613-141: Epoch: 0, Step: 86, Rank: 8, loss = 0.18359375
c622-071: Epoch: 0, Step: 86, Rank: 56, loss = 0.0020599365234375
c622-021: Epoch: 0, Step: 86, Rank: 46, loss = 0.478515625
c621-092: Epoch: 0, Step: 86, Rank: 29, loss = 0.1796875
c619-011: Epoch: 0, Step: 86, Rank: 14, loss = 0.416015625
c621-062: Epoch: 0, Step: 86, Rank: 23, loss = 0.36328125
c622-011: Epoch: 0, Step: 86, Rank: 44, loss = 0.4609375
c621-141: Epoch: 0, Step: 86, Rank: 38, loss = 0.5859375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2490234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 87, Rank: 43, loss = 0.35546875
c613-101: Epoch: 0, Step: 87, Rank: 0, loss = 0.34765625
c619-021: Epoch: 0, Step: 87, Rank: 16, loss = 0.474609375
c622-052: Epoch: 0, Step: 87, Rank: 53, loss = 0.08642578125
c619-002: Epoch: 0, Step: 87, Rank: 13, loss = 0.443359375
c621-111: Epoch: 0, Step: 87, Rank: 32, loss = 0.32421875
c622-071: Epoch: 0, Step: 87, Rank: 56, loss = 0.404296875
c621-151: Epoch: 0, Step: 87, Rank: 40, loss = 0.16015625
c622-001: Epoch: 0, Step: 87, Rank: 42, loss = 0.296875
c622-032: Epoch: 0, Step: 87, Rank: 49, loss = 0.2578125
c613-121: Epoch: 0, Step: 87, Rank: 4, loss = 0.3515625
c621-072: Epoch: 0, Step: 87, Rank: 25, loss = 0.36328125
c621-081: Epoch: 0, Step: 87, Rank: 26, loss = 0.484375
c619-022: Epoch: 0, Step: 87, Rank: 17, loss = 0.341796875
c619-041: Epoch: 0, Step: 87, Rank: 20, loss = 0.201171875
c621-091: Epoch: 0, Step: 87, Rank: 28, loss = 0.53125
c621-131: Epoch: 0, Step: 87, Rank: 36, loss = 0.17578125
c622-081: Epoch: 0, Step: 87, Rank: 58, loss = 0.427734375
c622-082: Epoch: 0, Step: 87, Rank: 59, loss = 0.1640625
c621-121: Epoch: 0, Step: 87, Rank: 34, loss = 0.193359375
c619-031: Epoch: 0, Step: 87, Rank: 18, loss = 0.51171875
c619-001: Epoch: 0, Step: 87, Rank: 12, loss = 0.34765625
c613-131: Epoch: 0, Step: 87, Rank: 6, loss = 0.28125
c621-132: Epoch: 0, Step: 87, Rank: 37, loss = 0.306640625
c613-132: Epoch: 0, Step: 87, Rank: 7, loss = 0.13671875
c621-122: Epoch: 0, Step: 87, Rank: 35, loss = 0.37890625
c613-152: Epoch: 0, Step: 87, Rank: 11, loss = 0.1982421875
c621-152: Epoch: 0, Step: 87, Rank: 41, loss = 0.359375
c621-101: Epoch: 0, Step: 87, Rank: 30, loss = 0.3125
c622-062: Epoch: 0, Step: 87, Rank: 55, loss = 0.46484375
c622-022: Epoch: 0, Step: 87, Rank: 47, loss = 0.048583984375
c622-061: Epoch: 0, Step: 87, Rank: 54, loss = 0.3515625
c621-082: Epoch: 0, Step: 87, Rank: 27, loss = 0.23046875
c622-101: Epoch: 0, Step: 87, Rank: 62, loss = 0.251953125
c622-012: Epoch: 0, Step: 87, Rank: 45, loss = 0.002471923828125
c622-091: Epoch: 0, Step: 87, Rank: 60, loss = 0.30859375
c613-141: Epoch: 0, Step: 87, Rank: 8, loss = 0.31640625
c622-051: Epoch: 0, Step: 87, Rank: 52, loss = 0.220703125
c621-071: Epoch: 0, Step: 87, Rank: 24, loss = 0.4375
c621-142: Epoch: 0, Step: 87, Rank: 39, loss = 0.458984375
c621-112: Epoch: 0, Step: 87, Rank: 33, loss = 0.220703125
c621-102: Epoch: 0, Step: 87, Rank: 31, loss = 0.18359375
c613-122: Epoch: 0, Step: 87, Rank: 5, loss = 0.220703125
c619-032: Epoch: 0, Step: 87, Rank: 19, loss = 0.220703125
c622-092: Epoch: 0, Step: 87, Rank: 61, loss = 0.0255126953125
c621-141: Epoch: 0, Step: 87, Rank: 38, loss = 0.17578125
c613-151: Epoch: 0, Step: 87, Rank: 10, loss = 0.01708984375
c622-041: Epoch: 0, Step: 87, Rank: 50, loss = 0.00164794921875
c619-012: Epoch: 0, Step: 87, Rank: 15, loss = 0.0186767578125
c619-011: Epoch: 0, Step: 87, Rank: 14, loss = 0.0478515625
c621-052: Epoch: 0, Step: 87, Rank: 21, loss = 0.37109375
c613-102: Epoch: 0, Step: 87, Rank: 1, loss = 0.359375
c622-031: Epoch: 0, Step: 87, Rank: 48, loss = 0.38671875
c613-142: Epoch: 0, Step: 87, Rank: 9, loss = 0.4375
c621-061: Epoch: 0, Step: 87, Rank: 22, loss = 0.345703125
c622-021: Epoch: 0, Step: 87, Rank: 46, loss = 0.275390625
c621-062: Epoch: 0, Step: 87, Rank: 23, loss = 0.57421875
c622-042: Epoch: 0, Step: 87, Rank: 51, loss = 0.0026397705078125
c622-011: Epoch: 0, Step: 87, Rank: 44, loss = 0.3828125
c613-111: Epoch: 0, Step: 87, Rank: 2, loss = 0.201171875
c613-112: Epoch: 0, Step: 87, Rank: 3, loss = 0.310546875
c622-072: Epoch: 0, Step: 87, Rank: 57, loss = 0.412109375
c621-092: Epoch: 0, Step: 87, Rank: 29, loss = 0.26171875
c622-102: Epoch: 0, Step: 87, Rank: 63, loss = 0.34765625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 88, Rank: 32, loss = 0.275390625
c622-002: Epoch: 0, Step: 88, Rank: 43, loss = 0.41015625
c619-021: Epoch: 0, Step: 88, Rank: 16, loss = 0.34765625
c622-062: Epoch: 0, Step: 88, Rank: 55, loss = 0.02978515625
c622-012: Epoch: 0, Step: 88, Rank: 45, loss = 0.58203125
c619-002: Epoch: 0, Step: 88, Rank: 13, loss = 0.333984375
c622-071: Epoch: 0, Step: 88, Rank: 56, loss = 0.51953125
c619-041: Epoch: 0, Step: 88, Rank: 20, loss = 0.039794921875
c621-091: Epoch: 0, Step: 88, Rank: 28, loss = 0.341796875
c622-052: Epoch: 0, Step: 88, Rank: 53, loss = 0.1533203125
c621-061: Epoch: 0, Step: 88, Rank: 22, loss = 0.057373046875
c621-072: Epoch: 0, Step: 88, Rank: 25, loss = 0.396484375
c613-151: Epoch: 0, Step: 88, Rank: 10, loss = 0.32421875
c621-081: Epoch: 0, Step: 88, Rank: 26, loss = 0.28125
c613-152: Epoch: 0, Step: 88, Rank: 11, loss = 0.287109375
c613-101: Epoch: 0, Step: 88, Rank: 0, loss = 0.474609375
c613-142: Epoch: 0, Step: 88, Rank: 9, loss = 0.3671875
c622-011: Epoch: 0, Step: 88, Rank: 44, loss = 0.59375
c619-031: Epoch: 0, Step: 88, Rank: 18, loss = 0.32421875
c619-001: Epoch: 0, Step: 88, Rank: 12, loss = 0.306640625
c619-032: Epoch: 0, Step: 88, Rank: 19, loss = 0.341796875
c621-151: Epoch: 0, Step: 88, Rank: 40, loss = 0.2412109375
c621-052: Epoch: 0, Step: 88, Rank: 21, loss = 0.3515625
c619-022: Epoch: 0, Step: 88, Rank: 17, loss = 0.359375
c622-061: Epoch: 0, Step: 88, Rank: 54, loss = 0.375
c621-082: Epoch: 0, Step: 88, Rank: 27, loss = 0.22265625
c622-032: Epoch: 0, Step: 88, Rank: 49, loss = 0.1396484375
c621-121: Epoch: 0, Step: 88, Rank: 34, loss = 0.0181884765625
c621-101: Epoch: 0, Step: 88, Rank: 30, loss = 0.484375
c613-132: Epoch: 0, Step: 88, Rank: 7, loss = 0.287109375
c622-081: Epoch: 0, Step: 88, Rank: 58, loss = 0.36328125
c619-011: Epoch: 0, Step: 88, Rank: 14, loss = 0.412109375
c622-041: Epoch: 0, Step: 88, Rank: 50, loss = 0.404296875
c622-051: Epoch: 0, Step: 88, Rank: 52, loss = 0.32421875
c621-131: Epoch: 0, Step: 88, Rank: 36, loss = 0.3125
c621-142: Epoch: 0, Step: 88, Rank: 39, loss = 0.498046875
c621-071: Epoch: 0, Step: 88, Rank: 24, loss = 0.29296875
c622-101: Epoch: 0, Step: 88, Rank: 62, loss = 0.69140625
c613-131: Epoch: 0, Step: 88, Rank: 6, loss = 0.37109375
c621-102: Epoch: 0, Step: 88, Rank: 31, loss = 0.287109375
c621-152: Epoch: 0, Step: 88, Rank: 41, loss = 0.251953125
c622-031: Epoch: 0, Step: 88, Rank: 48, loss = 0.296875
c622-082: Epoch: 0, Step: 88, Rank: 59, loss = 0.40234375
c621-122: Epoch: 0, Step: 88, Rank: 35, loss = 0.027099609375
c619-012: Epoch: 0, Step: 88, Rank: 15, loss = 0.072265625
c613-141: Epoch: 0, Step: 88, Rank: 8, loss = 0.423828125
c622-022: Epoch: 0, Step: 88, Rank: 47, loss = 0.29296875
c621-062: Epoch: 0, Step: 88, Rank: 23, loss = 0.39453125
c621-141: Epoch: 0, Step: 88, Rank: 38, loss = 0.453125
c622-001: Epoch: 0, Step: 88, Rank: 42, loss = 0.30078125
c621-112: Epoch: 0, Step: 88, Rank: 33, loss = 0.416015625
c613-121: Epoch: 0, Step: 88, Rank: 4, loss = 0.33984375
c622-042: Epoch: 0, Step: 88, Rank: 51, loss = 0.34765625
c622-092: Epoch: 0, Step: 88, Rank: 61, loss = 0.69140625
c622-021: Epoch: 0, Step: 88, Rank: 46, loss = 0.34765625
c613-102: Epoch: 0, Step: 88, Rank: 1, loss = 0.3203125
c622-072: Epoch: 0, Step: 88, Rank: 57, loss = 0.69140625
c613-112: Epoch: 0, Step: 88, Rank: 3, loss = 0.4375
c622-091: Epoch: 0, Step: 88, Rank: 60, loss = 0.390625
c621-132: Epoch: 0, Step: 88, Rank: 37, loss = 0.251953125
c613-111: Epoch: 0, Step: 88, Rank: 2, loss = 0.306640625
c622-102: Epoch: 0, Step: 88, Rank: 63, loss = 0.2578125
c621-092: Epoch: 0, Step: 88, Rank: 29, loss = 0.359375
c613-122: Epoch: 0, Step: 88, Rank: 5, loss = 0.220703125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 89, Rank: 43, loss = 0.34765625
c622-052: Epoch: 0, Step: 89, Rank: 53, loss = 0.26953125
c622-051: Epoch: 0, Step: 89, Rank: 52, loss = 0.53515625
c619-002: Epoch: 0, Step: 89, Rank: 13, loss = 0.32421875
c622-081: Epoch: 0, Step: 89, Rank: 58, loss = 0.69140625
c622-101: Epoch: 0, Step: 89, Rank: 62, loss = 0.412109375
c621-091: Epoch: 0, Step: 89, Rank: 28, loss = 0.2412109375
c619-021: Epoch: 0, Step: 89, Rank: 16, loss = 0.400390625
c622-041: Epoch: 0, Step: 89, Rank: 50, loss = 0.37109375
c621-082: Epoch: 0, Step: 89, Rank: 27, loss = 0.32421875
c613-101: Epoch: 0, Step: 89, Rank: 0, loss = 0.30078125
c622-012: Epoch: 0, Step: 89, Rank: 45, loss = 0.38671875
c621-131: Epoch: 0, Step: 89, Rank: 36, loss = 0.201171875
c621-122: Epoch: 0, Step: 89, Rank: 35, loss = 0.28515625
c621-102: Epoch: 0, Step: 89, Rank: 31, loss = 0.0311279296875
c622-032: Epoch: 0, Step: 89, Rank: 49, loss = 0.3984375
c621-081: Epoch: 0, Step: 89, Rank: 26, loss = 0.69140625
c621-052: Epoch: 0, Step: 89, Rank: 21, loss = 0.091796875
c621-072: Epoch: 0, Step: 89, Rank: 25, loss = 0.26171875
c621-121: Epoch: 0, Step: 89, Rank: 34, loss = 0.37109375
c622-061: Epoch: 0, Step: 89, Rank: 54, loss = 0.208984375
c621-152: Epoch: 0, Step: 89, Rank: 41, loss = 0.69140625
c621-151: Epoch: 0, Step: 89, Rank: 40, loss = 0.49609375
c619-022: Epoch: 0, Step: 89, Rank: 17, loss = 0.482421875
c622-082: Epoch: 0, Step: 89, Rank: 59, loss = 0.23046875
c621-101: Epoch: 0, Step: 89, Rank: 30, loss = 0.072265625
c622-092: Epoch: 0, Step: 89, Rank: 61, loss = 0.1162109375
c621-092: Epoch: 0, Step: 89, Rank: 29, loss = 0.0005035400390625
c622-001: Epoch: 0, Step: 89, Rank: 42, loss = 0.384765625
c622-022: Epoch: 0, Step: 89, Rank: 47, loss = 0.41015625
c622-042: Epoch: 0, Step: 89, Rank: 51, loss = 0.173828125
c621-142: Epoch: 0, Step: 89, Rank: 39, loss = 0.375
c619-041: Epoch: 0, Step: 89, Rank: 20, loss = 0.287109375
c622-091: Epoch: 0, Step: 89, Rank: 60, loss = 0.28125
c622-021: Epoch: 0, Step: 89, Rank: 46, loss = 0.2158203125
c619-012: Epoch: 0, Step: 89, Rank: 15, loss = 0.333984375
c619-031: Epoch: 0, Step: 89, Rank: 18, loss = 0.3515625
c622-011: Epoch: 0, Step: 89, Rank: 44, loss = 0.50390625
c621-062: Epoch: 0, Step: 89, Rank: 23, loss = 0.404296875
c619-011: Epoch: 0, Step: 89, Rank: 14, loss = 0.427734375
c621-111: Epoch: 0, Step: 89, Rank: 32, loss = 0.18359375
c621-071: Epoch: 0, Step: 89, Rank: 24, loss = 0.26171875
c613-121: Epoch: 0, Step: 89, Rank: 4, loss = 0.38671875
c621-061: Epoch: 0, Step: 89, Rank: 22, loss = 0.53125
c621-141: Epoch: 0, Step: 89, Rank: 38, loss = 0.2412109375
c619-001: Epoch: 0, Step: 89, Rank: 12, loss = 0.23046875
c622-031: Epoch: 0, Step: 89, Rank: 48, loss = 0.18359375
c613-151: Epoch: 0, Step: 89, Rank: 10, loss = 0.412109375
c621-112: Epoch: 0, Step: 89, Rank: 33, loss = 0.69140625
c613-102: Epoch: 0, Step: 89, Rank: 1, loss = 0.00714111328125
c619-032: Epoch: 0, Step: 89, Rank: 19, loss = 0.455078125
c613-111: Epoch: 0, Step: 89, Rank: 2, loss = 0.423828125
c613-152: Epoch: 0, Step: 89, Rank: 11, loss = 0.248046875
c622-072: Epoch: 0, Step: 89, Rank: 57, loss = 0.328125
c613-112: Epoch: 0, Step: 89, Rank: 3, loss = 0.3125
c622-062: Epoch: 0, Step: 89, Rank: 55, loss = 0.427734375
c621-132: Epoch: 0, Step: 89, Rank: 37, loss = 0.28125
c613-141: Epoch: 0, Step: 89, Rank: 8, loss = 0.484375
c622-071: Epoch: 0, Step: 89, Rank: 56, loss = 0.375
c613-142: Epoch: 0, Step: 89, Rank: 9, loss = 0.31640625
c613-122: Epoch: 0, Step: 89, Rank: 5, loss = 0.400390625
c613-132: Epoch: 0, Step: 89, Rank: 7, loss = 0.10498046875
c622-102: Epoch: 0, Step: 89, Rank: 63, loss = 0.478515625
c613-131: Epoch: 0, Step: 89, Rank: 6, loss = 0.35546875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24560546875 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 90, Rank: 16, loss = 0.38671875
c613-101: Epoch: 0, Step: 90, Rank: 0, loss = 0.412109375
c621-111: Epoch: 0, Step: 90, Rank: 32, loss = 0.447265625
c621-132: Epoch: 0, Step: 90, Rank: 37, loss = 0.69140625
c622-002: Epoch: 0, Step: 90, Rank: 43, loss = 0.388671875
c621-081: Epoch: 0, Step: 90, Rank: 26, loss = 0.439453125
c619-002: Epoch: 0, Step: 90, Rank: 13, loss = 0.37109375
c619-031: Epoch: 0, Step: 90, Rank: 18, loss = 0.1826171875
c619-001: Epoch: 0, Step: 90, Rank: 12, loss = 0.1826171875
c621-091: Epoch: 0, Step: 90, Rank: 28, loss = 0.287109375
c621-131: Epoch: 0, Step: 90, Rank: 36, loss = 0.37109375
c622-012: Epoch: 0, Step: 90, Rank: 45, loss = 0.3125
c622-052: Epoch: 0, Step: 90, Rank: 53, loss = 0.46484375
c621-072: Epoch: 0, Step: 90, Rank: 25, loss = 0.2177734375
c621-122: Epoch: 0, Step: 90, Rank: 35, loss = 0.36328125
c613-152: Epoch: 0, Step: 90, Rank: 11, loss = 0.267578125
c621-121: Epoch: 0, Step: 90, Rank: 34, loss = 0.69140625
c621-151: Epoch: 0, Step: 90, Rank: 40, loss = 0.37890625
c619-011: Epoch: 0, Step: 90, Rank: 14, loss = 0.341796875
c622-032: Epoch: 0, Step: 90, Rank: 49, loss = 0.388671875
c619-041: Epoch: 0, Step: 90, Rank: 20, loss = 0.2060546875
c621-061: Epoch: 0, Step: 90, Rank: 22, loss = 0.57421875
c621-112: Epoch: 0, Step: 90, Rank: 33, loss = 0.021484375
c621-142: Epoch: 0, Step: 90, Rank: 39, loss = 0.412109375
c622-011: Epoch: 0, Step: 90, Rank: 44, loss = 0.69140625
c622-081: Epoch: 0, Step: 90, Rank: 58, loss = 0.28125
c619-032: Epoch: 0, Step: 90, Rank: 19, loss = 0.5546875
c619-022: Epoch: 0, Step: 90, Rank: 17, loss = 0.341796875
c621-141: Epoch: 0, Step: 90, Rank: 38, loss = 0.3828125
c621-082: Epoch: 0, Step: 90, Rank: 27, loss = 0.29296875
c622-101: Epoch: 0, Step: 90, Rank: 62, loss = 0.38671875
c613-121: Epoch: 0, Step: 90, Rank: 4, loss = 0.059326171875
c621-101: Epoch: 0, Step: 90, Rank: 30, loss = 0.10498046875
c622-021: Epoch: 0, Step: 90, Rank: 46, loss = 0.52734375
c622-102: Epoch: 0, Step: 90, Rank: 63, loss = 0.072265625
c622-022: Epoch: 0, Step: 90, Rank: 47, loss = 0.23046875
c619-012: Epoch: 0, Step: 90, Rank: 15, loss = 0.416015625
c622-001: Epoch: 0, Step: 90, Rank: 42, loss = 0.39453125
c621-102: Epoch: 0, Step: 90, Rank: 31, loss = 0.30078125
c622-041: Epoch: 0, Step: 90, Rank: 50, loss = 0.37109375
c621-052: Epoch: 0, Step: 90, Rank: 21, loss = 0.01007080078125
c613-132: Epoch: 0, Step: 90, Rank: 7, loss = 0.142578125
c613-141: Epoch: 0, Step: 90, Rank: 8, loss = 0.388671875
c622-042: Epoch: 0, Step: 90, Rank: 51, loss = 0.328125
c622-092: Epoch: 0, Step: 90, Rank: 61, loss = 0.26953125
c613-142: Epoch: 0, Step: 90, Rank: 9, loss = 0.337890625
c622-082: Epoch: 0, Step: 90, Rank: 59, loss = 0.06201171875
c621-152: Epoch: 0, Step: 90, Rank: 41, loss = 0.095703125
c622-091: Epoch: 0, Step: 90, Rank: 60, loss = 0.2470703125
c622-051: Epoch: 0, Step: 90, Rank: 52, loss = 0.43359375
c621-062: Epoch: 0, Step: 90, Rank: 23, loss = 0.337890625
c622-031: Epoch: 0, Step: 90, Rank: 48, loss = 0.005584716796875
c613-112: Epoch: 0, Step: 90, Rank: 3, loss = 0.419921875
c613-102: Epoch: 0, Step: 90, Rank: 1, loss = 0.330078125
c621-071: Epoch: 0, Step: 90, Rank: 24, loss = 0.447265625
c613-122: Epoch: 0, Step: 90, Rank: 5, loss = 0.458984375
c613-151: Epoch: 0, Step: 90, Rank: 10, loss = 0.201171875
c621-092: Epoch: 0, Step: 90, Rank: 29, loss = 0.1533203125
c622-071: Epoch: 0, Step: 90, Rank: 56, loss = 0.193359375
c613-111: Epoch: 0, Step: 90, Rank: 2, loss = 0.400390625
c613-131: Epoch: 0, Step: 90, Rank: 6, loss = 0.400390625
c622-062: Epoch: 0, Step: 90, Rank: 55, loss = 0.404296875
c622-061: Epoch: 0, Step: 90, Rank: 54, loss = 0.341796875
c622-072: Epoch: 0, Step: 90, Rank: 57, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-152: Epoch: 0, Step: 91, Rank: 11, loss = 0.248046875
c619-001: Epoch: 0, Step: 91, Rank: 12, loss = 0.17578125
c619-002: Epoch: 0, Step: 91, Rank: 13, loss = 0.69140625
c622-002: Epoch: 0, Step: 91, Rank: 43, loss = 0.1904296875
c619-021: Epoch: 0, Step: 91, Rank: 16, loss = 0.306640625
c622-052: Epoch: 0, Step: 91, Rank: 53, loss = 0.400390625
c613-101: Epoch: 0, Step: 91, Rank: 0, loss = 0.37890625
c621-081: Epoch: 0, Step: 91, Rank: 26, loss = 0.427734375
c619-022: Epoch: 0, Step: 91, Rank: 17, loss = 0.12158203125
c619-031: Epoch: 0, Step: 91, Rank: 18, loss = 0.306640625
c622-001: Epoch: 0, Step: 91, Rank: 42, loss = 0.2431640625
c621-091: Epoch: 0, Step: 91, Rank: 28, loss = 0.27734375
c621-151: Epoch: 0, Step: 91, Rank: 40, loss = 0.197265625
c622-012: Epoch: 0, Step: 91, Rank: 45, loss = 0.34765625
c619-032: Epoch: 0, Step: 91, Rank: 19, loss = 0.306640625
c613-151: Epoch: 0, Step: 91, Rank: 10, loss = 0.0038299560546875
c613-142: Epoch: 0, Step: 91, Rank: 9, loss = 0.404296875
c621-111: Epoch: 0, Step: 91, Rank: 32, loss = 0.3125
c621-061: Epoch: 0, Step: 91, Rank: 22, loss = 0.2470703125
c621-072: Epoch: 0, Step: 91, Rank: 25, loss = 0.05322265625
c621-132: Epoch: 0, Step: 91, Rank: 37, loss = 0.328125
c613-132: Epoch: 0, Step: 91, Rank: 7, loss = 0.2255859375
c621-142: Epoch: 0, Step: 91, Rank: 39, loss = 0.1298828125
c621-082: Epoch: 0, Step: 91, Rank: 27, loss = 0.37109375
c621-112: Epoch: 0, Step: 91, Rank: 33, loss = 0.22265625
c622-041: Epoch: 0, Step: 91, Rank: 50, loss = 0.1015625
c622-021: Epoch: 0, Step: 91, Rank: 46, loss = 0.458984375
c621-121: Epoch: 0, Step: 91, Rank: 34, loss = 0.43359375
c619-011: Epoch: 0, Step: 91, Rank: 14, loss = 0.32421875
c622-032: Epoch: 0, Step: 91, Rank: 49, loss = 0.341796875
c621-101: Epoch: 0, Step: 91, Rank: 30, loss = 0.39453125
c622-022: Epoch: 0, Step: 91, Rank: 47, loss = 0.330078125
c619-012: Epoch: 0, Step: 91, Rank: 15, loss = 0.69140625
c621-052: Epoch: 0, Step: 91, Rank: 21, loss = 0.23046875
c622-061: Epoch: 0, Step: 91, Rank: 54, loss = 0.443359375
c621-102: Epoch: 0, Step: 91, Rank: 31, loss = 0.201171875
c613-131: Epoch: 0, Step: 91, Rank: 6, loss = 0.447265625
c621-152: Epoch: 0, Step: 91, Rank: 41, loss = 0.447265625
c613-141: Epoch: 0, Step: 91, Rank: 8, loss = 0.287109375
c619-041: Epoch: 0, Step: 91, Rank: 20, loss = 0.306640625
c622-031: Epoch: 0, Step: 91, Rank: 48, loss = 0.69140625
c622-062: Epoch: 0, Step: 91, Rank: 55, loss = 0.059326171875
c622-011: Epoch: 0, Step: 91, Rank: 44, loss = 0.423828125
c622-082: Epoch: 0, Step: 91, Rank: 59, loss = 0.55078125
c621-141: Epoch: 0, Step: 91, Rank: 38, loss = 0.16015625
c621-122: Epoch: 0, Step: 91, Rank: 35, loss = 0.041748046875
c622-101: Epoch: 0, Step: 91, Rank: 62, loss = 0.36328125
c622-042: Epoch: 0, Step: 91, Rank: 51, loss = 0.56640625
c621-131: Epoch: 0, Step: 91, Rank: 36, loss = 0.328125
c613-122: Epoch: 0, Step: 91, Rank: 5, loss = 0.341796875
c622-092: Epoch: 0, Step: 91, Rank: 61, loss = 0.416015625
c622-081: Epoch: 0, Step: 91, Rank: 58, loss = 0.5390625
c622-102: Epoch: 0, Step: 91, Rank: 63, loss = 0.435546875
c613-121: Epoch: 0, Step: 91, Rank: 4, loss = 0.431640625
c621-062: Epoch: 0, Step: 91, Rank: 23, loss = 0.228515625
c613-112: Epoch: 0, Step: 91, Rank: 3, loss = 0.000431060791015625
c621-092: Epoch: 0, Step: 91, Rank: 29, loss = 0.30078125
c622-071: Epoch: 0, Step: 91, Rank: 56, loss = 0.359375
c621-071: Epoch: 0, Step: 91, Rank: 24, loss = 0.36328125
c613-111: Epoch: 0, Step: 91, Rank: 2, loss = 0.23046875
c622-051: Epoch: 0, Step: 91, Rank: 52, loss = 0.37109375
c622-072: Epoch: 0, Step: 91, Rank: 57, loss = 0.322265625
c613-102: Epoch: 0, Step: 91, Rank: 1, loss = 0.50390625
c622-091: Epoch: 0, Step: 91, Rank: 60, loss = 0.43359375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.12s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.12s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 92, Rank: 0, loss = 0.2099609375
c622-081: Epoch: 0, Step: 92, Rank: 58, loss = 0.419921875
c619-002: Epoch: 0, Step: 92, Rank: 13, loss = 0.26171875
c621-091: Epoch: 0, Step: 92, Rank: 28, loss = 0.416015625
c619-021: Epoch: 0, Step: 92, Rank: 16, loss = 0.220703125
c622-002: Epoch: 0, Step: 92, Rank: 43, loss = 0.2470703125
c622-101: Epoch: 0, Step: 92, Rank: 62, loss = 0.02978515625
c619-001: Epoch: 0, Step: 92, Rank: 12, loss = 0.291015625
c619-031: Epoch: 0, Step: 92, Rank: 18, loss = 0.34765625
c622-082: Epoch: 0, Step: 92, Rank: 59, loss = 0.427734375
c622-012: Epoch: 0, Step: 92, Rank: 45, loss = 0.306640625
c619-032: Epoch: 0, Step: 92, Rank: 19, loss = 0.359375
c621-082: Epoch: 0, Step: 92, Rank: 27, loss = 0.287109375
c621-072: Epoch: 0, Step: 92, Rank: 25, loss = 0.470703125
c621-111: Epoch: 0, Step: 92, Rank: 32, loss = 0.412109375
c621-132: Epoch: 0, Step: 92, Rank: 37, loss = 0.34765625
c621-101: Epoch: 0, Step: 92, Rank: 30, loss = 0.38671875
c622-092: Epoch: 0, Step: 92, Rank: 61, loss = 0.427734375
c622-052: Epoch: 0, Step: 92, Rank: 53, loss = 0.419921875
c613-121: Epoch: 0, Step: 92, Rank: 4, loss = 0.390625
c621-052: Epoch: 0, Step: 92, Rank: 21, loss = 0.000335693359375
c613-142: Epoch: 0, Step: 92, Rank: 9, loss = 0.1533203125
c613-141: Epoch: 0, Step: 92, Rank: 8, loss = 0.39453125
c622-062: Epoch: 0, Step: 92, Rank: 55, loss = 0.341796875
c621-131: Epoch: 0, Step: 92, Rank: 36, loss = 0.32421875
c622-102: Epoch: 0, Step: 92, Rank: 63, loss = 0.451171875
c613-132: Epoch: 0, Step: 92, Rank: 7, loss = 0.341796875
c613-151: Epoch: 0, Step: 92, Rank: 10, loss = 0.38671875
c621-081: Epoch: 0, Step: 92, Rank: 26, loss = 0.486328125
c622-051: Epoch: 0, Step: 92, Rank: 52, loss = 0.36328125
c619-012: Epoch: 0, Step: 92, Rank: 15, loss = 0.2578125
c619-022: Epoch: 0, Step: 92, Rank: 17, loss = 0.27734375
c619-041: Epoch: 0, Step: 92, Rank: 20, loss = 0.546875
c613-152: Epoch: 0, Step: 92, Rank: 11, loss = 0.291015625
c621-151: Epoch: 0, Step: 92, Rank: 40, loss = 0.1669921875
c613-122: Epoch: 0, Step: 92, Rank: 5, loss = 0.38671875
c621-122: Epoch: 0, Step: 92, Rank: 35, loss = 0.004486083984375
c622-061: Epoch: 0, Step: 92, Rank: 54, loss = 0.51171875
c613-112: Epoch: 0, Step: 92, Rank: 3, loss = 0.00193023681640625
c613-131: Epoch: 0, Step: 92, Rank: 6, loss = 0.37890625
c621-061: Epoch: 0, Step: 92, Rank: 22, loss = 0.408203125
c622-001: Epoch: 0, Step: 92, Rank: 42, loss = 0.34765625
c622-091: Epoch: 0, Step: 92, Rank: 60, loss = 0.35546875
c621-121: Epoch: 0, Step: 92, Rank: 34, loss = 0.3203125
c621-112: Epoch: 0, Step: 92, Rank: 33, loss = 0.427734375
c622-031: Epoch: 0, Step: 92, Rank: 48, loss = 0.220703125
c621-092: Epoch: 0, Step: 92, Rank: 29, loss = 0.5234375
c622-041: Epoch: 0, Step: 92, Rank: 50, loss = 0.404296875
c619-011: Epoch: 0, Step: 92, Rank: 14, loss = 0.416015625
c621-071: Epoch: 0, Step: 92, Rank: 24, loss = 0.28125
c613-111: Epoch: 0, Step: 92, Rank: 2, loss = 0.0032806396484375
c622-022: Epoch: 0, Step: 92, Rank: 47, loss = 0.375
c621-102: Epoch: 0, Step: 92, Rank: 31, loss = 0.474609375
c622-021: Epoch: 0, Step: 92, Rank: 46, loss = 0.18359375
c622-011: Epoch: 0, Step: 92, Rank: 44, loss = 0.455078125
c622-072: Epoch: 0, Step: 92, Rank: 57, loss = 0.0791015625
c621-142: Epoch: 0, Step: 92, Rank: 39, loss = 0.4375
c613-102: Epoch: 0, Step: 92, Rank: 1, loss = 0.1943359375
c622-042: Epoch: 0, Step: 92, Rank: 51, loss = 0.35546875
c621-152: Epoch: 0, Step: 92, Rank: 41, loss = 0.1396484375
c621-062: Epoch: 0, Step: 92, Rank: 23, loss = 0.30078125
c622-071: Epoch: 0, Step: 92, Rank: 56, loss = 0.69140625
c622-032: Epoch: 0, Step: 92, Rank: 49, loss = 0.5546875
c621-141: Epoch: 0, Step: 92, Rank: 38, loss = 0.291015625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 93, Rank: 53, loss = 0.494140625
c622-002: Epoch: 0, Step: 93, Rank: 43, loss = 0.091796875
c621-132: Epoch: 0, Step: 93, Rank: 37, loss = 0.388671875
c622-012: Epoch: 0, Step: 93, Rank: 45, loss = 0.455078125
c621-091: Epoch: 0, Step: 93, Rank: 28, loss = 0.265625
c622-081: Epoch: 0, Step: 93, Rank: 58, loss = 0.328125
c622-051: Epoch: 0, Step: 93, Rank: 52, loss = 0.294921875
c621-131: Epoch: 0, Step: 93, Rank: 36, loss = 0.62890625
c613-101: Epoch: 0, Step: 93, Rank: 0, loss = 0.20703125
c622-001: Epoch: 0, Step: 93, Rank: 42, loss = 0.26171875
c621-122: Epoch: 0, Step: 93, Rank: 35, loss = 0.37109375
c622-061: Epoch: 0, Step: 93, Rank: 54, loss = 0.251953125
c621-121: Epoch: 0, Step: 93, Rank: 34, loss = 0.126953125
c622-022: Epoch: 0, Step: 93, Rank: 47, loss = 0.2099609375
c622-042: Epoch: 0, Step: 93, Rank: 51, loss = 0.5703125
c619-021: Epoch: 0, Step: 93, Rank: 16, loss = 0.69140625
c622-041: Epoch: 0, Step: 93, Rank: 50, loss = 0.220703125
c621-112: Epoch: 0, Step: 93, Rank: 33, loss = 0.423828125
c621-111: Epoch: 0, Step: 93, Rank: 32, loss = 0.37109375
c621-142: Epoch: 0, Step: 93, Rank: 39, loss = 0.29296875
c621-082: Epoch: 0, Step: 93, Rank: 27, loss = 0.1455078125
c621-101: Epoch: 0, Step: 93, Rank: 30, loss = 0.0849609375
c622-082: Epoch: 0, Step: 93, Rank: 59, loss = 0.220703125
c622-062: Epoch: 0, Step: 93, Rank: 55, loss = 0.2353515625
c619-001: Epoch: 0, Step: 93, Rank: 12, loss = 0.455078125
c621-102: Epoch: 0, Step: 93, Rank: 31, loss = 0.46484375
c621-152: Epoch: 0, Step: 93, Rank: 41, loss = 0.455078125
c622-032: Epoch: 0, Step: 93, Rank: 49, loss = 0.123046875
c622-102: Epoch: 0, Step: 93, Rank: 63, loss = 0.341796875
c622-092: Epoch: 0, Step: 93, Rank: 61, loss = 0.039306640625
c622-021: Epoch: 0, Step: 93, Rank: 46, loss = 0.37109375
c622-101: Epoch: 0, Step: 93, Rank: 62, loss = 0.23046875
c619-011: Epoch: 0, Step: 93, Rank: 14, loss = 0.35546875
c622-031: Epoch: 0, Step: 93, Rank: 48, loss = 0.26953125
c621-151: Epoch: 0, Step: 93, Rank: 40, loss = 0.49609375
c621-141: Epoch: 0, Step: 93, Rank: 38, loss = 0.1162109375
c622-071: Epoch: 0, Step: 93, Rank: 56, loss = 0.18359375
c621-081: Epoch: 0, Step: 93, Rank: 26, loss = 0.212890625
c622-011: Epoch: 0, Step: 93, Rank: 44, loss = 0.36328125
c621-072: Epoch: 0, Step: 93, Rank: 25, loss = 0.35546875
c621-061: Epoch: 0, Step: 93, Rank: 22, loss = 0.447265625
c613-151: Epoch: 0, Step: 93, Rank: 10, loss = 0.333984375
c619-002: Epoch: 0, Step: 93, Rank: 13, loss = 0.451171875
c613-152: Epoch: 0, Step: 93, Rank: 11, loss = 0.35546875
c613-132: Epoch: 0, Step: 93, Rank: 7, loss = 0.287109375
c621-092: Epoch: 0, Step: 93, Rank: 29, loss = 0.32421875
c613-141: Epoch: 0, Step: 93, Rank: 8, loss = 0.306640625
c619-031: Epoch: 0, Step: 93, Rank: 18, loss = 0.404296875
c619-022: Epoch: 0, Step: 93, Rank: 17, loss = 0.458984375
c621-052: Epoch: 0, Step: 93, Rank: 21, loss = 0.412109375
c613-121: Epoch: 0, Step: 93, Rank: 4, loss = 0.384765625
c613-131: Epoch: 0, Step: 93, Rank: 6, loss = 0.3203125
c613-122: Epoch: 0, Step: 93, Rank: 5, loss = 0.5234375
c613-142: Epoch: 0, Step: 93, Rank: 9, loss = 0.376953125
c613-102: Epoch: 0, Step: 93, Rank: 1, loss = 0.158203125
c619-041: Epoch: 0, Step: 93, Rank: 20, loss = 0.37109375
c619-032: Epoch: 0, Step: 93, Rank: 19, loss = 0.142578125
c619-012: Epoch: 0, Step: 93, Rank: 15, loss = 0.1455078125
c622-091: Epoch: 0, Step: 93, Rank: 60, loss = 0.0181884765625
c622-072: Epoch: 0, Step: 93, Rank: 57, loss = 0.1328125
c621-071: Epoch: 0, Step: 93, Rank: 24, loss = 0.306640625
c613-111: Epoch: 0, Step: 93, Rank: 2, loss = 0.341796875
c613-112: Epoch: 0, Step: 93, Rank: 3, loss = 0.2099609375
c621-062: Epoch: 0, Step: 93, Rank: 23, loss = 0.1533203125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.98s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.98s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 94, Rank: 0, loss = 0.29296875
c622-081: Epoch: 0, Step: 94, Rank: 58, loss = 0.69140625
c622-082: Epoch: 0, Step: 94, Rank: 59, loss = 0.18359375
c619-021: Epoch: 0, Step: 94, Rank: 16, loss = 0.1669921875
c622-092: Epoch: 0, Step: 94, Rank: 61, loss = 0.625
c622-091: Epoch: 0, Step: 94, Rank: 60, loss = 0.390625
c622-101: Epoch: 0, Step: 94, Rank: 62, loss = 0.0888671875
c619-001: Epoch: 0, Step: 94, Rank: 12, loss = 0.2060546875
c622-071: Epoch: 0, Step: 94, Rank: 56, loss = 0.375
c622-102: Epoch: 0, Step: 94, Rank: 63, loss = 0.18359375
c621-081: Epoch: 0, Step: 94, Rank: 26, loss = 0.287109375
c622-061: Epoch: 0, Step: 94, Rank: 54, loss = 0.423828125
c619-022: Epoch: 0, Step: 94, Rank: 17, loss = 0.69140625
c622-052: Epoch: 0, Step: 94, Rank: 53, loss = 0.171875
c621-052: Epoch: 0, Step: 94, Rank: 21, loss = 0.455078125
c622-032: Epoch: 0, Step: 94, Rank: 49, loss = 0.251953125
c622-002: Epoch: 0, Step: 94, Rank: 43, loss = 0.220703125
c613-132: Epoch: 0, Step: 94, Rank: 7, loss = 0.30859375
c613-152: Epoch: 0, Step: 94, Rank: 11, loss = 0.220703125
c622-001: Epoch: 0, Step: 94, Rank: 42, loss = 0.357421875
c619-041: Epoch: 0, Step: 94, Rank: 20, loss = 0.333984375
c622-012: Epoch: 0, Step: 94, Rank: 45, loss = 0.388671875
c619-002: Epoch: 0, Step: 94, Rank: 13, loss = 0.53515625
c613-131: Epoch: 0, Step: 94, Rank: 6, loss = 0.251953125
c613-151: Epoch: 0, Step: 94, Rank: 10, loss = 0.419921875
c619-012: Epoch: 0, Step: 94, Rank: 15, loss = 0.3203125
c619-031: Epoch: 0, Step: 94, Rank: 18, loss = 0.341796875
c622-062: Epoch: 0, Step: 94, Rank: 55, loss = 0.443359375
c619-011: Epoch: 0, Step: 94, Rank: 14, loss = 0.06396484375
c613-141: Epoch: 0, Step: 94, Rank: 8, loss = 0.3515625
c621-111: Epoch: 0, Step: 94, Rank: 32, loss = 0.091796875
c619-032: Epoch: 0, Step: 94, Rank: 19, loss = 0.310546875
c621-132: Epoch: 0, Step: 94, Rank: 37, loss = 0.2578125
c621-061: Epoch: 0, Step: 94, Rank: 22, loss = 0.220703125
c622-011: Epoch: 0, Step: 94, Rank: 44, loss = 0.3671875
c613-142: Epoch: 0, Step: 94, Rank: 9, loss = 0.287109375
c622-072: Epoch: 0, Step: 94, Rank: 57, loss = 0.69140625
c621-072: Epoch: 0, Step: 94, Rank: 25, loss = 0.30859375
c622-051: Epoch: 0, Step: 94, Rank: 52, loss = 0.1396484375
c621-131: Epoch: 0, Step: 94, Rank: 36, loss = 0.38671875
c621-151: Epoch: 0, Step: 94, Rank: 40, loss = 0.427734375
c622-041: Epoch: 0, Step: 94, Rank: 50, loss = 0.400390625
c613-121: Epoch: 0, Step: 94, Rank: 4, loss = 0.458984375
c613-122: Epoch: 0, Step: 94, Rank: 5, loss = 0.4375
c621-112: Epoch: 0, Step: 94, Rank: 33, loss = 0.32421875
c622-031: Epoch: 0, Step: 94, Rank: 48, loss = 0.36328125
c622-022: Epoch: 0, Step: 94, Rank: 47, loss = 0.275390625
c621-152: Epoch: 0, Step: 94, Rank: 41, loss = 0.26953125
c621-121: Epoch: 0, Step: 94, Rank: 34, loss = 0.431640625
c613-112: Epoch: 0, Step: 94, Rank: 3, loss = 0.1484375
c621-101: Epoch: 0, Step: 94, Rank: 30, loss = 0.359375
c622-042: Epoch: 0, Step: 94, Rank: 51, loss = 0.39453125
c621-091: Epoch: 0, Step: 94, Rank: 28, loss = 0.30078125
c621-142: Epoch: 0, Step: 94, Rank: 39, loss = 0.337890625
c613-102: Epoch: 0, Step: 94, Rank: 1, loss = 0.34765625
c621-071: Epoch: 0, Step: 94, Rank: 24, loss = 0.00421142578125
c621-141: Epoch: 0, Step: 94, Rank: 38, loss = 0.38671875
c621-082: Epoch: 0, Step: 94, Rank: 27, loss = 0.036376953125
c621-122: Epoch: 0, Step: 94, Rank: 35, loss = 0.26171875
c621-062: Epoch: 0, Step: 94, Rank: 23, loss = 0.00075531005859375
c622-021: Epoch: 0, Step: 94, Rank: 46, loss = 0.322265625
c621-102: Epoch: 0, Step: 94, Rank: 31, loss = 0.400390625
c621-092: Epoch: 0, Step: 94, Rank: 29, loss = 0.3203125
c613-111: Epoch: 0, Step: 94, Rank: 2, loss = 0.419921875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 95, Rank: 43, loss = 0.341796875
c619-021: Epoch: 0, Step: 95, Rank: 16, loss = 0.3515625
c619-002: Epoch: 0, Step: 95, Rank: 13, loss = 0.01287841796875
c613-151: Epoch: 0, Step: 95, Rank: 10, loss = 0.3125
c622-052: Epoch: 0, Step: 95, Rank: 53, loss = 0.341796875
c619-001: Epoch: 0, Step: 95, Rank: 12, loss = 0.458984375
c622-081: Epoch: 0, Step: 95, Rank: 58, loss = 0.1162109375
c621-082: Epoch: 0, Step: 95, Rank: 27, loss = 0.5
c622-051: Epoch: 0, Step: 95, Rank: 52, loss = 0.201171875
c622-012: Epoch: 0, Step: 95, Rank: 45, loss = 0.35546875
c621-072: Epoch: 0, Step: 95, Rank: 25, loss = 0.26953125
c613-152: Epoch: 0, Step: 95, Rank: 11, loss = 0.228515625
c622-082: Epoch: 0, Step: 95, Rank: 59, loss = 0.38671875
c613-101: Epoch: 0, Step: 95, Rank: 0, loss = 0.69140625
c619-011: Epoch: 0, Step: 95, Rank: 14, loss = 0.26171875
c621-142: Epoch: 0, Step: 95, Rank: 39, loss = 0.35546875
c621-091: Epoch: 0, Step: 95, Rank: 28, loss = 0.419921875
c621-131: Epoch: 0, Step: 95, Rank: 36, loss = 0.29296875
c622-001: Epoch: 0, Step: 95, Rank: 42, loss = 0.057373046875
c621-151: Epoch: 0, Step: 95, Rank: 40, loss = 0.3828125
c622-032: Epoch: 0, Step: 95, Rank: 49, loss = 0.41015625
c619-012: Epoch: 0, Step: 95, Rank: 15, loss = 0.0225830078125
c621-121: Epoch: 0, Step: 95, Rank: 34, loss = 0.291015625
c621-102: Epoch: 0, Step: 95, Rank: 31, loss = 0.26171875
c621-052: Epoch: 0, Step: 95, Rank: 21, loss = 0.00433349609375
c621-111: Epoch: 0, Step: 95, Rank: 32, loss = 0.451171875
c619-031: Epoch: 0, Step: 95, Rank: 18, loss = 0.000667572021484375
c621-081: Epoch: 0, Step: 95, Rank: 26, loss = 0.109375
c613-122: Epoch: 0, Step: 95, Rank: 5, loss = 0.52734375
c619-041: Epoch: 0, Step: 95, Rank: 20, loss = 0.251953125
c613-112: Epoch: 0, Step: 95, Rank: 3, loss = 0.404296875
c622-041: Epoch: 0, Step: 95, Rank: 50, loss = 0.16015625
c613-132: Epoch: 0, Step: 95, Rank: 7, loss = 0.287109375
c621-101: Epoch: 0, Step: 95, Rank: 30, loss = 0.1982421875
c622-022: Epoch: 0, Step: 95, Rank: 47, loss = 0.01507568359375
c622-061: Epoch: 0, Step: 95, Rank: 54, loss = 0.375
c621-061: Epoch: 0, Step: 95, Rank: 22, loss = 0.443359375
c621-122: Epoch: 0, Step: 95, Rank: 35, loss = 0.021240234375
c613-121: Epoch: 0, Step: 95, Rank: 4, loss = 0.29296875
c619-022: Epoch: 0, Step: 95, Rank: 17, loss = 0.427734375
c622-011: Epoch: 0, Step: 95, Rank: 44, loss = 0.69140625
c613-141: Epoch: 0, Step: 95, Rank: 8, loss = 0.490234375
c613-131: Epoch: 0, Step: 95, Rank: 6, loss = 0.1669921875
c619-032: Epoch: 0, Step: 95, Rank: 19, loss = 0.3671875
c613-111: Epoch: 0, Step: 95, Rank: 2, loss = 0.400390625
c622-102: Epoch: 0, Step: 95, Rank: 63, loss = 0.419921875
c613-142: Epoch: 0, Step: 95, Rank: 9, loss = 0.328125
c621-092: Epoch: 0, Step: 95, Rank: 29, loss = 0.287109375
c622-062: Epoch: 0, Step: 95, Rank: 55, loss = 0.28515625
c622-031: Epoch: 0, Step: 95, Rank: 48, loss = 0.00020313262939453125
c621-152: Epoch: 0, Step: 95, Rank: 41, loss = 0.296875
c621-141: Epoch: 0, Step: 95, Rank: 38, loss = 0.306640625
c621-071: Epoch: 0, Step: 95, Rank: 24, loss = 0.4765625
c622-101: Epoch: 0, Step: 95, Rank: 62, loss = 0.330078125
c621-112: Epoch: 0, Step: 95, Rank: 33, loss = 0.3671875
c621-062: Epoch: 0, Step: 95, Rank: 23, loss = 0.302734375
c622-072: Epoch: 0, Step: 95, Rank: 57, loss = 0.408203125
c613-102: Epoch: 0, Step: 95, Rank: 1, loss = 0.484375
c622-092: Epoch: 0, Step: 95, Rank: 61, loss = 0.072265625
c622-042: Epoch: 0, Step: 95, Rank: 51, loss = 0.38671875
c622-021: Epoch: 0, Step: 95, Rank: 46, loss = 0.00836181640625
c622-091: Epoch: 0, Step: 95, Rank: 60, loss = 0.3046875
c622-071: Epoch: 0, Step: 95, Rank: 56, loss = 0.328125
c621-132: Epoch: 0, Step: 95, Rank: 37, loss = 0.0242919921875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.19s, TFLOPs: 0.86, Samples/sec: 0.46, Time/seq 2.19s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 96, Rank: 0, loss = 0.154296875
c622-081: Epoch: 0, Step: 96, Rank: 58, loss = 0.2255859375
c622-052: Epoch: 0, Step: 96, Rank: 53, loss = 0.0037078857421875
c619-021: Epoch: 0, Step: 96, Rank: 16, loss = 0.06689453125
c619-001: Epoch: 0, Step: 96, Rank: 12, loss = 0.427734375
c622-002: Epoch: 0, Step: 96, Rank: 43, loss = 0.10009765625
c619-002: Epoch: 0, Step: 96, Rank: 13, loss = 0.328125
c619-022: Epoch: 0, Step: 96, Rank: 17, loss = 0.419921875
c619-031: Epoch: 0, Step: 96, Rank: 18, loss = 0.251953125
c613-131: Epoch: 0, Step: 96, Rank: 6, loss = 0.1162109375
c619-011: Epoch: 0, Step: 96, Rank: 14, loss = 0.12158203125
c619-012: Epoch: 0, Step: 96, Rank: 15, loss = 0.3515625
c613-152: Epoch: 0, Step: 96, Rank: 11, loss = 2.2649765014648438e-06
c622-082: Epoch: 0, Step: 96, Rank: 59, loss = 0.1796875
c613-122: Epoch: 0, Step: 96, Rank: 5, loss = 0.443359375
c622-061: Epoch: 0, Step: 96, Rank: 54, loss = 0.0091552734375
c622-062: Epoch: 0, Step: 96, Rank: 55, loss = 0.3203125
c622-071: Epoch: 0, Step: 96, Rank: 56, loss = 0.201171875
c613-121: Epoch: 0, Step: 96, Rank: 4, loss = 0.12158203125
c613-112: Epoch: 0, Step: 96, Rank: 3, loss = 0.1796875
c622-012: Epoch: 0, Step: 96, Rank: 45, loss = 0.228515625
c613-142: Epoch: 0, Step: 96, Rank: 9, loss = 0.08251953125
c621-111: Epoch: 0, Step: 96, Rank: 32, loss = 0.404296875
c621-152: Epoch: 0, Step: 96, Rank: 41, loss = 0.1826171875
c622-032: Epoch: 0, Step: 96, Rank: 49, loss = 0.18359375
c613-102: Epoch: 0, Step: 96, Rank: 1, loss = 0.16015625
c622-001: Epoch: 0, Step: 96, Rank: 42, loss = 0.73828125
c622-102: Epoch: 0, Step: 96, Rank: 63, loss = 0.29296875
c613-132: Epoch: 0, Step: 96, Rank: 7, loss = 0.330078125
c622-051: Epoch: 0, Step: 96, Rank: 52, loss = 0.345703125
c613-111: Epoch: 0, Step: 96, Rank: 2, loss = 2.014636993408203e-05
c621-151: Epoch: 0, Step: 96, Rank: 40, loss = 0.000518798828125
c622-042: Epoch: 0, Step: 96, Rank: 51, loss = 0.11279296875
c621-121: Epoch: 0, Step: 96, Rank: 34, loss = 0.10791015625
c622-092: Epoch: 0, Step: 96, Rank: 61, loss = 0.0849609375
c613-151: Epoch: 0, Step: 96, Rank: 10, loss = 0.2578125
c621-081: Epoch: 0, Step: 96, Rank: 26, loss = 0.28515625
c621-112: Epoch: 0, Step: 96, Rank: 33, loss = 0.00150299072265625
c619-032: Epoch: 0, Step: 96, Rank: 19, loss = 0.69140625
c621-132: Epoch: 0, Step: 96, Rank: 37, loss = 0.27734375
c622-041: Epoch: 0, Step: 96, Rank: 50, loss = 0.08251953125
c622-101: Epoch: 0, Step: 96, Rank: 62, loss = 0.10302734375
c621-052: Epoch: 0, Step: 96, Rank: 21, loss = 0.142578125
c613-141: Epoch: 0, Step: 96, Rank: 8, loss = 0.38671875
c621-082: Epoch: 0, Step: 96, Rank: 27, loss = 0.26953125
c621-101: Epoch: 0, Step: 96, Rank: 30, loss = 0.43359375
c621-102: Epoch: 0, Step: 96, Rank: 31, loss = 0.022216796875
c622-011: Epoch: 0, Step: 96, Rank: 44, loss = 0.20703125
c621-072: Epoch: 0, Step: 96, Rank: 25, loss = 0.1669921875
c621-122: Epoch: 0, Step: 96, Rank: 35, loss = 0.1875
c621-142: Epoch: 0, Step: 96, Rank: 39, loss = 0.1396484375
c621-141: Epoch: 0, Step: 96, Rank: 38, loss = 0.171875
c622-021: Epoch: 0, Step: 96, Rank: 46, loss = 0.12158203125
c622-031: Epoch: 0, Step: 96, Rank: 48, loss = 0.26171875
c622-091: Epoch: 0, Step: 96, Rank: 60, loss = 0.09716796875
c621-071: Epoch: 0, Step: 96, Rank: 24, loss = 0.32421875
c622-072: Epoch: 0, Step: 96, Rank: 57, loss = 0.48828125
c621-131: Epoch: 0, Step: 96, Rank: 36, loss = 0.10009765625
c621-062: Epoch: 0, Step: 96, Rank: 23, loss = 0.004486083984375
c621-091: Epoch: 0, Step: 96, Rank: 28, loss = 0.275390625
c621-061: Epoch: 0, Step: 96, Rank: 22, loss = 0.0008544921875
c622-022: Epoch: 0, Step: 96, Rank: 47, loss = 0.267578125
c621-092: Epoch: 0, Step: 96, Rank: 29, loss = 0.69140625
c619-041: Epoch: 0, Step: 96, Rank: 20, loss = 0.04443359375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 97, Rank: 58, loss = 0.048583984375
c622-002: Epoch: 0, Step: 97, Rank: 43, loss = 0.306640625
c619-021: Epoch: 0, Step: 97, Rank: 16, loss = 0.3203125
c621-111: Epoch: 0, Step: 97, Rank: 32, loss = 0.0791015625
c622-101: Epoch: 0, Step: 97, Rank: 62, loss = 0.39453125
c621-081: Epoch: 0, Step: 97, Rank: 26, loss = 0.1396484375
c613-101: Epoch: 0, Step: 97, Rank: 0, loss = 0.20703125
c619-002: Epoch: 0, Step: 97, Rank: 13, loss = 0.38671875
c621-072: Epoch: 0, Step: 97, Rank: 25, loss = 0.19140625
c613-152: Epoch: 0, Step: 97, Rank: 11, loss = 0.10498046875
c613-151: Epoch: 0, Step: 97, Rank: 10, loss = 0.1328125
c613-132: Epoch: 0, Step: 97, Rank: 7, loss = 0.13671875
c622-092: Epoch: 0, Step: 97, Rank: 61, loss = 0.28515625
c622-052: Epoch: 0, Step: 97, Rank: 53, loss = 0.10498046875
c619-032: Epoch: 0, Step: 97, Rank: 19, loss = 0.251953125
c619-022: Epoch: 0, Step: 97, Rank: 17, loss = 0.1943359375
c613-131: Epoch: 0, Step: 97, Rank: 6, loss = 0.43359375
c621-121: Epoch: 0, Step: 97, Rank: 34, loss = 0.43359375
c622-082: Epoch: 0, Step: 97, Rank: 59, loss = 0.2177734375
c622-061: Epoch: 0, Step: 97, Rank: 54, loss = 0.1328125
c613-142: Epoch: 0, Step: 97, Rank: 9, loss = 0.06689453125
c621-131: Epoch: 0, Step: 97, Rank: 36, loss = 0.3515625
c621-122: Epoch: 0, Step: 97, Rank: 35, loss = 0.06201171875
c621-112: Epoch: 0, Step: 97, Rank: 33, loss = 0.06787109375
c622-062: Epoch: 0, Step: 97, Rank: 55, loss = 0.271484375
c613-111: Epoch: 0, Step: 97, Rank: 2, loss = 0.201171875
c613-102: Epoch: 0, Step: 97, Rank: 1, loss = 0.1298828125
c613-141: Epoch: 0, Step: 97, Rank: 8, loss = 0.228515625
c622-022: Epoch: 0, Step: 97, Rank: 47, loss = 0.1455078125
c622-102: Epoch: 0, Step: 97, Rank: 63, loss = 0.193359375
c621-151: Epoch: 0, Step: 97, Rank: 40, loss = 0.16015625
c621-101: Epoch: 0, Step: 97, Rank: 30, loss = 0.291015625
c613-121: Epoch: 0, Step: 97, Rank: 4, loss = 0.12158203125
c621-132: Epoch: 0, Step: 97, Rank: 37, loss = 0.06494140625
c622-001: Epoch: 0, Step: 97, Rank: 42, loss = 0.1533203125
c621-142: Epoch: 0, Step: 97, Rank: 39, loss = 0.1796875
c621-082: Epoch: 0, Step: 97, Rank: 27, loss = 0.08642578125
c621-152: Epoch: 0, Step: 97, Rank: 41, loss = 0.248046875
c621-071: Epoch: 0, Step: 97, Rank: 24, loss = 0.29296875
c619-041: Epoch: 0, Step: 97, Rank: 20, loss = 0.419921875
c622-012: Epoch: 0, Step: 97, Rank: 45, loss = 0.69140625
c622-011: Epoch: 0, Step: 97, Rank: 44, loss = 0.28515625
c613-122: Epoch: 0, Step: 97, Rank: 5, loss = 0.1328125
c622-021: Epoch: 0, Step: 97, Rank: 46, loss = 0.1669921875
c621-052: Epoch: 0, Step: 97, Rank: 21, loss = 0.06201171875
c619-031: Epoch: 0, Step: 97, Rank: 18, loss = 0.095703125
c619-001: Epoch: 0, Step: 97, Rank: 12, loss = 0.255859375
c622-041: Epoch: 0, Step: 97, Rank: 50, loss = 0.154296875
c621-061: Epoch: 0, Step: 97, Rank: 22, loss = 0.2060546875
c621-102: Epoch: 0, Step: 97, Rank: 31, loss = 0.23046875
c619-012: Epoch: 0, Step: 97, Rank: 15, loss = 0.07080078125
c613-112: Epoch: 0, Step: 97, Rank: 3, loss = 0.04296875
c621-062: Epoch: 0, Step: 97, Rank: 23, loss = 0.000335693359375
c622-031: Epoch: 0, Step: 97, Rank: 48, loss = 0.251953125
c622-091: Epoch: 0, Step: 97, Rank: 60, loss = 5.424022674560547e-06
c621-092: Epoch: 0, Step: 97, Rank: 29, loss = 0.30078125
c622-072: Epoch: 0, Step: 97, Rank: 57, loss = 0.2255859375
c622-051: Epoch: 0, Step: 97, Rank: 52, loss = 0.125
c622-071: Epoch: 0, Step: 97, Rank: 56, loss = 0.08642578125
c619-011: Epoch: 0, Step: 97, Rank: 14, loss = 0.10302734375
c621-141: Epoch: 0, Step: 97, Rank: 38, loss = 0.1640625
c621-091: Epoch: 0, Step: 97, Rank: 28, loss = 0.330078125
c622-032: Epoch: 0, Step: 97, Rank: 49, loss = 0.337890625
c622-042: Epoch: 0, Step: 97, Rank: 51, loss = 3.688037395477295e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c621-142: Epoch: 0, Step: 98, Rank: 39, loss = 0.22265625
c619-002: Epoch: 0, Step: 98, Rank: 13, loss = 0.56640625
c621-111: Epoch: 0, Step: 98, Rank: 32, loss = 0.248046875
c621-091: Epoch: 0, Step: 98, Rank: 28, loss = 0.1494140625
c621-081: Epoch: 0, Step: 98, Rank: 26, loss = 0.14453125
c622-002: Epoch: 0, Step: 98, Rank: 43, loss = 0.328125
c621-082: Epoch: 0, Step: 98, Rank: 27, loss = 0.08642578125
c619-001: Epoch: 0, Step: 98, Rank: 12, loss = 0.291015625
c613-101: Epoch: 0, Step: 98, Rank: 0, loss = 0.337890625
c621-072: Epoch: 0, Step: 98, Rank: 25, loss = 0.404296875
c621-112: Epoch: 0, Step: 98, Rank: 33, loss = 0.0810546875
c621-121: Epoch: 0, Step: 98, Rank: 34, loss = 0.08642578125
c613-151: Epoch: 0, Step: 98, Rank: 10, loss = 0.54296875
c613-152: Epoch: 0, Step: 98, Rank: 11, loss = 0.109375
c613-121: Epoch: 0, Step: 98, Rank: 4, loss = 0.0380859375
c621-151: Epoch: 0, Step: 98, Rank: 40, loss = 0.126953125
c621-141: Epoch: 0, Step: 98, Rank: 38, loss = 0.2578125
c621-101: Epoch: 0, Step: 98, Rank: 30, loss = 0.5546875
c619-021: Epoch: 0, Step: 98, Rank: 16, loss = 0.000644683837890625
c621-131: Epoch: 0, Step: 98, Rank: 36, loss = 0.1142578125
c613-132: Epoch: 0, Step: 98, Rank: 7, loss = 0.1396484375
c622-001: Epoch: 0, Step: 98, Rank: 42, loss = 0.2578125
c621-092: Epoch: 0, Step: 98, Rank: 29, loss = 0.345703125
c621-132: Epoch: 0, Step: 98, Rank: 37, loss = 0.251953125
c621-052: Epoch: 0, Step: 98, Rank: 21, loss = 0.37109375
c619-011: Epoch: 0, Step: 98, Rank: 14, loss = 0.255859375
c613-111: Epoch: 0, Step: 98, Rank: 2, loss = 0.000335693359375
c613-122: Epoch: 0, Step: 98, Rank: 5, loss = 0.06494140625
c613-142: Epoch: 0, Step: 98, Rank: 9, loss = 0.1015625
c622-101: Epoch: 0, Step: 98, Rank: 62, loss = 0.43359375
c613-112: Epoch: 0, Step: 98, Rank: 3, loss = 0.1669921875
c613-141: Epoch: 0, Step: 98, Rank: 8, loss = 0.212890625
c621-122: Epoch: 0, Step: 98, Rank: 35, loss = 0.2734375
c622-081: Epoch: 0, Step: 98, Rank: 58, loss = 0.255859375
c622-092: Epoch: 0, Step: 98, Rank: 61, loss = 0.1455078125
c613-102: Epoch: 0, Step: 98, Rank: 1, loss = 0.123046875
c619-022: Epoch: 0, Step: 98, Rank: 17, loss = 0.0181884765625
c621-102: Epoch: 0, Step: 98, Rank: 31, loss = 0.00010251998901367188
c622-011: Epoch: 0, Step: 98, Rank: 44, loss = 0.3125
c619-041: Epoch: 0, Step: 98, Rank: 20, loss = 0.1328125
c619-032: Epoch: 0, Step: 98, Rank: 19, loss = 0.197265625
c622-052: Epoch: 0, Step: 98, Rank: 53, loss = 0.1533203125
c622-062: Epoch: 0, Step: 98, Rank: 55, loss = 0.2578125
c622-102: Epoch: 0, Step: 98, Rank: 63, loss = 0.28515625
c613-131: Epoch: 0, Step: 98, Rank: 6, loss = 0.03515625
c622-012: Epoch: 0, Step: 98, Rank: 45, loss = 0.26171875
c619-012: Epoch: 0, Step: 98, Rank: 15, loss = 0.166015625
c621-071: Epoch: 0, Step: 98, Rank: 24, loss = 0.06005859375
c619-031: Epoch: 0, Step: 98, Rank: 18, loss = 0.271484375
c621-061: Epoch: 0, Step: 98, Rank: 22, loss = 0.201171875
c622-082: Epoch: 0, Step: 98, Rank: 59, loss = 0.027587890625
c622-041: Epoch: 0, Step: 98, Rank: 50, loss = 0.140625
c622-021: Epoch: 0, Step: 98, Rank: 46, loss = 0.6328125
c622-091: Epoch: 0, Step: 98, Rank: 60, loss = 0.455078125
c621-062: Epoch: 0, Step: 98, Rank: 23, loss = 0.000911712646484375
c621-152: Epoch: 0, Step: 98, Rank: 41, loss = 0.11279296875
c622-061: Epoch: 0, Step: 98, Rank: 54, loss = 0.23046875
c622-022: Epoch: 0, Step: 98, Rank: 47, loss = 0.37890625
c622-032: Epoch: 0, Step: 98, Rank: 49, loss = 0.26953125
c622-031: Epoch: 0, Step: 98, Rank: 48, loss = 0.2333984375
c622-072: Epoch: 0, Step: 98, Rank: 57, loss = 0.1396484375
c622-071: Epoch: 0, Step: 98, Rank: 56, loss = 0.095703125
c622-042: Epoch: 0, Step: 98, Rank: 51, loss = 5.14984130859375e-05
c622-051: Epoch: 0, Step: 98, Rank: 52, loss = 0.302734375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 99, Rank: 58, loss = 0.06689453125
c613-101: Epoch: 0, Step: 99, Rank: 0, loss = 0.09716796875
c622-051: Epoch: 0, Step: 99, Rank: 52, loss = 0.208984375
c619-002: Epoch: 0, Step: 99, Rank: 13, loss = 0.0791015625
c622-002: Epoch: 0, Step: 99, Rank: 43, loss = 0.17578125
c622-062: Epoch: 0, Step: 99, Rank: 55, loss = 6.973743438720703e-06
c622-052: Epoch: 0, Step: 99, Rank: 53, loss = 0.494140625
c621-151: Epoch: 0, Step: 99, Rank: 40, loss = 0.193359375
c622-071: Epoch: 0, Step: 99, Rank: 56, loss = 0.1455078125
c622-001: Epoch: 0, Step: 99, Rank: 42, loss = 0.201171875
c622-032: Epoch: 0, Step: 99, Rank: 49, loss = 0.02978515625
c619-021: Epoch: 0, Step: 99, Rank: 16, loss = 0.0810546875
c613-111: Epoch: 0, Step: 99, Rank: 2, loss = 0.255859375
c622-042: Epoch: 0, Step: 99, Rank: 51, loss = 0.69140625
c613-152: Epoch: 0, Step: 99, Rank: 11, loss = 0.1328125
c622-012: Epoch: 0, Step: 99, Rank: 45, loss = 0.1943359375
c613-102: Epoch: 0, Step: 99, Rank: 1, loss = 0.201171875
c621-081: Epoch: 0, Step: 99, Rank: 26, loss = 0.08642578125
c622-102: Epoch: 0, Step: 99, Rank: 63, loss = 0.1396484375
c621-072: Epoch: 0, Step: 99, Rank: 25, loss = 0.37890625
c613-151: Epoch: 0, Step: 99, Rank: 10, loss = 0.2353515625
c619-001: Epoch: 0, Step: 99, Rank: 12, loss = 0.1181640625
c621-061: Epoch: 0, Step: 99, Rank: 22, loss = 0.482421875
c613-122: Epoch: 0, Step: 99, Rank: 5, loss = 0.1640625
c621-121: Epoch: 0, Step: 99, Rank: 34, loss = 0.494140625
c621-131: Epoch: 0, Step: 99, Rank: 36, loss = 0.41015625
c622-022: Epoch: 0, Step: 99, Rank: 47, loss = 0.173828125
c613-121: Epoch: 0, Step: 99, Rank: 4, loss = 0.59375
c621-082: Epoch: 0, Step: 99, Rank: 27, loss = 0.228515625
c622-031: Epoch: 0, Step: 99, Rank: 48, loss = 0.0791015625
c619-032: Epoch: 0, Step: 99, Rank: 19, loss = 0.341796875
c622-101: Epoch: 0, Step: 99, Rank: 62, loss = 0.2314453125
c613-131: Epoch: 0, Step: 99, Rank: 6, loss = 0.17578125
c621-052: Epoch: 0, Step: 99, Rank: 21, loss = 0.396484375
c621-152: Epoch: 0, Step: 99, Rank: 41, loss = 0.142578125
c622-072: Epoch: 0, Step: 99, Rank: 57, loss = 0.34765625
c619-031: Epoch: 0, Step: 99, Rank: 18, loss = 0.0927734375
c613-112: Epoch: 0, Step: 99, Rank: 3, loss = 0.07666015625
c619-022: Epoch: 0, Step: 99, Rank: 17, loss = 0.275390625
c622-092: Epoch: 0, Step: 99, Rank: 61, loss = 0.14453125
c621-142: Epoch: 0, Step: 99, Rank: 39, loss = 0.69140625
c622-011: Epoch: 0, Step: 99, Rank: 44, loss = 0.2353515625
c613-132: Epoch: 0, Step: 99, Rank: 7, loss = 0.1064453125
c621-091: Epoch: 0, Step: 99, Rank: 28, loss = 0.1328125
c622-082: Epoch: 0, Step: 99, Rank: 59, loss = 0.314453125
c613-141: Epoch: 0, Step: 99, Rank: 8, loss = 0.1953125
c621-122: Epoch: 0, Step: 99, Rank: 35, loss = 0.2255859375
c621-132: Epoch: 0, Step: 99, Rank: 37, loss = 0.26171875
c619-041: Epoch: 0, Step: 99, Rank: 20, loss = 0.51171875
c622-061: Epoch: 0, Step: 99, Rank: 54, loss = 0.1396484375
c613-142: Epoch: 0, Step: 99, Rank: 9, loss = 0.00170135498046875
c621-141: Epoch: 0, Step: 99, Rank: 38, loss = 0.1669921875
c621-062: Epoch: 0, Step: 99, Rank: 23, loss = 0.220703125
c621-111: Epoch: 0, Step: 99, Rank: 32, loss = 0.01708984375
c621-071: Epoch: 0, Step: 99, Rank: 24, loss = 0.69140625
c621-112: Epoch: 0, Step: 99, Rank: 33, loss = 0.2255859375
c622-021: Epoch: 0, Step: 99, Rank: 46, loss = 0.0380859375
c619-012: Epoch: 0, Step: 99, Rank: 15, loss = 0.26171875
c622-091: Epoch: 0, Step: 99, Rank: 60, loss = 0.1669921875
c619-011: Epoch: 0, Step: 99, Rank: 14, loss = 0.1455078125
c621-102: Epoch: 0, Step: 99, Rank: 31, loss = 0.69140625
c621-101: Epoch: 0, Step: 99, Rank: 30, loss = 0.1904296875
c622-041: Epoch: 0, Step: 99, Rank: 50, loss = 0.2412109375
c621-092: Epoch: 0, Step: 99, Rank: 29, loss = 0.2177734375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-002: Epoch: 0, Step: 100, Rank: 13, loss = 0.1796875
c619-001: Epoch: 0, Step: 100, Rank: 12, loss = 0.345703125
c619-021: Epoch: 0, Step: 100, Rank: 16, loss = 0.36328125
c622-002: Epoch: 0, Step: 100, Rank: 43, loss = 0.091796875
c619-041: Epoch: 0, Step: 100, Rank: 20, loss = 1.1641532182693481e-10
c622-052: Epoch: 0, Step: 100, Rank: 53, loss = 0.00150299072265625
c621-151: Epoch: 0, Step: 100, Rank: 40, loss = 0.193359375
c613-101: Epoch: 0, Step: 100, Rank: 0, loss = 0.345703125
c622-012: Epoch: 0, Step: 100, Rank: 45, loss = 0.2119140625
c621-111: Epoch: 0, Step: 100, Rank: 32, loss = 0.240234375
c621-082: Epoch: 0, Step: 100, Rank: 27, loss = 0.0186767578125
c622-032: Epoch: 0, Step: 100, Rank: 49, loss = 0.69140625
c619-022: Epoch: 0, Step: 100, Rank: 17, loss = 0.10009765625
c619-011: Epoch: 0, Step: 100, Rank: 14, loss = 0.494140625
c619-031: Epoch: 0, Step: 100, Rank: 18, loss = 0.193359375
c613-151: Epoch: 0, Step: 100, Rank: 10, loss = 0.002471923828125
c613-152: Epoch: 0, Step: 100, Rank: 11, loss = 0.16015625
c621-052: Epoch: 0, Step: 100, Rank: 21, loss = 0.17578125
c622-001: Epoch: 0, Step: 100, Rank: 42, loss = 0.328125
c622-061: Epoch: 0, Step: 100, Rank: 54, loss = 0.1669921875
c621-072: Epoch: 0, Step: 100, Rank: 25, loss = 0.69140625
c621-081: Epoch: 0, Step: 100, Rank: 26, loss = 0.13671875
c619-032: Epoch: 0, Step: 100, Rank: 19, loss = 0.00180816650390625
c613-141: Epoch: 0, Step: 100, Rank: 8, loss = 0.095703125
c621-142: Epoch: 0, Step: 100, Rank: 39, loss = 0.328125
c613-142: Epoch: 0, Step: 100, Rank: 9, loss = 0.2099609375
c621-091: Epoch: 0, Step: 100, Rank: 28, loss = 0.2060546875
c621-152: Epoch: 0, Step: 100, Rank: 41, loss = 0.1533203125
c621-112: Epoch: 0, Step: 100, Rank: 33, loss = 0.02392578125
c613-131: Epoch: 0, Step: 100, Rank: 6, loss = 0.17578125
c621-122: Epoch: 0, Step: 100, Rank: 35, loss = 0.162109375
c622-031: Epoch: 0, Step: 100, Rank: 48, loss = 0.17578125
c613-132: Epoch: 0, Step: 100, Rank: 7, loss = 0.197265625
c613-122: Epoch: 0, Step: 100, Rank: 5, loss = 0.1796875
c621-121: Epoch: 0, Step: 100, Rank: 34, loss = 3.123283386230469e-05
c621-061: Epoch: 0, Step: 100, Rank: 22, loss = 0.1943359375
c622-101: Epoch: 0, Step: 100, Rank: 62, loss = 0.345703125
c613-121: Epoch: 0, Step: 100, Rank: 4, loss = 0.2353515625
c621-132: Epoch: 0, Step: 100, Rank: 37, loss = 0.1181640625
c613-111: Epoch: 0, Step: 100, Rank: 2, loss = 0.23828125
c622-022: Epoch: 0, Step: 100, Rank: 47, loss = 0.13671875
c621-101: Epoch: 0, Step: 100, Rank: 30, loss = 0.08642578125
c613-112: Epoch: 0, Step: 100, Rank: 3, loss = 0.3203125
c621-141: Epoch: 0, Step: 100, Rank: 38, loss = 0.38671875
c621-102: Epoch: 0, Step: 100, Rank: 31, loss = 0.35546875
c621-092: Epoch: 0, Step: 100, Rank: 29, loss = 0.4375
c619-012: Epoch: 0, Step: 100, Rank: 15, loss = 0.197265625
c621-131: Epoch: 0, Step: 100, Rank: 36, loss = 0.0791015625
c621-071: Epoch: 0, Step: 100, Rank: 24, loss = 0.37109375
c622-051: Epoch: 0, Step: 100, Rank: 52, loss = 0.1396484375
c622-011: Epoch: 0, Step: 100, Rank: 44, loss = 0.08251953125
c622-092: Epoch: 0, Step: 100, Rank: 61, loss = 0.30078125
c621-062: Epoch: 0, Step: 100, Rank: 23, loss = 0.18359375
c622-081: Epoch: 0, Step: 100, Rank: 58, loss = 0.228515625
c613-102: Epoch: 0, Step: 100, Rank: 1, loss = 0.16015625
c622-021: Epoch: 0, Step: 100, Rank: 46, loss = 0.26171875
c622-042: Epoch: 0, Step: 100, Rank: 51, loss = 0.1328125
c622-102: Epoch: 0, Step: 100, Rank: 63, loss = 0.091796875
c622-082: Epoch: 0, Step: 100, Rank: 59, loss = 6.628036499023438e-05
c622-091: Epoch: 0, Step: 100, Rank: 60, loss = 0.10791015625
c622-041: Epoch: 0, Step: 100, Rank: 50, loss = 0.023193359375
c622-071: Epoch: 0, Step: 100, Rank: 56, loss = 0.302734375
c622-072: Epoch: 0, Step: 100, Rank: 57, loss = 0.10498046875
c622-062: Epoch: 0, Step: 100, Rank: 55, loss = 0.30859375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 101, Rank: 16, loss = 0.05078125
c622-002: Epoch: 0, Step: 101, Rank: 43, loss = 0.0032806396484375
c613-101: Epoch: 0, Step: 101, Rank: 0, loss = 0.0198974609375
c621-081: Epoch: 0, Step: 101, Rank: 26, loss = 0.201171875
c621-091: Epoch: 0, Step: 101, Rank: 28, loss = 0.271484375
c621-111: Epoch: 0, Step: 101, Rank: 32, loss = 0.0849609375
c622-081: Epoch: 0, Step: 101, Rank: 58, loss = 0.06396484375
c619-022: Epoch: 0, Step: 101, Rank: 17, loss = 0.23046875
c621-072: Epoch: 0, Step: 101, Rank: 25, loss = 0.16015625
c622-071: Epoch: 0, Step: 101, Rank: 56, loss = 0.1328125
c613-132: Epoch: 0, Step: 101, Rank: 7, loss = 0.01556396484375
c619-002: Epoch: 0, Step: 101, Rank: 13, loss = 0.69140625
c622-012: Epoch: 0, Step: 101, Rank: 45, loss = 0.01177978515625
c622-092: Epoch: 0, Step: 101, Rank: 61, loss = 0.3203125
c621-082: Epoch: 0, Step: 101, Rank: 27, loss = 0.3203125
c622-052: Epoch: 0, Step: 101, Rank: 53, loss = 0.0751953125
c613-121: Epoch: 0, Step: 101, Rank: 4, loss = 0.1396484375
c619-001: Epoch: 0, Step: 101, Rank: 12, loss = 0.1396484375
c621-101: Epoch: 0, Step: 101, Rank: 30, loss = 0.1494140625
c613-142: Epoch: 0, Step: 101, Rank: 9, loss = 0.283203125
c613-151: Epoch: 0, Step: 101, Rank: 10, loss = 0.69140625
c613-111: Epoch: 0, Step: 101, Rank: 2, loss = 6.198883056640625e-05
c621-151: Epoch: 0, Step: 101, Rank: 40, loss = 0.059326171875
c622-051: Epoch: 0, Step: 101, Rank: 52, loss = 0.01507568359375
c619-012: Epoch: 0, Step: 101, Rank: 15, loss = 0.255859375
c613-152: Epoch: 0, Step: 101, Rank: 11, loss = 0.345703125
c613-102: Epoch: 0, Step: 101, Rank: 1, loss = 0.16015625
c622-032: Epoch: 0, Step: 101, Rank: 49, loss = 0.10498046875
c619-011: Epoch: 0, Step: 101, Rank: 14, loss = 0.059326171875
c619-041: Epoch: 0, Step: 101, Rank: 20, loss = 0.1669921875
c621-142: Epoch: 0, Step: 101, Rank: 39, loss = 0.12158203125
c622-062: Epoch: 0, Step: 101, Rank: 55, loss = 0.220703125
c621-131: Epoch: 0, Step: 101, Rank: 36, loss = 0.1572265625
c622-001: Epoch: 0, Step: 101, Rank: 42, loss = 0.1162109375
c619-031: Epoch: 0, Step: 101, Rank: 18, loss = 0.28515625
c622-082: Epoch: 0, Step: 101, Rank: 59, loss = 0.201171875
c622-041: Epoch: 0, Step: 101, Rank: 50, loss = 0.08642578125
c613-131: Epoch: 0, Step: 101, Rank: 6, loss = 0.000606536865234375
c621-141: Epoch: 0, Step: 101, Rank: 38, loss = 0.341796875
c621-092: Epoch: 0, Step: 101, Rank: 29, loss = 0.419921875
c613-141: Epoch: 0, Step: 101, Rank: 8, loss = 0.2353515625
c621-112: Epoch: 0, Step: 101, Rank: 33, loss = 0.37890625
c622-101: Epoch: 0, Step: 101, Rank: 62, loss = 0.1455078125
c622-072: Epoch: 0, Step: 101, Rank: 57, loss = 0.095703125
c613-122: Epoch: 0, Step: 101, Rank: 5, loss = 0.48828125
c621-061: Epoch: 0, Step: 101, Rank: 22, loss = 0.01214599609375
c622-022: Epoch: 0, Step: 101, Rank: 47, loss = 0.126953125
c621-102: Epoch: 0, Step: 101, Rank: 31, loss = 0.3828125
c621-071: Epoch: 0, Step: 101, Rank: 24, loss = 0.023193359375
c613-112: Epoch: 0, Step: 101, Rank: 3, loss = 0.69140625
c621-121: Epoch: 0, Step: 101, Rank: 34, loss = 0.06787109375
c621-052: Epoch: 0, Step: 101, Rank: 21, loss = 0.474609375
c622-061: Epoch: 0, Step: 101, Rank: 54, loss = 0.2099609375
c622-042: Epoch: 0, Step: 101, Rank: 51, loss = 0.04443359375
c622-011: Epoch: 0, Step: 101, Rank: 44, loss = 0.1494140625
c621-122: Epoch: 0, Step: 101, Rank: 35, loss = 0.330078125
c622-102: Epoch: 0, Step: 101, Rank: 63, loss = 0.212890625
c621-152: Epoch: 0, Step: 101, Rank: 41, loss = 0.251953125
c622-031: Epoch: 0, Step: 101, Rank: 48, loss = 0.2099609375
c622-091: Epoch: 0, Step: 101, Rank: 60, loss = 0.0010986328125
c621-062: Epoch: 0, Step: 101, Rank: 23, loss = 0.00299072265625
c622-021: Epoch: 0, Step: 101, Rank: 46, loss = 0.287109375
c619-032: Epoch: 0, Step: 101, Rank: 19, loss = 0.111328125
c621-132: Epoch: 0, Step: 101, Rank: 37, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 102, Rank: 16, loss = 0.69140625
c622-081: Epoch: 0, Step: 102, Rank: 58, loss = 0.419921875
c621-121: Epoch: 0, Step: 102, Rank: 34, loss = 0.365234375
c613-101: Epoch: 0, Step: 102, Rank: 0, loss = 0.12158203125
c622-052: Epoch: 0, Step: 102, Rank: 53, loss = 0.287109375
c622-062: Epoch: 0, Step: 102, Rank: 55, loss = 0.17578125
c622-002: Epoch: 0, Step: 102, Rank: 43, loss = 0.01416015625
c622-012: Epoch: 0, Step: 102, Rank: 45, loss = 0.1162109375
c621-122: Epoch: 0, Step: 102, Rank: 35, loss = 0.34765625
c619-031: Epoch: 0, Step: 102, Rank: 18, loss = 0.2099609375
c619-001: Epoch: 0, Step: 102, Rank: 12, loss = 0.2333984375
c622-082: Epoch: 0, Step: 102, Rank: 59, loss = 0.28125
c621-111: Epoch: 0, Step: 102, Rank: 32, loss = 0.158203125
c622-092: Epoch: 0, Step: 102, Rank: 61, loss = 0.2578125
c613-152: Epoch: 0, Step: 102, Rank: 11, loss = 0.1328125
c622-061: Epoch: 0, Step: 102, Rank: 54, loss = 0.25
c621-081: Epoch: 0, Step: 102, Rank: 26, loss = 0.248046875
c619-002: Epoch: 0, Step: 102, Rank: 13, loss = 0.2333984375
c622-001: Epoch: 0, Step: 102, Rank: 42, loss = 0.0017547607421875
c621-112: Epoch: 0, Step: 102, Rank: 33, loss = 0.0751953125
c619-041: Epoch: 0, Step: 102, Rank: 20, loss = 1.7762184143066406e-05
c619-011: Epoch: 0, Step: 102, Rank: 14, loss = 0.002471923828125
c621-151: Epoch: 0, Step: 102, Rank: 40, loss = 0.23046875
c622-101: Epoch: 0, Step: 102, Rank: 62, loss = 0.287109375
c621-052: Epoch: 0, Step: 102, Rank: 21, loss = 0.2255859375
c613-131: Epoch: 0, Step: 102, Rank: 6, loss = 0.1494140625
c622-051: Epoch: 0, Step: 102, Rank: 52, loss = 0.000518798828125
c622-011: Epoch: 0, Step: 102, Rank: 44, loss = 0.248046875
c621-131: Epoch: 0, Step: 102, Rank: 36, loss = 0.291015625
c613-122: Epoch: 0, Step: 102, Rank: 5, loss = 0.291015625
c621-061: Epoch: 0, Step: 102, Rank: 22, loss = 0.361328125
c619-012: Epoch: 0, Step: 102, Rank: 15, loss = 0.17578125
c613-132: Epoch: 0, Step: 102, Rank: 7, loss = 0.2431640625
c619-022: Epoch: 0, Step: 102, Rank: 17, loss = 0.1494140625
c621-141: Epoch: 0, Step: 102, Rank: 38, loss = 0.201171875
c621-101: Epoch: 0, Step: 102, Rank: 30, loss = 0.2265625
c622-022: Epoch: 0, Step: 102, Rank: 47, loss = 0.3125
c621-072: Epoch: 0, Step: 102, Rank: 25, loss = 0.359375
c613-142: Epoch: 0, Step: 102, Rank: 9, loss = 0.0017547607421875
c622-032: Epoch: 0, Step: 102, Rank: 49, loss = 0.31640625
c613-102: Epoch: 0, Step: 102, Rank: 1, loss = 0.37109375
c621-142: Epoch: 0, Step: 102, Rank: 39, loss = 0.10498046875
c622-072: Epoch: 0, Step: 102, Rank: 57, loss = 0.091796875
c613-151: Epoch: 0, Step: 102, Rank: 10, loss = 0.69140625
c613-111: Epoch: 0, Step: 102, Rank: 2, loss = 6.139278411865234e-06
c621-091: Epoch: 0, Step: 102, Rank: 28, loss = 0.1181640625
c621-132: Epoch: 0, Step: 102, Rank: 37, loss = 0.2099609375
c619-032: Epoch: 0, Step: 102, Rank: 19, loss = 0.5
c613-121: Epoch: 0, Step: 102, Rank: 4, loss = 0.021484375
c613-112: Epoch: 0, Step: 102, Rank: 3, loss = 0.2353515625
c622-042: Epoch: 0, Step: 102, Rank: 51, loss = 0.3125
c622-041: Epoch: 0, Step: 102, Rank: 50, loss = 0.34765625
c622-091: Epoch: 0, Step: 102, Rank: 60, loss = 0.28515625
c621-092: Epoch: 0, Step: 102, Rank: 29, loss = 0.1494140625
c621-082: Epoch: 0, Step: 102, Rank: 27, loss = 0.00083160400390625
c621-102: Epoch: 0, Step: 102, Rank: 31, loss = 0.26171875
c622-031: Epoch: 0, Step: 102, Rank: 48, loss = 0.29296875
c621-062: Epoch: 0, Step: 102, Rank: 23, loss = 0.1328125
c621-071: Epoch: 0, Step: 102, Rank: 24, loss = 0.275390625
c622-021: Epoch: 0, Step: 102, Rank: 46, loss = 0.296875
c621-152: Epoch: 0, Step: 102, Rank: 41, loss = 0.30859375
c613-141: Epoch: 0, Step: 102, Rank: 8, loss = 0.2578125
c622-071: Epoch: 0, Step: 102, Rank: 56, loss = 0.126953125
c622-102: Epoch: 0, Step: 102, Rank: 63, loss = 0.109375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 103, Rank: 43, loss = 0.000431060791015625
c621-091: Epoch: 0, Step: 103, Rank: 28, loss = 0.208984375
c613-101: Epoch: 0, Step: 103, Rank: 0, loss = 0.00193023681640625
c622-081: Epoch: 0, Step: 103, Rank: 58, loss = 0.28125
c619-021: Epoch: 0, Step: 103, Rank: 16, loss = 0.0091552734375
c621-081: Epoch: 0, Step: 103, Rank: 26, loss = 0.2470703125
c622-012: Epoch: 0, Step: 103, Rank: 45, loss = 0.18359375
c619-002: Epoch: 0, Step: 103, Rank: 13, loss = 0.40234375
c621-151: Epoch: 0, Step: 103, Rank: 40, loss = 0.1328125
c622-001: Epoch: 0, Step: 103, Rank: 42, loss = 0.201171875
c619-011: Epoch: 0, Step: 103, Rank: 14, loss = 0.095703125
c619-032: Epoch: 0, Step: 103, Rank: 19, loss = 0.080078125
c621-072: Epoch: 0, Step: 103, Rank: 25, loss = 0.095703125
c621-131: Epoch: 0, Step: 103, Rank: 36, loss = 0.0242919921875
c621-082: Epoch: 0, Step: 103, Rank: 27, loss = 0.201171875
c622-051: Epoch: 0, Step: 103, Rank: 52, loss = 0.10302734375
c619-001: Epoch: 0, Step: 103, Rank: 12, loss = 0.00116729736328125
c619-031: Epoch: 0, Step: 103, Rank: 18, loss = 0.419921875
c622-062: Epoch: 0, Step: 103, Rank: 55, loss = 0.0732421875
c621-052: Epoch: 0, Step: 103, Rank: 21, loss = 0.2099609375
c613-151: Epoch: 0, Step: 103, Rank: 10, loss = 0.00811767578125
c619-041: Epoch: 0, Step: 103, Rank: 20, loss = 0.06201171875
c613-152: Epoch: 0, Step: 103, Rank: 11, loss = 0.1845703125
c619-022: Epoch: 0, Step: 103, Rank: 17, loss = 0.1162109375
c621-071: Epoch: 0, Step: 103, Rank: 24, loss = 0.34765625
c621-122: Epoch: 0, Step: 103, Rank: 35, loss = 0.400390625
c621-111: Epoch: 0, Step: 103, Rank: 32, loss = 0.2353515625
c621-061: Epoch: 0, Step: 103, Rank: 22, loss = 0.333984375
c621-132: Epoch: 0, Step: 103, Rank: 37, loss = 0.1796875
c613-121: Epoch: 0, Step: 103, Rank: 4, loss = 0.1904296875
c622-032: Epoch: 0, Step: 103, Rank: 49, loss = 0.11279296875
c621-152: Epoch: 0, Step: 103, Rank: 41, loss = 0.09716796875
c621-121: Epoch: 0, Step: 103, Rank: 34, loss = 0.041748046875
c622-052: Epoch: 0, Step: 103, Rank: 53, loss = 0.0791015625
c622-041: Epoch: 0, Step: 103, Rank: 50, loss = 0.220703125
c613-132: Epoch: 0, Step: 103, Rank: 7, loss = 0.30078125
c613-111: Epoch: 0, Step: 103, Rank: 2, loss = 0.3671875
c622-092: Epoch: 0, Step: 103, Rank: 61, loss = 0.2578125
c613-131: Epoch: 0, Step: 103, Rank: 6, loss = 0.039794921875
c621-142: Epoch: 0, Step: 103, Rank: 39, loss = 0.2470703125
c613-102: Epoch: 0, Step: 103, Rank: 1, loss = 0.328125
c622-101: Epoch: 0, Step: 103, Rank: 62, loss = 0.26953125
c621-141: Epoch: 0, Step: 103, Rank: 38, loss = 0.16015625
c622-061: Epoch: 0, Step: 103, Rank: 54, loss = 0.193359375
c621-101: Epoch: 0, Step: 103, Rank: 30, loss = 0.22265625
c622-082: Epoch: 0, Step: 103, Rank: 59, loss = 0.1669921875
c621-062: Epoch: 0, Step: 103, Rank: 23, loss = 0.2158203125
c621-112: Epoch: 0, Step: 103, Rank: 33, loss = 0.22265625
c613-141: Epoch: 0, Step: 103, Rank: 8, loss = 0.2177734375
c622-011: Epoch: 0, Step: 103, Rank: 44, loss = 0.35546875
c613-142: Epoch: 0, Step: 103, Rank: 9, loss = 0.2578125
c619-012: Epoch: 0, Step: 103, Rank: 15, loss = 0.1982421875
c613-122: Epoch: 0, Step: 103, Rank: 5, loss = 0.12158203125
c622-042: Epoch: 0, Step: 103, Rank: 51, loss = 0.11279296875
c622-072: Epoch: 0, Step: 103, Rank: 57, loss = 0.193359375
c622-022: Epoch: 0, Step: 103, Rank: 47, loss = 0.18359375
c621-102: Epoch: 0, Step: 103, Rank: 31, loss = 0.1162109375
c622-091: Epoch: 0, Step: 103, Rank: 60, loss = 0.04443359375
c622-031: Epoch: 0, Step: 103, Rank: 48, loss = 0.0003681182861328125
c622-021: Epoch: 0, Step: 103, Rank: 46, loss = 0.69140625
c613-112: Epoch: 0, Step: 103, Rank: 3, loss = 0.2353515625
c622-071: Epoch: 0, Step: 103, Rank: 56, loss = 0.1494140625
c621-092: Epoch: 0, Step: 103, Rank: 29, loss = 0.23046875
c622-102: Epoch: 0, Step: 103, Rank: 63, loss = 0.462890625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 104, Rank: 0, loss = 0.31640625
c622-052: Epoch: 0, Step: 104, Rank: 53, loss = 0.173828125
c622-001: Epoch: 0, Step: 104, Rank: 42, loss = 0.03515625
c621-091: Epoch: 0, Step: 104, Rank: 28, loss = 0.0181884765625
c622-081: Epoch: 0, Step: 104, Rank: 58, loss = 0.01904296875
c622-002: Epoch: 0, Step: 104, Rank: 43, loss = 0.08642578125
c621-151: Epoch: 0, Step: 104, Rank: 40, loss = 0.69140625
c622-012: Epoch: 0, Step: 104, Rank: 45, loss = 0.291015625
c622-062: Epoch: 0, Step: 104, Rank: 55, loss = 0.126953125
c619-001: Epoch: 0, Step: 104, Rank: 12, loss = 0.1669921875
c613-111: Epoch: 0, Step: 104, Rank: 2, loss = 0.3203125
c621-082: Epoch: 0, Step: 104, Rank: 27, loss = 0.056640625
c613-132: Epoch: 0, Step: 104, Rank: 7, loss = 0.197265625
c622-102: Epoch: 0, Step: 104, Rank: 63, loss = 0.1640625
c613-152: Epoch: 0, Step: 104, Rank: 11, loss = 0.16015625
c621-142: Epoch: 0, Step: 104, Rank: 39, loss = 0.00811767578125
c619-021: Epoch: 0, Step: 104, Rank: 16, loss = 0.142578125
c621-101: Epoch: 0, Step: 104, Rank: 30, loss = 0.027099609375
c613-122: Epoch: 0, Step: 104, Rank: 5, loss = 0.1181640625
c621-132: Epoch: 0, Step: 104, Rank: 37, loss = 0.2099609375
c622-032: Epoch: 0, Step: 104, Rank: 49, loss = 0.01177978515625
c622-092: Epoch: 0, Step: 104, Rank: 61, loss = 0.17578125
c613-131: Epoch: 0, Step: 104, Rank: 6, loss = 0.095703125
c622-031: Epoch: 0, Step: 104, Rank: 48, loss = 0.154296875
c613-151: Epoch: 0, Step: 104, Rank: 10, loss = 0.1376953125
c621-131: Epoch: 0, Step: 104, Rank: 36, loss = 0.2255859375
c621-121: Epoch: 0, Step: 104, Rank: 34, loss = 0.328125
c621-141: Epoch: 0, Step: 104, Rank: 38, loss = 0.1328125
c622-082: Epoch: 0, Step: 104, Rank: 59, loss = 0.375
c621-111: Epoch: 0, Step: 104, Rank: 32, loss = 0.1328125
c621-081: Epoch: 0, Step: 104, Rank: 26, loss = 0.0888671875
c613-142: Epoch: 0, Step: 104, Rank: 9, loss = 0.025146484375
c619-002: Epoch: 0, Step: 104, Rank: 13, loss = 0.140625
c622-022: Epoch: 0, Step: 104, Rank: 47, loss = 0.29296875
c621-122: Epoch: 0, Step: 104, Rank: 35, loss = 0.42578125
c621-112: Epoch: 0, Step: 104, Rank: 33, loss = 0.10009765625
c622-101: Epoch: 0, Step: 104, Rank: 62, loss = 0.00180816650390625
c622-041: Epoch: 0, Step: 104, Rank: 50, loss = 0.166015625
c613-121: Epoch: 0, Step: 104, Rank: 4, loss = 0.2158203125
c613-102: Epoch: 0, Step: 104, Rank: 1, loss = 0.18359375
c622-051: Epoch: 0, Step: 104, Rank: 52, loss = 0.41796875
c619-011: Epoch: 0, Step: 104, Rank: 14, loss = 0.18359375
c622-011: Epoch: 0, Step: 104, Rank: 44, loss = 0.1396484375
c613-141: Epoch: 0, Step: 104, Rank: 8, loss = 0.388671875
c621-102: Epoch: 0, Step: 104, Rank: 31, loss = 0.46484375
c621-072: Epoch: 0, Step: 104, Rank: 25, loss = 0.0791015625
c621-092: Epoch: 0, Step: 104, Rank: 29, loss = 0.330078125
c622-042: Epoch: 0, Step: 104, Rank: 51, loss = 0.341796875
c619-041: Epoch: 0, Step: 104, Rank: 20, loss = 0.2060546875
c613-112: Epoch: 0, Step: 104, Rank: 3, loss = 0.69140625
c619-022: Epoch: 0, Step: 104, Rank: 17, loss = 0.69140625
c622-061: Epoch: 0, Step: 104, Rank: 54, loss = 0.2158203125
c622-091: Epoch: 0, Step: 104, Rank: 60, loss = 2.2649765014648438e-06
c621-052: Epoch: 0, Step: 104, Rank: 21, loss = 0.056640625
c622-072: Epoch: 0, Step: 104, Rank: 57, loss = 0.1669921875
c619-031: Epoch: 0, Step: 104, Rank: 18, loss = 0.5078125
c621-061: Epoch: 0, Step: 104, Rank: 22, loss = 0.2001953125
c619-012: Epoch: 0, Step: 104, Rank: 15, loss = 0.208984375
c619-032: Epoch: 0, Step: 104, Rank: 19, loss = 0.002471923828125
c621-062: Epoch: 0, Step: 104, Rank: 23, loss = 0.1904296875
c622-021: Epoch: 0, Step: 104, Rank: 46, loss = 0.07666015625
c622-071: Epoch: 0, Step: 104, Rank: 56, loss = 0.036376953125
c621-152: Epoch: 0, Step: 104, Rank: 41, loss = 0.2470703125
c621-071: Epoch: 0, Step: 104, Rank: 24, loss = 0.13671875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.45s, TFLOPs: 0.77, Samples/sec: 0.41, Time/seq 2.45s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 105, Rank: 58, loss = 0.26171875
c613-101: Epoch: 0, Step: 105, Rank: 0, loss = 0.004486083984375
c621-091: Epoch: 0, Step: 105, Rank: 28, loss = 0.027099609375
c619-021: Epoch: 0, Step: 105, Rank: 16, loss = 0.69140625
c622-002: Epoch: 0, Step: 105, Rank: 43, loss = 0.35546875
c619-031: Epoch: 0, Step: 105, Rank: 18, loss = 0.228515625
c622-062: Epoch: 0, Step: 105, Rank: 55, loss = 0.1640625
c622-001: Epoch: 0, Step: 105, Rank: 42, loss = 0.1455078125
c621-101: Epoch: 0, Step: 105, Rank: 30, loss = 0.06201171875
c621-151: Epoch: 0, Step: 105, Rank: 40, loss = 0.306640625
c621-111: Epoch: 0, Step: 105, Rank: 32, loss = 0.2412109375
c621-121: Epoch: 0, Step: 105, Rank: 34, loss = 0.1455078125
c621-132: Epoch: 0, Step: 105, Rank: 37, loss = 0.2412109375
c619-032: Epoch: 0, Step: 105, Rank: 19, loss = 5.699694156646729e-07
c619-002: Epoch: 0, Step: 105, Rank: 13, loss = 0.2099609375
c621-081: Epoch: 0, Step: 105, Rank: 26, loss = 0.30078125
c622-061: Epoch: 0, Step: 105, Rank: 54, loss = 0.10498046875
c613-151: Epoch: 0, Step: 105, Rank: 10, loss = 0.1875
c613-112: Epoch: 0, Step: 105, Rank: 3, loss = 0.212890625
c622-092: Epoch: 0, Step: 105, Rank: 61, loss = 0.1669921875
c622-012: Epoch: 0, Step: 105, Rank: 45, loss = 0.091796875
c621-092: Epoch: 0, Step: 105, Rank: 29, loss = 0.447265625
c613-132: Epoch: 0, Step: 105, Rank: 7, loss = 6.628036499023438e-05
c621-131: Epoch: 0, Step: 105, Rank: 36, loss = 0.1826171875
c619-001: Epoch: 0, Step: 105, Rank: 12, loss = 0.3125
c622-102: Epoch: 0, Step: 105, Rank: 63, loss = 0.2451171875
c621-072: Epoch: 0, Step: 105, Rank: 25, loss = 0.46484375
c622-052: Epoch: 0, Step: 105, Rank: 53, loss = 0.171875
c613-142: Epoch: 0, Step: 105, Rank: 9, loss = 0.171875
c613-152: Epoch: 0, Step: 105, Rank: 11, loss = 0.03515625
c621-082: Epoch: 0, Step: 105, Rank: 27, loss = 0.2412109375
c621-141: Epoch: 0, Step: 105, Rank: 38, loss = 0.2099609375
c622-032: Epoch: 0, Step: 105, Rank: 49, loss = 0.1162109375
c622-051: Epoch: 0, Step: 105, Rank: 52, loss = 0.00099945068359375
c621-142: Epoch: 0, Step: 105, Rank: 39, loss = 0.10009765625
c619-041: Epoch: 0, Step: 105, Rank: 20, loss = 0.1865234375
c622-072: Epoch: 0, Step: 105, Rank: 57, loss = 0.1669921875
c622-011: Epoch: 0, Step: 105, Rank: 44, loss = 0.126953125
c619-011: Epoch: 0, Step: 105, Rank: 14, loss = 0.17578125
c621-152: Epoch: 0, Step: 105, Rank: 41, loss = 0.126953125
c621-122: Epoch: 0, Step: 105, Rank: 35, loss = 0.32421875
c619-022: Epoch: 0, Step: 105, Rank: 17, loss = 0.69140625
c622-101: Epoch: 0, Step: 105, Rank: 62, loss = 0.279296875
c613-131: Epoch: 0, Step: 105, Rank: 6, loss = 0.26171875
c613-102: Epoch: 0, Step: 105, Rank: 1, loss = 0.375
c613-121: Epoch: 0, Step: 105, Rank: 4, loss = 0.00738525390625
c621-061: Epoch: 0, Step: 105, Rank: 22, loss = 0.43359375
c622-082: Epoch: 0, Step: 105, Rank: 59, loss = 0.11962890625
c619-012: Epoch: 0, Step: 105, Rank: 15, loss = 0.5546875
c621-052: Epoch: 0, Step: 105, Rank: 21, loss = 0.69140625
c613-141: Epoch: 0, Step: 105, Rank: 8, loss = 0.0181884765625
c621-071: Epoch: 0, Step: 105, Rank: 24, loss = 0.71875
c622-041: Epoch: 0, Step: 105, Rank: 50, loss = 0.208984375
c622-091: Epoch: 0, Step: 105, Rank: 60, loss = 0.228515625
c622-022: Epoch: 0, Step: 105, Rank: 47, loss = 0.17578125
c621-112: Epoch: 0, Step: 105, Rank: 33, loss = 0.123046875
c621-102: Epoch: 0, Step: 105, Rank: 31, loss = 0.01904296875
c622-042: Epoch: 0, Step: 105, Rank: 51, loss = 0.003387451171875
c622-021: Epoch: 0, Step: 105, Rank: 46, loss = 0.28125
c622-071: Epoch: 0, Step: 105, Rank: 56, loss = 0.1455078125
c613-122: Epoch: 0, Step: 105, Rank: 5, loss = 0.01214599609375
c621-062: Epoch: 0, Step: 105, Rank: 23, loss = 0.1640625
c613-111: Epoch: 0, Step: 105, Rank: 2, loss = 0.10498046875
c622-031: Epoch: 0, Step: 105, Rank: 48, loss = 0.0038299560546875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 106, Rank: 43, loss = 0.01904296875
c622-001: Epoch: 0, Step: 106, Rank: 42, loss = 0.3515625
c621-151: Epoch: 0, Step: 106, Rank: 40, loss = 0.23046875
c621-152: Epoch: 0, Step: 106, Rank: 41, loss = 0.2255859375
c621-142: Epoch: 0, Step: 106, Rank: 39, loss = 0.2099609375
c622-011: Epoch: 0, Step: 106, Rank: 44, loss = 0.275390625
c621-131: Epoch: 0, Step: 106, Rank: 36, loss = 0.328125
c621-132: Epoch: 0, Step: 106, Rank: 37, loss = 0.1455078125
c622-012: Epoch: 0, Step: 106, Rank: 45, loss = 0.1767578125
c621-141: Epoch: 0, Step: 106, Rank: 38, loss = 0.37890625
c622-022: Epoch: 0, Step: 106, Rank: 47, loss = 0.73828125
c622-032: Epoch: 0, Step: 106, Rank: 49, loss = 0.1669921875
c621-111: Epoch: 0, Step: 106, Rank: 32, loss = 0.197265625
c622-031: Epoch: 0, Step: 106, Rank: 48, loss = 0.0322265625
c622-041: Epoch: 0, Step: 106, Rank: 50, loss = 0.000457763671875
c622-021: Epoch: 0, Step: 106, Rank: 46, loss = 0.12890625
c622-052: Epoch: 0, Step: 106, Rank: 53, loss = 0.67578125
c621-121: Epoch: 0, Step: 106, Rank: 34, loss = 0.1669921875
c621-112: Epoch: 0, Step: 106, Rank: 33, loss = 0.171875
c622-051: Epoch: 0, Step: 106, Rank: 52, loss = 0.357421875
c622-042: Epoch: 0, Step: 106, Rank: 51, loss = 0.1494140625
c613-101: Epoch: 0, Step: 106, Rank: 0, loss = 0.3515625
c619-021: Epoch: 0, Step: 106, Rank: 16, loss = 0.16015625
c621-122: Epoch: 0, Step: 106, Rank: 35, loss = 0.220703125
c613-151: Epoch: 0, Step: 106, Rank: 10, loss = 0.08642578125
c621-091: Epoch: 0, Step: 106, Rank: 28, loss = 0.2060546875
c619-001: Epoch: 0, Step: 106, Rank: 12, loss = 0.16015625
c621-081: Epoch: 0, Step: 106, Rank: 26, loss = 0.474609375
c622-061: Epoch: 0, Step: 106, Rank: 54, loss = 0.1181640625
c613-132: Epoch: 0, Step: 106, Rank: 7, loss = 0.0751953125
c619-002: Epoch: 0, Step: 106, Rank: 13, loss = 0.18359375
c621-102: Epoch: 0, Step: 106, Rank: 31, loss = 0.173828125
c613-152: Epoch: 0, Step: 106, Rank: 11, loss = 0.06591796875
c621-072: Epoch: 0, Step: 106, Rank: 25, loss = 0.1396484375
c621-082: Epoch: 0, Step: 106, Rank: 27, loss = 0.17578125
c619-031: Epoch: 0, Step: 106, Rank: 18, loss = 0.220703125
c619-012: Epoch: 0, Step: 106, Rank: 15, loss = 0.26171875
c622-071: Epoch: 0, Step: 106, Rank: 56, loss = 0.01141357421875
c622-101: Epoch: 0, Step: 106, Rank: 62, loss = 0.000431060791015625
c613-142: Epoch: 0, Step: 106, Rank: 9, loss = 0.0311279296875
c619-032: Epoch: 0, Step: 106, Rank: 19, loss = 0.18359375
c619-022: Epoch: 0, Step: 106, Rank: 17, loss = 0.2578125
c621-052: Epoch: 0, Step: 106, Rank: 21, loss = 0.69140625
c621-061: Epoch: 0, Step: 106, Rank: 22, loss = 0.0322265625
c621-101: Epoch: 0, Step: 106, Rank: 30, loss = 0.109375
c613-131: Epoch: 0, Step: 106, Rank: 6, loss = 0.109375
c619-041: Epoch: 0, Step: 106, Rank: 20, loss = 0.0181884765625
c613-141: Epoch: 0, Step: 106, Rank: 8, loss = 0.18359375
c613-112: Epoch: 0, Step: 106, Rank: 3, loss = 0.1943359375
c619-011: Epoch: 0, Step: 106, Rank: 14, loss = 0.10498046875
c613-111: Epoch: 0, Step: 106, Rank: 2, loss = 0.2060546875
c622-092: Epoch: 0, Step: 106, Rank: 61, loss = 0.11279296875
c613-122: Epoch: 0, Step: 106, Rank: 5, loss = 0.69140625
c622-062: Epoch: 0, Step: 106, Rank: 55, loss = 0.2001953125
c622-082: Epoch: 0, Step: 106, Rank: 59, loss = 0.36328125
c621-062: Epoch: 0, Step: 106, Rank: 23, loss = 0.00040435791015625
c622-081: Epoch: 0, Step: 106, Rank: 58, loss = 0.2001953125
c613-102: Epoch: 0, Step: 106, Rank: 1, loss = 0.0057373046875
c621-071: Epoch: 0, Step: 106, Rank: 24, loss = 0.08642578125
c621-092: Epoch: 0, Step: 106, Rank: 29, loss = 0.193359375
c622-072: Epoch: 0, Step: 106, Rank: 57, loss = 0.091796875
c622-091: Epoch: 0, Step: 106, Rank: 60, loss = 0.1669921875
c622-102: Epoch: 0, Step: 106, Rank: 63, loss = 0.287109375
c613-121: Epoch: 0, Step: 106, Rank: 4, loss = 0.10498046875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.14s, TFLOPs: 0.88, Samples/sec: 0.47, Time/seq 2.14s, Batch Size: 1, Sequence Length: 2048
c622-051: Epoch: 0, Step: 107, Rank: 52, loss = 1.3828277587890625e-05
c622-052: Epoch: 0, Step: 107, Rank: 53, loss = 0.125
c622-032: Epoch: 0, Step: 107, Rank: 49, loss = 0.1640625
c622-041: Epoch: 0, Step: 107, Rank: 50, loss = 0.06201171875
c619-021: Epoch: 0, Step: 107, Rank: 16, loss = 0.69140625
c622-042: Epoch: 0, Step: 107, Rank: 51, loss = 0.296875
c622-031: Epoch: 0, Step: 107, Rank: 48, loss = 0.333984375
c622-022: Epoch: 0, Step: 107, Rank: 47, loss = 0.0007781982421875
c619-002: Epoch: 0, Step: 107, Rank: 13, loss = 0.08251953125
c619-001: Epoch: 0, Step: 107, Rank: 12, loss = 0.1494140625
c622-061: Epoch: 0, Step: 107, Rank: 54, loss = 0.1318359375
c613-101: Epoch: 0, Step: 107, Rank: 0, loss = 0.26953125
c613-132: Epoch: 0, Step: 107, Rank: 7, loss = 0.23046875
c622-012: Epoch: 0, Step: 107, Rank: 45, loss = 0.197265625
c621-132: Epoch: 0, Step: 107, Rank: 37, loss = 0.06689453125
c622-001: Epoch: 0, Step: 107, Rank: 42, loss = 0.23046875
c622-081: Epoch: 0, Step: 107, Rank: 58, loss = 0.1162109375
c621-151: Epoch: 0, Step: 107, Rank: 40, loss = 0.27734375
c621-142: Epoch: 0, Step: 107, Rank: 39, loss = 0.26171875
c613-141: Epoch: 0, Step: 107, Rank: 8, loss = 0.1875
c621-111: Epoch: 0, Step: 107, Rank: 32, loss = 0.0849609375
c621-091: Epoch: 0, Step: 107, Rank: 28, loss = 0.296875
c621-082: Epoch: 0, Step: 107, Rank: 27, loss = 0.26953125
c619-031: Epoch: 0, Step: 107, Rank: 18, loss = 0.302734375
c621-081: Epoch: 0, Step: 107, Rank: 26, loss = 0.18359375
c622-002: Epoch: 0, Step: 107, Rank: 43, loss = 0.69140625
c619-022: Epoch: 0, Step: 107, Rank: 17, loss = 0.1669921875
c613-102: Epoch: 0, Step: 107, Rank: 1, loss = 0.38671875
c619-011: Epoch: 0, Step: 107, Rank: 14, loss = 0.1669921875
c613-131: Epoch: 0, Step: 107, Rank: 6, loss = 0.1455078125
c613-151: Epoch: 0, Step: 107, Rank: 10, loss = 0.228515625
c613-112: Epoch: 0, Step: 107, Rank: 3, loss = 0.50390625
c619-012: Epoch: 0, Step: 107, Rank: 15, loss = 0.109375
c619-032: Epoch: 0, Step: 107, Rank: 19, loss = 0.11279296875
c613-152: Epoch: 0, Step: 107, Rank: 11, loss = 0.2578125
c613-122: Epoch: 0, Step: 107, Rank: 5, loss = 0.287109375
c621-141: Epoch: 0, Step: 107, Rank: 38, loss = 0.4140625
c619-041: Epoch: 0, Step: 107, Rank: 20, loss = 0.69140625
c621-152: Epoch: 0, Step: 107, Rank: 41, loss = 0.00021648406982421875
c622-021: Epoch: 0, Step: 107, Rank: 46, loss = 0.341796875
c613-121: Epoch: 0, Step: 107, Rank: 4, loss = 0.23828125
c613-111: Epoch: 0, Step: 107, Rank: 2, loss = 0.109375
c621-102: Epoch: 0, Step: 107, Rank: 31, loss = 0.03271484375
c621-121: Epoch: 0, Step: 107, Rank: 34, loss = 0.1396484375
c622-092: Epoch: 0, Step: 107, Rank: 61, loss = 0.08642578125
c621-072: Epoch: 0, Step: 107, Rank: 25, loss = 0.5
c622-091: Epoch: 0, Step: 107, Rank: 60, loss = 0.01177978515625
c621-061: Epoch: 0, Step: 107, Rank: 22, loss = 0.00014019012451171875
c613-142: Epoch: 0, Step: 107, Rank: 9, loss = 0.16015625
c621-112: Epoch: 0, Step: 107, Rank: 33, loss = 0.00014019012451171875
c622-101: Epoch: 0, Step: 107, Rank: 62, loss = 0.1064453125
c621-122: Epoch: 0, Step: 107, Rank: 35, loss = 0.18359375
c622-011: Epoch: 0, Step: 107, Rank: 44, loss = 0.314453125
c621-092: Epoch: 0, Step: 107, Rank: 29, loss = 0.1875
c622-071: Epoch: 0, Step: 107, Rank: 56, loss = 0.26171875
c621-131: Epoch: 0, Step: 107, Rank: 36, loss = 0.173828125
c621-101: Epoch: 0, Step: 107, Rank: 30, loss = 0.10498046875
c622-072: Epoch: 0, Step: 107, Rank: 57, loss = 0.10009765625
c622-102: Epoch: 0, Step: 107, Rank: 63, loss = 0.1162109375
c622-082: Epoch: 0, Step: 107, Rank: 59, loss = 0.1953125
c622-062: Epoch: 0, Step: 107, Rank: 55, loss = 0.251953125
c621-052: Epoch: 0, Step: 107, Rank: 21, loss = 2.753734588623047e-05
c621-071: Epoch: 0, Step: 107, Rank: 24, loss = 0.17578125
c621-062: Epoch: 0, Step: 107, Rank: 23, loss = 0.048583984375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.234375 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 108, Rank: 16, loss = 0.13671875
c613-101: Epoch: 0, Step: 108, Rank: 0, loss = 0.0242919921875
c622-002: Epoch: 0, Step: 108, Rank: 43, loss = 0.002471923828125
c619-032: Epoch: 0, Step: 108, Rank: 19, loss = 0.041015625
c619-001: Epoch: 0, Step: 108, Rank: 12, loss = 0.197265625
c619-002: Epoch: 0, Step: 108, Rank: 13, loss = 0.330078125
c619-022: Epoch: 0, Step: 108, Rank: 17, loss = 0.25390625
c613-131: Epoch: 0, Step: 108, Rank: 6, loss = 0.69140625
c622-012: Epoch: 0, Step: 108, Rank: 45, loss = 0.2041015625
c613-112: Epoch: 0, Step: 108, Rank: 3, loss = 0.0133056640625
c613-151: Epoch: 0, Step: 108, Rank: 10, loss = 0.69140625
c622-052: Epoch: 0, Step: 108, Rank: 53, loss = 0.259765625
c613-152: Epoch: 0, Step: 108, Rank: 11, loss = 0.29296875
c613-122: Epoch: 0, Step: 108, Rank: 5, loss = 0.1796875
c613-111: Epoch: 0, Step: 108, Rank: 2, loss = 0.0284423828125
c619-031: Epoch: 0, Step: 108, Rank: 18, loss = 0.69140625
c621-111: Epoch: 0, Step: 108, Rank: 32, loss = 0.390625
c622-081: Epoch: 0, Step: 108, Rank: 58, loss = 0.04443359375
c621-121: Epoch: 0, Step: 108, Rank: 34, loss = 0.162109375
c619-012: Epoch: 0, Step: 108, Rank: 15, loss = 0.171875
c622-101: Epoch: 0, Step: 108, Rank: 62, loss = 0.17578125
c621-081: Epoch: 0, Step: 108, Rank: 26, loss = 0.1845703125
c613-102: Epoch: 0, Step: 108, Rank: 1, loss = 0.373046875
c613-132: Epoch: 0, Step: 108, Rank: 7, loss = 0.220703125
c621-132: Epoch: 0, Step: 108, Rank: 37, loss = 0.00016880035400390625
c622-102: Epoch: 0, Step: 108, Rank: 63, loss = 0.197265625
c619-011: Epoch: 0, Step: 108, Rank: 14, loss = 0.006103515625
c622-051: Epoch: 0, Step: 108, Rank: 52, loss = 0.1533203125
c621-082: Epoch: 0, Step: 108, Rank: 27, loss = 0.390625
c622-092: Epoch: 0, Step: 108, Rank: 61, loss = 0.009765625
c613-141: Epoch: 0, Step: 108, Rank: 8, loss = 0.36328125
c621-142: Epoch: 0, Step: 108, Rank: 39, loss = 0.2099609375
c613-121: Epoch: 0, Step: 108, Rank: 4, loss = 0.091796875
c621-101: Epoch: 0, Step: 108, Rank: 30, loss = 0.022216796875
c622-022: Epoch: 0, Step: 108, Rank: 47, loss = 0.26953125
c621-091: Epoch: 0, Step: 108, Rank: 28, loss = 0.0007781982421875
c621-112: Epoch: 0, Step: 108, Rank: 33, loss = 0.330078125
c621-151: Epoch: 0, Step: 108, Rank: 40, loss = 0.220703125
c621-131: Epoch: 0, Step: 108, Rank: 36, loss = 0.2412109375
c621-072: Epoch: 0, Step: 108, Rank: 25, loss = 0.0751953125
c622-032: Epoch: 0, Step: 108, Rank: 49, loss = 0.0181884765625
c622-061: Epoch: 0, Step: 108, Rank: 54, loss = 0.00150299072265625
c621-102: Epoch: 0, Step: 108, Rank: 31, loss = 0.296875
c622-091: Epoch: 0, Step: 108, Rank: 60, loss = 0.392578125
c622-031: Epoch: 0, Step: 108, Rank: 48, loss = 0.10498046875
c622-001: Epoch: 0, Step: 108, Rank: 42, loss = 0.27734375
c622-041: Epoch: 0, Step: 108, Rank: 50, loss = 0.10009765625
c621-122: Epoch: 0, Step: 108, Rank: 35, loss = 0.01214599609375
c621-141: Epoch: 0, Step: 108, Rank: 38, loss = 0.041015625
c622-042: Epoch: 0, Step: 108, Rank: 51, loss = 0.1455078125
c622-011: Epoch: 0, Step: 108, Rank: 44, loss = 0.337890625
c622-082: Epoch: 0, Step: 108, Rank: 59, loss = 0.171875
c622-072: Epoch: 0, Step: 108, Rank: 57, loss = 0.162109375
c621-061: Epoch: 0, Step: 108, Rank: 22, loss = 0.18359375
c619-041: Epoch: 0, Step: 108, Rank: 20, loss = 0.0028076171875
c621-152: Epoch: 0, Step: 108, Rank: 41, loss = 0.69140625
c621-052: Epoch: 0, Step: 108, Rank: 21, loss = 0.201171875
c621-071: Epoch: 0, Step: 108, Rank: 24, loss = 0.2470703125
c622-021: Epoch: 0, Step: 108, Rank: 46, loss = 0.333984375
c613-142: Epoch: 0, Step: 108, Rank: 9, loss = 0.2353515625
c622-071: Epoch: 0, Step: 108, Rank: 56, loss = 0.22265625
c621-092: Epoch: 0, Step: 108, Rank: 29, loss = 4.3655745685100555e-09
c622-062: Epoch: 0, Step: 108, Rank: 55, loss = 0.1455078125
c621-062: Epoch: 0, Step: 108, Rank: 23, loss = 0.0225830078125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2578125 | max allocated: 11957.24169921875 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 109, Rank: 43, loss = 0.341796875
c613-151: Epoch: 0, Step: 109, Rank: 10, loss = 0.26953125
c613-101: Epoch: 0, Step: 109, Rank: 0, loss = 0.21484375
c619-002: Epoch: 0, Step: 109, Rank: 13, loss = 0.384765625
c619-001: Epoch: 0, Step: 109, Rank: 12, loss = 0.3125
c622-081: Epoch: 0, Step: 109, Rank: 58, loss = 0.18359375
c621-081: Epoch: 0, Step: 109, Rank: 26, loss = 0.126953125
c622-001: Epoch: 0, Step: 109, Rank: 42, loss = 0.10791015625
c621-111: Epoch: 0, Step: 109, Rank: 32, loss = 0.306640625
c621-151: Epoch: 0, Step: 109, Rank: 40, loss = 0.1484375
c619-021: Epoch: 0, Step: 109, Rank: 16, loss = 0.296875
c622-012: Epoch: 0, Step: 109, Rank: 45, loss = 0.69140625
c613-121: Epoch: 0, Step: 109, Rank: 4, loss = 0.306640625
c621-131: Epoch: 0, Step: 109, Rank: 36, loss = 0.2578125
c621-072: Epoch: 0, Step: 109, Rank: 25, loss = 0.06787109375
c619-011: Epoch: 0, Step: 109, Rank: 14, loss = 0.228515625
c613-122: Epoch: 0, Step: 109, Rank: 5, loss = 0.69140625
c622-011: Epoch: 0, Step: 109, Rank: 44, loss = 0.1162109375
c621-121: Epoch: 0, Step: 109, Rank: 34, loss = 0.328125
c621-061: Epoch: 0, Step: 109, Rank: 22, loss = 0.29296875
c619-031: Epoch: 0, Step: 109, Rank: 18, loss = 0.154296875
c621-132: Epoch: 0, Step: 109, Rank: 37, loss = 0.027099609375
c613-152: Epoch: 0, Step: 109, Rank: 11, loss = 0.005584716796875
c613-112: Epoch: 0, Step: 109, Rank: 3, loss = 0.10498046875
c621-091: Epoch: 0, Step: 109, Rank: 28, loss = 0.220703125
c621-141: Epoch: 0, Step: 109, Rank: 38, loss = 0.22265625
c619-022: Epoch: 0, Step: 109, Rank: 17, loss = 0.11279296875
c622-032: Epoch: 0, Step: 109, Rank: 49, loss = 0.201171875
c613-141: Epoch: 0, Step: 109, Rank: 8, loss = 0.287109375
c621-122: Epoch: 0, Step: 109, Rank: 35, loss = 0.296875
c613-131: Epoch: 0, Step: 109, Rank: 6, loss = 0.69140625
c613-132: Epoch: 0, Step: 109, Rank: 7, loss = 0.306640625
c621-142: Epoch: 0, Step: 109, Rank: 39, loss = 0.039794921875
c622-072: Epoch: 0, Step: 109, Rank: 57, loss = 0.36328125
c622-041: Epoch: 0, Step: 109, Rank: 50, loss = 0.00946044921875
c619-012: Epoch: 0, Step: 109, Rank: 15, loss = 0.1396484375
c613-102: Epoch: 0, Step: 109, Rank: 1, loss = 0.059326171875
c622-051: Epoch: 0, Step: 109, Rank: 52, loss = 0.2255859375
c621-101: Epoch: 0, Step: 109, Rank: 30, loss = 0.03271484375
c613-111: Epoch: 0, Step: 109, Rank: 2, loss = 0.072265625
c622-022: Epoch: 0, Step: 109, Rank: 47, loss = 0.416015625
c622-062: Epoch: 0, Step: 109, Rank: 55, loss = 0.00433349609375
c621-082: Epoch: 0, Step: 109, Rank: 27, loss = 0.1572265625
c619-041: Epoch: 0, Step: 109, Rank: 20, loss = 0.0038299560546875
c621-152: Epoch: 0, Step: 109, Rank: 41, loss = 0.1396484375
c622-082: Epoch: 0, Step: 109, Rank: 59, loss = 0.30859375
c619-032: Epoch: 0, Step: 109, Rank: 19, loss = 0.00012302398681640625
c621-112: Epoch: 0, Step: 109, Rank: 33, loss = 7.963180541992188e-05
c622-042: Epoch: 0, Step: 109, Rank: 51, loss = 0.2099609375
c622-061: Epoch: 0, Step: 109, Rank: 54, loss = 1.3828277587890625e-05
c613-142: Epoch: 0, Step: 109, Rank: 9, loss = 0.1298828125
c622-102: Epoch: 0, Step: 109, Rank: 63, loss = 0.1669921875
c621-052: Epoch: 0, Step: 109, Rank: 21, loss = 0.126953125
c621-071: Epoch: 0, Step: 109, Rank: 24, loss = 0.193359375
c622-101: Epoch: 0, Step: 109, Rank: 62, loss = 0.1162109375
c622-031: Epoch: 0, Step: 109, Rank: 48, loss = 0.0037078857421875
c621-102: Epoch: 0, Step: 109, Rank: 31, loss = 0.404296875
c622-092: Epoch: 0, Step: 109, Rank: 61, loss = 0.21875
c622-091: Epoch: 0, Step: 109, Rank: 60, loss = 0.470703125
c621-062: Epoch: 0, Step: 109, Rank: 23, loss = 0.3828125
c622-021: Epoch: 0, Step: 109, Rank: 46, loss = 0.0986328125
c622-071: Epoch: 0, Step: 109, Rank: 56, loss = 0.1884765625
c622-052: Epoch: 0, Step: 109, Rank: 53, loss = 0.201171875
c621-092: Epoch: 0, Step: 109, Rank: 29, loss = 0.02978515625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 1.98s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.98s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 110, Rank: 0, loss = 0.1162109375
c621-132: Epoch: 0, Step: 110, Rank: 37, loss = 0.1640625
c622-081: Epoch: 0, Step: 110, Rank: 58, loss = 0.3125
c621-142: Epoch: 0, Step: 110, Rank: 39, loss = 0.228515625
c622-002: Epoch: 0, Step: 110, Rank: 43, loss = 0.1669921875
c621-111: Epoch: 0, Step: 110, Rank: 32, loss = 0.06494140625
c621-151: Epoch: 0, Step: 110, Rank: 40, loss = 0.12158203125
c622-001: Epoch: 0, Step: 110, Rank: 42, loss = 0.201171875
c622-062: Epoch: 0, Step: 110, Rank: 55, loss = 0.1298828125
c622-071: Epoch: 0, Step: 110, Rank: 56, loss = 0.126953125
c621-081: Epoch: 0, Step: 110, Rank: 26, loss = 0.10302734375
c622-101: Epoch: 0, Step: 110, Rank: 62, loss = 0.006103515625
c613-102: Epoch: 0, Step: 110, Rank: 1, loss = 0.2412109375
c622-102: Epoch: 0, Step: 110, Rank: 63, loss = 0.0028076171875
c622-072: Epoch: 0, Step: 110, Rank: 57, loss = 0.12158203125
c619-021: Epoch: 0, Step: 110, Rank: 16, loss = 0.07666015625
c622-012: Epoch: 0, Step: 110, Rank: 45, loss = 0.2236328125
c621-131: Epoch: 0, Step: 110, Rank: 36, loss = 0.0205078125
c621-091: Epoch: 0, Step: 110, Rank: 28, loss = 0.341796875
c622-092: Epoch: 0, Step: 110, Rank: 61, loss = 0.69140625
c619-001: Epoch: 0, Step: 110, Rank: 12, loss = 0.17578125
c621-141: Epoch: 0, Step: 110, Rank: 38, loss = 0.380859375
c621-072: Epoch: 0, Step: 110, Rank: 25, loss = 0.29296875
c622-032: Epoch: 0, Step: 110, Rank: 49, loss = 0.1572265625
c613-151: Epoch: 0, Step: 110, Rank: 10, loss = 0.69140625
c619-002: Epoch: 0, Step: 110, Rank: 13, loss = 0.22265625
c622-082: Epoch: 0, Step: 110, Rank: 59, loss = 0.041748046875
c622-091: Epoch: 0, Step: 110, Rank: 60, loss = 0.1162109375
c619-022: Epoch: 0, Step: 110, Rank: 17, loss = 0.072265625
c621-071: Epoch: 0, Step: 110, Rank: 24, loss = 0.34765625
c621-092: Epoch: 0, Step: 110, Rank: 29, loss = 0.3671875
c621-122: Epoch: 0, Step: 110, Rank: 35, loss = 0.267578125
c621-112: Epoch: 0, Step: 110, Rank: 33, loss = 0.08642578125
c613-132: Epoch: 0, Step: 110, Rank: 7, loss = 0.10498046875
c613-111: Epoch: 0, Step: 110, Rank: 2, loss = 0.1796875
c621-152: Epoch: 0, Step: 110, Rank: 41, loss = 0.002471923828125
c622-051: Epoch: 0, Step: 110, Rank: 52, loss = 0.06494140625
c621-082: Epoch: 0, Step: 110, Rank: 27, loss = 0.28515625
c622-022: Epoch: 0, Step: 110, Rank: 47, loss = 0.30859375
c621-121: Epoch: 0, Step: 110, Rank: 34, loss = 0.26171875
c619-031: Epoch: 0, Step: 110, Rank: 18, loss = 0.0791015625
c619-012: Epoch: 0, Step: 110, Rank: 15, loss = 0.1640625
c621-052: Epoch: 0, Step: 110, Rank: 21, loss = 0.17578125
c613-112: Epoch: 0, Step: 110, Rank: 3, loss = 0.16015625
c622-031: Epoch: 0, Step: 110, Rank: 48, loss = 0.0133056640625
c622-061: Epoch: 0, Step: 110, Rank: 54, loss = 0.197265625
c621-101: Epoch: 0, Step: 110, Rank: 30, loss = 0.35546875
c621-061: Epoch: 0, Step: 110, Rank: 22, loss = 0.036376953125
c619-032: Epoch: 0, Step: 110, Rank: 19, loss = 0.220703125
c622-011: Epoch: 0, Step: 110, Rank: 44, loss = 0.13671875
c619-011: Epoch: 0, Step: 110, Rank: 14, loss = 0.197265625
c621-062: Epoch: 0, Step: 110, Rank: 23, loss = 0.287109375
c613-122: Epoch: 0, Step: 110, Rank: 5, loss = 0.30859375
c619-041: Epoch: 0, Step: 110, Rank: 20, loss = 0.01708984375
c622-042: Epoch: 0, Step: 110, Rank: 51, loss = 0.03466796875
c613-121: Epoch: 0, Step: 110, Rank: 4, loss = 0.275390625
c613-142: Epoch: 0, Step: 110, Rank: 9, loss = 0.126953125
c622-041: Epoch: 0, Step: 110, Rank: 50, loss = 0.036376953125
c613-131: Epoch: 0, Step: 110, Rank: 6, loss = 0.1572265625
c621-102: Epoch: 0, Step: 110, Rank: 31, loss = 0.01507568359375
c613-141: Epoch: 0, Step: 110, Rank: 8, loss = 0.734375
c613-152: Epoch: 0, Step: 110, Rank: 11, loss = 0.158203125
c622-021: Epoch: 0, Step: 110, Rank: 46, loss = 0.0751953125
c622-052: Epoch: 0, Step: 110, Rank: 53, loss = 0.004486083984375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22636.0 | max reserved: 22636.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 111, Rank: 43, loss = 0.201171875
c621-091: Epoch: 0, Step: 111, Rank: 28, loss = 0.29296875
c619-031: Epoch: 0, Step: 111, Rank: 18, loss = 0.357421875
c619-021: Epoch: 0, Step: 111, Rank: 16, loss = 0.08642578125
c621-081: Epoch: 0, Step: 111, Rank: 26, loss = 0.197265625
c621-112: Epoch: 0, Step: 111, Rank: 33, loss = 0.17578125
c613-101: Epoch: 0, Step: 111, Rank: 0, loss = 0.1533203125
c622-001: Epoch: 0, Step: 111, Rank: 42, loss = 0.123046875
c621-151: Epoch: 0, Step: 111, Rank: 40, loss = 0.26171875
c621-111: Epoch: 0, Step: 111, Rank: 32, loss = 0.1328125
c619-002: Epoch: 0, Step: 111, Rank: 13, loss = 0.158203125
c622-012: Epoch: 0, Step: 111, Rank: 45, loss = 0.123046875
c621-072: Epoch: 0, Step: 111, Rank: 25, loss = 0.06494140625
c621-052: Epoch: 0, Step: 111, Rank: 21, loss = 0.408203125
c621-082: Epoch: 0, Step: 111, Rank: 27, loss = 0.13671875
c621-132: Epoch: 0, Step: 111, Rank: 37, loss = 0.154296875
c621-152: Epoch: 0, Step: 111, Rank: 41, loss = 0.1396484375
c622-041: Epoch: 0, Step: 111, Rank: 50, loss = 0.404296875
c621-142: Epoch: 0, Step: 111, Rank: 39, loss = 0.023193359375
c621-121: Epoch: 0, Step: 111, Rank: 34, loss = 0.00408935546875
c622-051: Epoch: 0, Step: 111, Rank: 52, loss = 0.126953125
c622-032: Epoch: 0, Step: 111, Rank: 49, loss = 0.41796875
c621-131: Epoch: 0, Step: 111, Rank: 36, loss = 0.26171875
c622-021: Epoch: 0, Step: 111, Rank: 46, loss = 0.1982421875
c619-032: Epoch: 0, Step: 111, Rank: 19, loss = 0.31640625
c619-001: Epoch: 0, Step: 111, Rank: 12, loss = 0.3203125
c619-012: Epoch: 0, Step: 111, Rank: 15, loss = 0.0021209716796875
c621-101: Epoch: 0, Step: 111, Rank: 30, loss = 0.0198974609375
c622-011: Epoch: 0, Step: 111, Rank: 44, loss = 2.2649765014648438e-06
c622-062: Epoch: 0, Step: 111, Rank: 55, loss = 0.267578125
c621-122: Epoch: 0, Step: 111, Rank: 35, loss = 0.3203125
c613-131: Epoch: 0, Step: 111, Rank: 6, loss = 0.318359375
c613-132: Epoch: 0, Step: 111, Rank: 7, loss = 0.06494140625
c619-041: Epoch: 0, Step: 111, Rank: 20, loss = 0.35546875
c622-101: Epoch: 0, Step: 111, Rank: 62, loss = 0.1669921875
c613-152: Epoch: 0, Step: 111, Rank: 11, loss = 0.69140625
c622-022: Epoch: 0, Step: 111, Rank: 47, loss = 0.337890625
c613-151: Epoch: 0, Step: 111, Rank: 10, loss = 0.306640625
c613-122: Epoch: 0, Step: 111, Rank: 5, loss = 0.2470703125
c621-061: Epoch: 0, Step: 111, Rank: 22, loss = 0.32421875
c621-141: Epoch: 0, Step: 111, Rank: 38, loss = 0.26953125
c613-142: Epoch: 0, Step: 111, Rank: 9, loss = 0.095703125
c619-022: Epoch: 0, Step: 111, Rank: 17, loss = 0.32421875
c622-061: Epoch: 0, Step: 111, Rank: 54, loss = 0.34765625
c621-102: Epoch: 0, Step: 111, Rank: 31, loss = 0.08740234375
c619-011: Epoch: 0, Step: 111, Rank: 14, loss = 0.36328125
c622-042: Epoch: 0, Step: 111, Rank: 51, loss = 0.08642578125
c622-081: Epoch: 0, Step: 111, Rank: 58, loss = 0.17578125
c621-062: Epoch: 0, Step: 111, Rank: 23, loss = 0.251953125
c622-031: Epoch: 0, Step: 111, Rank: 48, loss = 0.00075531005859375
c621-071: Epoch: 0, Step: 111, Rank: 24, loss = 0.1669921875
c613-111: Epoch: 0, Step: 111, Rank: 2, loss = 0.166015625
c622-102: Epoch: 0, Step: 111, Rank: 63, loss = 0.1904296875
c613-112: Epoch: 0, Step: 111, Rank: 3, loss = 0.0849609375
c621-092: Epoch: 0, Step: 111, Rank: 29, loss = 0.126953125
c613-121: Epoch: 0, Step: 111, Rank: 4, loss = 0.220703125
c613-102: Epoch: 0, Step: 111, Rank: 1, loss = 0.01214599609375
c622-072: Epoch: 0, Step: 111, Rank: 57, loss = 0.171875
c622-092: Epoch: 0, Step: 111, Rank: 61, loss = 0.1494140625
c613-141: Epoch: 0, Step: 111, Rank: 8, loss = 0.330078125
c622-071: Epoch: 0, Step: 111, Rank: 56, loss = 0.201171875
c622-082: Epoch: 0, Step: 111, Rank: 59, loss = 0.095703125
c622-052: Epoch: 0, Step: 111, Rank: 53, loss = 0.05078125
c622-091: Epoch: 0, Step: 111, Rank: 60, loss = 0.1904296875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.12s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.12s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 112, Rank: 0, loss = 0.00811767578125
c619-021: Epoch: 0, Step: 112, Rank: 16, loss = 0.1181640625
c622-071: Epoch: 0, Step: 112, Rank: 56, loss = 0.003173828125
c621-111: Epoch: 0, Step: 112, Rank: 32, loss = 0.06201171875
c622-062: Epoch: 0, Step: 112, Rank: 55, loss = 0.0023956298828125
c622-002: Epoch: 0, Step: 112, Rank: 43, loss = 0.0023193359375
c619-031: Epoch: 0, Step: 112, Rank: 18, loss = 0.69140625
c613-121: Epoch: 0, Step: 112, Rank: 4, loss = 0.69140625
c622-051: Epoch: 0, Step: 112, Rank: 52, loss = 0.0016021728515625
c613-131: Epoch: 0, Step: 112, Rank: 6, loss = 0.02978515625
c622-081: Epoch: 0, Step: 112, Rank: 58, loss = 0.0052490234375
c622-101: Epoch: 0, Step: 112, Rank: 62, loss = 0.0380859375
c619-001: Epoch: 0, Step: 112, Rank: 12, loss = 0.000553131103515625
c619-011: Epoch: 0, Step: 112, Rank: 14, loss = 0.1181640625
c613-151: Epoch: 0, Step: 112, Rank: 10, loss = 0.00360107421875
c613-112: Epoch: 0, Step: 112, Rank: 3, loss = 0.027099609375
c613-142: Epoch: 0, Step: 112, Rank: 9, loss = 0.01708984375
c613-102: Epoch: 0, Step: 112, Rank: 1, loss = 0.027587890625
c619-012: Epoch: 0, Step: 112, Rank: 15, loss = 0.00836181640625
c613-122: Epoch: 0, Step: 112, Rank: 5, loss = 0.057373046875
c613-111: Epoch: 0, Step: 112, Rank: 2, loss = 0.69140625
c621-132: Epoch: 0, Step: 112, Rank: 37, loss = 0.00075531005859375
c619-032: Epoch: 0, Step: 112, Rank: 19, loss = 0.123046875
c622-001: Epoch: 0, Step: 112, Rank: 42, loss = 0.06787109375
c622-092: Epoch: 0, Step: 112, Rank: 61, loss = 0.0003681182861328125
c613-132: Epoch: 0, Step: 112, Rank: 7, loss = 0.06005859375
c621-091: Epoch: 0, Step: 112, Rank: 28, loss = 1.0788440704345703e-05
c619-002: Epoch: 0, Step: 112, Rank: 13, loss = 0.03369140625
c613-152: Epoch: 0, Step: 112, Rank: 11, loss = 0.036376953125
c622-031: Epoch: 0, Step: 112, Rank: 48, loss = 0.69140625
c621-151: Epoch: 0, Step: 112, Rank: 40, loss = 0.005584716796875
c622-061: Epoch: 0, Step: 112, Rank: 54, loss = 0.10498046875
c621-121: Epoch: 0, Step: 112, Rank: 34, loss = 0.056640625
c622-011: Epoch: 0, Step: 112, Rank: 44, loss = 0.036376953125
c622-041: Epoch: 0, Step: 112, Rank: 50, loss = 0.053955078125
c622-102: Epoch: 0, Step: 112, Rank: 63, loss = 1.2656542480726785e-14
c619-022: Epoch: 0, Step: 112, Rank: 17, loss = 0.03271484375
c621-072: Epoch: 0, Step: 112, Rank: 25, loss = 0.1396484375
c621-081: Epoch: 0, Step: 112, Rank: 26, loss = 0.06005859375
c621-112: Epoch: 0, Step: 112, Rank: 33, loss = 0.69140625
c621-101: Epoch: 0, Step: 112, Rank: 30, loss = 0.06982421875
c622-012: Epoch: 0, Step: 112, Rank: 45, loss = 0.003387451171875
c613-141: Epoch: 0, Step: 112, Rank: 8, loss = 0.0133056640625
c622-072: Epoch: 0, Step: 112, Rank: 57, loss = 0.0242919921875
c622-091: Epoch: 0, Step: 112, Rank: 60, loss = 0.00193023681640625
c621-131: Epoch: 0, Step: 112, Rank: 36, loss = 0.00738525390625
c622-082: Epoch: 0, Step: 112, Rank: 59, loss = 0.03466796875
c621-071: Epoch: 0, Step: 112, Rank: 24, loss = 0.0181884765625
c621-082: Epoch: 0, Step: 112, Rank: 27, loss = 0.0034942626953125
c621-142: Epoch: 0, Step: 112, Rank: 39, loss = 0.027099609375
c621-102: Epoch: 0, Step: 112, Rank: 31, loss = 0.00433349609375
c621-061: Epoch: 0, Step: 112, Rank: 22, loss = 0.0186767578125
c622-042: Epoch: 0, Step: 112, Rank: 51, loss = 0.69140625
c621-152: Epoch: 0, Step: 112, Rank: 41, loss = 0.000606536865234375
c621-122: Epoch: 0, Step: 112, Rank: 35, loss = 0.00070953369140625
c621-092: Epoch: 0, Step: 112, Rank: 29, loss = 0.005584716796875
c622-052: Epoch: 0, Step: 112, Rank: 53, loss = 7.009506225585938e-05
c621-141: Epoch: 0, Step: 112, Rank: 38, loss = 0.03515625
c621-062: Epoch: 0, Step: 112, Rank: 23, loss = 0.69140625
c622-022: Epoch: 0, Step: 112, Rank: 47, loss = 0.0028076171875
c622-021: Epoch: 0, Step: 112, Rank: 46, loss = 0.056640625
c621-052: Epoch: 0, Step: 112, Rank: 21, loss = 0.000518798828125
c619-041: Epoch: 0, Step: 112, Rank: 20, loss = 0.0047607421875
c622-032: Epoch: 0, Step: 112, Rank: 49, loss = 0.050048828125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.52s, TFLOPs: 0.75, Samples/sec: 0.40, Time/seq 2.52s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 113, Rank: 16, loss = 0.059326171875
c613-101: Epoch: 0, Step: 113, Rank: 0, loss = 0.00714111328125
c621-091: Epoch: 0, Step: 113, Rank: 28, loss = 0.06494140625
c621-081: Epoch: 0, Step: 113, Rank: 26, loss = 0.022216796875
c619-031: Epoch: 0, Step: 113, Rank: 18, loss = 0.005584716796875
c621-061: Epoch: 0, Step: 113, Rank: 22, loss = 0.00099945068359375
c622-002: Epoch: 0, Step: 113, Rank: 43, loss = 0.0556640625
c622-001: Epoch: 0, Step: 113, Rank: 42, loss = 0.1015625
c621-072: Epoch: 0, Step: 113, Rank: 25, loss = 0.00811767578125
c621-152: Epoch: 0, Step: 113, Rank: 41, loss = 0.06787109375
c621-062: Epoch: 0, Step: 113, Rank: 23, loss = 0.005401611328125
c613-111: Epoch: 0, Step: 113, Rank: 2, loss = 0.05078125
c621-132: Epoch: 0, Step: 113, Rank: 37, loss = 0.0001087188720703125
c621-111: Epoch: 0, Step: 113, Rank: 32, loss = 0.0159912109375
c622-062: Epoch: 0, Step: 113, Rank: 55, loss = 0.06591796875
c622-081: Epoch: 0, Step: 113, Rank: 58, loss = 0.0255126953125
c621-151: Epoch: 0, Step: 113, Rank: 40, loss = 0.00136566162109375
c619-001: Epoch: 0, Step: 113, Rank: 12, loss = 0.01708984375
c621-131: Epoch: 0, Step: 113, Rank: 36, loss = 0.050048828125
c613-151: Epoch: 0, Step: 113, Rank: 10, loss = 0.02978515625
c613-131: Epoch: 0, Step: 113, Rank: 6, loss = 0.004486083984375
c619-002: Epoch: 0, Step: 113, Rank: 13, loss = 0.0026397705078125
c621-052: Epoch: 0, Step: 113, Rank: 21, loss = 0.0038299560546875
c613-132: Epoch: 0, Step: 113, Rank: 7, loss = 0.0103759765625
c621-142: Epoch: 0, Step: 113, Rank: 39, loss = 1.525040715932846e-08
c622-052: Epoch: 0, Step: 113, Rank: 53, loss = 1.2790197503539935e-19
c613-121: Epoch: 0, Step: 113, Rank: 4, loss = 3.3921018924503508e-28
c619-041: Epoch: 0, Step: 113, Rank: 20, loss = 0.01177978515625
c621-121: Epoch: 0, Step: 113, Rank: 34, loss = 0.0103759765625
c613-122: Epoch: 0, Step: 113, Rank: 5, loss = 1.2931877790833823e-12
c619-011: Epoch: 0, Step: 113, Rank: 14, loss = 0.007598876953125
c613-152: Epoch: 0, Step: 113, Rank: 11, loss = 0.23828125
c619-032: Epoch: 0, Step: 113, Rank: 19, loss = 0.07666015625
c622-102: Epoch: 0, Step: 113, Rank: 63, loss = 0.0037078857421875
c621-122: Epoch: 0, Step: 113, Rank: 35, loss = 0.0322265625
c613-102: Epoch: 0, Step: 113, Rank: 1, loss = 0.00012302398681640625
c622-101: Epoch: 0, Step: 113, Rank: 62, loss = 0.0002956390380859375
c622-061: Epoch: 0, Step: 113, Rank: 54, loss = 0.06787109375
c622-011: Epoch: 0, Step: 113, Rank: 44, loss = 0.1533203125
c621-101: Epoch: 0, Step: 113, Rank: 30, loss = 2.540190280342358e-13
c622-012: Epoch: 0, Step: 113, Rank: 45, loss = 0.08740234375
c621-112: Epoch: 0, Step: 113, Rank: 33, loss = 0.69140625
c619-022: Epoch: 0, Step: 113, Rank: 17, loss = 0.09423828125
c622-051: Epoch: 0, Step: 113, Rank: 52, loss = 0.00116729736328125
c622-041: Epoch: 0, Step: 113, Rank: 50, loss = 0.17578125
c621-071: Epoch: 0, Step: 113, Rank: 24, loss = 2.6756374893466273e-14
c622-022: Epoch: 0, Step: 113, Rank: 47, loss = 0.0198974609375
c619-012: Epoch: 0, Step: 113, Rank: 15, loss = 0.01507568359375
c622-072: Epoch: 0, Step: 113, Rank: 57, loss = 0.039306640625
c622-032: Epoch: 0, Step: 113, Rank: 49, loss = 0.69140625
c622-092: Epoch: 0, Step: 113, Rank: 61, loss = 0.048583984375
c621-141: Epoch: 0, Step: 113, Rank: 38, loss = 5.3085386753082275e-08
c621-092: Epoch: 0, Step: 113, Rank: 29, loss = 0.02880859375
c622-021: Epoch: 0, Step: 113, Rank: 46, loss = 0.00180816650390625
c613-141: Epoch: 0, Step: 113, Rank: 8, loss = 0.06005859375
c613-112: Epoch: 0, Step: 113, Rank: 3, loss = 0.0791015625
c622-031: Epoch: 0, Step: 113, Rank: 48, loss = 2.014636993408203e-05
c613-142: Epoch: 0, Step: 113, Rank: 9, loss = 0.06982421875
c622-071: Epoch: 0, Step: 113, Rank: 56, loss = 0.072265625
c622-042: Epoch: 0, Step: 113, Rank: 51, loss = 0.03515625
c621-082: Epoch: 0, Step: 113, Rank: 27, loss = 0.02392578125
c621-102: Epoch: 0, Step: 113, Rank: 31, loss = 0.06982421875
c622-091: Epoch: 0, Step: 113, Rank: 60, loss = 6.439293542825908e-14
c622-082: Epoch: 0, Step: 113, Rank: 59, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 114, Rank: 0, loss = 0.208984375
c621-072: Epoch: 0, Step: 114, Rank: 25, loss = 1.895427703857422e-05
c621-112: Epoch: 0, Step: 114, Rank: 33, loss = 0.00433349609375
c619-001: Epoch: 0, Step: 114, Rank: 12, loss = 0.08251953125
c622-002: Epoch: 0, Step: 114, Rank: 43, loss = 0.048583984375
c622-081: Epoch: 0, Step: 114, Rank: 58, loss = 1.7229467630386353e-08
c613-122: Epoch: 0, Step: 114, Rank: 5, loss = 0.03369140625
c621-111: Epoch: 0, Step: 114, Rank: 32, loss = 0.10791015625
c613-132: Epoch: 0, Step: 114, Rank: 7, loss = 0.0038299560546875
c622-032: Epoch: 0, Step: 114, Rank: 49, loss = 0.00396728515625
c621-132: Epoch: 0, Step: 114, Rank: 37, loss = 0.111328125
c613-131: Epoch: 0, Step: 114, Rank: 6, loss = 0.0181884765625
c621-081: Epoch: 0, Step: 114, Rank: 26, loss = 0.059326171875
c622-062: Epoch: 0, Step: 114, Rank: 55, loss = 0.027587890625
c621-061: Epoch: 0, Step: 114, Rank: 22, loss = 0.01416015625
c619-002: Epoch: 0, Step: 114, Rank: 13, loss = 0.0849609375
c622-101: Epoch: 0, Step: 114, Rank: 62, loss = 0.00714111328125
c613-152: Epoch: 0, Step: 114, Rank: 11, loss = 0.005584716796875
c622-071: Epoch: 0, Step: 114, Rank: 56, loss = 0.0255126953125
c622-012: Epoch: 0, Step: 114, Rank: 45, loss = 0.06005859375
c622-001: Epoch: 0, Step: 114, Rank: 42, loss = 0.0017547607421875
c621-121: Epoch: 0, Step: 114, Rank: 34, loss = 0.004913330078125
c622-052: Epoch: 0, Step: 114, Rank: 53, loss = 0.06494140625
c622-061: Epoch: 0, Step: 114, Rank: 54, loss = 8.083811398051921e-16
c622-092: Epoch: 0, Step: 114, Rank: 61, loss = 0.69140625
c613-111: Epoch: 0, Step: 114, Rank: 2, loss = 0.005584716796875
c622-041: Epoch: 0, Step: 114, Rank: 50, loss = 0.12158203125
c621-151: Epoch: 0, Step: 114, Rank: 40, loss = 0.10009765625
c619-022: Epoch: 0, Step: 114, Rank: 17, loss = 0.0810546875
c619-021: Epoch: 0, Step: 114, Rank: 16, loss = 0.03515625
c619-032: Epoch: 0, Step: 114, Rank: 19, loss = 3.979039320256561e-12
c622-051: Epoch: 0, Step: 114, Rank: 52, loss = 0.00124359130859375
c621-091: Epoch: 0, Step: 114, Rank: 28, loss = 0.0751953125
c621-082: Epoch: 0, Step: 114, Rank: 27, loss = 0.04296875
c613-151: Epoch: 0, Step: 114, Rank: 10, loss = 0.003387451171875
c622-102: Epoch: 0, Step: 114, Rank: 63, loss = 0.01416015625
c621-131: Epoch: 0, Step: 114, Rank: 36, loss = 0.004486083984375
c621-101: Epoch: 0, Step: 114, Rank: 30, loss = 0.00164794921875
c621-122: Epoch: 0, Step: 114, Rank: 35, loss = 0.01507568359375
c621-052: Epoch: 0, Step: 114, Rank: 21, loss = 0.07763671875
c622-031: Epoch: 0, Step: 114, Rank: 48, loss = 0.69140625
c613-141: Epoch: 0, Step: 114, Rank: 8, loss = 0.01708984375
c621-141: Epoch: 0, Step: 114, Rank: 38, loss = 0.02880859375
c619-011: Epoch: 0, Step: 114, Rank: 14, loss = 0.045654296875
c619-012: Epoch: 0, Step: 114, Rank: 15, loss = 0.04443359375
c619-041: Epoch: 0, Step: 114, Rank: 20, loss = 2.014636993408203e-05
c613-102: Epoch: 0, Step: 114, Rank: 1, loss = 0.027587890625
c621-071: Epoch: 0, Step: 114, Rank: 24, loss = 0.05322265625
c622-082: Epoch: 0, Step: 114, Rank: 59, loss = 0.043701171875
c621-142: Epoch: 0, Step: 114, Rank: 39, loss = 0.05078125
c619-031: Epoch: 0, Step: 114, Rank: 18, loss = 6.344635039567947e-09
c613-112: Epoch: 0, Step: 114, Rank: 3, loss = 0.03466796875
c621-062: Epoch: 0, Step: 114, Rank: 23, loss = 0.69140625
c621-152: Epoch: 0, Step: 114, Rank: 41, loss = 0.03271484375
c622-072: Epoch: 0, Step: 114, Rank: 57, loss = 0.048583984375
c621-102: Epoch: 0, Step: 114, Rank: 31, loss = 0.0306396484375
c622-022: Epoch: 0, Step: 114, Rank: 47, loss = 2.562999725341797e-06
c622-042: Epoch: 0, Step: 114, Rank: 51, loss = 0.0732421875
c613-142: Epoch: 0, Step: 114, Rank: 9, loss = 0.00136566162109375
c622-011: Epoch: 0, Step: 114, Rank: 44, loss = 0.0038299560546875
c621-092: Epoch: 0, Step: 114, Rank: 29, loss = 0.0164794921875
c622-021: Epoch: 0, Step: 114, Rank: 46, loss = 1.2759119272232056e-07
c622-091: Epoch: 0, Step: 114, Rank: 60, loss = 0.08642578125
c613-121: Epoch: 0, Step: 114, Rank: 4, loss = 7.566995918750763e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75439453125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 115, Rank: 0, loss = 0.00811767578125
c622-002: Epoch: 0, Step: 115, Rank: 43, loss = 0.000518798828125
c622-081: Epoch: 0, Step: 115, Rank: 58, loss = 1.1059455573558807e-09
c622-052: Epoch: 0, Step: 115, Rank: 53, loss = 0.03466796875
c621-091: Epoch: 0, Step: 115, Rank: 28, loss = 0.00433349609375
c622-062: Epoch: 0, Step: 115, Rank: 55, loss = 0.056640625
c619-001: Epoch: 0, Step: 115, Rank: 12, loss = 0.0002956390380859375
c621-072: Epoch: 0, Step: 115, Rank: 25, loss = 0.01416015625
c621-101: Epoch: 0, Step: 115, Rank: 30, loss = 0.006103515625
c621-081: Epoch: 0, Step: 115, Rank: 26, loss = 0.0380859375
c621-152: Epoch: 0, Step: 115, Rank: 41, loss = 0.0380859375
c622-101: Epoch: 0, Step: 115, Rank: 62, loss = 0.000392913818359375
c622-102: Epoch: 0, Step: 115, Rank: 63, loss = 0.006103515625
c613-151: Epoch: 0, Step: 115, Rank: 10, loss = 0.0047607421875
c621-132: Epoch: 0, Step: 115, Rank: 37, loss = 2.753734588623047e-05
c613-131: Epoch: 0, Step: 115, Rank: 6, loss = 0.00299072265625
c619-002: Epoch: 0, Step: 115, Rank: 13, loss = 0.0181884765625
c621-112: Epoch: 0, Step: 115, Rank: 33, loss = 0.054931640625
c621-111: Epoch: 0, Step: 115, Rank: 32, loss = 4.4517219066619873e-07
c613-122: Epoch: 0, Step: 115, Rank: 5, loss = 0.0181884765625
c613-152: Epoch: 0, Step: 115, Rank: 11, loss = 0.03369140625
c622-051: Epoch: 0, Step: 115, Rank: 52, loss = 0.01556396484375
c621-131: Epoch: 0, Step: 115, Rank: 36, loss = 0.03271484375
c622-001: Epoch: 0, Step: 115, Rank: 42, loss = 0.003082275390625
c613-132: Epoch: 0, Step: 115, Rank: 7, loss = 0.00180816650390625
c621-151: Epoch: 0, Step: 115, Rank: 40, loss = 0.00128173828125
c621-122: Epoch: 0, Step: 115, Rank: 35, loss = 0.021240234375
c619-031: Epoch: 0, Step: 115, Rank: 18, loss = 0.251953125
c613-121: Epoch: 0, Step: 115, Rank: 4, loss = 3.510081114654895e-12
c622-032: Epoch: 0, Step: 115, Rank: 49, loss = 0.142578125
c622-092: Epoch: 0, Step: 115, Rank: 61, loss = 0.10791015625
c621-121: Epoch: 0, Step: 115, Rank: 34, loss = 0.036376953125
c621-082: Epoch: 0, Step: 115, Rank: 27, loss = 0.0380859375
c619-032: Epoch: 0, Step: 115, Rank: 19, loss = 0.00136566162109375
c621-061: Epoch: 0, Step: 115, Rank: 22, loss = 0.0023956298828125
c621-142: Epoch: 0, Step: 115, Rank: 39, loss = 0.11962890625
c619-021: Epoch: 0, Step: 115, Rank: 16, loss = 0.0047607421875
c622-011: Epoch: 0, Step: 115, Rank: 44, loss = 7.338821887969971e-07
c622-042: Epoch: 0, Step: 115, Rank: 51, loss = 0.0038299560546875
c619-041: Epoch: 0, Step: 115, Rank: 20, loss = 0.00408935546875
c622-012: Epoch: 0, Step: 115, Rank: 45, loss = 0.0284423828125
c622-061: Epoch: 0, Step: 115, Rank: 54, loss = 0.01904296875
c613-102: Epoch: 0, Step: 115, Rank: 1, loss = 0.0255126953125
c621-071: Epoch: 0, Step: 115, Rank: 24, loss = 0.00811767578125
c622-072: Epoch: 0, Step: 115, Rank: 57, loss = 0.0810546875
c621-141: Epoch: 0, Step: 115, Rank: 38, loss = 0.03466796875
c622-041: Epoch: 0, Step: 115, Rank: 50, loss = 0.00946044921875
c613-142: Epoch: 0, Step: 115, Rank: 9, loss = 0.0262451171875
c619-012: Epoch: 0, Step: 115, Rank: 15, loss = 3.7670135498046875e-05
c622-031: Epoch: 0, Step: 115, Rank: 48, loss = 2.7284841053187847e-12
c622-022: Epoch: 0, Step: 115, Rank: 47, loss = 0.091796875
c621-102: Epoch: 0, Step: 115, Rank: 31, loss = 0.0091552734375
c613-112: Epoch: 0, Step: 115, Rank: 3, loss = 0.00012302398681640625
c621-052: Epoch: 0, Step: 115, Rank: 21, loss = 0.06396484375
c621-092: Epoch: 0, Step: 115, Rank: 29, loss = 0.06591796875
c613-141: Epoch: 0, Step: 115, Rank: 8, loss = 0.00433349609375
c621-062: Epoch: 0, Step: 115, Rank: 23, loss = 0.07080078125
c619-022: Epoch: 0, Step: 115, Rank: 17, loss = 0.0017547607421875
c622-021: Epoch: 0, Step: 115, Rank: 46, loss = 0.021240234375
c622-082: Epoch: 0, Step: 115, Rank: 59, loss = 0.00811767578125
c622-091: Epoch: 0, Step: 115, Rank: 60, loss = 0.0047607421875
c622-071: Epoch: 0, Step: 115, Rank: 56, loss = 0.00075531005859375
c619-011: Epoch: 0, Step: 115, Rank: 14, loss = 0.01214599609375
c613-111: Epoch: 0, Step: 115, Rank: 2, loss = 0.002899169921875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 116, Rank: 32, loss = 0.072265625
c619-021: Epoch: 0, Step: 116, Rank: 16, loss = 0.023193359375
c622-002: Epoch: 0, Step: 116, Rank: 43, loss = 0.0306396484375
c619-002: Epoch: 0, Step: 116, Rank: 13, loss = 0.0311279296875
c622-001: Epoch: 0, Step: 116, Rank: 42, loss = 0.056640625
c622-012: Epoch: 0, Step: 116, Rank: 45, loss = 0.18359375
c622-052: Epoch: 0, Step: 116, Rank: 53, loss = 1.6391277313232422e-07
c621-151: Epoch: 0, Step: 116, Rank: 40, loss = 0.01068115234375
c622-022: Epoch: 0, Step: 116, Rank: 47, loss = 1.6689300537109375e-05
c619-001: Epoch: 0, Step: 116, Rank: 12, loss = 0.69140625
c622-051: Epoch: 0, Step: 116, Rank: 52, loss = 0.05078125
c613-101: Epoch: 0, Step: 116, Rank: 0, loss = 0.01556396484375
c622-021: Epoch: 0, Step: 116, Rank: 46, loss = 0.0306396484375
c621-082: Epoch: 0, Step: 116, Rank: 27, loss = 0.002471923828125
c622-031: Epoch: 0, Step: 116, Rank: 48, loss = 0.0133056640625
c619-041: Epoch: 0, Step: 116, Rank: 20, loss = 0.025146484375
c619-011: Epoch: 0, Step: 116, Rank: 14, loss = 0.004913330078125
c622-011: Epoch: 0, Step: 116, Rank: 44, loss = 0.00180816650390625
c621-091: Epoch: 0, Step: 116, Rank: 28, loss = 0.69140625
c613-152: Epoch: 0, Step: 116, Rank: 11, loss = 0.05078125
c613-132: Epoch: 0, Step: 116, Rank: 7, loss = 1.0800249583553523e-11
c613-151: Epoch: 0, Step: 116, Rank: 10, loss = 0.0067138671875
c622-032: Epoch: 0, Step: 116, Rank: 49, loss = 0.2060546875
c621-142: Epoch: 0, Step: 116, Rank: 39, loss = 0.0284423828125
c622-061: Epoch: 0, Step: 116, Rank: 54, loss = 0.00433349609375
c621-072: Epoch: 0, Step: 116, Rank: 25, loss = 0.091796875
c621-122: Epoch: 0, Step: 116, Rank: 35, loss = 0.01104736328125
c621-121: Epoch: 0, Step: 116, Rank: 34, loss = 0.039794921875
c621-131: Epoch: 0, Step: 116, Rank: 36, loss = 9.74978320300579e-10
c619-012: Epoch: 0, Step: 116, Rank: 15, loss = 0.0023193359375
c621-061: Epoch: 0, Step: 116, Rank: 22, loss = 0.027099609375
c621-071: Epoch: 0, Step: 116, Rank: 24, loss = 0.000392913818359375
c621-081: Epoch: 0, Step: 116, Rank: 26, loss = 0.000667572021484375
c619-032: Epoch: 0, Step: 116, Rank: 19, loss = 0.0198974609375
c622-092: Epoch: 0, Step: 116, Rank: 61, loss = 0.00225830078125
c621-141: Epoch: 0, Step: 116, Rank: 38, loss = 0.0478515625
c613-131: Epoch: 0, Step: 116, Rank: 6, loss = 0.09716796875
c619-022: Epoch: 0, Step: 116, Rank: 17, loss = 0.000553131103515625
c613-141: Epoch: 0, Step: 116, Rank: 8, loss = 0.06396484375
c621-092: Epoch: 0, Step: 116, Rank: 29, loss = 0.005584716796875
c613-121: Epoch: 0, Step: 116, Rank: 4, loss = 0.022216796875
c621-062: Epoch: 0, Step: 116, Rank: 23, loss = 0.0322265625
c621-052: Epoch: 0, Step: 116, Rank: 21, loss = 0.006103515625
c622-081: Epoch: 0, Step: 116, Rank: 58, loss = 0.01251220703125
c621-152: Epoch: 0, Step: 116, Rank: 41, loss = 0.023193359375
c621-102: Epoch: 0, Step: 116, Rank: 31, loss = 0.0311279296875
c613-122: Epoch: 0, Step: 116, Rank: 5, loss = 0.00096893310546875
c622-082: Epoch: 0, Step: 116, Rank: 59, loss = 0.048583984375
c622-042: Epoch: 0, Step: 116, Rank: 51, loss = 0.02978515625
c621-132: Epoch: 0, Step: 116, Rank: 37, loss = 0.021484375
c613-111: Epoch: 0, Step: 116, Rank: 2, loss = 0.06494140625
c619-031: Epoch: 0, Step: 116, Rank: 18, loss = 0.048583984375
c622-062: Epoch: 0, Step: 116, Rank: 55, loss = 0.017578125
c622-101: Epoch: 0, Step: 116, Rank: 62, loss = 2.905726432800293e-06
c613-142: Epoch: 0, Step: 116, Rank: 9, loss = 0.00116729736328125
c622-102: Epoch: 0, Step: 116, Rank: 63, loss = 0.01007080078125
c621-101: Epoch: 0, Step: 116, Rank: 30, loss = 0.0284423828125
c613-102: Epoch: 0, Step: 116, Rank: 1, loss = 0.0225830078125
c613-112: Epoch: 0, Step: 116, Rank: 3, loss = 0.0008544921875
c622-041: Epoch: 0, Step: 116, Rank: 50, loss = 0.022216796875
c622-071: Epoch: 0, Step: 116, Rank: 56, loss = 0.044921875
c621-112: Epoch: 0, Step: 116, Rank: 33, loss = 0.00714111328125
c622-091: Epoch: 0, Step: 116, Rank: 60, loss = 0.006103515625
c622-072: Epoch: 0, Step: 116, Rank: 57, loss = 0.057373046875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.79s, TFLOPs: 0.68, Samples/sec: 0.36, Time/seq 2.79s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 117, Rank: 0, loss = 0.039306640625
c621-132: Epoch: 0, Step: 117, Rank: 37, loss = 0.1328125
c622-002: Epoch: 0, Step: 117, Rank: 43, loss = 0.0089111328125
c622-062: Epoch: 0, Step: 117, Rank: 55, loss = 0.004913330078125
c621-151: Epoch: 0, Step: 117, Rank: 40, loss = 0.0205078125
c621-081: Epoch: 0, Step: 117, Rank: 26, loss = 0.01904296875
c622-081: Epoch: 0, Step: 117, Rank: 58, loss = 0.05322265625
c619-021: Epoch: 0, Step: 117, Rank: 16, loss = 0.0791015625
c622-061: Epoch: 0, Step: 117, Rank: 54, loss = 0.054931640625
c621-072: Epoch: 0, Step: 117, Rank: 25, loss = 0.002716064453125
c621-142: Epoch: 0, Step: 117, Rank: 39, loss = 2.648448571562767e-09
c622-092: Epoch: 0, Step: 117, Rank: 61, loss = 0.1064453125
c622-052: Epoch: 0, Step: 117, Rank: 53, loss = 3.46451997756958e-07
c622-012: Epoch: 0, Step: 117, Rank: 45, loss = 0.023193359375
c613-152: Epoch: 0, Step: 117, Rank: 11, loss = 1.1059455573558807e-09
c621-131: Epoch: 0, Step: 117, Rank: 36, loss = 0.60546875
c621-141: Epoch: 0, Step: 117, Rank: 38, loss = 0.03271484375
c619-001: Epoch: 0, Step: 117, Rank: 12, loss = 0.01507568359375
c621-101: Epoch: 0, Step: 117, Rank: 30, loss = 0.0164794921875
c622-071: Epoch: 0, Step: 117, Rank: 56, loss = 0.004486083984375
c613-122: Epoch: 0, Step: 117, Rank: 5, loss = 0.002899169921875
c621-111: Epoch: 0, Step: 117, Rank: 32, loss = 5.424022674560547e-06
c622-101: Epoch: 0, Step: 117, Rank: 62, loss = 0.072265625
c622-051: Epoch: 0, Step: 117, Rank: 52, loss = 0.047119140625
c613-151: Epoch: 0, Step: 117, Rank: 10, loss = 1.0800249583553523e-11
c621-122: Epoch: 0, Step: 117, Rank: 35, loss = 0.00811767578125
c621-091: Epoch: 0, Step: 117, Rank: 28, loss = 0.043701171875
c619-002: Epoch: 0, Step: 117, Rank: 13, loss = 1.4637180356658064e-12
c619-011: Epoch: 0, Step: 117, Rank: 14, loss = 0.66796875
c613-112: Epoch: 0, Step: 117, Rank: 3, loss = 0.01251220703125
c622-082: Epoch: 0, Step: 117, Rank: 59, loss = 0.0198974609375
c613-132: Epoch: 0, Step: 117, Rank: 7, loss = 0.056640625
c621-071: Epoch: 0, Step: 117, Rank: 24, loss = 0.01251220703125
c622-072: Epoch: 0, Step: 117, Rank: 57, loss = 0.0057373046875
c621-061: Epoch: 0, Step: 117, Rank: 22, loss = 0.0242919921875
c622-011: Epoch: 0, Step: 117, Rank: 44, loss = 0.169921875
c613-121: Epoch: 0, Step: 117, Rank: 4, loss = 0.036865234375
c613-131: Epoch: 0, Step: 117, Rank: 6, loss = 0.047119140625
c622-001: Epoch: 0, Step: 117, Rank: 42, loss = 0.00150299072265625
c619-032: Epoch: 0, Step: 117, Rank: 19, loss = 0.00360107421875
c622-102: Epoch: 0, Step: 117, Rank: 63, loss = 0.00012302398681640625
c613-141: Epoch: 0, Step: 117, Rank: 8, loss = 0.08642578125
c621-121: Epoch: 0, Step: 117, Rank: 34, loss = 0.0322265625
c621-152: Epoch: 0, Step: 117, Rank: 41, loss = 0.02978515625
c619-012: Epoch: 0, Step: 117, Rank: 15, loss = 0.03369140625
c613-102: Epoch: 0, Step: 117, Rank: 1, loss = 0.00738525390625
c622-041: Epoch: 0, Step: 117, Rank: 50, loss = 0.000148773193359375
c621-082: Epoch: 0, Step: 117, Rank: 27, loss = 4.500150680541992e-06
c622-032: Epoch: 0, Step: 117, Rank: 49, loss = 0.1796875
c622-042: Epoch: 0, Step: 117, Rank: 51, loss = 0.0888671875
c613-142: Epoch: 0, Step: 117, Rank: 9, loss = 0.002899169921875
c621-052: Epoch: 0, Step: 117, Rank: 21, loss = 0.0205078125
c619-031: Epoch: 0, Step: 117, Rank: 18, loss = 0.0002460479736328125
c613-111: Epoch: 0, Step: 117, Rank: 2, loss = 0.07666015625
c619-041: Epoch: 0, Step: 117, Rank: 20, loss = 0.0001087188720703125
c622-031: Epoch: 0, Step: 117, Rank: 48, loss = 0.69140625
c622-021: Epoch: 0, Step: 117, Rank: 46, loss = 0.0181884765625
c622-022: Epoch: 0, Step: 117, Rank: 47, loss = 2.3245294578089215e-16
c621-102: Epoch: 0, Step: 117, Rank: 31, loss = 0.006317138671875
c621-062: Epoch: 0, Step: 117, Rank: 23, loss = 0.00164794921875
c621-112: Epoch: 0, Step: 117, Rank: 33, loss = 0.69140625
c619-022: Epoch: 0, Step: 117, Rank: 17, loss = 0.06982421875
c621-092: Epoch: 0, Step: 117, Rank: 29, loss = 0.023193359375
c622-091: Epoch: 0, Step: 117, Rank: 60, loss = 0.036865234375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.37s, TFLOPs: 0.80, Samples/sec: 0.42, Time/seq 2.37s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 118, Rank: 43, loss = 0.69140625
c613-101: Epoch: 0, Step: 118, Rank: 0, loss = 0.69140625
c622-081: Epoch: 0, Step: 118, Rank: 58, loss = 0.08251953125
c622-062: Epoch: 0, Step: 118, Rank: 55, loss = 0.04296875
c622-052: Epoch: 0, Step: 118, Rank: 53, loss = 0.0517578125
c622-012: Epoch: 0, Step: 118, Rank: 45, loss = 0.06005859375
c622-061: Epoch: 0, Step: 118, Rank: 54, loss = 0.025146484375
c619-021: Epoch: 0, Step: 118, Rank: 16, loss = 0.00225830078125
c619-002: Epoch: 0, Step: 118, Rank: 13, loss = 0.0052490234375
c622-011: Epoch: 0, Step: 118, Rank: 44, loss = 0.1162109375
c622-001: Epoch: 0, Step: 118, Rank: 42, loss = 7.009506225585938e-05
c619-022: Epoch: 0, Step: 118, Rank: 17, loss = 0.000667572021484375
c619-031: Epoch: 0, Step: 118, Rank: 18, loss = 1.2069940567016602e-06
c621-132: Epoch: 0, Step: 118, Rank: 37, loss = 0.10498046875
c622-042: Epoch: 0, Step: 118, Rank: 51, loss = 0.02880859375
c621-052: Epoch: 0, Step: 118, Rank: 21, loss = 0.050048828125
c621-061: Epoch: 0, Step: 118, Rank: 22, loss = 0.006927490234375
c621-151: Epoch: 0, Step: 118, Rank: 40, loss = 3.213062882423401e-08
c619-001: Epoch: 0, Step: 118, Rank: 12, loss = 0.08251953125
c621-152: Epoch: 0, Step: 118, Rank: 41, loss = 0.002716064453125
c622-051: Epoch: 0, Step: 118, Rank: 52, loss = 0.0322265625
c622-041: Epoch: 0, Step: 118, Rank: 50, loss = 0.006317138671875
c622-031: Epoch: 0, Step: 118, Rank: 48, loss = 0.69140625
c621-131: Epoch: 0, Step: 118, Rank: 36, loss = 0.06494140625
c621-111: Epoch: 0, Step: 118, Rank: 32, loss = 0.01214599609375
c622-022: Epoch: 0, Step: 118, Rank: 47, loss = 1.3732490638087374e-25
c622-021: Epoch: 0, Step: 118, Rank: 46, loss = 0.039794921875
c619-041: Epoch: 0, Step: 118, Rank: 20, loss = 0.0888671875
c622-092: Epoch: 0, Step: 118, Rank: 61, loss = 0.0205078125
c622-032: Epoch: 0, Step: 118, Rank: 49, loss = 0.027099609375
c621-081: Epoch: 0, Step: 118, Rank: 26, loss = 0.0311279296875
c613-151: Epoch: 0, Step: 118, Rank: 10, loss = 0.10498046875
c622-101: Epoch: 0, Step: 118, Rank: 62, loss = 0.05322265625
c613-121: Epoch: 0, Step: 118, Rank: 4, loss = 0.69140625
c621-142: Epoch: 0, Step: 118, Rank: 39, loss = 0.00096893310546875
c613-102: Epoch: 0, Step: 118, Rank: 1, loss = 0.01007080078125
c613-111: Epoch: 0, Step: 118, Rank: 2, loss = 0.05322265625
c613-152: Epoch: 0, Step: 118, Rank: 11, loss = 0.0888671875
c619-032: Epoch: 0, Step: 118, Rank: 19, loss = 0.69140625
c613-131: Epoch: 0, Step: 118, Rank: 6, loss = 0.0002460479736328125
c613-132: Epoch: 0, Step: 118, Rank: 7, loss = 0.0255126953125
c621-072: Epoch: 0, Step: 118, Rank: 25, loss = 0.004913330078125
c613-141: Epoch: 0, Step: 118, Rank: 8, loss = 0.039794921875
c619-012: Epoch: 0, Step: 118, Rank: 15, loss = 0.021484375
c621-122: Epoch: 0, Step: 118, Rank: 35, loss = 0.000431060791015625
c619-011: Epoch: 0, Step: 118, Rank: 14, loss = 0.000553131103515625
c621-141: Epoch: 0, Step: 118, Rank: 38, loss = 0.0057373046875
c621-112: Epoch: 0, Step: 118, Rank: 33, loss = 0.07666015625
c622-082: Epoch: 0, Step: 118, Rank: 59, loss = 0.021240234375
c613-122: Epoch: 0, Step: 118, Rank: 5, loss = 0.007598876953125
c613-142: Epoch: 0, Step: 118, Rank: 9, loss = 0.005584716796875
c621-062: Epoch: 0, Step: 118, Rank: 23, loss = 0.11279296875
c621-121: Epoch: 0, Step: 118, Rank: 34, loss = 0.69140625
c621-091: Epoch: 0, Step: 118, Rank: 28, loss = 0.0010986328125
c621-082: Epoch: 0, Step: 118, Rank: 27, loss = 0.00860595703125
c622-091: Epoch: 0, Step: 118, Rank: 60, loss = 0.314453125
c613-112: Epoch: 0, Step: 118, Rank: 3, loss = 0.12890625
c622-072: Epoch: 0, Step: 118, Rank: 57, loss = 0.00186920166015625
c622-071: Epoch: 0, Step: 118, Rank: 56, loss = 0.027587890625
c621-071: Epoch: 0, Step: 118, Rank: 24, loss = 0.006317138671875
c621-101: Epoch: 0, Step: 118, Rank: 30, loss = 8.940696716308594e-06
c621-102: Epoch: 0, Step: 118, Rank: 31, loss = 0.0052490234375
c622-102: Epoch: 0, Step: 118, Rank: 63, loss = 0.0103759765625
c621-092: Epoch: 0, Step: 118, Rank: 29, loss = 3.4897757426877174e-19
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 119, Rank: 0, loss = 0.0181884765625
c622-081: Epoch: 0, Step: 119, Rank: 58, loss = 0.0164794921875
c621-132: Epoch: 0, Step: 119, Rank: 37, loss = 0.0205078125
c622-002: Epoch: 0, Step: 119, Rank: 43, loss = 0.00124359130859375
c619-021: Epoch: 0, Step: 119, Rank: 16, loss = 0.03271484375
c619-001: Epoch: 0, Step: 119, Rank: 12, loss = 0.00014019012451171875
c619-002: Epoch: 0, Step: 119, Rank: 13, loss = 0.03369140625
c621-151: Epoch: 0, Step: 119, Rank: 40, loss = 0.043701171875
c622-071: Epoch: 0, Step: 119, Rank: 56, loss = 0.01104736328125
c613-131: Epoch: 0, Step: 119, Rank: 6, loss = 0.126953125
c621-111: Epoch: 0, Step: 119, Rank: 32, loss = 0.0198974609375
c621-142: Epoch: 0, Step: 119, Rank: 39, loss = 0.06005859375
c613-121: Epoch: 0, Step: 119, Rank: 4, loss = 5.115907697472721e-12
c621-091: Epoch: 0, Step: 119, Rank: 28, loss = 0.005584716796875
c613-102: Epoch: 0, Step: 119, Rank: 1, loss = 0.109375
c622-001: Epoch: 0, Step: 119, Rank: 42, loss = 0.000911712646484375
c622-092: Epoch: 0, Step: 119, Rank: 61, loss = 0.2177734375
c622-041: Epoch: 0, Step: 119, Rank: 50, loss = 0.01507568359375
c613-151: Epoch: 0, Step: 119, Rank: 10, loss = 0.0849609375
c613-111: Epoch: 0, Step: 119, Rank: 2, loss = 0.69140625
c613-152: Epoch: 0, Step: 119, Rank: 11, loss = 0.0037078857421875
c621-081: Epoch: 0, Step: 119, Rank: 26, loss = 0.04052734375
c622-051: Epoch: 0, Step: 119, Rank: 52, loss = 0.01556396484375
c619-022: Epoch: 0, Step: 119, Rank: 17, loss = 0.000179290771484375
c622-102: Epoch: 0, Step: 119, Rank: 63, loss = 0.072265625
c622-012: Epoch: 0, Step: 119, Rank: 45, loss = 0.0255126953125
c622-101: Epoch: 0, Step: 119, Rank: 62, loss = 0.06298828125
c622-011: Epoch: 0, Step: 119, Rank: 44, loss = 0.044921875
c621-092: Epoch: 0, Step: 119, Rank: 29, loss = 0.003387451171875
c621-121: Epoch: 0, Step: 119, Rank: 34, loss = 0.01068115234375
c622-062: Epoch: 0, Step: 119, Rank: 55, loss = 0.004913330078125
c621-072: Epoch: 0, Step: 119, Rank: 25, loss = 0.0133056640625
c621-101: Epoch: 0, Step: 119, Rank: 30, loss = 0.04296875
c622-032: Epoch: 0, Step: 119, Rank: 49, loss = 0.01177978515625
c621-141: Epoch: 0, Step: 119, Rank: 38, loss = 0.69140625
c613-122: Epoch: 0, Step: 119, Rank: 5, loss = 0.0026397705078125
c622-042: Epoch: 0, Step: 119, Rank: 51, loss = 0.04443359375
c613-112: Epoch: 0, Step: 119, Rank: 3, loss = 0.00714111328125
c622-031: Epoch: 0, Step: 119, Rank: 48, loss = 0.0037078857421875
c613-142: Epoch: 0, Step: 119, Rank: 9, loss = 0.01104736328125
c613-132: Epoch: 0, Step: 119, Rank: 7, loss = 7.048583938740194e-11
c621-082: Epoch: 0, Step: 119, Rank: 27, loss = 0.0133056640625
c621-061: Epoch: 0, Step: 119, Rank: 22, loss = 0.00787353515625
c621-102: Epoch: 0, Step: 119, Rank: 31, loss = 0.048583984375
c622-082: Epoch: 0, Step: 119, Rank: 59, loss = 0.00396728515625
c619-032: Epoch: 0, Step: 119, Rank: 19, loss = 0.14453125
c622-091: Epoch: 0, Step: 119, Rank: 60, loss = 0.017578125
c622-022: Epoch: 0, Step: 119, Rank: 47, loss = 0.23046875
c621-071: Epoch: 0, Step: 119, Rank: 24, loss = 0.0888671875
c619-011: Epoch: 0, Step: 119, Rank: 14, loss = 3.1650415621697903e-10
c621-052: Epoch: 0, Step: 119, Rank: 21, loss = 0.091796875
c619-012: Epoch: 0, Step: 119, Rank: 15, loss = 0.69140625
c613-141: Epoch: 0, Step: 119, Rank: 8, loss = 0.02978515625
c622-061: Epoch: 0, Step: 119, Rank: 54, loss = 0.01177978515625
c621-131: Epoch: 0, Step: 119, Rank: 36, loss = 0.06982421875
c622-072: Epoch: 0, Step: 119, Rank: 57, loss = 0.0026397705078125
c619-031: Epoch: 0, Step: 119, Rank: 18, loss = 0.053955078125
c621-112: Epoch: 0, Step: 119, Rank: 33, loss = 0.0089111328125
c621-152: Epoch: 0, Step: 119, Rank: 41, loss = 0.01556396484375
c621-122: Epoch: 0, Step: 119, Rank: 35, loss = 0.03369140625
c621-062: Epoch: 0, Step: 119, Rank: 23, loss = 0.003387451171875
c619-041: Epoch: 0, Step: 119, Rank: 20, loss = 8.754432201385498e-08
c622-021: Epoch: 0, Step: 119, Rank: 46, loss = 1.9190338207408786e-10
c622-052: Epoch: 0, Step: 119, Rank: 53, loss = 4.418687638008123e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 120, Rank: 43, loss = 0.00124359130859375
c622-001: Epoch: 0, Step: 120, Rank: 42, loss = 0.00946044921875
c621-152: Epoch: 0, Step: 120, Rank: 41, loss = 0.00360107421875
c621-142: Epoch: 0, Step: 120, Rank: 39, loss = 0.6171875
c621-151: Epoch: 0, Step: 120, Rank: 40, loss = 0.054931640625
c621-132: Epoch: 0, Step: 120, Rank: 37, loss = 0.021484375
c621-141: Epoch: 0, Step: 120, Rank: 38, loss = 0.083984375
c621-131: Epoch: 0, Step: 120, Rank: 36, loss = 0.0242919921875
c622-011: Epoch: 0, Step: 120, Rank: 44, loss = 0.0732421875
c621-121: Epoch: 0, Step: 120, Rank: 34, loss = 0.06787109375
c621-111: Epoch: 0, Step: 120, Rank: 32, loss = 0.005584716796875
c621-122: Epoch: 0, Step: 120, Rank: 35, loss = 0.0017547607421875
c621-112: Epoch: 0, Step: 120, Rank: 33, loss = 0.0001316070556640625
c622-012: Epoch: 0, Step: 120, Rank: 45, loss = 0.00860595703125
c621-101: Epoch: 0, Step: 120, Rank: 30, loss = 0.00714111328125
c621-102: Epoch: 0, Step: 120, Rank: 31, loss = 0.002716064453125
c621-092: Epoch: 0, Step: 120, Rank: 29, loss = 0.109375
c621-091: Epoch: 0, Step: 120, Rank: 28, loss = 0.01251220703125
c621-082: Epoch: 0, Step: 120, Rank: 27, loss = 0.0255126953125
c621-081: Epoch: 0, Step: 120, Rank: 26, loss = 0.04931640625
c621-072: Epoch: 0, Step: 120, Rank: 25, loss = 0.00433349609375
c622-021: Epoch: 0, Step: 120, Rank: 46, loss = 0.003082275390625
c621-071: Epoch: 0, Step: 120, Rank: 24, loss = 0.004608154296875
c621-061: Epoch: 0, Step: 120, Rank: 22, loss = 1.150369644165039e-05
c621-062: Epoch: 0, Step: 120, Rank: 23, loss = 0.0047607421875
c621-052: Epoch: 0, Step: 120, Rank: 21, loss = 0.0159912109375
c622-022: Epoch: 0, Step: 120, Rank: 47, loss = 0.08251953125
c619-041: Epoch: 0, Step: 120, Rank: 20, loss = 0.039306640625
c619-021: Epoch: 0, Step: 120, Rank: 16, loss = 0.00946044921875
c619-031: Epoch: 0, Step: 120, Rank: 18, loss = 2.9331204132176936e-11
c619-022: Epoch: 0, Step: 120, Rank: 17, loss = 0.050048828125
c619-032: Epoch: 0, Step: 120, Rank: 19, loss = 0.027587890625
c619-002: Epoch: 0, Step: 120, Rank: 13, loss = 0.0478515625
c619-012: Epoch: 0, Step: 120, Rank: 15, loss = 0.0004730224609375
c622-031: Epoch: 0, Step: 120, Rank: 48, loss = 0.123046875
c619-011: Epoch: 0, Step: 120, Rank: 14, loss = 0.04296875
c619-001: Epoch: 0, Step: 120, Rank: 12, loss = 0.0284423828125
c613-152: Epoch: 0, Step: 120, Rank: 11, loss = 0.0791015625
c613-151: Epoch: 0, Step: 120, Rank: 10, loss = 0.04296875
c613-142: Epoch: 0, Step: 120, Rank: 9, loss = 6.927791673660977e-13
c613-141: Epoch: 0, Step: 120, Rank: 8, loss = 0.69140625
c622-032: Epoch: 0, Step: 120, Rank: 49, loss = 0.14453125
c613-132: Epoch: 0, Step: 120, Rank: 7, loss = 0.0225830078125
c613-131: Epoch: 0, Step: 120, Rank: 6, loss = 0.08642578125
c613-122: Epoch: 0, Step: 120, Rank: 5, loss = 0.02978515625
c613-101: Epoch: 0, Step: 120, Rank: 0, loss = 0.0026397705078125
c613-121: Epoch: 0, Step: 120, Rank: 4, loss = 0.006500244140625
c613-112: Epoch: 0, Step: 120, Rank: 3, loss = 0.0380859375
c622-041: Epoch: 0, Step: 120, Rank: 50, loss = 0.0103759765625
c613-111: Epoch: 0, Step: 120, Rank: 2, loss = 0.390625
c613-102: Epoch: 0, Step: 120, Rank: 1, loss = 0.027587890625
c622-102: Epoch: 0, Step: 120, Rank: 63, loss = 0.10302734375
c622-101: Epoch: 0, Step: 120, Rank: 62, loss = 0.047119140625
c622-092: Epoch: 0, Step: 120, Rank: 61, loss = 0.0380859375
c622-052: Epoch: 0, Step: 120, Rank: 53, loss = 0.022216796875
c622-051: Epoch: 0, Step: 120, Rank: 52, loss = 0.039306640625
c622-042: Epoch: 0, Step: 120, Rank: 51, loss = 0.01708984375
c622-081: Epoch: 0, Step: 120, Rank: 58, loss = 0.000148773193359375
c622-082: Epoch: 0, Step: 120, Rank: 59, loss = 0.000553131103515625
c622-091: Epoch: 0, Step: 120, Rank: 60, loss = 0.003387451171875
c622-061: Epoch: 0, Step: 120, Rank: 54, loss = 0.057373046875
c622-071: Epoch: 0, Step: 120, Rank: 56, loss = 0.095703125
c622-062: Epoch: 0, Step: 120, Rank: 55, loss = 0.01507568359375
c622-072: Epoch: 0, Step: 120, Rank: 57, loss = 0.03369140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 121, Rank: 0, loss = 0.09423828125
c619-021: Epoch: 0, Step: 121, Rank: 16, loss = 0.08251953125
c619-002: Epoch: 0, Step: 121, Rank: 13, loss = 0.048583984375
c619-022: Epoch: 0, Step: 121, Rank: 17, loss = 0.69140625
c619-001: Epoch: 0, Step: 121, Rank: 12, loss = 0.000335693359375
c613-152: Epoch: 0, Step: 121, Rank: 11, loss = 0.004913330078125
c619-012: Epoch: 0, Step: 121, Rank: 15, loss = 3.841705620288849e-09
c619-031: Epoch: 0, Step: 121, Rank: 18, loss = 0.04296875
c622-002: Epoch: 0, Step: 121, Rank: 43, loss = 2.0057740190981832e-18
c619-011: Epoch: 0, Step: 121, Rank: 14, loss = 0.0004730224609375
c613-132: Epoch: 0, Step: 121, Rank: 7, loss = 0.01904296875
c622-081: Epoch: 0, Step: 121, Rank: 58, loss = 0.0311279296875
c621-111: Epoch: 0, Step: 121, Rank: 32, loss = 8.307397365570068e-07
c613-131: Epoch: 0, Step: 121, Rank: 6, loss = 0.0001583099365234375
c613-151: Epoch: 0, Step: 121, Rank: 10, loss = 0.021240234375
c613-122: Epoch: 0, Step: 121, Rank: 5, loss = 0.69140625
c621-091: Epoch: 0, Step: 121, Rank: 28, loss = 0.06494140625
c613-102: Epoch: 0, Step: 121, Rank: 1, loss = 0.004486083984375
c622-092: Epoch: 0, Step: 121, Rank: 61, loss = 7.188646122813225e-09
c613-121: Epoch: 0, Step: 121, Rank: 4, loss = 0.06494140625
c621-081: Epoch: 0, Step: 121, Rank: 26, loss = 0.0380859375
c613-112: Epoch: 0, Step: 121, Rank: 3, loss = 7.283063041541027e-14
c613-141: Epoch: 0, Step: 121, Rank: 8, loss = 9.74978320300579e-10
c622-052: Epoch: 0, Step: 121, Rank: 53, loss = 0.08642578125
c621-121: Epoch: 0, Step: 121, Rank: 34, loss = 0.0028076171875
c621-132: Epoch: 0, Step: 121, Rank: 37, loss = 0.00433349609375
c622-102: Epoch: 0, Step: 121, Rank: 63, loss = 0.02880859375
c621-092: Epoch: 0, Step: 121, Rank: 29, loss = 0.00433349609375
c622-012: Epoch: 0, Step: 121, Rank: 45, loss = 0.18359375
c621-142: Epoch: 0, Step: 121, Rank: 39, loss = 0.0008544921875
c622-101: Epoch: 0, Step: 121, Rank: 62, loss = 0.041015625
c621-151: Epoch: 0, Step: 121, Rank: 40, loss = 0.036376953125
c621-102: Epoch: 0, Step: 121, Rank: 31, loss = 0.00075531005859375
c621-082: Epoch: 0, Step: 121, Rank: 27, loss = 0.1455078125
c622-061: Epoch: 0, Step: 121, Rank: 54, loss = 0.0198974609375
c613-142: Epoch: 0, Step: 121, Rank: 9, loss = 0.16015625
c621-072: Epoch: 0, Step: 121, Rank: 25, loss = 0.050048828125
c622-032: Epoch: 0, Step: 121, Rank: 49, loss = 4.00543212890625e-05
c621-131: Epoch: 0, Step: 121, Rank: 36, loss = 8.003553375601768e-11
c621-101: Epoch: 0, Step: 121, Rank: 30, loss = 0.00070953369140625
c622-062: Epoch: 0, Step: 121, Rank: 55, loss = 0.0181884765625
c622-082: Epoch: 0, Step: 121, Rank: 59, loss = 0.022216796875
c613-111: Epoch: 0, Step: 121, Rank: 2, loss = 0.047119140625
c622-001: Epoch: 0, Step: 121, Rank: 42, loss = 0.039794921875
c622-071: Epoch: 0, Step: 121, Rank: 56, loss = 0.06396484375
c621-122: Epoch: 0, Step: 121, Rank: 35, loss = 0.044921875
c622-041: Epoch: 0, Step: 121, Rank: 50, loss = 0.10009765625
c621-141: Epoch: 0, Step: 121, Rank: 38, loss = 0.0091552734375
c622-051: Epoch: 0, Step: 121, Rank: 52, loss = 0.039794921875
c619-041: Epoch: 0, Step: 121, Rank: 20, loss = 0.00020313262939453125
c621-112: Epoch: 0, Step: 121, Rank: 33, loss = 0.000431060791015625
c621-062: Epoch: 0, Step: 121, Rank: 23, loss = 0.0205078125
c621-052: Epoch: 0, Step: 121, Rank: 21, loss = 0.06298828125
c622-022: Epoch: 0, Step: 121, Rank: 47, loss = 0.12158203125
c619-032: Epoch: 0, Step: 121, Rank: 19, loss = 0.00124359130859375
c622-072: Epoch: 0, Step: 121, Rank: 57, loss = 0.228515625
c622-031: Epoch: 0, Step: 121, Rank: 48, loss = 0.050048828125
c622-091: Epoch: 0, Step: 121, Rank: 60, loss = 0.01507568359375
c622-042: Epoch: 0, Step: 121, Rank: 51, loss = 1.2218952178955078e-05
c622-021: Epoch: 0, Step: 121, Rank: 46, loss = 0.039306640625
c621-071: Epoch: 0, Step: 121, Rank: 24, loss = 0.052490234375
c621-061: Epoch: 0, Step: 121, Rank: 22, loss = 4.94765117764473e-09
c621-152: Epoch: 0, Step: 121, Rank: 41, loss = 0.00014019012451171875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-011: Epoch: 0, Step: 121, Rank: 44, loss = 0.0008544921875
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 122, Rank: 43, loss = 0.054931640625
c622-052: Epoch: 0, Step: 122, Rank: 53, loss = 0.041748046875
c622-012: Epoch: 0, Step: 122, Rank: 45, loss = 0.0067138671875
c621-152: Epoch: 0, Step: 122, Rank: 41, loss = 0.04638671875
c621-151: Epoch: 0, Step: 122, Rank: 40, loss = 0.01904296875
c622-001: Epoch: 0, Step: 122, Rank: 42, loss = 0.0091552734375
c613-101: Epoch: 0, Step: 122, Rank: 0, loss = 0.021240234375
c622-051: Epoch: 0, Step: 122, Rank: 52, loss = 0.003387451171875
c621-132: Epoch: 0, Step: 122, Rank: 37, loss = 0.69140625
c622-041: Epoch: 0, Step: 122, Rank: 50, loss = 0.054931640625
c621-081: Epoch: 0, Step: 122, Rank: 26, loss = 0.0751953125
c622-081: Epoch: 0, Step: 122, Rank: 58, loss = 0.041015625
c622-011: Epoch: 0, Step: 122, Rank: 44, loss = 0.0181884765625
c621-111: Epoch: 0, Step: 122, Rank: 32, loss = 0.027099609375
c621-141: Epoch: 0, Step: 122, Rank: 38, loss = 2.7284841053187847e-12
c619-002: Epoch: 0, Step: 122, Rank: 13, loss = 0.0020599365234375
c622-061: Epoch: 0, Step: 122, Rank: 54, loss = 0.04638671875
c613-122: Epoch: 0, Step: 122, Rank: 5, loss = 0.06787109375
c621-142: Epoch: 0, Step: 122, Rank: 39, loss = 0.048583984375
c613-142: Epoch: 0, Step: 122, Rank: 9, loss = 2.5920599000528455e-11
c622-022: Epoch: 0, Step: 122, Rank: 47, loss = 0.0205078125
c621-082: Epoch: 0, Step: 122, Rank: 27, loss = 0.01068115234375
c613-151: Epoch: 0, Step: 122, Rank: 10, loss = 0.0306396484375
c613-132: Epoch: 0, Step: 122, Rank: 7, loss = 0.0198974609375
c622-032: Epoch: 0, Step: 122, Rank: 49, loss = 0.0322265625
c622-042: Epoch: 0, Step: 122, Rank: 51, loss = 0.04638671875
c621-131: Epoch: 0, Step: 122, Rank: 36, loss = 0.004486083984375
c613-131: Epoch: 0, Step: 122, Rank: 6, loss = 3.583409124985337e-10
c622-031: Epoch: 0, Step: 122, Rank: 48, loss = 0.044921875
c622-062: Epoch: 0, Step: 122, Rank: 55, loss = 1.8041124150158794e-16
c621-061: Epoch: 0, Step: 122, Rank: 22, loss = 0.0057373046875
c619-031: Epoch: 0, Step: 122, Rank: 18, loss = 8.940696716308594e-06
c619-021: Epoch: 0, Step: 122, Rank: 16, loss = 0.005401611328125
c621-101: Epoch: 0, Step: 122, Rank: 30, loss = 2.1457672119140625e-05
c621-091: Epoch: 0, Step: 122, Rank: 28, loss = 0.006500244140625
c622-092: Epoch: 0, Step: 122, Rank: 61, loss = 0.0010986328125
c613-141: Epoch: 0, Step: 122, Rank: 8, loss = 0.0849609375
c621-122: Epoch: 0, Step: 122, Rank: 35, loss = 0.022216796875
c613-152: Epoch: 0, Step: 122, Rank: 11, loss = 4.7222086809427244e-20
c621-071: Epoch: 0, Step: 122, Rank: 24, loss = 1.7848833522293717e-11
c619-012: Epoch: 0, Step: 122, Rank: 15, loss = 0.01416015625
c619-011: Epoch: 0, Step: 122, Rank: 14, loss = 0.01177978515625
c621-102: Epoch: 0, Step: 122, Rank: 31, loss = 0.0380859375
c613-121: Epoch: 0, Step: 122, Rank: 4, loss = 2.905726432800293e-06
c621-121: Epoch: 0, Step: 122, Rank: 34, loss = 0.027587890625
c619-032: Epoch: 0, Step: 122, Rank: 19, loss = 0.1875
c621-052: Epoch: 0, Step: 122, Rank: 21, loss = 0.023193359375
c622-021: Epoch: 0, Step: 122, Rank: 46, loss = 0.06494140625
c619-022: Epoch: 0, Step: 122, Rank: 17, loss = 0.04443359375
c613-111: Epoch: 0, Step: 122, Rank: 2, loss = 0.0003681182861328125
c622-101: Epoch: 0, Step: 122, Rank: 62, loss = 0.00787353515625
c622-082: Epoch: 0, Step: 122, Rank: 59, loss = 0.04296875
c619-001: Epoch: 0, Step: 122, Rank: 12, loss = 3.510081114654895e-12
c622-102: Epoch: 0, Step: 122, Rank: 63, loss = 0.69140625
c622-072: Epoch: 0, Step: 122, Rank: 57, loss = 0.0034942626953125
c619-041: Epoch: 0, Step: 122, Rank: 20, loss = 0.69140625
c622-071: Epoch: 0, Step: 122, Rank: 56, loss = 0.02392578125
c621-072: Epoch: 0, Step: 122, Rank: 25, loss = 1.4051260155412137e-16
c621-062: Epoch: 0, Step: 122, Rank: 23, loss = 0.0091552734375
c621-112: Epoch: 0, Step: 122, Rank: 33, loss = 0.69140625
c613-102: Epoch: 0, Step: 122, Rank: 1, loss = 0.07421875
c613-112: Epoch: 0, Step: 122, Rank: 3, loss = 0.0091552734375
c621-092: Epoch: 0, Step: 122, Rank: 29, loss = 0.69140625
c622-091: Epoch: 0, Step: 122, Rank: 60, loss = 0.005584716796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-121: Epoch: 0, Step: 123, Rank: 4, loss = 0.056640625
c613-101: Epoch: 0, Step: 123, Rank: 0, loss = 0.2412109375
c613-112: Epoch: 0, Step: 123, Rank: 3, loss = 0.01556396484375
c613-122: Epoch: 0, Step: 123, Rank: 5, loss = 0.03271484375
c621-111: Epoch: 0, Step: 123, Rank: 32, loss = 0.027587890625
c613-111: Epoch: 0, Step: 123, Rank: 2, loss = 0.1064453125
c613-131: Epoch: 0, Step: 123, Rank: 6, loss = 0.00811767578125
c622-012: Epoch: 0, Step: 123, Rank: 45, loss = 0.006317138671875
c621-081: Epoch: 0, Step: 123, Rank: 26, loss = 0.01904296875
c622-011: Epoch: 0, Step: 123, Rank: 44, loss = 0.0186767578125
c622-002: Epoch: 0, Step: 123, Rank: 43, loss = 0.01214599609375
c622-081: Epoch: 0, Step: 123, Rank: 58, loss = 0.0164794921875
c619-031: Epoch: 0, Step: 123, Rank: 18, loss = 0.0380859375
c621-132: Epoch: 0, Step: 123, Rank: 37, loss = 0.04296875
c622-001: Epoch: 0, Step: 123, Rank: 42, loss = 0.06298828125
c619-002: Epoch: 0, Step: 123, Rank: 13, loss = 0.03515625
c621-101: Epoch: 0, Step: 123, Rank: 30, loss = 5.617039278149605e-09
c613-102: Epoch: 0, Step: 123, Rank: 1, loss = 0.0242919921875
c619-032: Epoch: 0, Step: 123, Rank: 19, loss = 1.4637180356658064e-12
c622-052: Epoch: 0, Step: 123, Rank: 53, loss = 0.0017547607421875
c621-102: Epoch: 0, Step: 123, Rank: 31, loss = 0.021484375
c621-151: Epoch: 0, Step: 123, Rank: 40, loss = 0.0103759765625
c613-151: Epoch: 0, Step: 123, Rank: 10, loss = 0.69140625
c621-112: Epoch: 0, Step: 123, Rank: 33, loss = 1.0408340855860843e-15
c621-052: Epoch: 0, Step: 123, Rank: 21, loss = 0.083984375
c622-102: Epoch: 0, Step: 123, Rank: 63, loss = 0.69140625
c621-121: Epoch: 0, Step: 123, Rank: 34, loss = 0.00083160400390625
c613-142: Epoch: 0, Step: 123, Rank: 9, loss = 0.004608154296875
c621-131: Epoch: 0, Step: 123, Rank: 36, loss = 0.0186767578125
c619-011: Epoch: 0, Step: 123, Rank: 14, loss = 0.01507568359375
c621-141: Epoch: 0, Step: 123, Rank: 38, loss = 0.01214599609375
c622-092: Epoch: 0, Step: 123, Rank: 61, loss = 0.0002956390380859375
c622-032: Epoch: 0, Step: 123, Rank: 49, loss = 0.0242919921875
c613-132: Epoch: 0, Step: 123, Rank: 7, loss = 0.0322265625
c621-142: Epoch: 0, Step: 123, Rank: 39, loss = 5.995204332975845e-15
c619-021: Epoch: 0, Step: 123, Rank: 16, loss = 1.8775463104248047e-06
c622-101: Epoch: 0, Step: 123, Rank: 62, loss = 0.004913330078125
c621-082: Epoch: 0, Step: 123, Rank: 27, loss = 0.00738525390625
c622-041: Epoch: 0, Step: 123, Rank: 50, loss = 0.0751953125
c621-061: Epoch: 0, Step: 123, Rank: 22, loss = 0.00946044921875
c622-051: Epoch: 0, Step: 123, Rank: 52, loss = 0.09423828125
c621-091: Epoch: 0, Step: 123, Rank: 28, loss = 0.043701171875
c622-082: Epoch: 0, Step: 123, Rank: 59, loss = 0.039794921875
c613-141: Epoch: 0, Step: 123, Rank: 8, loss = 0.01904296875
c621-152: Epoch: 0, Step: 123, Rank: 41, loss = 0.69140625
c619-001: Epoch: 0, Step: 123, Rank: 12, loss = 0.00193023681640625
c621-122: Epoch: 0, Step: 123, Rank: 35, loss = 0.01104736328125
c619-022: Epoch: 0, Step: 123, Rank: 17, loss = 0.0010986328125
c621-092: Epoch: 0, Step: 123, Rank: 29, loss = 0.057373046875
c622-022: Epoch: 0, Step: 123, Rank: 47, loss = 0.0181884765625
c619-041: Epoch: 0, Step: 123, Rank: 20, loss = 0.025146484375
c622-062: Epoch: 0, Step: 123, Rank: 55, loss = 0.0004730224609375
c622-042: Epoch: 0, Step: 123, Rank: 51, loss = 0.01007080078125
c621-071: Epoch: 0, Step: 123, Rank: 24, loss = 0.0849609375
c613-152: Epoch: 0, Step: 123, Rank: 11, loss = 1.3597309589385986e-07
c622-031: Epoch: 0, Step: 123, Rank: 48, loss = 0.00180816650390625
c622-061: Epoch: 0, Step: 123, Rank: 54, loss = 0.00299072265625
c622-072: Epoch: 0, Step: 123, Rank: 57, loss = 2.831068712794149e-15
c621-062: Epoch: 0, Step: 123, Rank: 23, loss = 0.0186767578125
c622-091: Epoch: 0, Step: 123, Rank: 60, loss = 4.1443854570388794e-08
c622-021: Epoch: 0, Step: 123, Rank: 46, loss = 0.06494140625
c619-012: Epoch: 0, Step: 123, Rank: 15, loss = 6.891787052154541e-07
c622-071: Epoch: 0, Step: 123, Rank: 56, loss = 0.022216796875
c621-072: Epoch: 0, Step: 123, Rank: 25, loss = 0.0017547607421875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 124, Rank: 58, loss = 0.01416015625
c622-002: Epoch: 0, Step: 124, Rank: 43, loss = 0.080078125
c613-101: Epoch: 0, Step: 124, Rank: 0, loss = 0.091796875
c622-052: Epoch: 0, Step: 124, Rank: 53, loss = 0.050048828125
c621-082: Epoch: 0, Step: 124, Rank: 27, loss = 0.02978515625
c622-092: Epoch: 0, Step: 124, Rank: 61, loss = 0.00136566162109375
c621-121: Epoch: 0, Step: 124, Rank: 34, loss = 0.022216796875
c619-001: Epoch: 0, Step: 124, Rank: 12, loss = 0.0038299560546875
c621-142: Epoch: 0, Step: 124, Rank: 39, loss = 0.69140625
c619-021: Epoch: 0, Step: 124, Rank: 16, loss = 0.06298828125
c621-151: Epoch: 0, Step: 124, Rank: 40, loss = 0.039794921875
c621-081: Epoch: 0, Step: 124, Rank: 26, loss = 0.123046875
c621-111: Epoch: 0, Step: 124, Rank: 32, loss = 0.0198974609375
c621-072: Epoch: 0, Step: 124, Rank: 25, loss = 3.7670135498046875e-05
c622-061: Epoch: 0, Step: 124, Rank: 54, loss = 0.056640625
c622-062: Epoch: 0, Step: 124, Rank: 55, loss = 0.021484375
c622-012: Epoch: 0, Step: 124, Rank: 45, loss = 4.6798959374427795e-08
c622-102: Epoch: 0, Step: 124, Rank: 63, loss = 0.021240234375
c622-022: Epoch: 0, Step: 124, Rank: 47, loss = 0.00421142578125
c613-121: Epoch: 0, Step: 124, Rank: 4, loss = 0.0262451171875
c613-132: Epoch: 0, Step: 124, Rank: 7, loss = 0.0034942626953125
c622-101: Epoch: 0, Step: 124, Rank: 62, loss = 8.543513119185775e-17
c622-001: Epoch: 0, Step: 124, Rank: 42, loss = 0.01373291015625
c621-131: Epoch: 0, Step: 124, Rank: 36, loss = 1.6540288925170898e-06
c622-051: Epoch: 0, Step: 124, Rank: 52, loss = 0.0028076171875
c613-151: Epoch: 0, Step: 124, Rank: 10, loss = 0.000457763671875
c621-152: Epoch: 0, Step: 124, Rank: 41, loss = 0.00164794921875
c621-091: Epoch: 0, Step: 124, Rank: 28, loss = 0.000431060791015625
c619-032: Epoch: 0, Step: 124, Rank: 19, loss = 0.036376953125
c613-152: Epoch: 0, Step: 124, Rank: 11, loss = 0.01458740234375
c613-131: Epoch: 0, Step: 124, Rank: 6, loss = 6.3792168840089494e-21
c622-072: Epoch: 0, Step: 124, Rank: 57, loss = 4.267692565917969e-05
c619-002: Epoch: 0, Step: 124, Rank: 13, loss = 0.12158203125
c613-111: Epoch: 0, Step: 124, Rank: 2, loss = 1.826414792517085e-21
c621-112: Epoch: 0, Step: 124, Rank: 33, loss = 0.0016021728515625
c622-082: Epoch: 0, Step: 124, Rank: 59, loss = 0.0849609375
c622-041: Epoch: 0, Step: 124, Rank: 50, loss = 5.3085386753082275e-08
c622-071: Epoch: 0, Step: 124, Rank: 56, loss = 0.036376953125
c621-132: Epoch: 0, Step: 124, Rank: 37, loss = 0.00946044921875
c621-052: Epoch: 0, Step: 124, Rank: 21, loss = 0.03271484375
c621-101: Epoch: 0, Step: 124, Rank: 30, loss = 0.11279296875
c622-011: Epoch: 0, Step: 124, Rank: 44, loss = 0.0103759765625
c619-041: Epoch: 0, Step: 124, Rank: 20, loss = 5.617039278149605e-09
c619-011: Epoch: 0, Step: 124, Rank: 14, loss = 5.816113682013476e-24
c621-102: Epoch: 0, Step: 124, Rank: 31, loss = 0.0198974609375
c621-141: Epoch: 0, Step: 124, Rank: 38, loss = 0.083984375
c619-022: Epoch: 0, Step: 124, Rank: 17, loss = 0.69140625
c621-122: Epoch: 0, Step: 124, Rank: 35, loss = 0.1015625
c621-061: Epoch: 0, Step: 124, Rank: 22, loss = 0.04638671875
c619-031: Epoch: 0, Step: 124, Rank: 18, loss = 0.07666015625
c613-112: Epoch: 0, Step: 124, Rank: 3, loss = 0.002471923828125
c621-092: Epoch: 0, Step: 124, Rank: 29, loss = 5.14984130859375e-05
c622-031: Epoch: 0, Step: 124, Rank: 48, loss = 3.314018249511719e-05
c613-122: Epoch: 0, Step: 124, Rank: 5, loss = 0.000606536865234375
c621-071: Epoch: 0, Step: 124, Rank: 24, loss = 0.158203125
c619-012: Epoch: 0, Step: 124, Rank: 15, loss = 0.005584716796875
c622-042: Epoch: 0, Step: 124, Rank: 51, loss = 0.06396484375
c621-062: Epoch: 0, Step: 124, Rank: 23, loss = 1.5688783605583012e-11
c613-141: Epoch: 0, Step: 124, Rank: 8, loss = 0.00738525390625
c613-142: Epoch: 0, Step: 124, Rank: 9, loss = 0.0010986328125
c613-102: Epoch: 0, Step: 124, Rank: 1, loss = 9.74978320300579e-10
c622-091: Epoch: 0, Step: 124, Rank: 60, loss = 0.1162109375
c622-032: Epoch: 0, Step: 124, Rank: 49, loss = 0.00020313262939453125
c622-021: Epoch: 0, Step: 124, Rank: 46, loss = 0.01007080078125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 125, Rank: 0, loss = 0.0198974609375
c619-021: Epoch: 0, Step: 125, Rank: 16, loss = 0.01373291015625
c622-002: Epoch: 0, Step: 125, Rank: 43, loss = 0.0186767578125
c619-002: Epoch: 0, Step: 125, Rank: 13, loss = 1.525040715932846e-08
c622-081: Epoch: 0, Step: 125, Rank: 58, loss = 0.041015625
c622-052: Epoch: 0, Step: 125, Rank: 53, loss = 0.0004730224609375
c613-121: Epoch: 0, Step: 125, Rank: 4, loss = 0.02880859375
c619-012: Epoch: 0, Step: 125, Rank: 15, loss = 0.01416015625
c613-132: Epoch: 0, Step: 125, Rank: 7, loss = 0.0159912109375
c622-062: Epoch: 0, Step: 125, Rank: 55, loss = 0.01007080078125
c613-131: Epoch: 0, Step: 125, Rank: 6, loss = 0.00738525390625
c619-022: Epoch: 0, Step: 125, Rank: 17, loss = 0.00010251998901367188
c622-061: Epoch: 0, Step: 125, Rank: 54, loss = 0.01251220703125
c613-111: Epoch: 0, Step: 125, Rank: 2, loss = 0.109375
c619-001: Epoch: 0, Step: 125, Rank: 12, loss = 0.06201171875
c621-081: Epoch: 0, Step: 125, Rank: 26, loss = 0.005584716796875
c622-101: Epoch: 0, Step: 125, Rank: 62, loss = 0.1142578125
c613-142: Epoch: 0, Step: 125, Rank: 9, loss = 5.4836273193359375e-05
c621-111: Epoch: 0, Step: 125, Rank: 32, loss = 0.01214599609375
c613-102: Epoch: 0, Step: 125, Rank: 1, loss = 0.12158203125
c619-031: Epoch: 0, Step: 125, Rank: 18, loss = 9.74978320300579e-10
c613-152: Epoch: 0, Step: 125, Rank: 11, loss = 0.023193359375
c613-122: Epoch: 0, Step: 125, Rank: 5, loss = 0.0037078857421875
c622-092: Epoch: 0, Step: 125, Rank: 61, loss = 0.451171875
c621-091: Epoch: 0, Step: 125, Rank: 28, loss = 1.418811734765768e-09
c613-141: Epoch: 0, Step: 125, Rank: 8, loss = 0.0001087188720703125
c613-151: Epoch: 0, Step: 125, Rank: 10, loss = 0.0052490234375
c621-061: Epoch: 0, Step: 125, Rank: 22, loss = 0.036376953125
c622-001: Epoch: 0, Step: 125, Rank: 42, loss = 0.005584716796875
c621-082: Epoch: 0, Step: 125, Rank: 27, loss = 0.01214599609375
c622-102: Epoch: 0, Step: 125, Rank: 63, loss = 0.06787109375
c621-102: Epoch: 0, Step: 125, Rank: 31, loss = 4.3655745685100555e-09
c619-032: Epoch: 0, Step: 125, Rank: 19, loss = 0.01287841796875
c613-112: Epoch: 0, Step: 125, Rank: 3, loss = 0.006317138671875
c619-011: Epoch: 0, Step: 125, Rank: 14, loss = 0.06298828125
c622-051: Epoch: 0, Step: 125, Rank: 52, loss = 1.4722347259521484e-05
c619-041: Epoch: 0, Step: 125, Rank: 20, loss = 0.01287841796875
c621-052: Epoch: 0, Step: 125, Rank: 21, loss = 0.022216796875
c622-082: Epoch: 0, Step: 125, Rank: 59, loss = 0.0242919921875
c621-092: Epoch: 0, Step: 125, Rank: 29, loss = 0.00180816650390625
c622-012: Epoch: 0, Step: 125, Rank: 45, loss = 0.1767578125
c621-101: Epoch: 0, Step: 125, Rank: 30, loss = 0.0021209716796875
c621-151: Epoch: 0, Step: 125, Rank: 40, loss = 0.009765625
c622-041: Epoch: 0, Step: 125, Rank: 50, loss = 0.039794921875
c621-131: Epoch: 0, Step: 125, Rank: 36, loss = 0.00116729736328125
c621-152: Epoch: 0, Step: 125, Rank: 41, loss = 0.039794921875
c622-091: Epoch: 0, Step: 125, Rank: 60, loss = 0.00811767578125
c622-022: Epoch: 0, Step: 125, Rank: 47, loss = 0.1181640625
c621-072: Epoch: 0, Step: 125, Rank: 25, loss = 0.0306396484375
c622-032: Epoch: 0, Step: 125, Rank: 49, loss = 0.06982421875
c621-121: Epoch: 0, Step: 125, Rank: 34, loss = 0.69140625
c622-042: Epoch: 0, Step: 125, Rank: 51, loss = 0.04638671875
c622-072: Epoch: 0, Step: 125, Rank: 57, loss = 0.006317138671875
c621-062: Epoch: 0, Step: 125, Rank: 23, loss = 0.29296875
c622-011: Epoch: 0, Step: 125, Rank: 44, loss = 0.005584716796875
c621-112: Epoch: 0, Step: 125, Rank: 33, loss = 0.0986328125
c621-122: Epoch: 0, Step: 125, Rank: 35, loss = 0.01904296875
c621-071: Epoch: 0, Step: 125, Rank: 24, loss = 0.022216796875
c622-031: Epoch: 0, Step: 125, Rank: 48, loss = 0.1494140625
c622-071: Epoch: 0, Step: 125, Rank: 56, loss = 3.213062882423401e-08
c621-132: Epoch: 0, Step: 125, Rank: 37, loss = 0.021240234375
c621-141: Epoch: 0, Step: 125, Rank: 38, loss = 0.0198974609375
c621-142: Epoch: 0, Step: 125, Rank: 39, loss = 0.01708984375
c622-021: Epoch: 0, Step: 125, Rank: 46, loss = 0.2099609375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 126, Rank: 43, loss = 1.0408340855860843e-15
c622-052: Epoch: 0, Step: 126, Rank: 53, loss = 0.1455078125
c613-101: Epoch: 0, Step: 126, Rank: 0, loss = 3.293156623840332e-06
c622-001: Epoch: 0, Step: 126, Rank: 42, loss = 0.052490234375
c622-012: Epoch: 0, Step: 126, Rank: 45, loss = 0.0380859375
c619-031: Epoch: 0, Step: 126, Rank: 18, loss = 0.000644683837890625
c621-151: Epoch: 0, Step: 126, Rank: 40, loss = 0.0021820068359375
c621-111: Epoch: 0, Step: 126, Rank: 32, loss = 0.056640625
c621-081: Epoch: 0, Step: 126, Rank: 26, loss = 0.1162109375
c621-052: Epoch: 0, Step: 126, Rank: 21, loss = 0.00136566162109375
c619-001: Epoch: 0, Step: 126, Rank: 12, loss = 0.01214599609375
c622-041: Epoch: 0, Step: 126, Rank: 50, loss = 0.0091552734375
c619-021: Epoch: 0, Step: 126, Rank: 16, loss = 0.0255126953125
c619-041: Epoch: 0, Step: 126, Rank: 20, loss = 0.01416015625
c621-061: Epoch: 0, Step: 126, Rank: 22, loss = 1.8189894035458565e-09
c621-091: Epoch: 0, Step: 126, Rank: 28, loss = 0.02392578125
c619-002: Epoch: 0, Step: 126, Rank: 13, loss = 0.06494140625
c619-022: Epoch: 0, Step: 126, Rank: 17, loss = 0.021484375
c621-072: Epoch: 0, Step: 126, Rank: 25, loss = 0.0159912109375
c621-142: Epoch: 0, Step: 126, Rank: 39, loss = 0.0052490234375
c622-032: Epoch: 0, Step: 126, Rank: 49, loss = 0.050048828125
c622-011: Epoch: 0, Step: 126, Rank: 44, loss = 1.318767317570746e-10
c621-121: Epoch: 0, Step: 126, Rank: 34, loss = 0.06494140625
c621-152: Epoch: 0, Step: 126, Rank: 41, loss = 0.0001316070556640625
c622-031: Epoch: 0, Step: 126, Rank: 48, loss = 8.585629984736443e-10
c619-011: Epoch: 0, Step: 126, Rank: 14, loss = 8.881784197001252e-13
c621-131: Epoch: 0, Step: 126, Rank: 36, loss = 0.091796875
c622-051: Epoch: 0, Step: 126, Rank: 52, loss = 0.01556396484375
c619-032: Epoch: 0, Step: 126, Rank: 19, loss = 0.004608154296875
c621-112: Epoch: 0, Step: 126, Rank: 33, loss = 0.01556396484375
c621-122: Epoch: 0, Step: 126, Rank: 35, loss = 0.00714111328125
c622-022: Epoch: 0, Step: 126, Rank: 47, loss = 0.0810546875
c619-012: Epoch: 0, Step: 126, Rank: 15, loss = 0.0186767578125
c622-021: Epoch: 0, Step: 126, Rank: 46, loss = 0.01068115234375
c613-152: Epoch: 0, Step: 126, Rank: 11, loss = 0.083984375
c622-081: Epoch: 0, Step: 126, Rank: 58, loss = 0.0181884765625
c621-101: Epoch: 0, Step: 126, Rank: 30, loss = 0.002471923828125
c621-141: Epoch: 0, Step: 126, Rank: 38, loss = 0.00136566162109375
c622-061: Epoch: 0, Step: 126, Rank: 54, loss = 0.01507568359375
c621-082: Epoch: 0, Step: 126, Rank: 27, loss = 0.027587890625
c621-132: Epoch: 0, Step: 126, Rank: 37, loss = 0.02392578125
c622-101: Epoch: 0, Step: 126, Rank: 62, loss = 0.10009765625
c613-131: Epoch: 0, Step: 126, Rank: 6, loss = 0.0242919921875
c613-121: Epoch: 0, Step: 126, Rank: 4, loss = 0.023193359375
c622-042: Epoch: 0, Step: 126, Rank: 51, loss = 0.059326171875
c613-132: Epoch: 0, Step: 126, Rank: 7, loss = 0.017578125
c613-141: Epoch: 0, Step: 126, Rank: 8, loss = 0.07666015625
c621-102: Epoch: 0, Step: 126, Rank: 31, loss = 3.268496584496461e-13
c622-102: Epoch: 0, Step: 126, Rank: 63, loss = 6.927791673660977e-13
c621-071: Epoch: 0, Step: 126, Rank: 24, loss = 0.039794921875
c613-112: Epoch: 0, Step: 126, Rank: 3, loss = 0.01104736328125
c622-062: Epoch: 0, Step: 126, Rank: 55, loss = 0.0284423828125
c613-102: Epoch: 0, Step: 126, Rank: 1, loss = 0.1669921875
c613-111: Epoch: 0, Step: 126, Rank: 2, loss = 0.0001087188720703125
c622-071: Epoch: 0, Step: 126, Rank: 56, loss = 1.2656542480726785e-14
c621-092: Epoch: 0, Step: 126, Rank: 29, loss = 0.036376953125
c622-092: Epoch: 0, Step: 126, Rank: 61, loss = 0.09716796875
c613-122: Epoch: 0, Step: 126, Rank: 5, loss = 0.02978515625
c613-142: Epoch: 0, Step: 126, Rank: 9, loss = 1.2790197503539935e-19
c621-062: Epoch: 0, Step: 126, Rank: 23, loss = 0.03271484375
c622-072: Epoch: 0, Step: 126, Rank: 57, loss = 0.054931640625
c613-151: Epoch: 0, Step: 126, Rank: 10, loss = 0.0380859375
c622-082: Epoch: 0, Step: 126, Rank: 59, loss = 0.091796875
c622-091: Epoch: 0, Step: 126, Rank: 60, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.76123046875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 127, Rank: 58, loss = 0.69140625
c613-101: Epoch: 0, Step: 127, Rank: 0, loss = 1.8189894035458565e-09
c622-102: Epoch: 0, Step: 127, Rank: 63, loss = 0.00180816650390625
c622-052: Epoch: 0, Step: 127, Rank: 53, loss = 0.06396484375
c622-062: Epoch: 0, Step: 127, Rank: 55, loss = 0.054931640625
c621-081: Epoch: 0, Step: 127, Rank: 26, loss = 0.000392913818359375
c622-101: Epoch: 0, Step: 127, Rank: 62, loss = 3.528594970703125e-05
c622-071: Epoch: 0, Step: 127, Rank: 56, loss = 0.021484375
c622-002: Epoch: 0, Step: 127, Rank: 43, loss = 0.036376953125
c621-072: Epoch: 0, Step: 127, Rank: 25, loss = 0.06591796875
c619-001: Epoch: 0, Step: 127, Rank: 12, loss = 0.01507568359375
c613-111: Epoch: 0, Step: 127, Rank: 2, loss = 6.927791673660977e-13
c619-021: Epoch: 0, Step: 127, Rank: 16, loss = 0.17578125
c613-102: Epoch: 0, Step: 127, Rank: 1, loss = 0.00433349609375
c622-092: Epoch: 0, Step: 127, Rank: 61, loss = 5.0182080713057076e-14
c621-151: Epoch: 0, Step: 127, Rank: 40, loss = 0.0205078125
c622-051: Epoch: 0, Step: 127, Rank: 52, loss = 4.3655745685100555e-09
c613-112: Epoch: 0, Step: 127, Rank: 3, loss = 0.039794921875
c622-091: Epoch: 0, Step: 127, Rank: 60, loss = 0.0133056640625
c621-132: Epoch: 0, Step: 127, Rank: 37, loss = 0.025146484375
c622-001: Epoch: 0, Step: 127, Rank: 42, loss = 0.0005035400390625
c622-061: Epoch: 0, Step: 127, Rank: 54, loss = 0.036376953125
c622-072: Epoch: 0, Step: 127, Rank: 57, loss = 0.0284423828125
c622-082: Epoch: 0, Step: 127, Rank: 59, loss = 0.0103759765625
c613-121: Epoch: 0, Step: 127, Rank: 4, loss = 3.213062882423401e-08
c613-151: Epoch: 0, Step: 127, Rank: 10, loss = 0.06494140625
c621-142: Epoch: 0, Step: 127, Rank: 39, loss = 0.69140625
c621-131: Epoch: 0, Step: 127, Rank: 36, loss = 7.963180541992188e-05
c619-031: Epoch: 0, Step: 127, Rank: 18, loss = 0.55078125
c619-011: Epoch: 0, Step: 127, Rank: 14, loss = 0.69140625
c621-091: Epoch: 0, Step: 127, Rank: 28, loss = 6.927791673660977e-13
c613-122: Epoch: 0, Step: 127, Rank: 5, loss = 0.01904296875
c622-042: Epoch: 0, Step: 127, Rank: 51, loss = 0.69140625
c621-121: Epoch: 0, Step: 127, Rank: 34, loss = 0.0052490234375
c613-131: Epoch: 0, Step: 127, Rank: 6, loss = 0.045654296875
c619-032: Epoch: 0, Step: 127, Rank: 19, loss = 0.00150299072265625
c622-012: Epoch: 0, Step: 127, Rank: 45, loss = 0.003082275390625
c621-101: Epoch: 0, Step: 127, Rank: 30, loss = 0.000667572021484375
c613-152: Epoch: 0, Step: 127, Rank: 11, loss = 2.066371962428093e-09
c613-142: Epoch: 0, Step: 127, Rank: 9, loss = 0.07421875
c622-031: Epoch: 0, Step: 127, Rank: 48, loss = 0.021240234375
c621-112: Epoch: 0, Step: 127, Rank: 33, loss = 0.0067138671875
c621-082: Epoch: 0, Step: 127, Rank: 27, loss = 7.420778274536133e-06
c613-132: Epoch: 0, Step: 127, Rank: 7, loss = 0.0067138671875
c621-122: Epoch: 0, Step: 127, Rank: 35, loss = 0.00299072265625
c621-071: Epoch: 0, Step: 127, Rank: 24, loss = 0.045654296875
c621-092: Epoch: 0, Step: 127, Rank: 29, loss = 0.041015625
c621-152: Epoch: 0, Step: 127, Rank: 41, loss = 0.00433349609375
c613-141: Epoch: 0, Step: 127, Rank: 8, loss = 0.0242919921875
c621-141: Epoch: 0, Step: 127, Rank: 38, loss = 0.111328125
c622-041: Epoch: 0, Step: 127, Rank: 50, loss = 0.0751953125
c622-032: Epoch: 0, Step: 127, Rank: 49, loss = 0.021240234375
c619-022: Epoch: 0, Step: 127, Rank: 17, loss = 0.328125
c619-041: Epoch: 0, Step: 127, Rank: 20, loss = 0.00040435791015625
c619-002: Epoch: 0, Step: 127, Rank: 13, loss = 0.0091552734375
c621-062: Epoch: 0, Step: 127, Rank: 23, loss = 0.01251220703125
c622-022: Epoch: 0, Step: 127, Rank: 47, loss = 0.025146484375
c622-011: Epoch: 0, Step: 127, Rank: 44, loss = 0.125
c621-061: Epoch: 0, Step: 127, Rank: 22, loss = 5.3085386753082275e-08
c621-102: Epoch: 0, Step: 127, Rank: 31, loss = 5.424022674560547e-06
c621-052: Epoch: 0, Step: 127, Rank: 21, loss = 8.003553375601768e-11
c619-012: Epoch: 0, Step: 127, Rank: 15, loss = 0.000148773193359375
c621-111: Epoch: 0, Step: 127, Rank: 32, loss = 0.01507568359375
c622-021: Epoch: 0, Step: 127, Rank: 46, loss = 0.03271484375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.13s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.13s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 128, Rank: 16, loss = 0.01007080078125
c621-081: Epoch: 0, Step: 128, Rank: 26, loss = 0.00083160400390625
c621-132: Epoch: 0, Step: 128, Rank: 37, loss = 0.00075531005859375
c619-031: Epoch: 0, Step: 128, Rank: 18, loss = 0.00714111328125
c621-091: Epoch: 0, Step: 128, Rank: 28, loss = 0.0025482177734375
c621-111: Epoch: 0, Step: 128, Rank: 32, loss = 0.0016021728515625
c613-101: Epoch: 0, Step: 128, Rank: 0, loss = 0.01141357421875
c621-072: Epoch: 0, Step: 128, Rank: 25, loss = 0.0023956298828125
c619-022: Epoch: 0, Step: 128, Rank: 17, loss = 0.2412109375
c622-002: Epoch: 0, Step: 128, Rank: 43, loss = 0.0037078857421875
c621-121: Epoch: 0, Step: 128, Rank: 34, loss = 0.0032806396484375
c621-122: Epoch: 0, Step: 128, Rank: 35, loss = 0.0091552734375
c621-082: Epoch: 0, Step: 128, Rank: 27, loss = 0.003173828125
c621-112: Epoch: 0, Step: 128, Rank: 33, loss = 0.0001087188720703125
c621-131: Epoch: 0, Step: 128, Rank: 36, loss = 0.0284423828125
c622-001: Epoch: 0, Step: 128, Rank: 42, loss = 0.01556396484375
c619-032: Epoch: 0, Step: 128, Rank: 19, loss = 0.02978515625
c621-141: Epoch: 0, Step: 128, Rank: 38, loss = 0.0002956390380859375
c621-071: Epoch: 0, Step: 128, Rank: 24, loss = 0.01287841796875
c621-151: Epoch: 0, Step: 128, Rank: 40, loss = 0.06396484375
c621-062: Epoch: 0, Step: 128, Rank: 23, loss = 0.006103515625
c619-041: Epoch: 0, Step: 128, Rank: 20, loss = 2.562999725341797e-06
c622-051: Epoch: 0, Step: 128, Rank: 52, loss = 0.0284423828125
c619-012: Epoch: 0, Step: 128, Rank: 15, loss = 0.0002613067626953125
c621-052: Epoch: 0, Step: 128, Rank: 21, loss = 0.0103759765625
c621-102: Epoch: 0, Step: 128, Rank: 31, loss = 0.00836181640625
c621-142: Epoch: 0, Step: 128, Rank: 39, loss = 0.0016021728515625
c622-012: Epoch: 0, Step: 128, Rank: 45, loss = 6.5635692504717e-31
c619-001: Epoch: 0, Step: 128, Rank: 12, loss = 0.00180816650390625
c619-011: Epoch: 0, Step: 128, Rank: 14, loss = 0.000644683837890625
c621-152: Epoch: 0, Step: 128, Rank: 41, loss = 6.993104012531504e-18
c621-061: Epoch: 0, Step: 128, Rank: 22, loss = 0.0001583099365234375
c619-002: Epoch: 0, Step: 128, Rank: 13, loss = 0.0008544921875
c622-032: Epoch: 0, Step: 128, Rank: 49, loss = 3.841705620288849e-09
c622-031: Epoch: 0, Step: 128, Rank: 48, loss = 0.07080078125
c613-152: Epoch: 0, Step: 128, Rank: 11, loss = 0.0159912109375
c613-121: Epoch: 0, Step: 128, Rank: 4, loss = 0.000431060791015625
c622-061: Epoch: 0, Step: 128, Rank: 54, loss = 0.025146484375
c613-142: Epoch: 0, Step: 128, Rank: 9, loss = 0.004913330078125
c622-022: Epoch: 0, Step: 128, Rank: 47, loss = 0.0133056640625
c621-101: Epoch: 0, Step: 128, Rank: 30, loss = 0.036376953125
c613-151: Epoch: 0, Step: 128, Rank: 10, loss = 1.4722347259521484e-05
c613-132: Epoch: 0, Step: 128, Rank: 7, loss = 1.126900315284729e-07
c622-011: Epoch: 0, Step: 128, Rank: 44, loss = 0.000644683837890625
c622-062: Epoch: 0, Step: 128, Rank: 55, loss = 0.000644683837890625
c622-071: Epoch: 0, Step: 128, Rank: 56, loss = 0.212890625
c613-111: Epoch: 0, Step: 128, Rank: 2, loss = 0.0196533203125
c622-021: Epoch: 0, Step: 128, Rank: 46, loss = 0.004913330078125
c622-042: Epoch: 0, Step: 128, Rank: 51, loss = 0.0002460479736328125
c613-102: Epoch: 0, Step: 128, Rank: 1, loss = 0.01416015625
c622-041: Epoch: 0, Step: 128, Rank: 50, loss = 4.839897155761719e-05
c613-122: Epoch: 0, Step: 128, Rank: 5, loss = 0.005584716796875
c613-112: Epoch: 0, Step: 128, Rank: 3, loss = 0.000335693359375
c613-141: Epoch: 0, Step: 128, Rank: 8, loss = 4.00543212890625e-05
c622-101: Epoch: 0, Step: 128, Rank: 62, loss = 0.00299072265625
c621-092: Epoch: 0, Step: 128, Rank: 29, loss = 0.00180816650390625
c613-131: Epoch: 0, Step: 128, Rank: 6, loss = 0.002899169921875
c622-081: Epoch: 0, Step: 128, Rank: 58, loss = 0.00714111328125
c622-052: Epoch: 0, Step: 128, Rank: 53, loss = 0.01287841796875
c622-102: Epoch: 0, Step: 128, Rank: 63, loss = 0.0322265625
c622-092: Epoch: 0, Step: 128, Rank: 61, loss = 4.7222086809427244e-20
c622-072: Epoch: 0, Step: 128, Rank: 57, loss = 0.0017547607421875
c622-091: Epoch: 0, Step: 128, Rank: 60, loss = 0.00180816650390625
c622-082: Epoch: 0, Step: 128, Rank: 59, loss = 0.001983642578125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 129, Rank: 26, loss = 0.00128173828125
c621-091: Epoch: 0, Step: 129, Rank: 28, loss = 8.487701416015625e-05
c621-072: Epoch: 0, Step: 129, Rank: 25, loss = 0.13671875
c621-132: Epoch: 0, Step: 129, Rank: 37, loss = 0.04443359375
c621-082: Epoch: 0, Step: 129, Rank: 27, loss = 0.000553131103515625
c621-131: Epoch: 0, Step: 129, Rank: 36, loss = 6.993104012531504e-18
c622-002: Epoch: 0, Step: 129, Rank: 43, loss = 7.496324301261813e-24
c622-052: Epoch: 0, Step: 129, Rank: 53, loss = 0.006103515625
c622-012: Epoch: 0, Step: 129, Rank: 45, loss = 0.01708984375
c621-111: Epoch: 0, Step: 129, Rank: 32, loss = 0.00408935546875
c619-031: Epoch: 0, Step: 129, Rank: 18, loss = 0.0517578125
c613-101: Epoch: 0, Step: 129, Rank: 0, loss = 0.002471923828125
c621-151: Epoch: 0, Step: 129, Rank: 40, loss = 0.0007781982421875
c619-022: Epoch: 0, Step: 129, Rank: 17, loss = 0.002716064453125
c621-052: Epoch: 0, Step: 129, Rank: 21, loss = 0.69140625
c619-002: Epoch: 0, Step: 129, Rank: 13, loss = 0.126953125
c619-021: Epoch: 0, Step: 129, Rank: 16, loss = 0.00124359130859375
c622-001: Epoch: 0, Step: 129, Rank: 42, loss = 1.1874362826347351e-08
c621-142: Epoch: 0, Step: 129, Rank: 39, loss = 0.0010986328125
c619-001: Epoch: 0, Step: 129, Rank: 12, loss = 0.036865234375
c621-152: Epoch: 0, Step: 129, Rank: 41, loss = 0.00193023681640625
c622-081: Epoch: 0, Step: 129, Rank: 58, loss = 0.023193359375
c619-041: Epoch: 0, Step: 129, Rank: 20, loss = 7.486343383789062e-05
c613-132: Epoch: 0, Step: 129, Rank: 7, loss = 0.0038299560546875
c613-131: Epoch: 0, Step: 129, Rank: 6, loss = 4.05634636990726e-10
c613-151: Epoch: 0, Step: 129, Rank: 10, loss = 0.00150299072265625
c621-071: Epoch: 0, Step: 129, Rank: 24, loss = 0.00946044921875
c621-061: Epoch: 0, Step: 129, Rank: 22, loss = 7.963180541992188e-05
c622-041: Epoch: 0, Step: 129, Rank: 50, loss = 0.0023193359375
c621-141: Epoch: 0, Step: 129, Rank: 38, loss = 9.012222290039062e-05
c621-122: Epoch: 0, Step: 129, Rank: 35, loss = 0.00836181640625
c622-011: Epoch: 0, Step: 129, Rank: 44, loss = 0.00150299072265625
c621-101: Epoch: 0, Step: 129, Rank: 30, loss = 0.03466796875
c622-032: Epoch: 0, Step: 129, Rank: 49, loss = 0.00128173828125
c613-111: Epoch: 0, Step: 129, Rank: 2, loss = 0.06494140625
c621-121: Epoch: 0, Step: 129, Rank: 34, loss = 0.02880859375
c621-102: Epoch: 0, Step: 129, Rank: 31, loss = 0.00860595703125
c613-152: Epoch: 0, Step: 129, Rank: 11, loss = 0.03515625
c622-022: Epoch: 0, Step: 129, Rank: 47, loss = 0.00811767578125
c621-092: Epoch: 0, Step: 129, Rank: 29, loss = 5.3085386753082275e-08
c622-061: Epoch: 0, Step: 129, Rank: 54, loss = 0.000667572021484375
c619-011: Epoch: 0, Step: 129, Rank: 14, loss = 0.000335693359375
c621-112: Epoch: 0, Step: 129, Rank: 33, loss = 0.00170135498046875
c621-062: Epoch: 0, Step: 129, Rank: 23, loss = 0.00811767578125
c622-071: Epoch: 0, Step: 129, Rank: 56, loss = 0.00225830078125
c622-021: Epoch: 0, Step: 129, Rank: 46, loss = 0.01556396484375
c613-112: Epoch: 0, Step: 129, Rank: 3, loss = 0.01416015625
c613-122: Epoch: 0, Step: 129, Rank: 5, loss = 9.370282327836321e-14
c622-051: Epoch: 0, Step: 129, Rank: 52, loss = 0.00083160400390625
c622-072: Epoch: 0, Step: 129, Rank: 57, loss = 0.00396728515625
c613-102: Epoch: 0, Step: 129, Rank: 1, loss = 0.003082275390625
c622-031: Epoch: 0, Step: 129, Rank: 48, loss = 0.041748046875
c622-082: Epoch: 0, Step: 129, Rank: 59, loss = 0.0198974609375
c622-101: Epoch: 0, Step: 129, Rank: 62, loss = 0.0026397705078125
c622-091: Epoch: 0, Step: 129, Rank: 60, loss = 0.01507568359375
c613-141: Epoch: 0, Step: 129, Rank: 8, loss = 0.0003566741943359375
c613-121: Epoch: 0, Step: 129, Rank: 4, loss = 0.01416015625
c619-012: Epoch: 0, Step: 129, Rank: 15, loss = 0.00164794921875
c622-092: Epoch: 0, Step: 129, Rank: 61, loss = 0.01214599609375
c622-042: Epoch: 0, Step: 129, Rank: 51, loss = 0.00099945068359375
c613-142: Epoch: 0, Step: 129, Rank: 9, loss = 1.0788440704345703e-05
c619-032: Epoch: 0, Step: 129, Rank: 19, loss = 0.002716064453125
c622-102: Epoch: 0, Step: 129, Rank: 63, loss = 0.0016021728515625
c622-062: Epoch: 0, Step: 129, Rank: 55, loss = 0.005584716796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 130, Rank: 43, loss = 7.283063041541027e-14
c613-101: Epoch: 0, Step: 130, Rank: 0, loss = 0.0
c622-081: Epoch: 0, Step: 130, Rank: 58, loss = 0.02978515625
c622-071: Epoch: 0, Step: 130, Rank: 56, loss = 0.0002956390380859375
c622-011: Epoch: 0, Step: 130, Rank: 44, loss = 0.01104736328125
c622-012: Epoch: 0, Step: 130, Rank: 45, loss = 0.00010251998901367188
c622-001: Epoch: 0, Step: 130, Rank: 42, loss = 3.655441105365753e-08
c622-101: Epoch: 0, Step: 130, Rank: 62, loss = 0.0205078125
c621-081: Epoch: 0, Step: 130, Rank: 26, loss = 0.003387451171875
c622-052: Epoch: 0, Step: 130, Rank: 53, loss = 0.00099945068359375
c619-021: Epoch: 0, Step: 130, Rank: 16, loss = 0.0089111328125
c622-051: Epoch: 0, Step: 130, Rank: 52, loss = 0.0023193359375
c622-062: Epoch: 0, Step: 130, Rank: 55, loss = 7.338821887969971e-07
c621-061: Epoch: 0, Step: 130, Rank: 22, loss = 0.0067138671875
c613-111: Epoch: 0, Step: 130, Rank: 2, loss = 0.69140625
c613-112: Epoch: 0, Step: 130, Rank: 3, loss = 0.0181884765625
c613-121: Epoch: 0, Step: 130, Rank: 4, loss = 0.0037078857421875
c613-141: Epoch: 0, Step: 130, Rank: 8, loss = 0.6171875
c621-072: Epoch: 0, Step: 130, Rank: 25, loss = 0.0164794921875
c621-132: Epoch: 0, Step: 130, Rank: 37, loss = 0.000606536865234375
c621-151: Epoch: 0, Step: 130, Rank: 40, loss = 2.726912498474121e-06
c622-072: Epoch: 0, Step: 130, Rank: 57, loss = 0.00180816650390625
c613-142: Epoch: 0, Step: 130, Rank: 9, loss = 0.06201171875
c613-102: Epoch: 0, Step: 130, Rank: 1, loss = 9.632110595703125e-05
c613-132: Epoch: 0, Step: 130, Rank: 7, loss = 1.6015625
c621-111: Epoch: 0, Step: 130, Rank: 32, loss = 2.2351741790771484e-07
c613-131: Epoch: 0, Step: 130, Rank: 6, loss = 0.69140625
c613-152: Epoch: 0, Step: 130, Rank: 11, loss = 0.00225830078125
c622-032: Epoch: 0, Step: 130, Rank: 49, loss = 0.00020313262939453125
c619-031: Epoch: 0, Step: 130, Rank: 18, loss = 0.006317138671875
c622-022: Epoch: 0, Step: 130, Rank: 47, loss = 0.0010986328125
c621-152: Epoch: 0, Step: 130, Rank: 41, loss = 0.69140625
c622-102: Epoch: 0, Step: 130, Rank: 63, loss = 9.012222290039062e-05
c619-001: Epoch: 0, Step: 130, Rank: 12, loss = 0.0091552734375
c619-002: Epoch: 0, Step: 130, Rank: 13, loss = 0.00299072265625
c613-151: Epoch: 0, Step: 130, Rank: 10, loss = 0.00124359130859375
c622-091: Epoch: 0, Step: 130, Rank: 60, loss = 0.0020599365234375
c619-041: Epoch: 0, Step: 130, Rank: 20, loss = 0.0007781982421875
c622-041: Epoch: 0, Step: 130, Rank: 50, loss = 0.0067138671875
c622-092: Epoch: 0, Step: 130, Rank: 61, loss = 3.7670135498046875e-05
c622-061: Epoch: 0, Step: 130, Rank: 54, loss = 0.002471923828125
c621-142: Epoch: 0, Step: 130, Rank: 39, loss = 0.00040435791015625
c621-052: Epoch: 0, Step: 130, Rank: 21, loss = 0.000179290771484375
c622-021: Epoch: 0, Step: 130, Rank: 46, loss = 0.0067138671875
c621-141: Epoch: 0, Step: 130, Rank: 38, loss = 0.0004730224609375
c622-042: Epoch: 0, Step: 130, Rank: 51, loss = 0.275390625
c621-122: Epoch: 0, Step: 130, Rank: 35, loss = 0.0057373046875
c621-091: Epoch: 0, Step: 130, Rank: 28, loss = 0.021484375
c621-131: Epoch: 0, Step: 130, Rank: 36, loss = 0.0003681182861328125
c621-071: Epoch: 0, Step: 130, Rank: 24, loss = 0.00083160400390625
c621-121: Epoch: 0, Step: 130, Rank: 34, loss = 0.006103515625
c622-031: Epoch: 0, Step: 130, Rank: 48, loss = 0.027587890625
c621-062: Epoch: 0, Step: 130, Rank: 23, loss = 0.0020599365234375
c621-112: Epoch: 0, Step: 130, Rank: 33, loss = 0.006103515625
c622-082: Epoch: 0, Step: 130, Rank: 59, loss = 0.0005035400390625
c613-122: Epoch: 0, Step: 130, Rank: 5, loss = 0.0005035400390625
c619-012: Epoch: 0, Step: 130, Rank: 15, loss = 0.00099945068359375
c619-022: Epoch: 0, Step: 130, Rank: 17, loss = 7.009506225585938e-05
c621-082: Epoch: 0, Step: 130, Rank: 27, loss = 0.00433349609375
c619-032: Epoch: 0, Step: 130, Rank: 19, loss = 0.00083160400390625
c621-092: Epoch: 0, Step: 130, Rank: 29, loss = 0.0198974609375
c621-101: Epoch: 0, Step: 130, Rank: 30, loss = 6.439293542825908e-14
c621-102: Epoch: 0, Step: 130, Rank: 31, loss = 3.725290298461914e-06
c619-011: Epoch: 0, Step: 130, Rank: 14, loss = 5.995204332975845e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 131, Rank: 58, loss = 0.000431060791015625
c622-052: Epoch: 0, Step: 131, Rank: 53, loss = 0.006927490234375
c622-071: Epoch: 0, Step: 131, Rank: 56, loss = 0.0008544921875
c622-031: Epoch: 0, Step: 131, Rank: 48, loss = 0.006927490234375
c622-032: Epoch: 0, Step: 131, Rank: 49, loss = 5.424022674560547e-06
c622-062: Epoch: 0, Step: 131, Rank: 55, loss = 0.0005035400390625
c622-041: Epoch: 0, Step: 131, Rank: 50, loss = 0.00860595703125
c622-022: Epoch: 0, Step: 131, Rank: 47, loss = 0.1640625
c622-061: Epoch: 0, Step: 131, Rank: 54, loss = 0.00150299072265625
c622-082: Epoch: 0, Step: 131, Rank: 59, loss = 0.0181884765625
c622-051: Epoch: 0, Step: 131, Rank: 52, loss = 0.0017547607421875
c622-042: Epoch: 0, Step: 131, Rank: 51, loss = 9.098986738083304e-23
c622-091: Epoch: 0, Step: 131, Rank: 60, loss = 0.0038299560546875
c622-072: Epoch: 0, Step: 131, Rank: 57, loss = 0.00116729736328125
c622-012: Epoch: 0, Step: 131, Rank: 45, loss = 0.005584716796875
c622-092: Epoch: 0, Step: 131, Rank: 61, loss = 0.0181884765625
c622-101: Epoch: 0, Step: 131, Rank: 62, loss = 0.0322265625
c622-002: Epoch: 0, Step: 131, Rank: 43, loss = 0.03466796875
c622-102: Epoch: 0, Step: 131, Rank: 63, loss = 0.006317138671875
c622-021: Epoch: 0, Step: 131, Rank: 46, loss = 0.01507568359375
c622-011: Epoch: 0, Step: 131, Rank: 44, loss = 0.01416015625
c622-001: Epoch: 0, Step: 131, Rank: 42, loss = 0.002899169921875
c613-101: Epoch: 0, Step: 131, Rank: 0, loss = 0.69140625
c621-152: Epoch: 0, Step: 131, Rank: 41, loss = 0.0038299560546875
c621-151: Epoch: 0, Step: 131, Rank: 40, loss = 0.01068115234375
c621-142: Epoch: 0, Step: 131, Rank: 39, loss = 7.009506225585938e-05
c621-132: Epoch: 0, Step: 131, Rank: 37, loss = 0.0181884765625
c621-141: Epoch: 0, Step: 131, Rank: 38, loss = 0.00738525390625
c621-131: Epoch: 0, Step: 131, Rank: 36, loss = 2.2118911147117615e-08
c621-122: Epoch: 0, Step: 131, Rank: 35, loss = 0.006103515625
c621-081: Epoch: 0, Step: 131, Rank: 26, loss = 0.00396728515625
c621-111: Epoch: 0, Step: 131, Rank: 32, loss = 0.0091552734375
c621-112: Epoch: 0, Step: 131, Rank: 33, loss = 0.01104736328125
c619-021: Epoch: 0, Step: 131, Rank: 16, loss = 0.0017547607421875
c621-072: Epoch: 0, Step: 131, Rank: 25, loss = 0.01507568359375
c619-002: Epoch: 0, Step: 131, Rank: 13, loss = 0.044921875
c621-082: Epoch: 0, Step: 131, Rank: 27, loss = 0.0026397705078125
c621-091: Epoch: 0, Step: 131, Rank: 28, loss = 0.0002460479736328125
c621-121: Epoch: 0, Step: 131, Rank: 34, loss = 6.198883056640625e-05
c621-102: Epoch: 0, Step: 131, Rank: 31, loss = 6.628036499023438e-05
c613-131: Epoch: 0, Step: 131, Rank: 6, loss = 0.00408935546875
c621-101: Epoch: 0, Step: 131, Rank: 30, loss = 0.00299072265625
c621-052: Epoch: 0, Step: 131, Rank: 21, loss = 0.000667572021484375
c619-001: Epoch: 0, Step: 131, Rank: 12, loss = 0.00506591796875
c613-132: Epoch: 0, Step: 131, Rank: 7, loss = 0.00180816650390625
c613-102: Epoch: 0, Step: 131, Rank: 1, loss = 4.00543212890625e-05
c613-151: Epoch: 0, Step: 131, Rank: 10, loss = 0.0262451171875
c619-012: Epoch: 0, Step: 131, Rank: 15, loss = 0.0133056640625
c613-152: Epoch: 0, Step: 131, Rank: 11, loss = 0.0038299560546875
c619-031: Epoch: 0, Step: 131, Rank: 18, loss = 0.00070953369140625
c621-061: Epoch: 0, Step: 131, Rank: 22, loss = 0.000911712646484375
c613-121: Epoch: 0, Step: 131, Rank: 4, loss = 0.004913330078125
c619-032: Epoch: 0, Step: 131, Rank: 19, loss = 1.8041124150158794e-16
c621-062: Epoch: 0, Step: 131, Rank: 23, loss = 8.487701416015625e-05
c613-141: Epoch: 0, Step: 131, Rank: 8, loss = 6.198883056640625e-05
c613-142: Epoch: 0, Step: 131, Rank: 9, loss = 0.01287841796875
c619-041: Epoch: 0, Step: 131, Rank: 20, loss = 0.00506591796875
c613-112: Epoch: 0, Step: 131, Rank: 3, loss = 0.0001087188720703125
c619-022: Epoch: 0, Step: 131, Rank: 17, loss = 0.00738525390625
c621-071: Epoch: 0, Step: 131, Rank: 24, loss = 0.0133056640625
c613-122: Epoch: 0, Step: 131, Rank: 5, loss = 0.0026397705078125
c613-111: Epoch: 0, Step: 131, Rank: 2, loss = 0.0034942626953125
c619-011: Epoch: 0, Step: 131, Rank: 14, loss = 0.69140625
c621-092: Epoch: 0, Step: 131, Rank: 29, loss = 0.006927490234375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 132, Rank: 43, loss = 2.983724378680108e-16
c619-002: Epoch: 0, Step: 132, Rank: 13, loss = 6.993104012531504e-18
c619-021: Epoch: 0, Step: 132, Rank: 16, loss = 1.55717134475708e-06
c613-101: Epoch: 0, Step: 132, Rank: 0, loss = 0.0026397705078125
c622-052: Epoch: 0, Step: 132, Rank: 53, loss = 0.041015625
c621-072: Epoch: 0, Step: 132, Rank: 25, loss = 0.0003681182861328125
c622-012: Epoch: 0, Step: 132, Rank: 45, loss = 1.0132789611816406e-05
c619-031: Epoch: 0, Step: 132, Rank: 18, loss = 0.12158203125
c621-151: Epoch: 0, Step: 132, Rank: 40, loss = 2.753734588623047e-05
c621-081: Epoch: 0, Step: 132, Rank: 26, loss = 2.3647750424515834e-14
c622-011: Epoch: 0, Step: 132, Rank: 44, loss = 1.3828277587890625e-05
c619-022: Epoch: 0, Step: 132, Rank: 17, loss = 0.00360107421875
c622-001: Epoch: 0, Step: 132, Rank: 42, loss = 0.03369140625
c621-091: Epoch: 0, Step: 132, Rank: 28, loss = 0.69140625
c613-152: Epoch: 0, Step: 132, Rank: 11, loss = 0.69140625
c621-111: Epoch: 0, Step: 132, Rank: 32, loss = 4.7222086809427244e-20
c619-032: Epoch: 0, Step: 132, Rank: 19, loss = 0.0255126953125
c621-082: Epoch: 0, Step: 132, Rank: 27, loss = 0.00946044921875
c621-121: Epoch: 0, Step: 132, Rank: 34, loss = 0.01708984375
c619-012: Epoch: 0, Step: 132, Rank: 15, loss = 0.000392913818359375
c622-022: Epoch: 0, Step: 132, Rank: 47, loss = 1.4722347259521484e-05
c622-061: Epoch: 0, Step: 132, Rank: 54, loss = 0.00396728515625
c621-131: Epoch: 0, Step: 132, Rank: 36, loss = 0.0091552734375
c613-112: Epoch: 0, Step: 132, Rank: 3, loss = 0.0025482177734375
c621-061: Epoch: 0, Step: 132, Rank: 22, loss = 0.000179290771484375
c613-132: Epoch: 0, Step: 132, Rank: 7, loss = 1.2790197503539935e-19
c613-102: Epoch: 0, Step: 132, Rank: 1, loss = 0.0001583099365234375
c621-052: Epoch: 0, Step: 132, Rank: 21, loss = 0.00714111328125
c622-101: Epoch: 0, Step: 132, Rank: 62, loss = 0.01251220703125
c622-102: Epoch: 0, Step: 132, Rank: 63, loss = 0.00136566162109375
c621-142: Epoch: 0, Step: 132, Rank: 39, loss = 0.072265625
c621-141: Epoch: 0, Step: 132, Rank: 38, loss = 0.000148773193359375
c613-122: Epoch: 0, Step: 132, Rank: 5, loss = 1.8041124150158794e-16
c622-032: Epoch: 0, Step: 132, Rank: 49, loss = 0.00014019012451171875
c622-051: Epoch: 0, Step: 132, Rank: 52, loss = 0.000335693359375
c613-131: Epoch: 0, Step: 132, Rank: 6, loss = 0.00014019012451171875
c613-121: Epoch: 0, Step: 132, Rank: 4, loss = 1.7139067942650854e-15
c622-081: Epoch: 0, Step: 132, Rank: 58, loss = 1.2931877790833823e-12
c619-001: Epoch: 0, Step: 132, Rank: 12, loss = 0.00299072265625
c622-042: Epoch: 0, Step: 132, Rank: 51, loss = 0.005584716796875
c619-011: Epoch: 0, Step: 132, Rank: 14, loss = 0.02392578125
c613-141: Epoch: 0, Step: 132, Rank: 8, loss = 0.007598876953125
c619-041: Epoch: 0, Step: 132, Rank: 20, loss = 0.003387451171875
c621-102: Epoch: 0, Step: 132, Rank: 31, loss = 0.0751953125
c621-152: Epoch: 0, Step: 132, Rank: 41, loss = 4.843059286940843e-11
c622-062: Epoch: 0, Step: 132, Rank: 55, loss = 0.0002956390380859375
c621-112: Epoch: 0, Step: 132, Rank: 33, loss = 0.036376953125
c622-041: Epoch: 0, Step: 132, Rank: 50, loss = 7.486343383789062e-05
c621-101: Epoch: 0, Step: 132, Rank: 30, loss = 8.404254913330078e-06
c613-151: Epoch: 0, Step: 132, Rank: 10, loss = 0.0091552734375
c621-071: Epoch: 0, Step: 132, Rank: 24, loss = 0.1455078125
c613-111: Epoch: 0, Step: 132, Rank: 2, loss = 0.02880859375
c622-092: Epoch: 0, Step: 132, Rank: 61, loss = 0.0306396484375
c622-031: Epoch: 0, Step: 132, Rank: 48, loss = 0.04052734375
c621-122: Epoch: 0, Step: 132, Rank: 35, loss = 1.318767317570746e-10
c622-021: Epoch: 0, Step: 132, Rank: 46, loss = 0.001068115234375
c622-091: Epoch: 0, Step: 132, Rank: 60, loss = 1.55717134475708e-06
c622-082: Epoch: 0, Step: 132, Rank: 59, loss = 0.000911712646484375
c621-062: Epoch: 0, Step: 132, Rank: 23, loss = 2.439454888092385e-17
c621-132: Epoch: 0, Step: 132, Rank: 37, loss = 0.005401611328125
c622-072: Epoch: 0, Step: 132, Rank: 57, loss = 0.003173828125
c622-071: Epoch: 0, Step: 132, Rank: 56, loss = 0.00180816650390625
c621-092: Epoch: 0, Step: 132, Rank: 29, loss = 0.01507568359375
c613-142: Epoch: 0, Step: 132, Rank: 9, loss = 0.0255126953125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 133, Rank: 58, loss = 0.003173828125
c622-002: Epoch: 0, Step: 133, Rank: 43, loss = 0.0181884765625
c622-062: Epoch: 0, Step: 133, Rank: 55, loss = 7.486343383789062e-05
c622-052: Epoch: 0, Step: 133, Rank: 53, loss = 0.005584716796875
c613-101: Epoch: 0, Step: 133, Rank: 0, loss = 0.000148773193359375
c622-012: Epoch: 0, Step: 133, Rank: 45, loss = 5.0961971282958984e-06
c622-071: Epoch: 0, Step: 133, Rank: 56, loss = 0.004608154296875
c622-101: Epoch: 0, Step: 133, Rank: 62, loss = 0.0007781982421875
c622-061: Epoch: 0, Step: 133, Rank: 54, loss = 0.69140625
c622-082: Epoch: 0, Step: 133, Rank: 59, loss = 0.017578125
c619-001: Epoch: 0, Step: 133, Rank: 12, loss = 0.000606536865234375
c619-002: Epoch: 0, Step: 133, Rank: 13, loss = 0.69140625
c621-151: Epoch: 0, Step: 133, Rank: 40, loss = 0.00299072265625
c619-031: Epoch: 0, Step: 133, Rank: 18, loss = 0.0003566741943359375
c613-112: Epoch: 0, Step: 133, Rank: 3, loss = 0.69140625
c619-021: Epoch: 0, Step: 133, Rank: 16, loss = 0.002471923828125
c621-131: Epoch: 0, Step: 133, Rank: 36, loss = 6.628036499023438e-05
c613-111: Epoch: 0, Step: 133, Rank: 2, loss = 0.000431060791015625
c613-121: Epoch: 0, Step: 133, Rank: 4, loss = 0.003387451171875
c622-072: Epoch: 0, Step: 133, Rank: 57, loss = 0.35546875
c622-001: Epoch: 0, Step: 133, Rank: 42, loss = 0.0198974609375
c622-102: Epoch: 0, Step: 133, Rank: 63, loss = 2.276897430419922e-05
c621-111: Epoch: 0, Step: 133, Rank: 32, loss = 0.021240234375
c622-041: Epoch: 0, Step: 133, Rank: 50, loss = 0.0091552734375
c622-042: Epoch: 0, Step: 133, Rank: 51, loss = 3.123283386230469e-05
c622-092: Epoch: 0, Step: 133, Rank: 61, loss = 0.00433349609375
c613-132: Epoch: 0, Step: 133, Rank: 7, loss = 0.00019073486328125
c613-122: Epoch: 0, Step: 133, Rank: 5, loss = 2.4158453015843406e-12
c622-032: Epoch: 0, Step: 133, Rank: 49, loss = 0.0133056640625
c621-091: Epoch: 0, Step: 133, Rank: 28, loss = 0.201171875
c619-012: Epoch: 0, Step: 133, Rank: 15, loss = 0.00433349609375
c621-061: Epoch: 0, Step: 133, Rank: 22, loss = 0.0181884765625
c621-052: Epoch: 0, Step: 133, Rank: 21, loss = 1.6689300537109375e-05
c622-091: Epoch: 0, Step: 133, Rank: 60, loss = 0.041748046875
c621-132: Epoch: 0, Step: 133, Rank: 37, loss = 0.00180816650390625
c622-051: Epoch: 0, Step: 133, Rank: 52, loss = 1.053497228147536e-20
c621-121: Epoch: 0, Step: 133, Rank: 34, loss = 0.01068115234375
c619-041: Epoch: 0, Step: 133, Rank: 20, loss = 0.000457763671875
c613-141: Epoch: 0, Step: 133, Rank: 8, loss = 0.0047607421875
c621-101: Epoch: 0, Step: 133, Rank: 30, loss = 1.8041124150158794e-16
c621-141: Epoch: 0, Step: 133, Rank: 38, loss = 0.022216796875
c613-131: Epoch: 0, Step: 133, Rank: 6, loss = 0.00421142578125
c621-122: Epoch: 0, Step: 133, Rank: 35, loss = 0.0089111328125
c613-152: Epoch: 0, Step: 133, Rank: 11, loss = 2.514570951461792e-08
c621-081: Epoch: 0, Step: 133, Rank: 26, loss = 0.002716064453125
c613-102: Epoch: 0, Step: 133, Rank: 1, loss = 0.004608154296875
c621-152: Epoch: 0, Step: 133, Rank: 41, loss = 8.881784197001252e-13
c622-031: Epoch: 0, Step: 133, Rank: 48, loss = 0.00141143798828125
c621-142: Epoch: 0, Step: 133, Rank: 39, loss = 0.00433349609375
c621-112: Epoch: 0, Step: 133, Rank: 33, loss = 0.000431060791015625
c622-011: Epoch: 0, Step: 133, Rank: 44, loss = 6.891787052154541e-07
c621-102: Epoch: 0, Step: 133, Rank: 31, loss = 0.69140625
c622-022: Epoch: 0, Step: 133, Rank: 47, loss = 0.0038299560546875
c613-142: Epoch: 0, Step: 133, Rank: 9, loss = 0.0002460479736328125
c619-011: Epoch: 0, Step: 133, Rank: 14, loss = 0.000148773193359375
c619-022: Epoch: 0, Step: 133, Rank: 17, loss = 8.487701416015625e-05
c621-082: Epoch: 0, Step: 133, Rank: 27, loss = 0.050048828125
c621-062: Epoch: 0, Step: 133, Rank: 23, loss = 0.0380859375
c621-072: Epoch: 0, Step: 133, Rank: 25, loss = 0.0026397705078125
c621-092: Epoch: 0, Step: 133, Rank: 29, loss = 0.0002956390380859375
c622-021: Epoch: 0, Step: 133, Rank: 46, loss = 0.0002956390380859375
c619-032: Epoch: 0, Step: 133, Rank: 19, loss = 0.00083160400390625
c621-071: Epoch: 0, Step: 133, Rank: 24, loss = 0.00811767578125
c613-151: Epoch: 0, Step: 133, Rank: 10, loss = 0.01556396484375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2451171875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 134, Rank: 43, loss = 0.00170135498046875
c622-081: Epoch: 0, Step: 134, Rank: 58, loss = 0.00836181640625
c613-101: Epoch: 0, Step: 134, Rank: 0, loss = 0.022216796875
c622-052: Epoch: 0, Step: 134, Rank: 53, loss = 0.01177978515625
c621-081: Epoch: 0, Step: 134, Rank: 26, loss = 0.021240234375
c621-101: Epoch: 0, Step: 134, Rank: 30, loss = 0.04638671875
c622-001: Epoch: 0, Step: 134, Rank: 42, loss = 0.00170135498046875
c622-012: Epoch: 0, Step: 134, Rank: 45, loss = 0.00164794921875
c622-062: Epoch: 0, Step: 134, Rank: 55, loss = 0.08642578125
c621-111: Epoch: 0, Step: 134, Rank: 32, loss = 0.00136566162109375
c619-021: Epoch: 0, Step: 134, Rank: 16, loss = 0.0181884765625
c613-132: Epoch: 0, Step: 134, Rank: 7, loss = 0.00811767578125
c622-061: Epoch: 0, Step: 134, Rank: 54, loss = 0.01141357421875
c622-011: Epoch: 0, Step: 134, Rank: 44, loss = 0.00433349609375
c622-022: Epoch: 0, Step: 134, Rank: 47, loss = 0.0002460479736328125
c613-141: Epoch: 0, Step: 134, Rank: 8, loss = 8.487701416015625e-05
c613-131: Epoch: 0, Step: 134, Rank: 6, loss = 0.0038299560546875
c621-072: Epoch: 0, Step: 134, Rank: 25, loss = 0.01458740234375
c613-122: Epoch: 0, Step: 134, Rank: 5, loss = 0.0198974609375
c621-091: Epoch: 0, Step: 134, Rank: 28, loss = 0.06787109375
c619-001: Epoch: 0, Step: 134, Rank: 12, loss = 0.002471923828125
c613-111: Epoch: 0, Step: 134, Rank: 2, loss = 0.00083160400390625
c621-152: Epoch: 0, Step: 134, Rank: 41, loss = 8.487701416015625e-05
c621-131: Epoch: 0, Step: 134, Rank: 36, loss = 0.0052490234375
c621-151: Epoch: 0, Step: 134, Rank: 40, loss = 4.13994203059987e-27
c621-142: Epoch: 0, Step: 134, Rank: 39, loss = 0.00096893310546875
c622-071: Epoch: 0, Step: 134, Rank: 56, loss = 8.487701416015625e-05
c621-132: Epoch: 0, Step: 134, Rank: 37, loss = 0.0037078857421875
c621-082: Epoch: 0, Step: 134, Rank: 27, loss = 1.1864468014524018e-27
c622-032: Epoch: 0, Step: 134, Rank: 49, loss = 0.000667572021484375
c613-152: Epoch: 0, Step: 134, Rank: 11, loss = 0.0034942626953125
c622-051: Epoch: 0, Step: 134, Rank: 52, loss = 5.4836273193359375e-05
c613-121: Epoch: 0, Step: 134, Rank: 4, loss = 0.69140625
c622-082: Epoch: 0, Step: 134, Rank: 59, loss = 0.08251953125
c619-022: Epoch: 0, Step: 134, Rank: 17, loss = 0.0003681182861328125
c622-031: Epoch: 0, Step: 134, Rank: 48, loss = 0.01287841796875
c622-041: Epoch: 0, Step: 134, Rank: 50, loss = 0.0005035400390625
c621-052: Epoch: 0, Step: 134, Rank: 21, loss = 0.0057373046875
c621-061: Epoch: 0, Step: 134, Rank: 22, loss = 0.00096893310546875
c619-002: Epoch: 0, Step: 134, Rank: 13, loss = 0.00186920166015625
c613-102: Epoch: 0, Step: 134, Rank: 1, loss = 0.0016021728515625
c619-031: Epoch: 0, Step: 134, Rank: 18, loss = 0.00836181640625
c621-122: Epoch: 0, Step: 134, Rank: 35, loss = 1.8533319234848022e-07
c622-101: Epoch: 0, Step: 134, Rank: 62, loss = 0.005401611328125
c613-112: Epoch: 0, Step: 134, Rank: 3, loss = 0.03369140625
c621-141: Epoch: 0, Step: 134, Rank: 38, loss = 0.0002307891845703125
c621-112: Epoch: 0, Step: 134, Rank: 33, loss = 0.0026397705078125
c622-092: Epoch: 0, Step: 134, Rank: 61, loss = 0.0047607421875
c619-041: Epoch: 0, Step: 134, Rank: 20, loss = 0.00040435791015625
c621-121: Epoch: 0, Step: 134, Rank: 34, loss = 1.8775463104248047e-06
c621-071: Epoch: 0, Step: 134, Rank: 24, loss = 1.1399388313293457e-06
c613-142: Epoch: 0, Step: 134, Rank: 9, loss = 0.0322265625
c622-072: Epoch: 0, Step: 134, Rank: 57, loss = 4.231929779052734e-06
c622-102: Epoch: 0, Step: 134, Rank: 63, loss = 0.0159912109375
c622-091: Epoch: 0, Step: 134, Rank: 60, loss = 0.003173828125
c621-092: Epoch: 0, Step: 134, Rank: 29, loss = 0.003082275390625
c622-021: Epoch: 0, Step: 134, Rank: 46, loss = 0.000553131103515625
c621-102: Epoch: 0, Step: 134, Rank: 31, loss = 0.0262451171875
c619-012: Epoch: 0, Step: 134, Rank: 15, loss = 0.01708984375
c621-062: Epoch: 0, Step: 134, Rank: 23, loss = 0.00096893310546875
c622-042: Epoch: 0, Step: 134, Rank: 51, loss = 1.525040715932846e-08
c619-011: Epoch: 0, Step: 134, Rank: 14, loss = 7.496324301261813e-24
c619-032: Epoch: 0, Step: 134, Rank: 19, loss = 0.03515625
c613-151: Epoch: 0, Step: 134, Rank: 10, loss = 0.005584716796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 135, Rank: 53, loss = 0.0025482177734375
c613-101: Epoch: 0, Step: 135, Rank: 0, loss = 0.004913330078125
c622-002: Epoch: 0, Step: 135, Rank: 43, loss = 0.0023956298828125
c621-091: Epoch: 0, Step: 135, Rank: 28, loss = 0.2099609375
c621-111: Epoch: 0, Step: 135, Rank: 32, loss = 0.0004730224609375
c621-132: Epoch: 0, Step: 135, Rank: 37, loss = 0.00193023681640625
c622-062: Epoch: 0, Step: 135, Rank: 55, loss = 0.0005035400390625
c619-031: Epoch: 0, Step: 135, Rank: 18, loss = 0.01068115234375
c622-061: Epoch: 0, Step: 135, Rank: 54, loss = 1.0662875083691372e-25
c621-101: Epoch: 0, Step: 135, Rank: 30, loss = 0.000644683837890625
c621-081: Epoch: 0, Step: 135, Rank: 26, loss = 0.69140625
c622-012: Epoch: 0, Step: 135, Rank: 45, loss = 0.002716064453125
c613-141: Epoch: 0, Step: 135, Rank: 8, loss = 0.0032806396484375
c613-131: Epoch: 0, Step: 135, Rank: 6, loss = 0.01141357421875
c619-021: Epoch: 0, Step: 135, Rank: 16, loss = 0.01104736328125
c622-072: Epoch: 0, Step: 135, Rank: 57, loss = 0.0003147125244140625
c613-122: Epoch: 0, Step: 135, Rank: 5, loss = 0.0556640625
c622-071: Epoch: 0, Step: 135, Rank: 56, loss = 0.69140625
c622-092: Epoch: 0, Step: 135, Rank: 61, loss = 0.0255126953125
c613-121: Epoch: 0, Step: 135, Rank: 4, loss = 1.84297022087776e-14
c619-001: Epoch: 0, Step: 135, Rank: 12, loss = 0.006103515625
c621-131: Epoch: 0, Step: 135, Rank: 36, loss = 0.005584716796875
c619-002: Epoch: 0, Step: 135, Rank: 13, loss = 1.2993812561035156e-05
c622-081: Epoch: 0, Step: 135, Rank: 58, loss = 1.1059455573558807e-09
c622-051: Epoch: 0, Step: 135, Rank: 52, loss = 0.002899169921875
c622-101: Epoch: 0, Step: 135, Rank: 62, loss = 0.00225830078125
c613-142: Epoch: 0, Step: 135, Rank: 9, loss = 0.01214599609375
c622-032: Epoch: 0, Step: 135, Rank: 49, loss = 0.000431060791015625
c613-132: Epoch: 0, Step: 135, Rank: 7, loss = 5.029141902923584e-07
c613-111: Epoch: 0, Step: 135, Rank: 2, loss = 0.0016021728515625
c621-072: Epoch: 0, Step: 135, Rank: 25, loss = 5.0961971282958984e-06
c622-001: Epoch: 0, Step: 135, Rank: 42, loss = 0.009765625
c622-042: Epoch: 0, Step: 135, Rank: 51, loss = 0.00070953369140625
c619-022: Epoch: 0, Step: 135, Rank: 17, loss = 0.00150299072265625
c622-091: Epoch: 0, Step: 135, Rank: 60, loss = 9.098986738083304e-23
c622-041: Epoch: 0, Step: 135, Rank: 50, loss = 5.182486384480711e-17
c621-092: Epoch: 0, Step: 135, Rank: 29, loss = 9.918585419654846e-08
c621-151: Epoch: 0, Step: 135, Rank: 40, loss = 0.0091552734375
c619-041: Epoch: 0, Step: 135, Rank: 20, loss = 5.182486384480711e-17
c613-152: Epoch: 0, Step: 135, Rank: 11, loss = 0.022216796875
c621-052: Epoch: 0, Step: 135, Rank: 21, loss = 0.01708984375
c621-121: Epoch: 0, Step: 135, Rank: 34, loss = 0.000392913818359375
c622-011: Epoch: 0, Step: 135, Rank: 44, loss = 0.005584716796875
c622-082: Epoch: 0, Step: 135, Rank: 59, loss = 0.0004730224609375
c621-112: Epoch: 0, Step: 135, Rank: 33, loss = 0.0089111328125
c621-061: Epoch: 0, Step: 135, Rank: 22, loss = 0.002899169921875
c621-122: Epoch: 0, Step: 135, Rank: 35, loss = 0.007598876953125
c619-032: Epoch: 0, Step: 135, Rank: 19, loss = 0.000667572021484375
c622-102: Epoch: 0, Step: 135, Rank: 63, loss = 0.00136566162109375
c621-082: Epoch: 0, Step: 135, Rank: 27, loss = 1.3869794202037156e-11
c613-112: Epoch: 0, Step: 135, Rank: 3, loss = 4.418687638008123e-14
c622-031: Epoch: 0, Step: 135, Rank: 48, loss = 0.00010251998901367188
c621-152: Epoch: 0, Step: 135, Rank: 41, loss = 0.0052490234375
c621-102: Epoch: 0, Step: 135, Rank: 31, loss = 0.01708984375
c621-071: Epoch: 0, Step: 135, Rank: 24, loss = 2.574980159653073e-18
c613-102: Epoch: 0, Step: 135, Rank: 1, loss = 1.2514647096395493e-09
c619-011: Epoch: 0, Step: 135, Rank: 14, loss = 0.01177978515625
c621-142: Epoch: 0, Step: 135, Rank: 39, loss = 0.0004730224609375
c619-012: Epoch: 0, Step: 135, Rank: 15, loss = 0.0020599365234375
c621-062: Epoch: 0, Step: 135, Rank: 23, loss = 0.0037078857421875
c621-141: Epoch: 0, Step: 135, Rank: 38, loss = 0.000457763671875
c622-022: Epoch: 0, Step: 135, Rank: 47, loss = 0.00408935546875
c622-021: Epoch: 0, Step: 135, Rank: 46, loss = 0.1328125
c613-151: Epoch: 0, Step: 135, Rank: 10, loss = 0.004486083984375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 136, Rank: 0, loss = 3.583409124985337e-10
c622-002: Epoch: 0, Step: 136, Rank: 43, loss = 0.05078125
c621-111: Epoch: 0, Step: 136, Rank: 32, loss = 0.158203125
c622-092: Epoch: 0, Step: 136, Rank: 61, loss = 0.000278472900390625
c622-012: Epoch: 0, Step: 136, Rank: 45, loss = 0.00299072265625
c619-021: Epoch: 0, Step: 136, Rank: 16, loss = 0.0186767578125
c619-002: Epoch: 0, Step: 136, Rank: 13, loss = 3.694822225952521e-13
c613-111: Epoch: 0, Step: 136, Rank: 2, loss = 0.025146484375
c621-081: Epoch: 0, Step: 136, Rank: 26, loss = 0.0028076171875
c613-132: Epoch: 0, Step: 136, Rank: 7, loss = 0.01214599609375
c619-001: Epoch: 0, Step: 136, Rank: 12, loss = 2.1736923372372985e-10
c621-151: Epoch: 0, Step: 136, Rank: 40, loss = 0.0010986328125
c622-001: Epoch: 0, Step: 136, Rank: 42, loss = 0.002716064453125
c621-072: Epoch: 0, Step: 136, Rank: 25, loss = 0.0732421875
c622-081: Epoch: 0, Step: 136, Rank: 58, loss = 0.006103515625
c622-052: Epoch: 0, Step: 136, Rank: 53, loss = 0.0002613067626953125
c613-121: Epoch: 0, Step: 136, Rank: 4, loss = 0.02978515625
c622-101: Epoch: 0, Step: 136, Rank: 62, loss = 0.0010986328125
c613-131: Epoch: 0, Step: 136, Rank: 6, loss = 1.0408340855860843e-15
c621-091: Epoch: 0, Step: 136, Rank: 28, loss = 0.0103759765625
c622-032: Epoch: 0, Step: 136, Rank: 49, loss = 0.00787353515625
c619-031: Epoch: 0, Step: 136, Rank: 18, loss = 0.00136566162109375
c621-121: Epoch: 0, Step: 136, Rank: 34, loss = 0.000911712646484375
c622-082: Epoch: 0, Step: 136, Rank: 59, loss = 0.01287841796875
c621-052: Epoch: 0, Step: 136, Rank: 21, loss = 0.0002613067626953125
c613-122: Epoch: 0, Step: 136, Rank: 5, loss = 6.344635039567947e-09
c613-152: Epoch: 0, Step: 136, Rank: 11, loss = 0.002716064453125
c622-091: Epoch: 0, Step: 136, Rank: 60, loss = 0.0047607421875
c621-142: Epoch: 0, Step: 136, Rank: 39, loss = 6.3792168840089494e-21
c621-092: Epoch: 0, Step: 136, Rank: 29, loss = 0.00811767578125
c622-062: Epoch: 0, Step: 136, Rank: 55, loss = 0.00116729736328125
c613-102: Epoch: 0, Step: 136, Rank: 1, loss = 0.005584716796875
c621-082: Epoch: 0, Step: 136, Rank: 27, loss = 0.0026397705078125
c619-022: Epoch: 0, Step: 136, Rank: 17, loss = 0.0020599365234375
c622-011: Epoch: 0, Step: 136, Rank: 44, loss = 0.0067138671875
c621-131: Epoch: 0, Step: 136, Rank: 36, loss = 0.0037078857421875
c621-061: Epoch: 0, Step: 136, Rank: 22, loss = 0.000431060791015625
c619-032: Epoch: 0, Step: 136, Rank: 19, loss = 0.0002307891845703125
c621-101: Epoch: 0, Step: 136, Rank: 30, loss = 0.00186920166015625
c621-152: Epoch: 0, Step: 136, Rank: 41, loss = 0.002899169921875
c613-112: Epoch: 0, Step: 136, Rank: 3, loss = 0.09716796875
c619-041: Epoch: 0, Step: 136, Rank: 20, loss = 0.00124359130859375
c621-112: Epoch: 0, Step: 136, Rank: 33, loss = 0.005584716796875
c613-141: Epoch: 0, Step: 136, Rank: 8, loss = 0.10302734375
c622-061: Epoch: 0, Step: 136, Rank: 54, loss = 0.00738525390625
c613-151: Epoch: 0, Step: 136, Rank: 10, loss = 0.0003681182861328125
c619-012: Epoch: 0, Step: 136, Rank: 15, loss = 0.0205078125
c622-031: Epoch: 0, Step: 136, Rank: 48, loss = 0.00299072265625
c622-102: Epoch: 0, Step: 136, Rank: 63, loss = 0.01251220703125
c622-041: Epoch: 0, Step: 136, Rank: 50, loss = 0.007598876953125
c619-011: Epoch: 0, Step: 136, Rank: 14, loss = 0.0089111328125
c621-071: Epoch: 0, Step: 136, Rank: 24, loss = 0.0004730224609375
c622-051: Epoch: 0, Step: 136, Rank: 52, loss = 0.001983642578125
c621-062: Epoch: 0, Step: 136, Rank: 23, loss = 0.00860595703125
c622-042: Epoch: 0, Step: 136, Rank: 51, loss = 0.007598876953125
c621-122: Epoch: 0, Step: 136, Rank: 35, loss = 0.00141143798828125
c613-142: Epoch: 0, Step: 136, Rank: 9, loss = 3.4897757426877174e-19
c621-141: Epoch: 0, Step: 136, Rank: 38, loss = 0.000553131103515625
c621-102: Epoch: 0, Step: 136, Rank: 31, loss = 0.01507568359375
c622-022: Epoch: 0, Step: 136, Rank: 47, loss = 0.0016021728515625
c622-071: Epoch: 0, Step: 136, Rank: 56, loss = 0.0133056640625
c622-021: Epoch: 0, Step: 136, Rank: 46, loss = 0.0017547607421875
c622-072: Epoch: 0, Step: 136, Rank: 57, loss = 0.0002307891845703125
c621-132: Epoch: 0, Step: 136, Rank: 37, loss = 0.0284423828125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24658203125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 137, Rank: 32, loss = 0.01214599609375
c622-081: Epoch: 0, Step: 137, Rank: 58, loss = 0.005584716796875
c621-081: Epoch: 0, Step: 137, Rank: 26, loss = 0.0003681182861328125
c619-021: Epoch: 0, Step: 137, Rank: 16, loss = 0.02392578125
c613-101: Epoch: 0, Step: 137, Rank: 0, loss = 0.005584716796875
c621-112: Epoch: 0, Step: 137, Rank: 33, loss = 0.0133056640625
c621-142: Epoch: 0, Step: 137, Rank: 39, loss = 0.00180816650390625
c621-132: Epoch: 0, Step: 137, Rank: 37, loss = 0.00016880035400390625
c622-002: Epoch: 0, Step: 137, Rank: 43, loss = 0.091796875
c622-062: Epoch: 0, Step: 137, Rank: 55, loss = 0.00592041015625
c621-151: Epoch: 0, Step: 137, Rank: 40, loss = 0.01007080078125
c621-052: Epoch: 0, Step: 137, Rank: 21, loss = 0.0017547607421875
c622-001: Epoch: 0, Step: 137, Rank: 42, loss = 0.000457763671875
c622-052: Epoch: 0, Step: 137, Rank: 53, loss = 0.03466796875
c619-001: Epoch: 0, Step: 137, Rank: 12, loss = 0.00010251998901367188
c622-051: Epoch: 0, Step: 137, Rank: 52, loss = 0.0103759765625
c621-101: Epoch: 0, Step: 137, Rank: 30, loss = 8.149072527885437e-09
c621-082: Epoch: 0, Step: 137, Rank: 27, loss = 0.0003566741943359375
c621-141: Epoch: 0, Step: 137, Rank: 38, loss = 0.0037078857421875
c619-022: Epoch: 0, Step: 137, Rank: 17, loss = 0.0091552734375
c621-091: Epoch: 0, Step: 137, Rank: 28, loss = 6.993104012531504e-18
c621-072: Epoch: 0, Step: 137, Rank: 25, loss = 0.470703125
c619-031: Epoch: 0, Step: 137, Rank: 18, loss = 9.486769009248164e-19
c619-012: Epoch: 0, Step: 137, Rank: 15, loss = 1.895427703857422e-05
c622-012: Epoch: 0, Step: 137, Rank: 45, loss = 0.01177978515625
c613-121: Epoch: 0, Step: 137, Rank: 4, loss = 0.001983642578125
c622-071: Epoch: 0, Step: 137, Rank: 56, loss = 0.0003681182861328125
c619-032: Epoch: 0, Step: 137, Rank: 19, loss = 1.7497114868092467e-13
c621-131: Epoch: 0, Step: 137, Rank: 36, loss = 0.003387451171875
c613-152: Epoch: 0, Step: 137, Rank: 11, loss = 6.198883056640625e-05
c613-151: Epoch: 0, Step: 137, Rank: 10, loss = 3.123283386230469e-05
c613-142: Epoch: 0, Step: 137, Rank: 9, loss = 0.003173828125
c619-041: Epoch: 0, Step: 137, Rank: 20, loss = 0.0380859375
c622-041: Epoch: 0, Step: 137, Rank: 50, loss = 3.688037395477295e-07
c622-032: Epoch: 0, Step: 137, Rank: 49, loss = 2.014636993408203e-05
c622-101: Epoch: 0, Step: 137, Rank: 62, loss = 0.00019073486328125
c621-121: Epoch: 0, Step: 137, Rank: 34, loss = 0.0025482177734375
c621-152: Epoch: 0, Step: 137, Rank: 41, loss = 0.00083160400390625
c622-061: Epoch: 0, Step: 137, Rank: 54, loss = 1.6171875
c613-102: Epoch: 0, Step: 137, Rank: 1, loss = 0.00016880035400390625
c619-002: Epoch: 0, Step: 137, Rank: 13, loss = 0.000911712646484375
c622-011: Epoch: 0, Step: 137, Rank: 44, loss = 0.00714111328125
c621-122: Epoch: 0, Step: 137, Rank: 35, loss = 0.000667572021484375
c621-102: Epoch: 0, Step: 137, Rank: 31, loss = 0.0255126953125
c622-082: Epoch: 0, Step: 137, Rank: 59, loss = 5.699694156646729e-07
c621-061: Epoch: 0, Step: 137, Rank: 22, loss = 1.6432439176733427e-19
c613-111: Epoch: 0, Step: 137, Rank: 2, loss = 0.0002460479736328125
c613-122: Epoch: 0, Step: 137, Rank: 5, loss = 0.69140625
c619-011: Epoch: 0, Step: 137, Rank: 14, loss = 1.9190338207408786e-10
c613-132: Epoch: 0, Step: 137, Rank: 7, loss = 0.0067138671875
c621-092: Epoch: 0, Step: 137, Rank: 29, loss = 3.213062882423401e-08
c622-031: Epoch: 0, Step: 137, Rank: 48, loss = 0.0038299560546875
c613-131: Epoch: 0, Step: 137, Rank: 6, loss = 0.0133056640625
c622-042: Epoch: 0, Step: 137, Rank: 51, loss = 0.00180816650390625
c622-072: Epoch: 0, Step: 137, Rank: 57, loss = 0.0001087188720703125
c622-092: Epoch: 0, Step: 137, Rank: 61, loss = 0.0026397705078125
c622-021: Epoch: 0, Step: 137, Rank: 46, loss = 0.00186920166015625
c622-022: Epoch: 0, Step: 137, Rank: 47, loss = 0.001983642578125
c621-062: Epoch: 0, Step: 137, Rank: 23, loss = 6.927791673660977e-13
c621-071: Epoch: 0, Step: 137, Rank: 24, loss = 2.014636993408203e-05
c622-091: Epoch: 0, Step: 137, Rank: 60, loss = 0.0002307891845703125
c622-102: Epoch: 0, Step: 137, Rank: 63, loss = 0.01177978515625
c613-112: Epoch: 0, Step: 137, Rank: 3, loss = 0.0032806396484375
c613-141: Epoch: 0, Step: 137, Rank: 8, loss = 0.0026397705078125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 138, Rank: 0, loss = 0.69140625
c621-081: Epoch: 0, Step: 138, Rank: 26, loss = 0.000911712646484375
c619-021: Epoch: 0, Step: 138, Rank: 16, loss = 0.006317138671875
c621-072: Epoch: 0, Step: 138, Rank: 25, loss = 2.6056189295420372e-23
c622-002: Epoch: 0, Step: 138, Rank: 43, loss = 0.0010986328125
c619-022: Epoch: 0, Step: 138, Rank: 17, loss = 0.00738525390625
c613-102: Epoch: 0, Step: 138, Rank: 1, loss = 1.6209256159527285e-14
c619-032: Epoch: 0, Step: 138, Rank: 19, loss = 0.69140625
c621-091: Epoch: 0, Step: 138, Rank: 28, loss = 0.00592041015625
c621-052: Epoch: 0, Step: 138, Rank: 21, loss = 0.00040435791015625
c619-031: Epoch: 0, Step: 138, Rank: 18, loss = 0.01416015625
c621-111: Epoch: 0, Step: 138, Rank: 32, loss = 0.039794921875
c619-041: Epoch: 0, Step: 138, Rank: 20, loss = 0.01507568359375
c622-052: Epoch: 0, Step: 138, Rank: 53, loss = 0.109375
c619-011: Epoch: 0, Step: 138, Rank: 14, loss = 0.000553131103515625
c621-131: Epoch: 0, Step: 138, Rank: 36, loss = 0.01708984375
c619-002: Epoch: 0, Step: 138, Rank: 13, loss = 0.01416015625
c621-101: Epoch: 0, Step: 138, Rank: 30, loss = 0.0001583099365234375
c622-061: Epoch: 0, Step: 138, Rank: 54, loss = 0.00714111328125
c621-132: Epoch: 0, Step: 138, Rank: 37, loss = 0.0010986328125
c621-061: Epoch: 0, Step: 138, Rank: 22, loss = 2.2649765014648438e-06
c613-111: Epoch: 0, Step: 138, Rank: 2, loss = 0.00021648406982421875
c619-001: Epoch: 0, Step: 138, Rank: 12, loss = 0.00714111328125
c613-132: Epoch: 0, Step: 138, Rank: 7, loss = 0.470703125
c619-012: Epoch: 0, Step: 138, Rank: 15, loss = 3.8163916471489756e-16
c622-001: Epoch: 0, Step: 138, Rank: 42, loss = 0.0159912109375
c622-012: Epoch: 0, Step: 138, Rank: 45, loss = 0.01708984375
c622-051: Epoch: 0, Step: 138, Rank: 52, loss = 0.0038299560546875
c621-141: Epoch: 0, Step: 138, Rank: 38, loss = 0.00164794921875
c613-142: Epoch: 0, Step: 138, Rank: 9, loss = 0.006500244140625
c621-082: Epoch: 0, Step: 138, Rank: 27, loss = 0.00136566162109375
c621-151: Epoch: 0, Step: 138, Rank: 40, loss = 0.0001087188720703125
c613-151: Epoch: 0, Step: 138, Rank: 10, loss = 0.027587890625
c622-032: Epoch: 0, Step: 138, Rank: 49, loss = 0.01507568359375
c621-121: Epoch: 0, Step: 138, Rank: 34, loss = 0.021484375
c613-112: Epoch: 0, Step: 138, Rank: 3, loss = 0.00811767578125
c613-121: Epoch: 0, Step: 138, Rank: 4, loss = 7.963180541992188e-05
c613-152: Epoch: 0, Step: 138, Rank: 11, loss = 0.0016021728515625
c622-011: Epoch: 0, Step: 138, Rank: 44, loss = 0.000278472900390625
c622-092: Epoch: 0, Step: 138, Rank: 61, loss = 0.00141143798828125
c622-101: Epoch: 0, Step: 138, Rank: 62, loss = 0.004486083984375
c622-081: Epoch: 0, Step: 138, Rank: 58, loss = 1.150369644165039e-05
c613-131: Epoch: 0, Step: 138, Rank: 6, loss = 0.0091552734375
c622-102: Epoch: 0, Step: 138, Rank: 63, loss = 7.729977369308472e-08
c621-092: Epoch: 0, Step: 138, Rank: 29, loss = 0.0023193359375
c622-041: Epoch: 0, Step: 138, Rank: 50, loss = 0.000606536865234375
c621-102: Epoch: 0, Step: 138, Rank: 31, loss = 0.006103515625
c621-142: Epoch: 0, Step: 138, Rank: 39, loss = 0.06396484375
c621-062: Epoch: 0, Step: 138, Rank: 23, loss = 0.001983642578125
c621-122: Epoch: 0, Step: 138, Rank: 35, loss = 0.01007080078125
c622-031: Epoch: 0, Step: 138, Rank: 48, loss = 0.004486083984375
c622-091: Epoch: 0, Step: 138, Rank: 60, loss = 0.00136566162109375
c622-022: Epoch: 0, Step: 138, Rank: 47, loss = 0.00299072265625
c613-141: Epoch: 0, Step: 138, Rank: 8, loss = 0.69140625
c621-112: Epoch: 0, Step: 138, Rank: 33, loss = 0.0004730224609375
c622-062: Epoch: 0, Step: 138, Rank: 55, loss = 0.0205078125
c622-082: Epoch: 0, Step: 138, Rank: 59, loss = 0.00180816650390625
c613-122: Epoch: 0, Step: 138, Rank: 5, loss = 0.0225830078125
c622-071: Epoch: 0, Step: 138, Rank: 56, loss = 0.047119140625
c622-021: Epoch: 0, Step: 138, Rank: 46, loss = 0.005584716796875
c622-042: Epoch: 0, Step: 138, Rank: 51, loss = 0.00019073486328125
c621-152: Epoch: 0, Step: 138, Rank: 41, loss = 3.583409124985337e-10
c622-072: Epoch: 0, Step: 138, Rank: 57, loss = 0.0023193359375
c621-071: Epoch: 0, Step: 138, Rank: 24, loss = 0.02978515625
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75341796875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 139, Rank: 0, loss = 0.00225830078125
c622-052: Epoch: 0, Step: 139, Rank: 53, loss = 0.0089111328125
c613-152: Epoch: 0, Step: 139, Rank: 11, loss = 0.04296875
c613-112: Epoch: 0, Step: 139, Rank: 3, loss = 0.00124359130859375
c613-121: Epoch: 0, Step: 139, Rank: 4, loss = 4.5299530029296875e-05
c613-111: Epoch: 0, Step: 139, Rank: 2, loss = 0.0002956390380859375
c621-081: Epoch: 0, Step: 139, Rank: 26, loss = 0.009765625
c619-002: Epoch: 0, Step: 139, Rank: 13, loss = 0.00010251998901367188
c622-012: Epoch: 0, Step: 139, Rank: 45, loss = 0.0002613067626953125
c619-021: Epoch: 0, Step: 139, Rank: 16, loss = 0.0021820068359375
c621-072: Epoch: 0, Step: 139, Rank: 25, loss = 0.0164794921875
c622-061: Epoch: 0, Step: 139, Rank: 54, loss = 0.0007781982421875
c613-102: Epoch: 0, Step: 139, Rank: 1, loss = 0.000911712646484375
c619-001: Epoch: 0, Step: 139, Rank: 12, loss = 0.000606536865234375
c622-002: Epoch: 0, Step: 139, Rank: 43, loss = 0.00225830078125
c622-001: Epoch: 0, Step: 139, Rank: 42, loss = 0.003387451171875
c613-142: Epoch: 0, Step: 139, Rank: 9, loss = 8.543513119185775e-17
c613-132: Epoch: 0, Step: 139, Rank: 7, loss = 0.003387451171875
c619-011: Epoch: 0, Step: 139, Rank: 14, loss = 6.629788138637702e-36
c613-151: Epoch: 0, Step: 139, Rank: 10, loss = 7.188646122813225e-09
c622-051: Epoch: 0, Step: 139, Rank: 52, loss = 2.9331204132176936e-11
c621-111: Epoch: 0, Step: 139, Rank: 32, loss = 0.01507568359375
c621-091: Epoch: 0, Step: 139, Rank: 28, loss = 0.0242919921875
c621-132: Epoch: 0, Step: 139, Rank: 37, loss = 0.69140625
c622-081: Epoch: 0, Step: 139, Rank: 58, loss = 0.039794921875
c621-061: Epoch: 0, Step: 139, Rank: 22, loss = 0.0020599365234375
c621-152: Epoch: 0, Step: 139, Rank: 41, loss = 0.0057373046875
c622-042: Epoch: 0, Step: 139, Rank: 51, loss = 0.003082275390625
c619-032: Epoch: 0, Step: 139, Rank: 19, loss = 0.021484375
c621-151: Epoch: 0, Step: 139, Rank: 40, loss = 0.00136566162109375
c622-032: Epoch: 0, Step: 139, Rank: 49, loss = 0.00811767578125
c613-131: Epoch: 0, Step: 139, Rank: 6, loss = 0.0002460479736328125
c622-022: Epoch: 0, Step: 139, Rank: 47, loss = 0.000553131103515625
c621-142: Epoch: 0, Step: 139, Rank: 39, loss = 0.00396728515625
c621-101: Epoch: 0, Step: 139, Rank: 30, loss = 0.0028076171875
c622-041: Epoch: 0, Step: 139, Rank: 50, loss = 2.342858351767063e-09
c621-131: Epoch: 0, Step: 139, Rank: 36, loss = 3.123283386230469e-05
c622-062: Epoch: 0, Step: 139, Rank: 55, loss = 0.00075531005859375
c622-071: Epoch: 0, Step: 139, Rank: 56, loss = 1.3322676295501878e-15
c613-141: Epoch: 0, Step: 139, Rank: 8, loss = 0.0003681182861328125
c621-122: Epoch: 0, Step: 139, Rank: 35, loss = 0.341796875
c622-031: Epoch: 0, Step: 139, Rank: 48, loss = 7.338821887969971e-07
c621-052: Epoch: 0, Step: 139, Rank: 21, loss = 0.03271484375
c621-121: Epoch: 0, Step: 139, Rank: 34, loss = 0.00408935546875
c619-012: Epoch: 0, Step: 139, Rank: 15, loss = 0.005584716796875
c622-101: Epoch: 0, Step: 139, Rank: 62, loss = 0.000644683837890625
c619-031: Epoch: 0, Step: 139, Rank: 18, loss = 5.424022674560547e-06
c622-021: Epoch: 0, Step: 139, Rank: 46, loss = 5.115907697472721e-12
c622-092: Epoch: 0, Step: 139, Rank: 61, loss = 7.194411455615628e-28
c621-141: Epoch: 0, Step: 139, Rank: 38, loss = 2.753734588623047e-05
c619-041: Epoch: 0, Step: 139, Rank: 20, loss = 0.01507568359375
c621-102: Epoch: 0, Step: 139, Rank: 31, loss = 9.012222290039062e-05
c622-091: Epoch: 0, Step: 139, Rank: 60, loss = 0.06787109375
c621-071: Epoch: 0, Step: 139, Rank: 24, loss = 0.0010986328125
c619-022: Epoch: 0, Step: 139, Rank: 17, loss = 0.00124359130859375
c621-062: Epoch: 0, Step: 139, Rank: 23, loss = 1.1546753136970622e-17
c621-082: Epoch: 0, Step: 139, Rank: 27, loss = 0.007598876953125
c622-011: Epoch: 0, Step: 139, Rank: 44, loss = 4.9763185651190145e-21
c621-092: Epoch: 0, Step: 139, Rank: 29, loss = 0.023193359375
c613-122: Epoch: 0, Step: 139, Rank: 5, loss = 0.00150299072265625
c622-102: Epoch: 0, Step: 139, Rank: 63, loss = 1.2790197503539935e-19
c622-082: Epoch: 0, Step: 139, Rank: 59, loss = 0.000553131103515625
c622-072: Epoch: 0, Step: 139, Rank: 57, loss = 0.03369140625
c621-112: Epoch: 0, Step: 139, Rank: 33, loss = 0.001983642578125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 140, Rank: 53, loss = 0.027587890625
c622-081: Epoch: 0, Step: 140, Rank: 58, loss = 0.000667572021484375
c622-061: Epoch: 0, Step: 140, Rank: 54, loss = 3.635980405647388e-15
c622-062: Epoch: 0, Step: 140, Rank: 55, loss = 1.3597309589385986e-07
c622-071: Epoch: 0, Step: 140, Rank: 56, loss = 0.01458740234375
c622-051: Epoch: 0, Step: 140, Rank: 52, loss = 0.004913330078125
c622-072: Epoch: 0, Step: 140, Rank: 57, loss = 0.01556396484375
c622-042: Epoch: 0, Step: 140, Rank: 51, loss = 0.00408935546875
c622-082: Epoch: 0, Step: 140, Rank: 59, loss = 4.481572212509971e-38
c622-041: Epoch: 0, Step: 140, Rank: 50, loss = 0.0002460479736328125
c622-032: Epoch: 0, Step: 140, Rank: 49, loss = 0.0255126953125
c622-091: Epoch: 0, Step: 140, Rank: 60, loss = 0.02978515625
c622-022: Epoch: 0, Step: 140, Rank: 47, loss = 0.17578125
c622-031: Epoch: 0, Step: 140, Rank: 48, loss = 0.69140625
c622-012: Epoch: 0, Step: 140, Rank: 45, loss = 0.00164794921875
c622-021: Epoch: 0, Step: 140, Rank: 46, loss = 0.0034942626953125
c622-092: Epoch: 0, Step: 140, Rank: 61, loss = 0.00136566162109375
c622-011: Epoch: 0, Step: 140, Rank: 44, loss = 0.0067138671875
c622-002: Epoch: 0, Step: 140, Rank: 43, loss = 0.0003566741943359375
c621-152: Epoch: 0, Step: 140, Rank: 41, loss = 0.0002460479736328125
c622-001: Epoch: 0, Step: 140, Rank: 42, loss = 0.00180816650390625
c621-151: Epoch: 0, Step: 140, Rank: 40, loss = 0.0001087188720703125
c622-101: Epoch: 0, Step: 140, Rank: 62, loss = 0.00170135498046875
c621-142: Epoch: 0, Step: 140, Rank: 39, loss = 0.0016021728515625
c621-132: Epoch: 0, Step: 140, Rank: 37, loss = 0.0380859375
c621-141: Epoch: 0, Step: 140, Rank: 38, loss = 7.009506225585938e-05
c621-131: Epoch: 0, Step: 140, Rank: 36, loss = 6.198883056640625e-05
c621-122: Epoch: 0, Step: 140, Rank: 35, loss = 2.4318695068359375e-05
c622-102: Epoch: 0, Step: 140, Rank: 63, loss = 0.083984375
c613-101: Epoch: 0, Step: 140, Rank: 0, loss = 0.021240234375
c621-121: Epoch: 0, Step: 140, Rank: 34, loss = 0.00193023681640625
c621-111: Epoch: 0, Step: 140, Rank: 32, loss = 0.021240234375
c621-112: Epoch: 0, Step: 140, Rank: 33, loss = 1.4811228820360823e-36
c621-101: Epoch: 0, Step: 140, Rank: 30, loss = 4.00543212890625e-05
c621-102: Epoch: 0, Step: 140, Rank: 31, loss = 4.6798959374427795e-08
c621-091: Epoch: 0, Step: 140, Rank: 28, loss = 2.8919009696678116e-25
c621-081: Epoch: 0, Step: 140, Rank: 26, loss = 0.69140625
c621-072: Epoch: 0, Step: 140, Rank: 25, loss = 6.927791673660977e-13
c621-082: Epoch: 0, Step: 140, Rank: 27, loss = 0.00040435791015625
c621-092: Epoch: 0, Step: 140, Rank: 29, loss = 0.021240234375
c621-052: Epoch: 0, Step: 140, Rank: 21, loss = 0.001068115234375
c621-061: Epoch: 0, Step: 140, Rank: 22, loss = 0.0810546875
c621-062: Epoch: 0, Step: 140, Rank: 23, loss = 8.149072527885437e-09
c621-071: Epoch: 0, Step: 140, Rank: 24, loss = 0.000518798828125
c613-102: Epoch: 0, Step: 140, Rank: 1, loss = 0.004608154296875
c619-041: Epoch: 0, Step: 140, Rank: 20, loss = 0.0034942626953125
c619-031: Epoch: 0, Step: 140, Rank: 18, loss = 1.6672859221771964e-24
c619-021: Epoch: 0, Step: 140, Rank: 16, loss = 8.66885281955573e-22
c619-032: Epoch: 0, Step: 140, Rank: 19, loss = 0.005584716796875
c619-002: Epoch: 0, Step: 140, Rank: 13, loss = 0.004913330078125
c619-022: Epoch: 0, Step: 140, Rank: 17, loss = 0.10498046875
c613-111: Epoch: 0, Step: 140, Rank: 2, loss = 0.004608154296875
c619-012: Epoch: 0, Step: 140, Rank: 15, loss = 0.0017547607421875
c619-011: Epoch: 0, Step: 140, Rank: 14, loss = 9.5367431640625e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c619-001: Epoch: 0, Step: 140, Rank: 12, loss = 0.08251953125
c613-152: Epoch: 0, Step: 140, Rank: 11, loss = 0.0052490234375
c613-151: Epoch: 0, Step: 140, Rank: 10, loss = 0.002471923828125
c613-112: Epoch: 0, Step: 140, Rank: 3, loss = 0.00096893310546875
c613-121: Epoch: 0, Step: 140, Rank: 4, loss = 0.0284423828125
c613-132: Epoch: 0, Step: 140, Rank: 7, loss = 0.006317138671875
c613-141: Epoch: 0, Step: 140, Rank: 8, loss = 0.007598876953125
c613-142: Epoch: 0, Step: 140, Rank: 9, loss = 0.00128173828125
c613-131: Epoch: 0, Step: 140, Rank: 6, loss = 0.000518798828125
c613-122: Epoch: 0, Step: 140, Rank: 5, loss = 0.004913330078125
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 141, Rank: 16, loss = 0.000553131103515625
c619-001: Epoch: 0, Step: 141, Rank: 12, loss = 0.0186767578125
c613-101: Epoch: 0, Step: 141, Rank: 0, loss = 0.0007781982421875
c622-002: Epoch: 0, Step: 141, Rank: 43, loss = 4.231929779052734e-06
c622-092: Epoch: 0, Step: 141, Rank: 61, loss = 0.0052490234375
c622-091: Epoch: 0, Step: 141, Rank: 60, loss = 0.01104736328125
c613-131: Epoch: 0, Step: 141, Rank: 6, loss = 5.817413330078125e-05
c622-081: Epoch: 0, Step: 141, Rank: 58, loss = 1.3709068298339844e-06
c619-011: Epoch: 0, Step: 141, Rank: 14, loss = 0.000179290771484375
c613-141: Epoch: 0, Step: 141, Rank: 8, loss = 0.041748046875
c619-012: Epoch: 0, Step: 141, Rank: 15, loss = 0.01141357421875
c613-152: Epoch: 0, Step: 141, Rank: 11, loss = 0.0002956390380859375
c619-031: Epoch: 0, Step: 141, Rank: 18, loss = 0.0047607421875
c619-002: Epoch: 0, Step: 141, Rank: 13, loss = 4.926614671774132e-16
c621-132: Epoch: 0, Step: 141, Rank: 37, loss = 0.021240234375
c622-001: Epoch: 0, Step: 141, Rank: 42, loss = 3.017554874593445e-21
c621-111: Epoch: 0, Step: 141, Rank: 32, loss = 1.4637180356658064e-12
c621-091: Epoch: 0, Step: 141, Rank: 28, loss = 1.2931877790833823e-12
c619-022: Epoch: 0, Step: 141, Rank: 17, loss = 0.0751953125
c613-132: Epoch: 0, Step: 141, Rank: 7, loss = 0.0002307891845703125
c621-081: Epoch: 0, Step: 141, Rank: 26, loss = 0.00408935546875
c621-142: Epoch: 0, Step: 141, Rank: 39, loss = 0.000518798828125
c619-032: Epoch: 0, Step: 141, Rank: 19, loss = 0.0034942626953125
c613-121: Epoch: 0, Step: 141, Rank: 4, loss = 0.006317138671875
c621-151: Epoch: 0, Step: 141, Rank: 40, loss = 6.628036499023438e-05
c621-152: Epoch: 0, Step: 141, Rank: 41, loss = 0.0284423828125
c621-082: Epoch: 0, Step: 141, Rank: 27, loss = 0.043701171875
c622-101: Epoch: 0, Step: 141, Rank: 62, loss = 0.0004730224609375
c621-052: Epoch: 0, Step: 141, Rank: 21, loss = 0.06005859375
c622-011: Epoch: 0, Step: 141, Rank: 44, loss = 0.01708984375
c622-082: Epoch: 0, Step: 141, Rank: 59, loss = 0.0010986328125
c622-062: Epoch: 0, Step: 141, Rank: 55, loss = 0.00506591796875
c622-071: Epoch: 0, Step: 141, Rank: 56, loss = 2.276897430419922e-05
c613-102: Epoch: 0, Step: 141, Rank: 1, loss = 0.0284423828125
c613-142: Epoch: 0, Step: 141, Rank: 9, loss = 0.000278472900390625
c613-122: Epoch: 0, Step: 141, Rank: 5, loss = 0.000667572021484375
c613-151: Epoch: 0, Step: 141, Rank: 10, loss = 0.0007781982421875
c622-012: Epoch: 0, Step: 141, Rank: 45, loss = 0.00714111328125
c622-052: Epoch: 0, Step: 141, Rank: 53, loss = 1.9983403963978888e-37
c621-122: Epoch: 0, Step: 141, Rank: 35, loss = 0.0186767578125
c621-121: Epoch: 0, Step: 141, Rank: 34, loss = 0.00787353515625
c621-101: Epoch: 0, Step: 141, Rank: 30, loss = 0.0091552734375
c621-102: Epoch: 0, Step: 141, Rank: 31, loss = 0.00738525390625
c621-141: Epoch: 0, Step: 141, Rank: 38, loss = 8.754432201385498e-08
c613-111: Epoch: 0, Step: 141, Rank: 2, loss = 0.0091552734375
c613-112: Epoch: 0, Step: 141, Rank: 3, loss = 0.03515625
c621-112: Epoch: 0, Step: 141, Rank: 33, loss = 0.02880859375
c622-041: Epoch: 0, Step: 141, Rank: 50, loss = 0.00360107421875
c622-031: Epoch: 0, Step: 141, Rank: 48, loss = 0.69140625
c621-062: Epoch: 0, Step: 141, Rank: 23, loss = 0.003387451171875
c622-032: Epoch: 0, Step: 141, Rank: 49, loss = 0.00116729736328125
c622-051: Epoch: 0, Step: 141, Rank: 52, loss = 0.000278472900390625
c621-061: Epoch: 0, Step: 141, Rank: 22, loss = 0.01214599609375
c622-022: Epoch: 0, Step: 141, Rank: 47, loss = 0.00225830078125
c621-071: Epoch: 0, Step: 141, Rank: 24, loss = 2.710505431213761e-19
c622-102: Epoch: 0, Step: 141, Rank: 63, loss = 2.648448571562767e-09
c622-042: Epoch: 0, Step: 141, Rank: 51, loss = 8.412825991399586e-12
c619-041: Epoch: 0, Step: 141, Rank: 20, loss = 0.022216796875
c622-021: Epoch: 0, Step: 141, Rank: 46, loss = 3.725290298461914e-06
c621-092: Epoch: 0, Step: 141, Rank: 29, loss = 0.0103759765625
c622-072: Epoch: 0, Step: 141, Rank: 57, loss = 1.0408340855860843e-15
c621-131: Epoch: 0, Step: 141, Rank: 36, loss = 0.0021209716796875
c621-072: Epoch: 0, Step: 141, Rank: 25, loss = 0.00150299072265625
c622-061: Epoch: 0, Step: 141, Rank: 54, loss = 0.01556396484375
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.7548828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.11s, TFLOPs: 0.90, Samples/sec: 0.47, Time/seq 2.11s, Batch Size: 1, Sequence Length: 2048
c619-002: Epoch: 0, Step: 142, Rank: 13, loss = 0.0198974609375
c621-111: Epoch: 0, Step: 142, Rank: 32, loss = 0.003387451171875
c619-001: Epoch: 0, Step: 142, Rank: 12, loss = 0.01287841796875
c622-002: Epoch: 0, Step: 142, Rank: 43, loss = 0.01177978515625
c622-081: Epoch: 0, Step: 142, Rank: 58, loss = 5.14984130859375e-05
c619-021: Epoch: 0, Step: 142, Rank: 16, loss = 3.0547380447387695e-07
c621-122: Epoch: 0, Step: 142, Rank: 35, loss = 2.71833068627654e-38
c621-132: Epoch: 0, Step: 142, Rank: 37, loss = 0.0306396484375
c621-142: Epoch: 0, Step: 142, Rank: 39, loss = 0.01507568359375
c621-061: Epoch: 0, Step: 142, Rank: 22, loss = 0.00083160400390625
c613-151: Epoch: 0, Step: 142, Rank: 10, loss = 2.586841583251953e-05
c621-121: Epoch: 0, Step: 142, Rank: 34, loss = 0.004486083984375
c621-112: Epoch: 0, Step: 142, Rank: 33, loss = 0.005584716796875
c621-072: Epoch: 0, Step: 142, Rank: 25, loss = 2.9325485229492188e-05
c613-101: Epoch: 0, Step: 142, Rank: 0, loss = 0.006317138671875
c621-131: Epoch: 0, Step: 142, Rank: 36, loss = 0.00075531005859375
c621-151: Epoch: 0, Step: 142, Rank: 40, loss = 1.5819829215076654e-23
c619-031: Epoch: 0, Step: 142, Rank: 18, loss = 0.0021820068359375
c622-032: Epoch: 0, Step: 142, Rank: 49, loss = 0.036376953125
c622-051: Epoch: 0, Step: 142, Rank: 52, loss = 0.00083160400390625
c619-032: Epoch: 0, Step: 142, Rank: 19, loss = 2.983724378680108e-16
c613-132: Epoch: 0, Step: 142, Rank: 7, loss = 0.027587890625
c621-141: Epoch: 0, Step: 142, Rank: 38, loss = 0.00396728515625
c621-052: Epoch: 0, Step: 142, Rank: 21, loss = 0.0001316070556640625
c622-012: Epoch: 0, Step: 142, Rank: 45, loss = 0.0091552734375
c613-152: Epoch: 0, Step: 142, Rank: 11, loss = 4.7222086809427244e-20
c619-041: Epoch: 0, Step: 142, Rank: 20, loss = 0.69140625
c622-001: Epoch: 0, Step: 142, Rank: 42, loss = 1.84297022087776e-14
c621-102: Epoch: 0, Step: 142, Rank: 31, loss = 6.891787052154541e-07
c622-072: Epoch: 0, Step: 142, Rank: 57, loss = 0.0091552734375
c619-022: Epoch: 0, Step: 142, Rank: 17, loss = 1.3499587596865412e-20
c622-061: Epoch: 0, Step: 142, Rank: 54, loss = 0.0103759765625
c619-011: Epoch: 0, Step: 142, Rank: 14, loss = 1.4811228820360823e-36
c621-152: Epoch: 0, Step: 142, Rank: 41, loss = 0.000553131103515625
c621-101: Epoch: 0, Step: 142, Rank: 30, loss = 4.5299530029296875e-05
c621-091: Epoch: 0, Step: 142, Rank: 28, loss = 0.004913330078125
c622-041: Epoch: 0, Step: 142, Rank: 50, loss = 3.213062882423401e-08
c621-081: Epoch: 0, Step: 142, Rank: 26, loss = 0.00016880035400390625
c622-082: Epoch: 0, Step: 142, Rank: 59, loss = 0.00075531005859375
c621-082: Epoch: 0, Step: 142, Rank: 27, loss = 0.0003681182861328125
c613-142: Epoch: 0, Step: 142, Rank: 9, loss = 0.0164794921875
c622-031: Epoch: 0, Step: 142, Rank: 48, loss = 0.0020599365234375
c619-012: Epoch: 0, Step: 142, Rank: 15, loss = 0.00136566162109375
c621-092: Epoch: 0, Step: 142, Rank: 29, loss = 1.9273308272485545e-22
c622-071: Epoch: 0, Step: 142, Rank: 56, loss = 0.006317138671875
c613-121: Epoch: 0, Step: 142, Rank: 4, loss = 0.006927490234375
c621-071: Epoch: 0, Step: 142, Rank: 24, loss = 0.002899169921875
c622-011: Epoch: 0, Step: 142, Rank: 44, loss = 0.00433349609375
c622-042: Epoch: 0, Step: 142, Rank: 51, loss = 0.04443359375
c613-112: Epoch: 0, Step: 142, Rank: 3, loss = 0.023193359375
c613-141: Epoch: 0, Step: 142, Rank: 8, loss = 0.017578125
c622-092: Epoch: 0, Step: 142, Rank: 61, loss = 0.072265625
c621-062: Epoch: 0, Step: 142, Rank: 23, loss = 2.014636993408203e-05
c622-022: Epoch: 0, Step: 142, Rank: 47, loss = 0.005584716796875
c613-131: Epoch: 0, Step: 142, Rank: 6, loss = 0.00019073486328125
c622-091: Epoch: 0, Step: 142, Rank: 60, loss = 0.08251953125
c622-052: Epoch: 0, Step: 142, Rank: 53, loss = 0.00083160400390625
c613-111: Epoch: 0, Step: 142, Rank: 2, loss = 5.4836273193359375e-05
c622-101: Epoch: 0, Step: 142, Rank: 62, loss = 0.0196533203125
c622-062: Epoch: 0, Step: 142, Rank: 55, loss = 0.69140625
c613-122: Epoch: 0, Step: 142, Rank: 5, loss = 0.01068115234375
c613-102: Epoch: 0, Step: 142, Rank: 1, loss = 1.828125
c622-021: Epoch: 0, Step: 142, Rank: 46, loss = 0.00433349609375
c622-102: Epoch: 0, Step: 142, Rank: 63, loss = 0.0242919921875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 143, Rank: 0, loss = 0.000667572021484375
c622-002: Epoch: 0, Step: 143, Rank: 43, loss = 0.009765625
c619-021: Epoch: 0, Step: 143, Rank: 16, loss = 1.6689300537109375e-05
c622-081: Epoch: 0, Step: 143, Rank: 58, loss = 0.005584716796875
c619-001: Epoch: 0, Step: 143, Rank: 12, loss = 0.0001087188720703125
c621-081: Epoch: 0, Step: 143, Rank: 26, loss = 0.006500244140625
c619-002: Epoch: 0, Step: 143, Rank: 13, loss = 0.03466796875
c622-101: Epoch: 0, Step: 143, Rank: 62, loss = 0.0003566741943359375
c622-082: Epoch: 0, Step: 143, Rank: 59, loss = 0.06787109375
c613-152: Epoch: 0, Step: 143, Rank: 11, loss = 1.150369644165039e-05
c613-111: Epoch: 0, Step: 143, Rank: 2, loss = 0.000606536865234375
c621-132: Epoch: 0, Step: 143, Rank: 37, loss = 0.0002460479736328125
c621-151: Epoch: 0, Step: 143, Rank: 40, loss = 0.0284423828125
c621-091: Epoch: 0, Step: 143, Rank: 28, loss = 0.0001583099365234375
c613-141: Epoch: 0, Step: 143, Rank: 8, loss = 0.0242919921875
c619-011: Epoch: 0, Step: 143, Rank: 14, loss = 0.042236328125
c619-031: Epoch: 0, Step: 143, Rank: 18, loss = 0.01177978515625
c613-132: Epoch: 0, Step: 143, Rank: 7, loss = 0.0052490234375
c622-062: Epoch: 0, Step: 143, Rank: 55, loss = 0.0164794921875
c622-022: Epoch: 0, Step: 143, Rank: 47, loss = 4.267692565917969e-05
c622-061: Epoch: 0, Step: 143, Rank: 54, loss = 1.318767317570746e-10
c613-112: Epoch: 0, Step: 143, Rank: 3, loss = 1.150369644165039e-05
c621-131: Epoch: 0, Step: 143, Rank: 36, loss = 0.0311279296875
c621-072: Epoch: 0, Step: 143, Rank: 25, loss = 0.3515625
c619-022: Epoch: 0, Step: 143, Rank: 17, loss = 6.5267086029052734e-06
c621-122: Epoch: 0, Step: 143, Rank: 35, loss = 0.01287841796875
c613-142: Epoch: 0, Step: 143, Rank: 9, loss = 0.00021648406982421875
c621-141: Epoch: 0, Step: 143, Rank: 38, loss = 0.000606536865234375
c621-082: Epoch: 0, Step: 143, Rank: 27, loss = 3.7670135498046875e-05
c622-102: Epoch: 0, Step: 143, Rank: 63, loss = 0.0005035400390625
c613-131: Epoch: 0, Step: 143, Rank: 6, loss = 0.000553131103515625
c622-012: Epoch: 0, Step: 143, Rank: 45, loss = 0.00421142578125
c622-041: Epoch: 0, Step: 143, Rank: 50, loss = 2.2649765014648438e-06
c622-001: Epoch: 0, Step: 143, Rank: 42, loss = 0.006317138671875
c613-151: Epoch: 0, Step: 143, Rank: 10, loss = 1.4637180356658064e-12
c622-011: Epoch: 0, Step: 143, Rank: 44, loss = 0.0021209716796875
c613-121: Epoch: 0, Step: 143, Rank: 4, loss = 0.003387451171875
c613-102: Epoch: 0, Step: 143, Rank: 1, loss = 9.742432179479496e-29
c613-122: Epoch: 0, Step: 143, Rank: 5, loss = 4.94765117764473e-09
c619-041: Epoch: 0, Step: 143, Rank: 20, loss = 0.0005035400390625
c622-052: Epoch: 0, Step: 143, Rank: 53, loss = 0.0010986328125
c619-012: Epoch: 0, Step: 143, Rank: 15, loss = 0.00058746337890625
c621-052: Epoch: 0, Step: 143, Rank: 21, loss = 0.01458740234375
c622-071: Epoch: 0, Step: 143, Rank: 56, loss = 0.0023193359375
c621-142: Epoch: 0, Step: 143, Rank: 39, loss = 0.00193023681640625
c622-091: Epoch: 0, Step: 143, Rank: 60, loss = 0.004608154296875
c622-021: Epoch: 0, Step: 143, Rank: 46, loss = 0.00016880035400390625
c621-101: Epoch: 0, Step: 143, Rank: 30, loss = 0.000148773193359375
c621-121: Epoch: 0, Step: 143, Rank: 34, loss = 0.0732421875
c619-032: Epoch: 0, Step: 143, Rank: 19, loss = 8.404254913330078e-06
c621-111: Epoch: 0, Step: 143, Rank: 32, loss = 0.0023193359375
c622-042: Epoch: 0, Step: 143, Rank: 51, loss = 0.00040435791015625
c621-152: Epoch: 0, Step: 143, Rank: 41, loss = 0.01068115234375
c622-051: Epoch: 0, Step: 143, Rank: 52, loss = 4.7222086809427244e-20
c621-102: Epoch: 0, Step: 143, Rank: 31, loss = 0.0016021728515625
c622-072: Epoch: 0, Step: 143, Rank: 57, loss = 0.052490234375
c622-032: Epoch: 0, Step: 143, Rank: 49, loss = 0.01177978515625
c621-071: Epoch: 0, Step: 143, Rank: 24, loss = 1.9273308272485545e-22
c621-061: Epoch: 0, Step: 143, Rank: 22, loss = 0.021484375
c622-031: Epoch: 0, Step: 143, Rank: 48, loss = 0.0001583099365234375
c621-112: Epoch: 0, Step: 143, Rank: 33, loss = 0.00506591796875
c622-092: Epoch: 0, Step: 143, Rank: 61, loss = 0.0196533203125
c621-062: Epoch: 0, Step: 143, Rank: 23, loss = 0.0047607421875
c621-092: Epoch: 0, Step: 143, Rank: 29, loss = 0.00020313262939453125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.12s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.12s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 144, Rank: 16, loss = 2.276897430419922e-05
c619-002: Epoch: 0, Step: 144, Rank: 13, loss = 0.0003681182861328125
c619-011: Epoch: 0, Step: 144, Rank: 14, loss = 0.69140625
c619-012: Epoch: 0, Step: 144, Rank: 15, loss = 5.424022674560547e-06
c619-031: Epoch: 0, Step: 144, Rank: 18, loss = 0.01904296875
c619-001: Epoch: 0, Step: 144, Rank: 12, loss = 0.003387451171875
c613-152: Epoch: 0, Step: 144, Rank: 11, loss = 7.963180541992188e-05
c619-022: Epoch: 0, Step: 144, Rank: 17, loss = 1.0132789611816406e-05
c621-081: Epoch: 0, Step: 144, Rank: 26, loss = 0.0
c613-151: Epoch: 0, Step: 144, Rank: 10, loss = 0.0
c621-052: Epoch: 0, Step: 144, Rank: 21, loss = 0.004913330078125
c619-041: Epoch: 0, Step: 144, Rank: 20, loss = 1.4637067577342992e-31
c621-061: Epoch: 0, Step: 144, Rank: 22, loss = 8.859277744181285e-32
c621-072: Epoch: 0, Step: 144, Rank: 25, loss = 6.3792168840089494e-21
c613-142: Epoch: 0, Step: 144, Rank: 9, loss = 1.3499587596865412e-20
c621-071: Epoch: 0, Step: 144, Rank: 24, loss = 3.841705620288849e-09
c619-032: Epoch: 0, Step: 144, Rank: 19, loss = 6.198883056640625e-05
c621-062: Epoch: 0, Step: 144, Rank: 23, loss = 8.404254913330078e-06
c621-082: Epoch: 0, Step: 144, Rank: 27, loss = 0.00124359130859375
c613-132: Epoch: 0, Step: 144, Rank: 7, loss = 0.0025482177734375
c613-141: Epoch: 0, Step: 144, Rank: 8, loss = 1.3316000006114873e-34
c613-131: Epoch: 0, Step: 144, Rank: 6, loss = 5.424022674560547e-06
c613-121: Epoch: 0, Step: 144, Rank: 4, loss = 0.0181884765625
c613-122: Epoch: 0, Step: 144, Rank: 5, loss = 1.6689300537109375e-05
c613-101: Epoch: 0, Step: 144, Rank: 0, loss = 6.198883056640625e-05
c621-091: Epoch: 0, Step: 144, Rank: 28, loss = 0.00136566162109375
c613-111: Epoch: 0, Step: 144, Rank: 2, loss = 0.455078125
c613-112: Epoch: 0, Step: 144, Rank: 3, loss = 0.03271484375
c622-101: Epoch: 0, Step: 144, Rank: 62, loss = 0.00075531005859375
c613-102: Epoch: 0, Step: 144, Rank: 1, loss = 0.00116729736328125
c622-002: Epoch: 0, Step: 144, Rank: 43, loss = 2.1457672119140625e-05
c622-102: Epoch: 0, Step: 144, Rank: 63, loss = 5.3085386753082275e-08
c622-062: Epoch: 0, Step: 144, Rank: 55, loss = 0.0133056640625
c621-111: Epoch: 0, Step: 144, Rank: 32, loss = 0.05078125
c622-052: Epoch: 0, Step: 144, Rank: 53, loss = 6.007030606269836e-08
c621-132: Epoch: 0, Step: 144, Rank: 37, loss = 6.973743438720703e-06
c622-012: Epoch: 0, Step: 144, Rank: 45, loss = 0.00011587142944335938
c622-071: Epoch: 0, Step: 144, Rank: 56, loss = 0.0002956390380859375
c622-092: Epoch: 0, Step: 144, Rank: 61, loss = 5.781650543212891e-06
c622-001: Epoch: 0, Step: 144, Rank: 42, loss = 7.867813110351562e-06
c621-151: Epoch: 0, Step: 144, Rank: 40, loss = 9.012222290039062e-05
c622-032: Epoch: 0, Step: 144, Rank: 49, loss = 0.000335693359375
c621-142: Epoch: 0, Step: 144, Rank: 39, loss = 9.255018085241318e-09
c621-092: Epoch: 0, Step: 144, Rank: 29, loss = 1.857925203976527e-26
c621-152: Epoch: 0, Step: 144, Rank: 41, loss = 2.971649718878743e-35
c622-022: Epoch: 0, Step: 144, Rank: 47, loss = 0.0010986328125
c621-131: Epoch: 0, Step: 144, Rank: 36, loss = 0.0016021728515625
c621-122: Epoch: 0, Step: 144, Rank: 35, loss = 1.4722347259521484e-05
c622-051: Epoch: 0, Step: 144, Rank: 52, loss = 7.048583938740194e-11
c622-031: Epoch: 0, Step: 144, Rank: 48, loss = 5.20230969414115e-10
c621-121: Epoch: 0, Step: 144, Rank: 34, loss = 1.895427703857422e-05
c621-141: Epoch: 0, Step: 144, Rank: 38, loss = 0.01007080078125
c622-041: Epoch: 0, Step: 144, Rank: 50, loss = 3.314018249511719e-05
c622-061: Epoch: 0, Step: 144, Rank: 54, loss = 0.0003147125244140625
c622-082: Epoch: 0, Step: 144, Rank: 59, loss = 0.0
c622-021: Epoch: 0, Step: 144, Rank: 46, loss = 0.00016880035400390625
c622-072: Epoch: 0, Step: 144, Rank: 57, loss = 1.1399388313293457e-06
c621-112: Epoch: 0, Step: 144, Rank: 33, loss = 7.486343383789062e-05
c621-102: Epoch: 0, Step: 144, Rank: 31, loss = 0.0002460479736328125
c622-091: Epoch: 0, Step: 144, Rank: 60, loss = 1.895427703857422e-05
c622-042: Epoch: 0, Step: 144, Rank: 51, loss = 3.0547380447387695e-07
c622-011: Epoch: 0, Step: 144, Rank: 44, loss = 3.7670135498046875e-05
c622-081: Epoch: 0, Step: 144, Rank: 58, loss = 6.628036499023438e-05
c621-101: Epoch: 0, Step: 144, Rank: 30, loss = 6.973743438720703e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 145, Rank: 0, loss = 2.204051907791789e-39
c622-101: Epoch: 0, Step: 145, Rank: 62, loss = 0.01214599609375
c619-001: Epoch: 0, Step: 145, Rank: 12, loss = 0.00012302398681640625
c622-052: Epoch: 0, Step: 145, Rank: 53, loss = 4.00543212890625e-05
c619-002: Epoch: 0, Step: 145, Rank: 13, loss = 1.0089706847793423e-12
c622-002: Epoch: 0, Step: 145, Rank: 43, loss = 1.0277290130034089e-10
c622-092: Epoch: 0, Step: 145, Rank: 61, loss = 0.00128173828125
c621-111: Epoch: 0, Step: 145, Rank: 32, loss = 6.007030606269836e-08
c622-082: Epoch: 0, Step: 145, Rank: 59, loss = 0.000518798828125
c621-091: Epoch: 0, Step: 145, Rank: 28, loss = 0.000335693359375
c621-081: Epoch: 0, Step: 145, Rank: 26, loss = 0.00096893310546875
c622-071: Epoch: 0, Step: 145, Rank: 56, loss = 0.0002460479736328125
c622-062: Epoch: 0, Step: 145, Rank: 55, loss = 1.7762184143066406e-05
c621-052: Epoch: 0, Step: 145, Rank: 21, loss = 3.123283386230469e-05
c619-031: Epoch: 0, Step: 145, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 145, Rank: 16, loss = 8.754432201385498e-08
c622-001: Epoch: 0, Step: 145, Rank: 42, loss = 0.000518798828125
c622-012: Epoch: 0, Step: 145, Rank: 45, loss = 1.3709068298339844e-06
c619-032: Epoch: 0, Step: 145, Rank: 19, loss = 1.5366822481155396e-07
c621-132: Epoch: 0, Step: 145, Rank: 37, loss = 0.00164794921875
c613-122: Epoch: 0, Step: 145, Rank: 5, loss = 5.0961971282958984e-06
c613-152: Epoch: 0, Step: 145, Rank: 11, loss = 2.726912498474121e-06
c621-121: Epoch: 0, Step: 145, Rank: 34, loss = 0.6875
c621-082: Epoch: 0, Step: 145, Rank: 27, loss = 3.0547380447387695e-07
c613-111: Epoch: 0, Step: 145, Rank: 2, loss = 0.69140625
c621-122: Epoch: 0, Step: 145, Rank: 35, loss = 3.144186300207963e-17
c613-131: Epoch: 0, Step: 145, Rank: 6, loss = 0.00124359130859375
c619-041: Epoch: 0, Step: 145, Rank: 20, loss = 1.6079866327345371e-09
c621-072: Epoch: 0, Step: 145, Rank: 25, loss = 4.267692565917969e-05
c621-142: Epoch: 0, Step: 145, Rank: 39, loss = 0.0001316070556640625
c619-022: Epoch: 0, Step: 145, Rank: 17, loss = 0.00014019012451171875
c621-061: Epoch: 0, Step: 145, Rank: 22, loss = 0.00011587142944335938
c613-121: Epoch: 0, Step: 145, Rank: 4, loss = 1.4722347259521484e-05
c619-011: Epoch: 0, Step: 145, Rank: 14, loss = 0.00075531005859375
c613-132: Epoch: 0, Step: 145, Rank: 7, loss = 0.00408935546875
c621-151: Epoch: 0, Step: 145, Rank: 40, loss = 4.500150680541992e-06
c622-041: Epoch: 0, Step: 145, Rank: 50, loss = 0.00010251998901367188
c622-061: Epoch: 0, Step: 145, Rank: 54, loss = 3.123283386230469e-05
c622-011: Epoch: 0, Step: 145, Rank: 44, loss = 0.005584716796875
c619-012: Epoch: 0, Step: 145, Rank: 15, loss = 3.528594970703125e-05
c621-101: Epoch: 0, Step: 145, Rank: 30, loss = 4.4517219066619873e-07
c621-131: Epoch: 0, Step: 145, Rank: 36, loss = 0.00141143798828125
c613-151: Epoch: 0, Step: 145, Rank: 10, loss = 0.0
c613-102: Epoch: 0, Step: 145, Rank: 1, loss = 0.00099945068359375
c622-091: Epoch: 0, Step: 145, Rank: 60, loss = 0.0002956390380859375
c621-112: Epoch: 0, Step: 145, Rank: 33, loss = 0.005584716796875
c621-141: Epoch: 0, Step: 145, Rank: 38, loss = 1.4722347259521484e-05
c622-051: Epoch: 0, Step: 145, Rank: 52, loss = 0.0002460479736328125
c622-072: Epoch: 0, Step: 145, Rank: 57, loss = 0.0020599365234375
c621-062: Epoch: 0, Step: 145, Rank: 23, loss = 7.420778274536133e-06
c613-112: Epoch: 0, Step: 145, Rank: 3, loss = 0.000553131103515625
c613-142: Epoch: 0, Step: 145, Rank: 9, loss = 0.000457763671875
c622-022: Epoch: 0, Step: 145, Rank: 47, loss = 0.00124359130859375
c622-042: Epoch: 0, Step: 145, Rank: 51, loss = 2.5331974029541016e-07
c621-152: Epoch: 0, Step: 145, Rank: 41, loss = 0.00164794921875
c622-032: Epoch: 0, Step: 145, Rank: 49, loss = 0.000667572021484375
c622-031: Epoch: 0, Step: 145, Rank: 48, loss = 0.67578125
c621-102: Epoch: 0, Step: 145, Rank: 31, loss = 0.00021648406982421875
c622-021: Epoch: 0, Step: 145, Rank: 46, loss = 8.487701416015625e-05
c621-092: Epoch: 0, Step: 145, Rank: 29, loss = 0.000518798828125
c622-102: Epoch: 0, Step: 145, Rank: 63, loss = 0.000278472900390625
c613-141: Epoch: 0, Step: 145, Rank: 8, loss = 0.0021820068359375
c621-071: Epoch: 0, Step: 145, Rank: 24, loss = 0.00083160400390625
c622-081: Epoch: 0, Step: 145, Rank: 58, loss = 2.71833068627654e-38
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 146, Rank: 58, loss = 0.001983642578125
c622-082: Epoch: 0, Step: 146, Rank: 59, loss = 8.404254913330078e-06
c622-071: Epoch: 0, Step: 146, Rank: 56, loss = 0.000179290771484375
c622-072: Epoch: 0, Step: 146, Rank: 57, loss = 8.487701416015625e-05
c622-062: Epoch: 0, Step: 146, Rank: 55, loss = 0.00811767578125
c622-061: Epoch: 0, Step: 146, Rank: 54, loss = 1.84297022087776e-14
c622-052: Epoch: 0, Step: 146, Rank: 53, loss = 0.0010986328125
c622-091: Epoch: 0, Step: 146, Rank: 60, loss = 0.10498046875
c622-051: Epoch: 0, Step: 146, Rank: 52, loss = 1.2069940567016602e-06
c622-042: Epoch: 0, Step: 146, Rank: 51, loss = 0.0
c622-092: Epoch: 0, Step: 146, Rank: 61, loss = 0.00136566162109375
c622-041: Epoch: 0, Step: 146, Rank: 50, loss = 0.69140625
c622-032: Epoch: 0, Step: 146, Rank: 49, loss = 4.00543212890625e-05
c622-101: Epoch: 0, Step: 146, Rank: 62, loss = 0.00040435791015625
c622-031: Epoch: 0, Step: 146, Rank: 48, loss = 0.0032806396484375
c622-022: Epoch: 0, Step: 146, Rank: 47, loss = 1.895427703857422e-05
c622-012: Epoch: 0, Step: 146, Rank: 45, loss = 0.69140625
c622-002: Epoch: 0, Step: 146, Rank: 43, loss = 4.6629367034256575e-15
c622-021: Epoch: 0, Step: 146, Rank: 46, loss = 3.123283386230469e-05
c622-011: Epoch: 0, Step: 146, Rank: 44, loss = 0.000667572021484375
c622-001: Epoch: 0, Step: 146, Rank: 42, loss = 7.270385540061815e-33
c621-152: Epoch: 0, Step: 146, Rank: 41, loss = 0.0010986328125
c621-151: Epoch: 0, Step: 146, Rank: 40, loss = 0.00136566162109375
c621-142: Epoch: 0, Step: 146, Rank: 39, loss = 2.726912498474121e-06
c622-102: Epoch: 0, Step: 146, Rank: 63, loss = 0.69140625
c621-132: Epoch: 0, Step: 146, Rank: 37, loss = 0.0091552734375
c613-101: Epoch: 0, Step: 146, Rank: 0, loss = 9.424984455108643e-07
c621-141: Epoch: 0, Step: 146, Rank: 38, loss = 0.69140625
c621-131: Epoch: 0, Step: 146, Rank: 36, loss = 0.00014019012451171875
c621-111: Epoch: 0, Step: 146, Rank: 32, loss = 0.00150299072265625
c621-122: Epoch: 0, Step: 146, Rank: 35, loss = 4.231929779052734e-06
c621-121: Epoch: 0, Step: 146, Rank: 34, loss = 0.00020313262939453125
c621-112: Epoch: 0, Step: 146, Rank: 33, loss = 4.831773044478697e-30
c621-101: Epoch: 0, Step: 146, Rank: 30, loss = 0.006103515625
c621-091: Epoch: 0, Step: 146, Rank: 28, loss = 0.00360107421875
c621-102: Epoch: 0, Step: 146, Rank: 31, loss = 0.006317138671875
c621-081: Epoch: 0, Step: 146, Rank: 26, loss = 0.0023193359375
c621-082: Epoch: 0, Step: 146, Rank: 27, loss = 1.8533319234848022e-07
c621-092: Epoch: 0, Step: 146, Rank: 29, loss = 2.2649765014648438e-06
c621-072: Epoch: 0, Step: 146, Rank: 25, loss = 4.00543212890625e-05
c613-102: Epoch: 0, Step: 146, Rank: 1, loss = 3.0547380447387695e-07
c621-071: Epoch: 0, Step: 146, Rank: 24, loss = 0.00116729736328125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-111: Epoch: 0, Step: 146, Rank: 2, loss = 0.00099945068359375
c621-061: Epoch: 0, Step: 146, Rank: 22, loss = 0.00014019012451171875
c621-052: Epoch: 0, Step: 146, Rank: 21, loss = 3.3060778616876836e-37
c621-062: Epoch: 0, Step: 146, Rank: 23, loss = 1.6540288925170898e-06
c619-032: Epoch: 0, Step: 146, Rank: 19, loss = 0.0003147125244140625
c619-041: Epoch: 0, Step: 146, Rank: 20, loss = 2.1047890186309814e-07
c619-021: Epoch: 0, Step: 146, Rank: 16, loss = 4.839897155761719e-05
c619-031: Epoch: 0, Step: 146, Rank: 18, loss = 3.979039320256561e-12
c619-022: Epoch: 0, Step: 146, Rank: 17, loss = 7.963180541992188e-05
c619-002: Epoch: 0, Step: 146, Rank: 13, loss = 0.00136566162109375
c619-011: Epoch: 0, Step: 146, Rank: 14, loss = 0.00180816650390625
c619-012: Epoch: 0, Step: 146, Rank: 15, loss = 3.1650415621697903e-10
c613-112: Epoch: 0, Step: 146, Rank: 3, loss = 0.0089111328125
c619-001: Epoch: 0, Step: 146, Rank: 12, loss = 3.123283386230469e-05
c613-152: Epoch: 0, Step: 146, Rank: 11, loss = 0.69140625
c613-121: Epoch: 0, Step: 146, Rank: 4, loss = 5.4836273193359375e-05
c613-151: Epoch: 0, Step: 146, Rank: 10, loss = 0.01251220703125
c613-132: Epoch: 0, Step: 146, Rank: 7, loss = 0.00738525390625
c613-142: Epoch: 0, Step: 146, Rank: 9, loss = 1.8775463104248047e-06
c613-122: Epoch: 0, Step: 146, Rank: 5, loss = 7.963180541992188e-05
c613-131: Epoch: 0, Step: 146, Rank: 6, loss = 1.4722347259521484e-05
c613-141: Epoch: 0, Step: 146, Rank: 8, loss = 0.0007781982421875
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 147, Rank: 43, loss = 0.0002956390380859375
c621-072: Epoch: 0, Step: 147, Rank: 25, loss = 9.5367431640625e-06
c621-142: Epoch: 0, Step: 147, Rank: 39, loss = 0.00058746337890625
c621-132: Epoch: 0, Step: 147, Rank: 37, loss = 7.566995918750763e-10
c621-081: Epoch: 0, Step: 147, Rank: 26, loss = 0.000667572021484375
c621-151: Epoch: 0, Step: 147, Rank: 40, loss = 3.725290298461914e-06
c621-131: Epoch: 0, Step: 147, Rank: 36, loss = 2.5331974029541016e-07
c621-111: Epoch: 0, Step: 147, Rank: 32, loss = 1.895427703857422e-05
c622-011: Epoch: 0, Step: 147, Rank: 44, loss = 1.6079866327345371e-09
c621-141: Epoch: 0, Step: 147, Rank: 38, loss = 0.006927490234375
c621-121: Epoch: 0, Step: 147, Rank: 34, loss = 2.1457672119140625e-05
c621-091: Epoch: 0, Step: 147, Rank: 28, loss = 3.6845933205562065e-20
c621-112: Epoch: 0, Step: 147, Rank: 33, loss = 7.486343383789062e-05
c621-122: Epoch: 0, Step: 147, Rank: 35, loss = 7.729977369308472e-08
c621-082: Epoch: 0, Step: 147, Rank: 27, loss = 0.00299072265625
c621-092: Epoch: 0, Step: 147, Rank: 29, loss = 8.754432201385498e-08
c622-012: Epoch: 0, Step: 147, Rank: 45, loss = 5.14984130859375e-05
c621-101: Epoch: 0, Step: 147, Rank: 30, loss = 1.3445969671010971e-08
c621-102: Epoch: 0, Step: 147, Rank: 31, loss = 3.7670135498046875e-05
c622-001: Epoch: 0, Step: 147, Rank: 42, loss = 6.139278411865234e-06
c621-152: Epoch: 0, Step: 147, Rank: 41, loss = 0.0007781982421875
c619-031: Epoch: 0, Step: 147, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 147, Rank: 16, loss = 0.00299072265625
c621-071: Epoch: 0, Step: 147, Rank: 24, loss = 0.0005035400390625
c621-062: Epoch: 0, Step: 147, Rank: 23, loss = 0.00421142578125
c619-032: Epoch: 0, Step: 147, Rank: 19, loss = 0.000179290771484375
c621-061: Epoch: 0, Step: 147, Rank: 22, loss = 4.5299530029296875e-05
c621-052: Epoch: 0, Step: 147, Rank: 21, loss = 0.000606536865234375
c619-041: Epoch: 0, Step: 147, Rank: 20, loss = 2.1457672119140625e-05
c619-022: Epoch: 0, Step: 147, Rank: 17, loss = 6.198883056640625e-05
c622-021: Epoch: 0, Step: 147, Rank: 46, loss = 6.056285572868247e-20
c619-012: Epoch: 0, Step: 147, Rank: 15, loss = 0.0005035400390625
c619-002: Epoch: 0, Step: 147, Rank: 13, loss = 5.817413330078125e-05
c619-011: Epoch: 0, Step: 147, Rank: 14, loss = 0.00010251998901367188
c619-001: Epoch: 0, Step: 147, Rank: 12, loss = 2.1047890186309814e-07
c613-152: Epoch: 0, Step: 147, Rank: 11, loss = 0.0002460479736328125
c613-151: Epoch: 0, Step: 147, Rank: 10, loss = 0.00010251998901367188
c613-142: Epoch: 0, Step: 147, Rank: 9, loss = 1.0089706847793423e-12
c622-022: Epoch: 0, Step: 147, Rank: 47, loss = 0.0034942626953125
c613-141: Epoch: 0, Step: 147, Rank: 8, loss = 0.00170135498046875
c613-132: Epoch: 0, Step: 147, Rank: 7, loss = 4.500150680541992e-06
c613-131: Epoch: 0, Step: 147, Rank: 6, loss = 1.0132789611816406e-05
c622-031: Epoch: 0, Step: 147, Rank: 48, loss = 5.893525667488575e-10
c613-122: Epoch: 0, Step: 147, Rank: 5, loss = 7.009506225585938e-05
c613-101: Epoch: 0, Step: 147, Rank: 0, loss = 9.5367431640625e-06
c613-121: Epoch: 0, Step: 147, Rank: 4, loss = 0.00021648406982421875
c613-112: Epoch: 0, Step: 147, Rank: 3, loss = 1.150369644165039e-05
c613-111: Epoch: 0, Step: 147, Rank: 2, loss = 4.1443854570388794e-08
c613-102: Epoch: 0, Step: 147, Rank: 1, loss = 5.3085386753082275e-08
c622-032: Epoch: 0, Step: 147, Rank: 49, loss = 0.022216796875
c622-101: Epoch: 0, Step: 147, Rank: 62, loss = 2.2351741790771484e-07
c622-102: Epoch: 0, Step: 147, Rank: 63, loss = 9.918585419654846e-08
c622-092: Epoch: 0, Step: 147, Rank: 61, loss = 9.870390964984584e-34
c622-082: Epoch: 0, Step: 147, Rank: 59, loss = 0.00019073486328125
c622-041: Epoch: 0, Step: 147, Rank: 50, loss = 0.00193023681640625
c622-081: Epoch: 0, Step: 147, Rank: 58, loss = 1.8775463104248047e-06
c622-091: Epoch: 0, Step: 147, Rank: 60, loss = 0.00012302398681640625
c622-052: Epoch: 0, Step: 147, Rank: 53, loss = 0.0004730224609375
c622-051: Epoch: 0, Step: 147, Rank: 52, loss = 0.000911712646484375
c622-061: Epoch: 0, Step: 147, Rank: 54, loss = 0.0038299560546875
c622-072: Epoch: 0, Step: 147, Rank: 57, loss = 5.781650543212891e-06
c622-042: Epoch: 0, Step: 147, Rank: 51, loss = 0.00128173828125
c622-062: Epoch: 0, Step: 147, Rank: 55, loss = 8.487701416015625e-05
c622-071: Epoch: 0, Step: 147, Rank: 56, loss = 8.754432201385498e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75390625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 148, Rank: 0, loss = 0.0008544921875
c622-012: Epoch: 0, Step: 148, Rank: 45, loss = 7.009506225585938e-05
c619-031: Epoch: 0, Step: 148, Rank: 18, loss = 0.00096893310546875
c619-021: Epoch: 0, Step: 148, Rank: 16, loss = 0.000553131103515625
c621-111: Epoch: 0, Step: 148, Rank: 32, loss = 0.006317138671875
c619-002: Epoch: 0, Step: 148, Rank: 13, loss = 4.231929779052734e-06
c621-072: Epoch: 0, Step: 148, Rank: 25, loss = 0.0380859375
c621-132: Epoch: 0, Step: 148, Rank: 37, loss = 0.69140625
c621-052: Epoch: 0, Step: 148, Rank: 21, loss = 0.00124359130859375
c621-081: Epoch: 0, Step: 148, Rank: 26, loss = 0.00058746337890625
c613-141: Epoch: 0, Step: 148, Rank: 8, loss = 0.69140625
c622-081: Epoch: 0, Step: 148, Rank: 58, loss = 0.69140625
c613-132: Epoch: 0, Step: 148, Rank: 7, loss = 0.0
c622-002: Epoch: 0, Step: 148, Rank: 43, loss = 1.1484375
c613-151: Epoch: 0, Step: 148, Rank: 10, loss = 0.0
c619-011: Epoch: 0, Step: 148, Rank: 14, loss = 3.314018249511719e-05
c619-022: Epoch: 0, Step: 148, Rank: 17, loss = 3.293156623840332e-06
c621-151: Epoch: 0, Step: 148, Rank: 40, loss = 3.528594970703125e-05
c622-001: Epoch: 0, Step: 148, Rank: 42, loss = 1.1874362826347351e-08
c619-032: Epoch: 0, Step: 148, Rank: 19, loss = 0.00040435791015625
c613-142: Epoch: 0, Step: 148, Rank: 9, loss = 0.00021648406982421875
c622-042: Epoch: 0, Step: 148, Rank: 51, loss = 6.628036499023438e-05
c621-131: Epoch: 0, Step: 148, Rank: 36, loss = 6.198883056640625e-05
c621-121: Epoch: 0, Step: 148, Rank: 34, loss = 4.1443854570388794e-08
c621-101: Epoch: 0, Step: 148, Rank: 30, loss = 0.01007080078125
c622-032: Epoch: 0, Step: 148, Rank: 49, loss = 0.69140625
c613-131: Epoch: 0, Step: 148, Rank: 6, loss = 0.01104736328125
c613-121: Epoch: 0, Step: 148, Rank: 4, loss = 0.0
c622-031: Epoch: 0, Step: 148, Rank: 48, loss = 0.00193023681640625
c619-001: Epoch: 0, Step: 148, Rank: 12, loss = 1.3709068298339844e-06
c613-112: Epoch: 0, Step: 148, Rank: 3, loss = 0.69140625
c622-041: Epoch: 0, Step: 148, Rank: 50, loss = 1.6689300537109375e-05
c621-091: Epoch: 0, Step: 148, Rank: 28, loss = 4.500150680541992e-06
c622-092: Epoch: 0, Step: 148, Rank: 61, loss = 0.0002307891845703125
c621-092: Epoch: 0, Step: 148, Rank: 29, loss = 0.0021820068359375
c622-052: Epoch: 0, Step: 148, Rank: 53, loss = 5.781650543212891e-06
c621-061: Epoch: 0, Step: 148, Rank: 22, loss = 0.0052490234375
c622-011: Epoch: 0, Step: 148, Rank: 44, loss = 5.029141902923584e-07
c619-041: Epoch: 0, Step: 148, Rank: 20, loss = 6.628036499023438e-05
c613-122: Epoch: 0, Step: 148, Rank: 5, loss = 0.0002307891845703125
c613-152: Epoch: 0, Step: 148, Rank: 11, loss = 1.1874362826347351e-08
c622-101: Epoch: 0, Step: 148, Rank: 62, loss = 1.2218952178955078e-05
c622-071: Epoch: 0, Step: 148, Rank: 56, loss = 0.002899169921875
c621-152: Epoch: 0, Step: 148, Rank: 41, loss = 0.69140625
c621-142: Epoch: 0, Step: 148, Rank: 39, loss = 4.231929779052734e-06
c621-141: Epoch: 0, Step: 148, Rank: 38, loss = 5.4836273193359375e-05
c621-082: Epoch: 0, Step: 148, Rank: 27, loss = 2.514570951461792e-08
c622-062: Epoch: 0, Step: 148, Rank: 55, loss = 1.3322676295501878e-15
c613-111: Epoch: 0, Step: 148, Rank: 2, loss = 0.000392913818359375
c622-061: Epoch: 0, Step: 148, Rank: 54, loss = 6.628036499023438e-05
c622-082: Epoch: 0, Step: 148, Rank: 59, loss = 9.183549615799121e-41
c622-051: Epoch: 0, Step: 148, Rank: 52, loss = 1.053497228147536e-20
c619-012: Epoch: 0, Step: 148, Rank: 15, loss = 0.005584716796875
c622-091: Epoch: 0, Step: 148, Rank: 60, loss = 0.00011587142944335938
c621-071: Epoch: 0, Step: 148, Rank: 24, loss = 0.0002307891845703125
c622-022: Epoch: 0, Step: 148, Rank: 47, loss = 1.150369644165039e-05
c621-062: Epoch: 0, Step: 148, Rank: 23, loss = 2.4318695068359375e-05
c621-122: Epoch: 0, Step: 148, Rank: 35, loss = 0.000392913818359375
c622-072: Epoch: 0, Step: 148, Rank: 57, loss = 0.69140625
c622-021: Epoch: 0, Step: 148, Rank: 46, loss = 0.005584716796875
c622-102: Epoch: 0, Step: 148, Rank: 63, loss = 0.08251953125
c621-112: Epoch: 0, Step: 148, Rank: 33, loss = 0.0001087188720703125
c613-102: Epoch: 0, Step: 148, Rank: 1, loss = 0.0
c621-102: Epoch: 0, Step: 148, Rank: 31, loss = 7.194411455615628e-28
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 149, Rank: 32, loss = 0.0002307891845703125
c619-002: Epoch: 0, Step: 149, Rank: 13, loss = 5.14984130859375e-05
c622-081: Epoch: 0, Step: 149, Rank: 58, loss = 0.00040435791015625
c621-081: Epoch: 0, Step: 149, Rank: 26, loss = 0.00010251998901367188
c619-001: Epoch: 0, Step: 149, Rank: 12, loss = 2.2351741790771484e-07
c613-101: Epoch: 0, Step: 149, Rank: 0, loss = 0.06787109375
c622-052: Epoch: 0, Step: 149, Rank: 53, loss = 7.486343383789062e-05
c622-002: Epoch: 0, Step: 149, Rank: 43, loss = 4.00543212890625e-05
c619-021: Epoch: 0, Step: 149, Rank: 16, loss = 4.798173904418945e-06
c619-031: Epoch: 0, Step: 149, Rank: 18, loss = 8.003553375601768e-11
c622-012: Epoch: 0, Step: 149, Rank: 45, loss = 0.0001583099365234375
c621-072: Epoch: 0, Step: 149, Rank: 25, loss = 2.9331204132176936e-11
c621-121: Epoch: 0, Step: 149, Rank: 34, loss = 4.6629367034256575e-15
c621-151: Epoch: 0, Step: 149, Rank: 40, loss = 2.2649765014648438e-06
c622-001: Epoch: 0, Step: 149, Rank: 42, loss = 2.586841583251953e-05
c619-041: Epoch: 0, Step: 149, Rank: 20, loss = 3.123283386230469e-05
c621-091: Epoch: 0, Step: 149, Rank: 28, loss = 1.6540288925170898e-06
c622-082: Epoch: 0, Step: 149, Rank: 59, loss = 7.486343383789062e-05
c621-101: Epoch: 0, Step: 149, Rank: 30, loss = 0.000518798828125
c613-141: Epoch: 0, Step: 149, Rank: 8, loss = 0.00019073486328125
c621-132: Epoch: 0, Step: 149, Rank: 37, loss = 1.3213420162451948e-29
c621-082: Epoch: 0, Step: 149, Rank: 27, loss = 5.44811591673966e-18
c613-131: Epoch: 0, Step: 149, Rank: 6, loss = 0.0005035400390625
c622-062: Epoch: 0, Step: 149, Rank: 55, loss = 0.00811767578125
c619-022: Epoch: 0, Step: 149, Rank: 17, loss = 0.05322265625
c622-101: Epoch: 0, Step: 149, Rank: 62, loss = 2.648448571562767e-09
c621-131: Epoch: 0, Step: 149, Rank: 36, loss = 0.004486083984375
c613-142: Epoch: 0, Step: 149, Rank: 9, loss = 7.420778274536133e-06
c619-032: Epoch: 0, Step: 149, Rank: 19, loss = 0.00014019012451171875
c622-071: Epoch: 0, Step: 149, Rank: 56, loss = 0.5859375
c622-051: Epoch: 0, Step: 149, Rank: 52, loss = 0.0002956390380859375
c613-152: Epoch: 0, Step: 149, Rank: 11, loss = 2.514570951461792e-08
c621-142: Epoch: 0, Step: 149, Rank: 39, loss = 1.2993812561035156e-05
c621-052: Epoch: 0, Step: 149, Rank: 21, loss = 0.000606536865234375
c613-121: Epoch: 0, Step: 149, Rank: 4, loss = 2.2649765014648438e-06
c619-011: Epoch: 0, Step: 149, Rank: 14, loss = 5.14984130859375e-05
c613-132: Epoch: 0, Step: 149, Rank: 7, loss = 0.000179290771484375
c622-042: Epoch: 0, Step: 149, Rank: 51, loss = 4.5299530029296875e-05
c613-102: Epoch: 0, Step: 149, Rank: 1, loss = 0.23046875
c621-061: Epoch: 0, Step: 149, Rank: 22, loss = 0.00150299072265625
c622-032: Epoch: 0, Step: 149, Rank: 49, loss = 0.69140625
c621-141: Epoch: 0, Step: 149, Rank: 38, loss = 0.00012302398681640625
c613-122: Epoch: 0, Step: 149, Rank: 5, loss = 1.7497114868092467e-13
c621-122: Epoch: 0, Step: 149, Rank: 35, loss = 0.0021209716796875
c622-061: Epoch: 0, Step: 149, Rank: 54, loss = 0.003387451171875
c622-011: Epoch: 0, Step: 149, Rank: 44, loss = 0.000667572021484375
c622-102: Epoch: 0, Step: 149, Rank: 63, loss = 2.7830537874251604e-10
c621-152: Epoch: 0, Step: 149, Rank: 41, loss = 0.000431060791015625
c613-151: Epoch: 0, Step: 149, Rank: 10, loss = 2.753734588623047e-05
c622-031: Epoch: 0, Step: 149, Rank: 48, loss = 0.00099945068359375
c621-102: Epoch: 0, Step: 149, Rank: 31, loss = 0.00020313262939453125
c622-021: Epoch: 0, Step: 149, Rank: 46, loss = 5.424022674560547e-06
c622-072: Epoch: 0, Step: 149, Rank: 57, loss = 0.69140625
c621-112: Epoch: 0, Step: 149, Rank: 33, loss = 5.424022674560547e-06
c613-112: Epoch: 0, Step: 149, Rank: 3, loss = 0.00040435791015625
c622-092: Epoch: 0, Step: 149, Rank: 61, loss = 2.2649765014648438e-06
c619-012: Epoch: 0, Step: 149, Rank: 15, loss = 6.139278411865234e-06
c622-041: Epoch: 0, Step: 149, Rank: 50, loss = 0.005584716796875
c621-071: Epoch: 0, Step: 149, Rank: 24, loss = 0.00075531005859375
c621-062: Epoch: 0, Step: 149, Rank: 23, loss = 2.753734588623047e-05
c622-091: Epoch: 0, Step: 149, Rank: 60, loss = 2.68426485998971e-33
c622-022: Epoch: 0, Step: 149, Rank: 47, loss = 0.00075531005859375
c621-092: Epoch: 0, Step: 149, Rank: 29, loss = 0.001068115234375
c613-111: Epoch: 0, Step: 149, Rank: 2, loss = 5.424022674560547e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2470703125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 150, Rank: 0, loss = 0.0026397705078125
c622-081: Epoch: 0, Step: 150, Rank: 58, loss = 0.00150299072265625
c619-021: Epoch: 0, Step: 150, Rank: 16, loss = 3.3060778616876836e-37
c622-062: Epoch: 0, Step: 150, Rank: 55, loss = 0.0003566741943359375
c622-002: Epoch: 0, Step: 150, Rank: 43, loss = 0.00180816650390625
c621-132: Epoch: 0, Step: 150, Rank: 37, loss = 5.699694156646729e-07
c622-052: Epoch: 0, Step: 150, Rank: 53, loss = 0.00083160400390625
c622-012: Epoch: 0, Step: 150, Rank: 45, loss = 0.000457763671875
c621-151: Epoch: 0, Step: 150, Rank: 40, loss = 0.0002613067626953125
c622-071: Epoch: 0, Step: 150, Rank: 56, loss = 2.204051907791789e-39
c622-101: Epoch: 0, Step: 150, Rank: 62, loss = 0.69140625
c622-001: Epoch: 0, Step: 150, Rank: 42, loss = 4.172325134277344e-07
c619-002: Epoch: 0, Step: 150, Rank: 13, loss = 7.963180541992188e-05
c622-082: Epoch: 0, Step: 150, Rank: 59, loss = 3.46451997756958e-07
c621-081: Epoch: 0, Step: 150, Rank: 26, loss = 1.996755599975586e-06
c621-091: Epoch: 0, Step: 150, Rank: 28, loss = 0.00193023681640625
c622-011: Epoch: 0, Step: 150, Rank: 44, loss = 1.1864468014524018e-27
c622-092: Epoch: 0, Step: 150, Rank: 61, loss = 0.0001087188720703125
c621-101: Epoch: 0, Step: 150, Rank: 30, loss = 0.0021820068359375
c613-151: Epoch: 0, Step: 150, Rank: 10, loss = 0.0028076171875
c613-132: Epoch: 0, Step: 150, Rank: 7, loss = 0.001068115234375
c613-121: Epoch: 0, Step: 150, Rank: 4, loss = 1.3869794202037156e-11
c613-111: Epoch: 0, Step: 150, Rank: 2, loss = 5.817413330078125e-05
c621-142: Epoch: 0, Step: 150, Rank: 39, loss = 0.69140625
c622-051: Epoch: 0, Step: 150, Rank: 52, loss = 6.628036499023438e-05
c621-131: Epoch: 0, Step: 150, Rank: 36, loss = 0.00014019012451171875
c621-111: Epoch: 0, Step: 150, Rank: 32, loss = 0.0005035400390625
c613-152: Epoch: 0, Step: 150, Rank: 11, loss = 3.123283386230469e-05
c621-102: Epoch: 0, Step: 150, Rank: 31, loss = 2.014636993408203e-05
c622-041: Epoch: 0, Step: 150, Rank: 50, loss = 0.00150299072265625
c622-061: Epoch: 0, Step: 150, Rank: 54, loss = 0.00021648406982421875
c613-122: Epoch: 0, Step: 150, Rank: 5, loss = 0.0002956390380859375
c622-032: Epoch: 0, Step: 150, Rank: 49, loss = 0.69140625
c619-011: Epoch: 0, Step: 150, Rank: 14, loss = 0.002471923828125
c619-001: Epoch: 0, Step: 150, Rank: 12, loss = 0.69140625
c619-032: Epoch: 0, Step: 150, Rank: 19, loss = 0.0038299560546875
c621-052: Epoch: 0, Step: 150, Rank: 21, loss = 0.000644683837890625
c621-072: Epoch: 0, Step: 150, Rank: 25, loss = 0.000148773193359375
c613-131: Epoch: 0, Step: 150, Rank: 6, loss = 1.2218952178955078e-05
c619-031: Epoch: 0, Step: 150, Rank: 18, loss = 0.69140625
c613-102: Epoch: 0, Step: 150, Rank: 1, loss = 4.00543212890625e-05
c622-031: Epoch: 0, Step: 150, Rank: 48, loss = 4.231929779052734e-06
c619-012: Epoch: 0, Step: 150, Rank: 15, loss = 8.487701416015625e-05
c622-072: Epoch: 0, Step: 150, Rank: 57, loss = 7.338821887969971e-07
c619-022: Epoch: 0, Step: 150, Rank: 17, loss = 7.283063041541027e-14
c622-042: Epoch: 0, Step: 150, Rank: 51, loss = 8.149072527885437e-09
c621-141: Epoch: 0, Step: 150, Rank: 38, loss = 0.00225830078125
c621-082: Epoch: 0, Step: 150, Rank: 27, loss = 8.940696716308594e-06
c613-112: Epoch: 0, Step: 150, Rank: 3, loss = 0.00164794921875
c613-142: Epoch: 0, Step: 150, Rank: 9, loss = 0.001068115234375
c622-021: Epoch: 0, Step: 150, Rank: 46, loss = 8.307397365570068e-07
c619-041: Epoch: 0, Step: 150, Rank: 20, loss = 0.00421142578125
c621-061: Epoch: 0, Step: 150, Rank: 22, loss = 1.6391277313232422e-07
c621-112: Epoch: 0, Step: 150, Rank: 33, loss = 0.00016880035400390625
c621-122: Epoch: 0, Step: 150, Rank: 35, loss = 0.003387451171875
c621-062: Epoch: 0, Step: 150, Rank: 23, loss = 1.0662875083691372e-25
c621-152: Epoch: 0, Step: 150, Rank: 41, loss = 0.00040435791015625
c622-022: Epoch: 0, Step: 150, Rank: 47, loss = 4.843059286940843e-11
c622-102: Epoch: 0, Step: 150, Rank: 63, loss = 3.213062882423401e-08
c621-092: Epoch: 0, Step: 150, Rank: 29, loss = 2.9331204132176936e-11
c621-071: Epoch: 0, Step: 150, Rank: 24, loss = 0.000335693359375
c613-141: Epoch: 0, Step: 150, Rank: 8, loss = 5.3085386753082275e-08
c621-121: Epoch: 0, Step: 150, Rank: 34, loss = 0.00010251998901367188
c622-091: Epoch: 0, Step: 150, Rank: 60, loss = 3.7670135498046875e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 151, Rank: 16, loss = 1.4722347259521484e-05
c613-101: Epoch: 0, Step: 151, Rank: 0, loss = 0.69140625
c622-002: Epoch: 0, Step: 151, Rank: 43, loss = 1.8533319234848022e-07
c622-052: Epoch: 0, Step: 151, Rank: 53, loss = 0.00180816650390625
c621-082: Epoch: 0, Step: 151, Rank: 27, loss = 0.00014019012451171875
c621-081: Epoch: 0, Step: 151, Rank: 26, loss = 9.424984455108643e-07
c619-022: Epoch: 0, Step: 151, Rank: 17, loss = 8.940696716308594e-06
c621-101: Epoch: 0, Step: 151, Rank: 30, loss = 6.891787052154541e-07
c622-101: Epoch: 0, Step: 151, Rank: 62, loss = 0.0002307891845703125
c619-031: Epoch: 0, Step: 151, Rank: 18, loss = 0.0
c622-092: Epoch: 0, Step: 151, Rank: 61, loss = 0.0007781982421875
c621-091: Epoch: 0, Step: 151, Rank: 28, loss = 3.694822225952521e-13
c622-012: Epoch: 0, Step: 151, Rank: 45, loss = 1.0788440704345703e-05
c621-092: Epoch: 0, Step: 151, Rank: 29, loss = 0.0026397705078125
c621-072: Epoch: 0, Step: 151, Rank: 25, loss = 1.0132789611816406e-05
c622-051: Epoch: 0, Step: 151, Rank: 52, loss = 0.000553131103515625
c619-032: Epoch: 0, Step: 151, Rank: 19, loss = 0.05078125
c621-132: Epoch: 0, Step: 151, Rank: 37, loss = 0.0021209716796875
c613-121: Epoch: 0, Step: 151, Rank: 4, loss = 0.000667572021484375
c619-011: Epoch: 0, Step: 151, Rank: 14, loss = 0.00070953369140625
c619-012: Epoch: 0, Step: 151, Rank: 15, loss = 0.0010986328125
c622-061: Epoch: 0, Step: 151, Rank: 54, loss = 9.918585419654846e-08
c622-102: Epoch: 0, Step: 151, Rank: 63, loss = 1.6391277313232422e-07
c621-111: Epoch: 0, Step: 151, Rank: 32, loss = 3.123283386230469e-05
c622-001: Epoch: 0, Step: 151, Rank: 42, loss = 0.00860595703125
c619-002: Epoch: 0, Step: 151, Rank: 13, loss = 0.0
c621-151: Epoch: 0, Step: 151, Rank: 40, loss = 1.6540288925170898e-06
c619-041: Epoch: 0, Step: 151, Rank: 20, loss = 0.0003147125244140625
c613-112: Epoch: 0, Step: 151, Rank: 3, loss = 0.01177978515625
c613-132: Epoch: 0, Step: 151, Rank: 7, loss = 9.049472282640636e-11
c622-032: Epoch: 0, Step: 151, Rank: 49, loss = 0.000335693359375
c613-152: Epoch: 0, Step: 151, Rank: 11, loss = 5.115907697472721e-12
c622-082: Epoch: 0, Step: 151, Rank: 59, loss = 0.69140625
c621-102: Epoch: 0, Step: 151, Rank: 31, loss = 9.424984455108643e-07
c621-131: Epoch: 0, Step: 151, Rank: 36, loss = 2.7008354663848877e-07
c619-001: Epoch: 0, Step: 151, Rank: 12, loss = 0.00058746337890625
c613-151: Epoch: 0, Step: 151, Rank: 10, loss = 2.5331974029541016e-07
c613-111: Epoch: 0, Step: 151, Rank: 2, loss = 0.0255126953125
c621-071: Epoch: 0, Step: 151, Rank: 24, loss = 0.00141143798828125
c622-041: Epoch: 0, Step: 151, Rank: 50, loss = 0.00075531005859375
c622-071: Epoch: 0, Step: 151, Rank: 56, loss = 0.000148773193359375
c621-141: Epoch: 0, Step: 151, Rank: 38, loss = 0.01507568359375
c621-061: Epoch: 0, Step: 151, Rank: 22, loss = 1.6209256159527285e-14
c621-122: Epoch: 0, Step: 151, Rank: 35, loss = 2.3245294578089215e-16
c613-131: Epoch: 0, Step: 151, Rank: 6, loss = 1.0058283805847168e-06
c622-031: Epoch: 0, Step: 151, Rank: 48, loss = 0.0003566741943359375
c613-102: Epoch: 0, Step: 151, Rank: 1, loss = 0.0005035400390625
c621-142: Epoch: 0, Step: 151, Rank: 39, loss = 5.3085386753082275e-08
c622-091: Epoch: 0, Step: 151, Rank: 60, loss = 1.996755599975586e-06
c621-052: Epoch: 0, Step: 151, Rank: 21, loss = 5.424022674560547e-06
c622-042: Epoch: 0, Step: 151, Rank: 51, loss = 0.0001087188720703125
c622-011: Epoch: 0, Step: 151, Rank: 44, loss = 0.00011587142944335938
c621-121: Epoch: 0, Step: 151, Rank: 34, loss = 0.00021648406982421875
c621-062: Epoch: 0, Step: 151, Rank: 23, loss = 0.00811767578125
c622-062: Epoch: 0, Step: 151, Rank: 55, loss = 0.00164794921875
c622-021: Epoch: 0, Step: 151, Rank: 46, loss = 6.198883056640625e-05
c621-112: Epoch: 0, Step: 151, Rank: 33, loss = 0.000667572021484375
c613-141: Epoch: 0, Step: 151, Rank: 8, loss = 0.00019073486328125
c622-022: Epoch: 0, Step: 151, Rank: 47, loss = 5.4836273193359375e-05
c613-142: Epoch: 0, Step: 151, Rank: 9, loss = 0.025146484375
c622-081: Epoch: 0, Step: 151, Rank: 58, loss = 1.55717134475708e-06
c621-152: Epoch: 0, Step: 151, Rank: 41, loss = 9.486769009248164e-19
c622-072: Epoch: 0, Step: 151, Rank: 57, loss = 3.123283386230469e-05
c613-122: Epoch: 0, Step: 151, Rank: 5, loss = 0.00136566162109375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 152, Rank: 0, loss = 4.3655745685100555e-09
c621-091: Epoch: 0, Step: 152, Rank: 28, loss = 0.0002307891845703125
c621-072: Epoch: 0, Step: 152, Rank: 25, loss = 0.00020313262939453125
c621-132: Epoch: 0, Step: 152, Rank: 37, loss = 0.000278472900390625
c621-151: Epoch: 0, Step: 152, Rank: 40, loss = 0.0
c619-002: Epoch: 0, Step: 152, Rank: 13, loss = 0.00433349609375
c622-002: Epoch: 0, Step: 152, Rank: 43, loss = 1.4722347259521484e-05
c622-081: Epoch: 0, Step: 152, Rank: 58, loss = 2.6756374893466273e-14
c622-001: Epoch: 0, Step: 152, Rank: 42, loss = 0.0003147125244140625
c613-152: Epoch: 0, Step: 152, Rank: 11, loss = 0.000518798828125
c613-132: Epoch: 0, Step: 152, Rank: 7, loss = 0.002899169921875
c622-101: Epoch: 0, Step: 152, Rank: 62, loss = 0.00020313262939453125
c619-022: Epoch: 0, Step: 152, Rank: 17, loss = 0.000911712646484375
c622-062: Epoch: 0, Step: 152, Rank: 55, loss = 0.000644683837890625
c621-081: Epoch: 0, Step: 152, Rank: 26, loss = 0.0
c622-012: Epoch: 0, Step: 152, Rank: 45, loss = 0.00011587142944335938
c619-001: Epoch: 0, Step: 152, Rank: 12, loss = 0.006103515625
c613-121: Epoch: 0, Step: 152, Rank: 4, loss = 1.1399388313293457e-06
c622-052: Epoch: 0, Step: 152, Rank: 53, loss = 0.0016021728515625
c613-151: Epoch: 0, Step: 152, Rank: 10, loss = 5.817413330078125e-05
c621-121: Epoch: 0, Step: 152, Rank: 34, loss = 0.0002307891845703125
c621-141: Epoch: 0, Step: 152, Rank: 38, loss = 4.405564747785802e-33
c622-092: Epoch: 0, Step: 152, Rank: 61, loss = 0.00020313262939453125
c621-082: Epoch: 0, Step: 152, Rank: 27, loss = 6.628036499023438e-05
c621-152: Epoch: 0, Step: 152, Rank: 41, loss = 4.267692565917969e-05
c621-061: Epoch: 0, Step: 152, Rank: 22, loss = 0.000518798828125
c621-052: Epoch: 0, Step: 152, Rank: 21, loss = 9.012222290039062e-05
c619-031: Epoch: 0, Step: 152, Rank: 18, loss = 0.00075531005859375
c621-111: Epoch: 0, Step: 152, Rank: 32, loss = 0.0010986328125
c613-111: Epoch: 0, Step: 152, Rank: 2, loss = 0.69140625
c619-021: Epoch: 0, Step: 152, Rank: 16, loss = 0.0023956298828125
c619-032: Epoch: 0, Step: 152, Rank: 19, loss = 4.4517219066619873e-07
c621-101: Epoch: 0, Step: 152, Rank: 30, loss = 4.839897155761719e-05
c622-061: Epoch: 0, Step: 152, Rank: 54, loss = 0.0284423828125
c622-011: Epoch: 0, Step: 152, Rank: 44, loss = 0.69140625
c622-102: Epoch: 0, Step: 152, Rank: 63, loss = 3.54136699749265e-24
c613-122: Epoch: 0, Step: 152, Rank: 5, loss = 0.00811767578125
c613-141: Epoch: 0, Step: 152, Rank: 8, loss = 0.003173828125
c621-131: Epoch: 0, Step: 152, Rank: 36, loss = 0.193359375
c622-041: Epoch: 0, Step: 152, Rank: 50, loss = 0.00019073486328125
c621-142: Epoch: 0, Step: 152, Rank: 39, loss = 0.00075531005859375
c613-131: Epoch: 0, Step: 152, Rank: 6, loss = 7.338821887969971e-07
c619-012: Epoch: 0, Step: 152, Rank: 15, loss = 0.00070953369140625
c621-122: Epoch: 0, Step: 152, Rank: 35, loss = 0.000278472900390625
c613-112: Epoch: 0, Step: 152, Rank: 3, loss = 0.00136566162109375
c622-082: Epoch: 0, Step: 152, Rank: 59, loss = 0.000606536865234375
c619-041: Epoch: 0, Step: 152, Rank: 20, loss = 0.0020599365234375
c619-011: Epoch: 0, Step: 152, Rank: 14, loss = 0.0003566741943359375
c621-112: Epoch: 0, Step: 152, Rank: 33, loss = 7.486343383789062e-05
c622-032: Epoch: 0, Step: 152, Rank: 49, loss = 0.0001087188720703125
c622-072: Epoch: 0, Step: 152, Rank: 57, loss = 8.940696716308594e-06
c613-102: Epoch: 0, Step: 152, Rank: 1, loss = 1.895427703857422e-05
c621-092: Epoch: 0, Step: 152, Rank: 29, loss = 3.688037395477295e-07
c621-071: Epoch: 0, Step: 152, Rank: 24, loss = 0.004913330078125
c622-031: Epoch: 0, Step: 152, Rank: 48, loss = 0.0002307891845703125
c621-102: Epoch: 0, Step: 152, Rank: 31, loss = 1.418811734765768e-09
c622-022: Epoch: 0, Step: 152, Rank: 47, loss = 7.338821887969971e-07
c622-051: Epoch: 0, Step: 152, Rank: 52, loss = 3.979039320256561e-12
c622-071: Epoch: 0, Step: 152, Rank: 56, loss = 0.000606536865234375
c621-062: Epoch: 0, Step: 152, Rank: 23, loss = 0.0020599365234375
c622-042: Epoch: 0, Step: 152, Rank: 51, loss = 0.0002460479736328125
c622-091: Epoch: 0, Step: 152, Rank: 60, loss = 0.0026397705078125
c613-142: Epoch: 0, Step: 152, Rank: 9, loss = 3.510081114654895e-12
c622-021: Epoch: 0, Step: 152, Rank: 46, loss = 7.486343383789062e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24560546875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 153, Rank: 0, loss = 0.001983642578125
c622-081: Epoch: 0, Step: 153, Rank: 58, loss = 0.0002956390380859375
c622-012: Epoch: 0, Step: 153, Rank: 45, loss = 3.0547380447387695e-07
c622-002: Epoch: 0, Step: 153, Rank: 43, loss = 7.420778274536133e-06
c621-132: Epoch: 0, Step: 153, Rank: 37, loss = 1.2993812561035156e-05
c622-092: Epoch: 0, Step: 153, Rank: 61, loss = 0.00058746337890625
c622-082: Epoch: 0, Step: 153, Rank: 59, loss = 3.293156623840332e-06
c621-111: Epoch: 0, Step: 153, Rank: 32, loss = 0.000667572021484375
c622-001: Epoch: 0, Step: 153, Rank: 42, loss = 1.525040715932846e-08
c619-001: Epoch: 0, Step: 153, Rank: 12, loss = 0.69140625
c622-101: Epoch: 0, Step: 153, Rank: 62, loss = 3.0547380447387695e-07
c621-151: Epoch: 0, Step: 153, Rank: 40, loss = 4.839897155761719e-05
c613-132: Epoch: 0, Step: 153, Rank: 7, loss = 7.283063041541027e-14
c613-152: Epoch: 0, Step: 153, Rank: 11, loss = 0.0016021728515625
c619-021: Epoch: 0, Step: 153, Rank: 16, loss = 1.895427703857422e-05
c622-071: Epoch: 0, Step: 153, Rank: 56, loss = 1.996755599975586e-06
c619-011: Epoch: 0, Step: 153, Rank: 14, loss = 0.000148773193359375
c621-082: Epoch: 0, Step: 153, Rank: 27, loss = 4.500150680541992e-06
c613-111: Epoch: 0, Step: 153, Rank: 2, loss = 0.0001316070556640625
c622-051: Epoch: 0, Step: 153, Rank: 52, loss = 0.00787353515625
c619-041: Epoch: 0, Step: 153, Rank: 20, loss = 1.895427703857422e-05
c621-052: Epoch: 0, Step: 153, Rank: 21, loss = 0.000431060791015625
c621-081: Epoch: 0, Step: 153, Rank: 26, loss = 7.188646122813225e-09
c622-052: Epoch: 0, Step: 153, Rank: 53, loss = 0.00021648406982421875
c622-062: Epoch: 0, Step: 153, Rank: 55, loss = 0.0003566741943359375
c619-022: Epoch: 0, Step: 153, Rank: 17, loss = 2.753734588623047e-05
c621-121: Epoch: 0, Step: 153, Rank: 34, loss = 0.0
c622-011: Epoch: 0, Step: 153, Rank: 44, loss = 0.000553131103515625
c621-091: Epoch: 0, Step: 153, Rank: 28, loss = 2.4318695068359375e-05
c613-112: Epoch: 0, Step: 153, Rank: 3, loss = 3.123283386230469e-05
c613-131: Epoch: 0, Step: 153, Rank: 6, loss = 0.003082275390625
c622-102: Epoch: 0, Step: 153, Rank: 63, loss = 0.0242919921875
c613-122: Epoch: 0, Step: 153, Rank: 5, loss = 1.895427703857422e-05
c613-142: Epoch: 0, Step: 153, Rank: 9, loss = 0.000553131103515625
c621-152: Epoch: 0, Step: 153, Rank: 41, loss = 3.7670135498046875e-05
c621-101: Epoch: 0, Step: 153, Rank: 30, loss = 0.00738525390625
c622-032: Epoch: 0, Step: 153, Rank: 49, loss = 0.69140625
c621-072: Epoch: 0, Step: 153, Rank: 25, loss = 3.5017728805541992e-06
c621-061: Epoch: 0, Step: 153, Rank: 22, loss = 2.276897430419922e-05
c619-002: Epoch: 0, Step: 153, Rank: 13, loss = 0.04638671875
c621-141: Epoch: 0, Step: 153, Rank: 38, loss = 0.00164794921875
c613-151: Epoch: 0, Step: 153, Rank: 10, loss = 8.754432201385498e-08
c622-091: Epoch: 0, Step: 153, Rank: 60, loss = 7.113753267956038e-23
c622-061: Epoch: 0, Step: 153, Rank: 54, loss = 5.424022674560547e-06
c613-121: Epoch: 0, Step: 153, Rank: 4, loss = 0.00193023681640625
c621-142: Epoch: 0, Step: 153, Rank: 39, loss = 1.6672859221771964e-24
c621-112: Epoch: 0, Step: 153, Rank: 33, loss = 0.00020313262939453125
c621-131: Epoch: 0, Step: 153, Rank: 36, loss = 3.688037395477295e-07
c622-031: Epoch: 0, Step: 153, Rank: 48, loss = 0.0023193359375
c613-102: Epoch: 0, Step: 153, Rank: 1, loss = 3.144186300207963e-17
c622-022: Epoch: 0, Step: 153, Rank: 47, loss = 0.00141143798828125
c622-041: Epoch: 0, Step: 153, Rank: 50, loss = 0.0016021728515625
c622-042: Epoch: 0, Step: 153, Rank: 51, loss = 0.00136566162109375
c621-102: Epoch: 0, Step: 153, Rank: 31, loss = 2.2649765014648438e-06
c613-141: Epoch: 0, Step: 153, Rank: 8, loss = 4.00543212890625e-05
c622-072: Epoch: 0, Step: 153, Rank: 57, loss = 3.975119405215255e-31
c621-092: Epoch: 0, Step: 153, Rank: 29, loss = 0.00020313262939453125
c619-012: Epoch: 0, Step: 153, Rank: 15, loss = 0.000392913818359375
c621-062: Epoch: 0, Step: 153, Rank: 23, loss = 0.00116729736328125
c621-071: Epoch: 0, Step: 153, Rank: 24, loss = 0.0010986328125
c619-031: Epoch: 0, Step: 153, Rank: 18, loss = 6.5267086029052734e-06
c621-122: Epoch: 0, Step: 153, Rank: 35, loss = 3.46451997756958e-07
c619-032: Epoch: 0, Step: 153, Rank: 19, loss = 0.140625
c622-021: Epoch: 0, Step: 153, Rank: 46, loss = 0.00075531005859375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c619-001: Epoch: 0, Step: 154, Rank: 12, loss = 2.1736923372372985e-10
c621-111: Epoch: 0, Step: 154, Rank: 32, loss = 0.0003681182861328125
c622-002: Epoch: 0, Step: 154, Rank: 43, loss = 3.123283386230469e-05
c619-021: Epoch: 0, Step: 154, Rank: 16, loss = 0.000667572021484375
c619-002: Epoch: 0, Step: 154, Rank: 13, loss = 4.5917748078995606e-40
c619-011: Epoch: 0, Step: 154, Rank: 14, loss = 5.0961971282958984e-06
c621-081: Epoch: 0, Step: 154, Rank: 26, loss = 0.00136566162109375
c613-151: Epoch: 0, Step: 154, Rank: 10, loss = 6.198883056640625e-05
c622-012: Epoch: 0, Step: 154, Rank: 45, loss = 1.3828277587890625e-05
c621-072: Epoch: 0, Step: 154, Rank: 25, loss = 0.00070953369140625
c613-132: Epoch: 0, Step: 154, Rank: 7, loss = 0.0
c613-142: Epoch: 0, Step: 154, Rank: 9, loss = 1.2218952178955078e-05
c621-151: Epoch: 0, Step: 154, Rank: 40, loss = 0.8828125
c621-121: Epoch: 0, Step: 154, Rank: 34, loss = 2.234049398383217e-20
c619-022: Epoch: 0, Step: 154, Rank: 17, loss = 3.841705620288849e-09
c613-152: Epoch: 0, Step: 154, Rank: 11, loss = 0.00012302398681640625
c622-052: Epoch: 0, Step: 154, Rank: 53, loss = 0.000911712646484375
c621-091: Epoch: 0, Step: 154, Rank: 28, loss = 2.753734588623047e-05
c621-102: Epoch: 0, Step: 154, Rank: 31, loss = 2.4158453015843406e-12
c613-131: Epoch: 0, Step: 154, Rank: 6, loss = 5.14984130859375e-05
c619-012: Epoch: 0, Step: 154, Rank: 15, loss = 0.0
c613-101: Epoch: 0, Step: 154, Rank: 0, loss = 2.726912498474121e-06
c622-001: Epoch: 0, Step: 154, Rank: 42, loss = 0.126953125
c621-112: Epoch: 0, Step: 154, Rank: 33, loss = 0.69140625
c613-141: Epoch: 0, Step: 154, Rank: 8, loss = 4.172325134277344e-07
c622-011: Epoch: 0, Step: 154, Rank: 44, loss = 3.725290298461914e-06
c622-031: Epoch: 0, Step: 154, Rank: 48, loss = 0.00299072265625
c621-101: Epoch: 0, Step: 154, Rank: 30, loss = 5.4836273193359375e-05
c622-022: Epoch: 0, Step: 154, Rank: 47, loss = 0.0
c622-041: Epoch: 0, Step: 154, Rank: 50, loss = 0.000457763671875
c621-152: Epoch: 0, Step: 154, Rank: 41, loss = 0.00408935546875
c621-142: Epoch: 0, Step: 154, Rank: 39, loss = 0.000278472900390625
c622-032: Epoch: 0, Step: 154, Rank: 49, loss = 0.000457763671875
c613-121: Epoch: 0, Step: 154, Rank: 4, loss = 0.0010986328125
c621-132: Epoch: 0, Step: 154, Rank: 37, loss = 0.00433349609375
c621-082: Epoch: 0, Step: 154, Rank: 27, loss = 0.000553131103515625
c622-051: Epoch: 0, Step: 154, Rank: 52, loss = 0.0038299560546875
c621-092: Epoch: 0, Step: 154, Rank: 29, loss = 0.0002613067626953125
c613-122: Epoch: 0, Step: 154, Rank: 5, loss = 5.424022674560547e-06
c622-021: Epoch: 0, Step: 154, Rank: 46, loss = 5.4836273193359375e-05
c619-041: Epoch: 0, Step: 154, Rank: 20, loss = 0.0001316070556640625
c621-061: Epoch: 0, Step: 154, Rank: 22, loss = 1.895427703857422e-05
c621-131: Epoch: 0, Step: 154, Rank: 36, loss = 0.69140625
c613-112: Epoch: 0, Step: 154, Rank: 3, loss = 0.0038299560546875
c621-141: Epoch: 0, Step: 154, Rank: 38, loss = 0.00012302398681640625
c621-071: Epoch: 0, Step: 154, Rank: 24, loss = 1.955777406692505e-08
c613-102: Epoch: 0, Step: 154, Rank: 1, loss = 0.0002956390380859375
c622-101: Epoch: 0, Step: 154, Rank: 62, loss = 0.00058746337890625
c621-052: Epoch: 0, Step: 154, Rank: 21, loss = 1.895427703857422e-05
c622-102: Epoch: 0, Step: 154, Rank: 63, loss = 0.0002613067626953125
c613-111: Epoch: 0, Step: 154, Rank: 2, loss = 5.4836273193359375e-05
c622-092: Epoch: 0, Step: 154, Rank: 61, loss = 1.601387637598654e-28
c619-032: Epoch: 0, Step: 154, Rank: 19, loss = 4.231929779052734e-06
c622-081: Epoch: 0, Step: 154, Rank: 58, loss = 2.4318695068359375e-05
c622-042: Epoch: 0, Step: 154, Rank: 51, loss = 1.4722347259521484e-05
c619-031: Epoch: 0, Step: 154, Rank: 18, loss = 0.0003566741943359375
c621-062: Epoch: 0, Step: 154, Rank: 23, loss = 6.891787052154541e-07
c622-061: Epoch: 0, Step: 154, Rank: 54, loss = 0.000335693359375
c621-122: Epoch: 0, Step: 154, Rank: 35, loss = 0.000148773193359375
c622-082: Epoch: 0, Step: 154, Rank: 59, loss = 0.000457763671875
c622-072: Epoch: 0, Step: 154, Rank: 57, loss = 0.00040435791015625
c622-091: Epoch: 0, Step: 154, Rank: 60, loss = 1.996755599975586e-06
c622-062: Epoch: 0, Step: 154, Rank: 55, loss = 0.00124359130859375
c622-071: Epoch: 0, Step: 154, Rank: 56, loss = 0.00070953369140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 155, Rank: 58, loss = 0.000911712646484375
c613-101: Epoch: 0, Step: 155, Rank: 0, loss = 1.4722347259521484e-05
c622-002: Epoch: 0, Step: 155, Rank: 43, loss = 1.895427703857422e-05
c622-082: Epoch: 0, Step: 155, Rank: 59, loss = 0.0021820068359375
c621-072: Epoch: 0, Step: 155, Rank: 25, loss = 3.293156623840332e-06
c621-091: Epoch: 0, Step: 155, Rank: 28, loss = 0.000179290771484375
c613-121: Epoch: 0, Step: 155, Rank: 4, loss = 7.867813110351562e-06
c619-031: Epoch: 0, Step: 155, Rank: 18, loss = 1.4637180356658064e-12
c619-002: Epoch: 0, Step: 155, Rank: 13, loss = 0.00299072265625
c622-092: Epoch: 0, Step: 155, Rank: 61, loss = 0.00124359130859375
c619-021: Epoch: 0, Step: 155, Rank: 16, loss = 0.000431060791015625
c619-001: Epoch: 0, Step: 155, Rank: 12, loss = 0.0020599365234375
c619-032: Epoch: 0, Step: 155, Rank: 19, loss = 0.000392913818359375
c622-052: Epoch: 0, Step: 155, Rank: 53, loss = 0.00136566162109375
c613-111: Epoch: 0, Step: 155, Rank: 2, loss = 0.000606536865234375
c621-081: Epoch: 0, Step: 155, Rank: 26, loss = 0.0037078857421875
c613-132: Epoch: 0, Step: 155, Rank: 7, loss = 0.00096893310546875
c621-132: Epoch: 0, Step: 155, Rank: 37, loss = 1.0788440704345703e-05
c621-082: Epoch: 0, Step: 155, Rank: 27, loss = 0.00021648406982421875
c613-102: Epoch: 0, Step: 155, Rank: 1, loss = 3.528594970703125e-05
c613-141: Epoch: 0, Step: 155, Rank: 8, loss = 0.0
c622-072: Epoch: 0, Step: 155, Rank: 57, loss = 0.00014019012451171875
c613-142: Epoch: 0, Step: 155, Rank: 9, loss = 0.005584716796875
c619-011: Epoch: 0, Step: 155, Rank: 14, loss = 3.528594970703125e-05
c613-112: Epoch: 0, Step: 155, Rank: 3, loss = 1.4915713109076023e-10
c622-091: Epoch: 0, Step: 155, Rank: 60, loss = 8.083811398051921e-16
c622-071: Epoch: 0, Step: 155, Rank: 56, loss = 0.005584716796875
c621-092: Epoch: 0, Step: 155, Rank: 29, loss = 1.3828277587890625e-05
c619-041: Epoch: 0, Step: 155, Rank: 20, loss = 0.000335693359375
c622-102: Epoch: 0, Step: 155, Rank: 63, loss = 0.07080078125
c621-061: Epoch: 0, Step: 155, Rank: 22, loss = 1.6689300537109375e-05
c622-012: Epoch: 0, Step: 155, Rank: 45, loss = 0.00193023681640625
c622-062: Epoch: 0, Step: 155, Rank: 55, loss = 0.10009765625
c621-102: Epoch: 0, Step: 155, Rank: 31, loss = 2.2065682614424986e-15
c621-152: Epoch: 0, Step: 155, Rank: 41, loss = 1.150369644165039e-05
c613-131: Epoch: 0, Step: 155, Rank: 6, loss = 8.307397365570068e-07
c621-151: Epoch: 0, Step: 155, Rank: 40, loss = 5.4836273193359375e-05
c622-011: Epoch: 0, Step: 155, Rank: 44, loss = 0.000911712646484375
c613-151: Epoch: 0, Step: 155, Rank: 10, loss = 4.00543212890625e-05
c621-052: Epoch: 0, Step: 155, Rank: 21, loss = 1.3732490638087374e-25
c619-022: Epoch: 0, Step: 155, Rank: 17, loss = 0.69140625
c622-101: Epoch: 0, Step: 155, Rank: 62, loss = 0.00299072265625
c622-001: Epoch: 0, Step: 155, Rank: 42, loss = 1.6689300537109375e-05
c621-122: Epoch: 0, Step: 155, Rank: 35, loss = 0.004486083984375
c622-061: Epoch: 0, Step: 155, Rank: 54, loss = 0.000148773193359375
c621-071: Epoch: 0, Step: 155, Rank: 24, loss = 0.000606536865234375
c621-101: Epoch: 0, Step: 155, Rank: 30, loss = 0.002716064453125
c613-122: Epoch: 0, Step: 155, Rank: 5, loss = 0.000518798828125
c613-152: Epoch: 0, Step: 155, Rank: 11, loss = 6.628036499023438e-05
c619-012: Epoch: 0, Step: 155, Rank: 15, loss = 5.424022674560547e-06
c621-121: Epoch: 0, Step: 155, Rank: 34, loss = 0.00014019012451171875
c621-131: Epoch: 0, Step: 155, Rank: 36, loss = 8.404254913330078e-06
c622-051: Epoch: 0, Step: 155, Rank: 52, loss = 0.0028076171875
c621-062: Epoch: 0, Step: 155, Rank: 23, loss = 0.000606536865234375
c622-042: Epoch: 0, Step: 155, Rank: 51, loss = 4.4517219066619873e-07
c622-041: Epoch: 0, Step: 155, Rank: 50, loss = 7.44648787076585e-12
c621-142: Epoch: 0, Step: 155, Rank: 39, loss = 7.963180541992188e-05
c622-032: Epoch: 0, Step: 155, Rank: 49, loss = 0.00058746337890625
c622-022: Epoch: 0, Step: 155, Rank: 47, loss = 0.001068115234375
c621-111: Epoch: 0, Step: 155, Rank: 32, loss = 6.973743438720703e-06
c621-141: Epoch: 0, Step: 155, Rank: 38, loss = 1.318767317570746e-10
c622-031: Epoch: 0, Step: 155, Rank: 48, loss = 4.839897155761719e-05
c621-112: Epoch: 0, Step: 155, Rank: 33, loss = 0.0001583099365234375
c622-021: Epoch: 0, Step: 155, Rank: 46, loss = 1.5735626220703125e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 156, Rank: 16, loss = 3.7670135498046875e-05
c619-022: Epoch: 0, Step: 156, Rank: 17, loss = 5.0961971282958984e-06
c619-031: Epoch: 0, Step: 156, Rank: 18, loss = 0.00714111328125
c619-012: Epoch: 0, Step: 156, Rank: 15, loss = 0.0032806396484375
c619-041: Epoch: 0, Step: 156, Rank: 20, loss = 5.182486384480711e-17
c619-002: Epoch: 0, Step: 156, Rank: 13, loss = 2.5117331222237016e-27
c619-011: Epoch: 0, Step: 156, Rank: 14, loss = 0.000667572021484375
c619-001: Epoch: 0, Step: 156, Rank: 12, loss = 0.00099945068359375
c621-052: Epoch: 0, Step: 156, Rank: 21, loss = 0.004913330078125
c619-032: Epoch: 0, Step: 156, Rank: 19, loss = 3.4051481634378433e-09
c621-072: Epoch: 0, Step: 156, Rank: 25, loss = 1.0788440704345703e-05
c621-081: Epoch: 0, Step: 156, Rank: 26, loss = 0.0020599365234375
c613-151: Epoch: 0, Step: 156, Rank: 10, loss = 6.198883056640625e-05
c621-091: Epoch: 0, Step: 156, Rank: 28, loss = 2.71833068627654e-38
c613-152: Epoch: 0, Step: 156, Rank: 11, loss = 0.00040435791015625
c621-061: Epoch: 0, Step: 156, Rank: 22, loss = 0.003173828125
c621-082: Epoch: 0, Step: 156, Rank: 27, loss = 0.000667572021484375
c621-111: Epoch: 0, Step: 156, Rank: 32, loss = 0.69140625
c621-102: Epoch: 0, Step: 156, Rank: 31, loss = 0.0
c621-101: Epoch: 0, Step: 156, Rank: 30, loss = 0.003387451171875
c621-071: Epoch: 0, Step: 156, Rank: 24, loss = 1.6540288925170898e-06
c621-121: Epoch: 0, Step: 156, Rank: 34, loss = 5.0961971282958984e-06
c621-062: Epoch: 0, Step: 156, Rank: 23, loss = 0.0008544921875
c621-092: Epoch: 0, Step: 156, Rank: 29, loss = 7.867813110351562e-06
c621-112: Epoch: 0, Step: 156, Rank: 33, loss = 0.000667572021484375
c613-142: Epoch: 0, Step: 156, Rank: 9, loss = 2.562999725341797e-06
c621-122: Epoch: 0, Step: 156, Rank: 35, loss = 3.7670135498046875e-05
c613-131: Epoch: 0, Step: 156, Rank: 6, loss = 0.0002307891845703125
c613-132: Epoch: 0, Step: 156, Rank: 7, loss = 0.002716064453125
c613-141: Epoch: 0, Step: 156, Rank: 8, loss = 9.74978320300579e-10
c613-121: Epoch: 0, Step: 156, Rank: 4, loss = 2.1693674893577825e-29
c613-122: Epoch: 0, Step: 156, Rank: 5, loss = 4.5299530029296875e-05
c621-131: Epoch: 0, Step: 156, Rank: 36, loss = 3.7670135498046875e-05
c613-112: Epoch: 0, Step: 156, Rank: 3, loss = 8.754432201385498e-08
c613-101: Epoch: 0, Step: 156, Rank: 0, loss = 2.5331974029541016e-07
c621-132: Epoch: 0, Step: 156, Rank: 37, loss = 0.0002307891845703125
c613-111: Epoch: 0, Step: 156, Rank: 2, loss = 0.000667572021484375
c622-002: Epoch: 0, Step: 156, Rank: 43, loss = 0.69140625
c622-092: Epoch: 0, Step: 156, Rank: 61, loss = 0.00225830078125
c613-102: Epoch: 0, Step: 156, Rank: 1, loss = 0.0010986328125
c622-081: Epoch: 0, Step: 156, Rank: 58, loss = 0.004913330078125
c622-001: Epoch: 0, Step: 156, Rank: 42, loss = 0.00860595703125
c622-052: Epoch: 0, Step: 156, Rank: 53, loss = 6.845220923423767e-08
c622-051: Epoch: 0, Step: 156, Rank: 52, loss = 0.03466796875
c622-072: Epoch: 0, Step: 156, Rank: 57, loss = 0.00180816650390625
c622-082: Epoch: 0, Step: 156, Rank: 59, loss = 0.0020599365234375
c622-061: Epoch: 0, Step: 156, Rank: 54, loss = 0.0001316070556640625
c622-101: Epoch: 0, Step: 156, Rank: 62, loss = 0.007598876953125
c622-102: Epoch: 0, Step: 156, Rank: 63, loss = 7.729977369308472e-08
c621-151: Epoch: 0, Step: 156, Rank: 40, loss = 5.14984130859375e-05
c622-012: Epoch: 0, Step: 156, Rank: 45, loss = 4.843059286940843e-11
c622-091: Epoch: 0, Step: 156, Rank: 60, loss = 0.004913330078125
c622-071: Epoch: 0, Step: 156, Rank: 56, loss = 3.655441105365753e-08
c622-062: Epoch: 0, Step: 156, Rank: 55, loss = 0.000148773193359375
c622-031: Epoch: 0, Step: 156, Rank: 48, loss = 0.0091552734375
c622-011: Epoch: 0, Step: 156, Rank: 44, loss = 5.4836273193359375e-05
c622-042: Epoch: 0, Step: 156, Rank: 51, loss = 0.00433349609375
c621-152: Epoch: 0, Step: 156, Rank: 41, loss = 1.3445969671010971e-08
c621-142: Epoch: 0, Step: 156, Rank: 39, loss = 6.462348535570529e-26
c622-022: Epoch: 0, Step: 156, Rank: 47, loss = 2.9335764912906377e-30
c621-141: Epoch: 0, Step: 156, Rank: 38, loss = 2.276897430419922e-05
c622-032: Epoch: 0, Step: 156, Rank: 49, loss = 0.01708984375
c622-041: Epoch: 0, Step: 156, Rank: 50, loss = 0.0
c622-021: Epoch: 0, Step: 156, Rank: 46, loss = 0.0021209716796875
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75439453125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.13s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.13s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 157, Rank: 16, loss = 5.14984130859375e-05
c622-052: Epoch: 0, Step: 157, Rank: 53, loss = 9.012222290039062e-05
c613-101: Epoch: 0, Step: 157, Rank: 0, loss = 0.0010986328125
c622-002: Epoch: 0, Step: 157, Rank: 43, loss = 0.003387451171875
c621-132: Epoch: 0, Step: 157, Rank: 37, loss = 0.000457763671875
c621-111: Epoch: 0, Step: 157, Rank: 32, loss = 3.528594970703125e-05
c619-012: Epoch: 0, Step: 157, Rank: 15, loss = 0.00099945068359375
c622-012: Epoch: 0, Step: 157, Rank: 45, loss = 5.585135208964764e-28
c619-011: Epoch: 0, Step: 157, Rank: 14, loss = 0.69140625
c621-081: Epoch: 0, Step: 157, Rank: 26, loss = 0.0003147125244140625
c619-022: Epoch: 0, Step: 157, Rank: 17, loss = 1.0058283805847168e-06
c621-072: Epoch: 0, Step: 157, Rank: 25, loss = 5.4836273193359375e-05
c621-082: Epoch: 0, Step: 157, Rank: 27, loss = 1.8533319234848022e-07
c621-131: Epoch: 0, Step: 157, Rank: 36, loss = 0.00014019012451171875
c622-061: Epoch: 0, Step: 157, Rank: 54, loss = 0.000179290771484375
c619-001: Epoch: 0, Step: 157, Rank: 12, loss = 0.00193023681640625
c613-132: Epoch: 0, Step: 157, Rank: 7, loss = 0.0002956390380859375
c619-002: Epoch: 0, Step: 157, Rank: 13, loss = 0.0021209716796875
c622-051: Epoch: 0, Step: 157, Rank: 52, loss = 3.213062882423401e-08
c613-151: Epoch: 0, Step: 157, Rank: 10, loss = 0.000518798828125
c621-091: Epoch: 0, Step: 157, Rank: 28, loss = 0.0
c621-152: Epoch: 0, Step: 157, Rank: 41, loss = 0.000553131103515625
c621-151: Epoch: 0, Step: 157, Rank: 40, loss = 2.586841583251953e-05
c622-001: Epoch: 0, Step: 157, Rank: 42, loss = 1.895427703857422e-05
c622-022: Epoch: 0, Step: 157, Rank: 47, loss = 0.00058746337890625
c621-122: Epoch: 0, Step: 157, Rank: 35, loss = 1.0788440704345703e-05
c613-152: Epoch: 0, Step: 157, Rank: 11, loss = 2.9325485229492188e-05
c613-111: Epoch: 0, Step: 157, Rank: 2, loss = 0.0001087188720703125
c613-141: Epoch: 0, Step: 157, Rank: 8, loss = 1.0058283805847168e-06
c622-081: Epoch: 0, Step: 157, Rank: 58, loss = 0.059326171875
c622-011: Epoch: 0, Step: 157, Rank: 44, loss = 2.276897430419922e-05
c622-032: Epoch: 0, Step: 157, Rank: 49, loss = 0.0021820068359375
c613-121: Epoch: 0, Step: 157, Rank: 4, loss = 0.0002613067626953125
c613-142: Epoch: 0, Step: 157, Rank: 9, loss = 2.4318695068359375e-05
c621-141: Epoch: 0, Step: 157, Rank: 38, loss = 2.4318695068359375e-05
c622-082: Epoch: 0, Step: 157, Rank: 59, loss = 0.0002613067626953125
c613-102: Epoch: 0, Step: 157, Rank: 1, loss = 0.69140625
c621-101: Epoch: 0, Step: 157, Rank: 30, loss = 4.926614671774132e-16
c621-061: Epoch: 0, Step: 157, Rank: 22, loss = 6.891787052154541e-07
c621-142: Epoch: 0, Step: 157, Rank: 39, loss = 0.002471923828125
c619-041: Epoch: 0, Step: 157, Rank: 20, loss = 4.5299530029296875e-05
c621-052: Epoch: 0, Step: 157, Rank: 21, loss = 0.000606536865234375
c622-092: Epoch: 0, Step: 157, Rank: 61, loss = 2.753734588623047e-05
c621-121: Epoch: 0, Step: 157, Rank: 34, loss = 1.5735626220703125e-05
c613-131: Epoch: 0, Step: 157, Rank: 6, loss = 4.231929779052734e-06
c622-031: Epoch: 0, Step: 157, Rank: 48, loss = 5.4836273193359375e-05
c619-032: Epoch: 0, Step: 157, Rank: 19, loss = 1.4722347259521484e-05
c622-041: Epoch: 0, Step: 157, Rank: 50, loss = 4.5299530029296875e-05
c622-101: Epoch: 0, Step: 157, Rank: 62, loss = 5.4836273193359375e-05
c621-112: Epoch: 0, Step: 157, Rank: 33, loss = 8.487701416015625e-05
c622-102: Epoch: 0, Step: 157, Rank: 63, loss = 0.000179290771484375
c621-092: Epoch: 0, Step: 157, Rank: 29, loss = 1.2218952178955078e-05
c613-122: Epoch: 0, Step: 157, Rank: 5, loss = 2.4318695068359375e-05
c621-062: Epoch: 0, Step: 157, Rank: 23, loss = 3.6734198463196485e-39
c622-071: Epoch: 0, Step: 157, Rank: 56, loss = 1.8533319234848022e-07
c622-091: Epoch: 0, Step: 157, Rank: 60, loss = 0.00014019012451171875
c621-102: Epoch: 0, Step: 157, Rank: 31, loss = 0.0010986328125
c621-071: Epoch: 0, Step: 157, Rank: 24, loss = 9.424984455108643e-07
c622-021: Epoch: 0, Step: 157, Rank: 46, loss = 4.5299530029296875e-05
c613-112: Epoch: 0, Step: 157, Rank: 3, loss = 0.0002956390380859375
c619-031: Epoch: 0, Step: 157, Rank: 18, loss = 0.00150299072265625
c622-042: Epoch: 0, Step: 157, Rank: 51, loss = 0.00170135498046875
c622-062: Epoch: 0, Step: 157, Rank: 55, loss = 3.123283386230469e-05
c622-072: Epoch: 0, Step: 157, Rank: 57, loss = 0.00141143798828125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 158, Rank: 28, loss = 0.00141143798828125
c621-072: Epoch: 0, Step: 158, Rank: 25, loss = 0.69140625
c621-081: Epoch: 0, Step: 158, Rank: 26, loss = 8.307397365570068e-07
c621-082: Epoch: 0, Step: 158, Rank: 27, loss = 1.8775463104248047e-06
c621-111: Epoch: 0, Step: 158, Rank: 32, loss = 7.338821887969971e-07
c619-021: Epoch: 0, Step: 158, Rank: 16, loss = 2.2649765014648438e-06
c621-071: Epoch: 0, Step: 158, Rank: 24, loss = 4.5299530029296875e-05
c613-101: Epoch: 0, Step: 158, Rank: 0, loss = 7.963180541992188e-05
c622-002: Epoch: 0, Step: 158, Rank: 43, loss = 7.792703114739563e-20
c621-092: Epoch: 0, Step: 158, Rank: 29, loss = 1.2218952178955078e-05
c621-061: Epoch: 0, Step: 158, Rank: 22, loss = 0.000148773193359375
c621-052: Epoch: 0, Step: 158, Rank: 21, loss = 0.00811767578125
c619-002: Epoch: 0, Step: 158, Rank: 13, loss = 7.188646122813225e-09
c621-101: Epoch: 0, Step: 158, Rank: 30, loss = 1.6689300537109375e-05
c619-031: Epoch: 0, Step: 158, Rank: 18, loss = 4.94765117764473e-09
c622-081: Epoch: 0, Step: 158, Rank: 58, loss = 3.688037395477295e-07
c619-022: Epoch: 0, Step: 158, Rank: 17, loss = 1.6171875
c619-041: Epoch: 0, Step: 158, Rank: 20, loss = 4.798173904418945e-06
c621-132: Epoch: 0, Step: 158, Rank: 37, loss = 0.00040435791015625
c621-121: Epoch: 0, Step: 158, Rank: 34, loss = 0.004486083984375
c621-062: Epoch: 0, Step: 158, Rank: 23, loss = 0.00019073486328125
c621-131: Epoch: 0, Step: 158, Rank: 36, loss = 8.754432201385498e-08
c621-112: Epoch: 0, Step: 158, Rank: 33, loss = 0.00019073486328125
c621-102: Epoch: 0, Step: 158, Rank: 31, loss = 5.817413330078125e-05
c613-121: Epoch: 0, Step: 158, Rank: 4, loss = 0.00099945068359375
c622-001: Epoch: 0, Step: 158, Rank: 42, loss = 7.338821887969971e-07
c619-032: Epoch: 0, Step: 158, Rank: 19, loss = 0.00116729736328125
c619-011: Epoch: 0, Step: 158, Rank: 14, loss = 0.00083160400390625
c619-001: Epoch: 0, Step: 158, Rank: 12, loss = 4.267692565917969e-05
c622-101: Epoch: 0, Step: 158, Rank: 62, loss = 0.000553131103515625
c622-061: Epoch: 0, Step: 158, Rank: 54, loss = 0.0021820068359375
c621-151: Epoch: 0, Step: 158, Rank: 40, loss = 1.955777406692505e-08
c613-122: Epoch: 0, Step: 158, Rank: 5, loss = 0.69140625
c619-012: Epoch: 0, Step: 158, Rank: 15, loss = 0.0026397705078125
c613-112: Epoch: 0, Step: 158, Rank: 3, loss = 9.012222290039062e-05
c613-111: Epoch: 0, Step: 158, Rank: 2, loss = 0.00070953369140625
c622-071: Epoch: 0, Step: 158, Rank: 56, loss = 4.500150680541992e-06
c622-052: Epoch: 0, Step: 158, Rank: 53, loss = 1.7848833522293717e-11
c622-012: Epoch: 0, Step: 158, Rank: 45, loss = 0.0003566741943359375
c622-082: Epoch: 0, Step: 158, Rank: 59, loss = 0.00099945068359375
c622-072: Epoch: 0, Step: 158, Rank: 57, loss = 6.198883056640625e-05
c621-152: Epoch: 0, Step: 158, Rank: 41, loss = 4.231929779052734e-06
c622-062: Epoch: 0, Step: 158, Rank: 55, loss = 2.1457672119140625e-05
c613-102: Epoch: 0, Step: 158, Rank: 1, loss = 0.0005035400390625
c622-102: Epoch: 0, Step: 158, Rank: 63, loss = 0.00136566162109375
c622-092: Epoch: 0, Step: 158, Rank: 61, loss = 8.487701416015625e-05
c613-151: Epoch: 0, Step: 158, Rank: 10, loss = 3.314018249511719e-05
c622-051: Epoch: 0, Step: 158, Rank: 52, loss = 2.726912498474121e-06
c622-042: Epoch: 0, Step: 158, Rank: 51, loss = 0.004913330078125
c613-152: Epoch: 0, Step: 158, Rank: 11, loss = 5.14984130859375e-05
c621-141: Epoch: 0, Step: 158, Rank: 38, loss = 0.000179290771484375
c622-041: Epoch: 0, Step: 158, Rank: 50, loss = 0.00058746337890625
c613-131: Epoch: 0, Step: 158, Rank: 6, loss = 9.012222290039062e-05
c622-091: Epoch: 0, Step: 158, Rank: 60, loss = 0.00099945068359375
c622-032: Epoch: 0, Step: 158, Rank: 49, loss = 5.14984130859375e-05
c621-142: Epoch: 0, Step: 158, Rank: 39, loss = 2.7284841053187847e-12
c622-031: Epoch: 0, Step: 158, Rank: 48, loss = 3.841705620288849e-09
c622-022: Epoch: 0, Step: 158, Rank: 47, loss = 0.000278472900390625
c622-011: Epoch: 0, Step: 158, Rank: 44, loss = 2.831068712794149e-15
c613-141: Epoch: 0, Step: 158, Rank: 8, loss = 3.725290298461914e-06
c613-132: Epoch: 0, Step: 158, Rank: 7, loss = 0.01708984375
c621-122: Epoch: 0, Step: 158, Rank: 35, loss = 0.000431060791015625
c613-142: Epoch: 0, Step: 158, Rank: 9, loss = 0.00099945068359375
c622-021: Epoch: 0, Step: 158, Rank: 46, loss = 0.00016880035400390625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 159, Rank: 16, loss = 4.267692565917969e-05
c622-081: Epoch: 0, Step: 159, Rank: 58, loss = 0.000553131103515625
c619-031: Epoch: 0, Step: 159, Rank: 18, loss = 0.0026397705078125
c613-101: Epoch: 0, Step: 159, Rank: 0, loss = 5.14984130859375e-05
c619-002: Epoch: 0, Step: 159, Rank: 13, loss = 7.009506225585938e-05
c619-001: Epoch: 0, Step: 159, Rank: 12, loss = 0.00096893310546875
c613-152: Epoch: 0, Step: 159, Rank: 11, loss = 1.895427703857422e-05
c619-011: Epoch: 0, Step: 159, Rank: 14, loss = 1.7848833522293717e-11
c622-101: Epoch: 0, Step: 159, Rank: 62, loss = 0.00150299072265625
c622-062: Epoch: 0, Step: 159, Rank: 55, loss = 3.314018249511719e-05
c613-151: Epoch: 0, Step: 159, Rank: 10, loss = 7.963180541992188e-05
c621-132: Epoch: 0, Step: 159, Rank: 37, loss = 0.0001583099365234375
c621-072: Epoch: 0, Step: 159, Rank: 25, loss = 0.004913330078125
c622-052: Epoch: 0, Step: 159, Rank: 53, loss = 2.5331974029541016e-07
c622-092: Epoch: 0, Step: 159, Rank: 61, loss = 7.383573891102493e-38
c622-001: Epoch: 0, Step: 159, Rank: 42, loss = 1.6689300537109375e-05
c622-002: Epoch: 0, Step: 159, Rank: 43, loss = 0.027587890625
c621-122: Epoch: 0, Step: 159, Rank: 35, loss = 0.01104736328125
c621-151: Epoch: 0, Step: 159, Rank: 40, loss = 1.2993812561035156e-05
c619-022: Epoch: 0, Step: 159, Rank: 17, loss = 0.00170135498046875
c622-071: Epoch: 0, Step: 159, Rank: 56, loss = 0.000179290771484375
c621-142: Epoch: 0, Step: 159, Rank: 39, loss = 0.0002956390380859375
c613-141: Epoch: 0, Step: 159, Rank: 8, loss = 7.987216665362745e-30
c621-091: Epoch: 0, Step: 159, Rank: 28, loss = 2.276897430419922e-05
c621-081: Epoch: 0, Step: 159, Rank: 26, loss = 0.00433349609375
c622-082: Epoch: 0, Step: 159, Rank: 59, loss = 0.0005035400390625
c619-012: Epoch: 0, Step: 159, Rank: 15, loss = 0.00016880035400390625
c613-142: Epoch: 0, Step: 159, Rank: 9, loss = 0.69140625
c621-131: Epoch: 0, Step: 159, Rank: 36, loss = 3.123283386230469e-05
c621-052: Epoch: 0, Step: 159, Rank: 21, loss = 0.002716064453125
c622-012: Epoch: 0, Step: 159, Rank: 45, loss = 0.000278472900390625
c621-121: Epoch: 0, Step: 159, Rank: 34, loss = 4.00543212890625e-05
c613-132: Epoch: 0, Step: 159, Rank: 7, loss = 5.029141902923584e-07
c613-131: Epoch: 0, Step: 159, Rank: 6, loss = 4.4517219066619873e-07
c621-071: Epoch: 0, Step: 159, Rank: 24, loss = 0.000606536865234375
c622-032: Epoch: 0, Step: 159, Rank: 49, loss = 8.940696716308594e-06
c621-141: Epoch: 0, Step: 159, Rank: 38, loss = 0.000278472900390625
c613-122: Epoch: 0, Step: 159, Rank: 5, loss = 1.150369644165039e-05
c621-152: Epoch: 0, Step: 159, Rank: 41, loss = 4.00543212890625e-05
c622-022: Epoch: 0, Step: 159, Rank: 47, loss = 0.0002956390380859375
c622-072: Epoch: 0, Step: 159, Rank: 57, loss = 0.0016021728515625
c622-102: Epoch: 0, Step: 159, Rank: 63, loss = 0.0007781982421875
c619-041: Epoch: 0, Step: 159, Rank: 20, loss = 7.009506225585938e-05
c621-102: Epoch: 0, Step: 159, Rank: 31, loss = 0.00014019012451171875
c613-112: Epoch: 0, Step: 159, Rank: 3, loss = 0.000179290771484375
c613-102: Epoch: 0, Step: 159, Rank: 1, loss = 9.012222290039062e-05
c613-111: Epoch: 0, Step: 159, Rank: 2, loss = 0.0001087188720703125
c622-051: Epoch: 0, Step: 159, Rank: 52, loss = 0.000667572021484375
c621-061: Epoch: 0, Step: 159, Rank: 22, loss = 4.1443854570388794e-08
c621-112: Epoch: 0, Step: 159, Rank: 33, loss = 0.000278472900390625
c613-121: Epoch: 0, Step: 159, Rank: 4, loss = 0.001068115234375
c622-061: Epoch: 0, Step: 159, Rank: 54, loss = 0.00099945068359375
c621-101: Epoch: 0, Step: 159, Rank: 30, loss = 0.00010251998901367188
c622-011: Epoch: 0, Step: 159, Rank: 44, loss = 0.0002956390380859375
c622-041: Epoch: 0, Step: 159, Rank: 50, loss = 5.424022674560547e-06
c621-062: Epoch: 0, Step: 159, Rank: 23, loss = 0.00058746337890625
c622-031: Epoch: 0, Step: 159, Rank: 48, loss = 0.69140625
c622-091: Epoch: 0, Step: 159, Rank: 60, loss = 0.0002956390380859375
c622-042: Epoch: 0, Step: 159, Rank: 51, loss = 6.973743438720703e-06
c621-092: Epoch: 0, Step: 159, Rank: 29, loss = 0.17578125
c621-082: Epoch: 0, Step: 159, Rank: 27, loss = 6.628036499023438e-05
c619-032: Epoch: 0, Step: 159, Rank: 19, loss = 8.307397365570068e-07
c622-021: Epoch: 0, Step: 159, Rank: 46, loss = 1.418811734765768e-09
c621-111: Epoch: 0, Step: 159, Rank: 32, loss = 6.927791673660977e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: [2025-03-01 17:57:56,407] [INFO] [logging.py:128:log_dist] [Rank 0] step=10, skipped=0, lr=[9.65e-06, 9.65e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
c613-101: [2025-03-01 17:57:56,407] [INFO] [timer.py:264:stop] epoch=0/micro_step=160/global_step=10, RunningAvgSamplesPerSec=30.985315882063738, CurrSamplesPerSec=30.99543838916153, MemAllocated=3.69GB, MaxMemAllocated=11.68GB
c613-101: Model Parameters: 7.505 B, Latency: 2.16s, TFLOPs: 0.87, Samples/sec: 0.46, Time/seq 2.16s, Batch Size: 1, Sequence Length: 2048
c621-142: Epoch: 0, Step: 160, Rank: 39, loss = 0.69140625
c621-151: Epoch: 0, Step: 160, Rank: 40, loss = 7.867813110351562e-06
c621-131: Epoch: 0, Step: 160, Rank: 36, loss = 1.525040715932846e-08
c621-132: Epoch: 0, Step: 160, Rank: 37, loss = 0.69140625
c621-111: Epoch: 0, Step: 160, Rank: 32, loss = 1.126900315284729e-07
c621-122: Epoch: 0, Step: 160, Rank: 35, loss = 5.4836273193359375e-05
c621-121: Epoch: 0, Step: 160, Rank: 34, loss = 3.314018249511719e-05
c621-141: Epoch: 0, Step: 160, Rank: 38, loss = 1.996755599975586e-06
c621-112: Epoch: 0, Step: 160, Rank: 33, loss = 3.655441105365753e-08
c621-081: Epoch: 0, Step: 160, Rank: 26, loss = 2.014636993408203e-05
c621-072: Epoch: 0, Step: 160, Rank: 25, loss = 2.562999725341797e-06
c621-091: Epoch: 0, Step: 160, Rank: 28, loss = 4.6798959374427795e-08
c621-102: Epoch: 0, Step: 160, Rank: 31, loss = 0.0002956390380859375
c621-082: Epoch: 0, Step: 160, Rank: 27, loss = 4.839897155761719e-05
c621-101: Epoch: 0, Step: 160, Rank: 30, loss = 3.144186300207963e-17
c622-001: Epoch: 0, Step: 160, Rank: 42, loss = 1.5735626220703125e-05
c619-021: Epoch: 0, Step: 160, Rank: 16, loss = 1.9081958235744878e-17
c621-152: Epoch: 0, Step: 160, Rank: 41, loss = 0.0001087188720703125
c619-022: Epoch: 0, Step: 160, Rank: 17, loss = 3.0547380447387695e-07
c621-092: Epoch: 0, Step: 160, Rank: 29, loss = 2.648448571562767e-09
c613-101: Epoch: 0, Step: 160, Rank: 0, loss = 0.000606536865234375
c619-031: Epoch: 0, Step: 160, Rank: 18, loss = 3.293156623840332e-06
c622-002: Epoch: 0, Step: 160, Rank: 43, loss = 0.000148773193359375
c619-032: Epoch: 0, Step: 160, Rank: 19, loss = 4.4517219066619873e-07
c621-071: Epoch: 0, Step: 160, Rank: 24, loss = 2.586841583251953e-05
c619-011: Epoch: 0, Step: 160, Rank: 14, loss = 0.0
c621-052: Epoch: 0, Step: 160, Rank: 21, loss = 5.029141902923584e-07
c619-002: Epoch: 0, Step: 160, Rank: 13, loss = 1.0132789611816406e-05
c619-001: Epoch: 0, Step: 160, Rank: 12, loss = 2.726912498474121e-06
c619-041: Epoch: 0, Step: 160, Rank: 20, loss = 0.00019073486328125
c619-012: Epoch: 0, Step: 160, Rank: 15, loss = 3.46451997756958e-07
c621-061: Epoch: 0, Step: 160, Rank: 22, loss = 1.8775463104248047e-06
c613-152: Epoch: 0, Step: 160, Rank: 11, loss = 0.000606536865234375
c621-062: Epoch: 0, Step: 160, Rank: 23, loss = 1.3709068298339844e-06
c613-151: Epoch: 0, Step: 160, Rank: 10, loss = 5.3085386753082275e-08
c613-132: Epoch: 0, Step: 160, Rank: 7, loss = 4.3655745685100555e-09
c613-122: Epoch: 0, Step: 160, Rank: 5, loss = 0.00014019012451171875
c613-141: Epoch: 0, Step: 160, Rank: 8, loss = 4.00543212890625e-05
c613-142: Epoch: 0, Step: 160, Rank: 9, loss = 0.007598876953125
c613-112: Epoch: 0, Step: 160, Rank: 3, loss = 0.000179290771484375
c613-131: Epoch: 0, Step: 160, Rank: 6, loss = 2.8405338525772095e-08
c622-011: Epoch: 0, Step: 160, Rank: 44, loss = 7.009506225585938e-05
c622-071: Epoch: 0, Step: 160, Rank: 56, loss = 0.0
c613-111: Epoch: 0, Step: 160, Rank: 2, loss = 2.5920599000528455e-11
c613-121: Epoch: 0, Step: 160, Rank: 4, loss = 1.6079866327345371e-09
c622-101: Epoch: 0, Step: 160, Rank: 62, loss = 0.0005035400390625
c622-062: Epoch: 0, Step: 160, Rank: 55, loss = 2.2851054382044822e-11
c622-052: Epoch: 0, Step: 160, Rank: 53, loss = 3.528594970703125e-05
c613-102: Epoch: 0, Step: 160, Rank: 1, loss = 1.0058283805847168e-06
c622-102: Epoch: 0, Step: 160, Rank: 63, loss = 0.0
c622-051: Epoch: 0, Step: 160, Rank: 52, loss = 0.00136566162109375
c622-012: Epoch: 0, Step: 160, Rank: 45, loss = 0.00180816650390625
c622-081: Epoch: 0, Step: 160, Rank: 58, loss = 1.2218952178955078e-05
c622-072: Epoch: 0, Step: 160, Rank: 57, loss = 4.418687638008123e-14
c622-061: Epoch: 0, Step: 160, Rank: 54, loss = 0.0020599365234375
c622-032: Epoch: 0, Step: 160, Rank: 49, loss = 0.0001087188720703125
c622-041: Epoch: 0, Step: 160, Rank: 50, loss = 0.01507568359375
c622-092: Epoch: 0, Step: 160, Rank: 61, loss = 0.00014019012451171875
c622-042: Epoch: 0, Step: 160, Rank: 51, loss = 8.404254913330078e-06
c622-082: Epoch: 0, Step: 160, Rank: 59, loss = 5.424022674560547e-06
c622-091: Epoch: 0, Step: 160, Rank: 60, loss = 6.5267086029052734e-06
c622-021: Epoch: 0, Step: 160, Rank: 46, loss = 3.293156623840332e-06
c622-031: Epoch: 0, Step: 160, Rank: 48, loss = 8.412825991399586e-12
c622-022: Epoch: 0, Step: 160, Rank: 47, loss = 1.6540288925170898e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 161, Rank: 16, loss = 0.0
c613-101: Epoch: 0, Step: 161, Rank: 0, loss = 1.7229467630386353e-08
c622-002: Epoch: 0, Step: 161, Rank: 43, loss = 0.0003566741943359375
c622-052: Epoch: 0, Step: 161, Rank: 53, loss = 0.00012302398681640625
c621-072: Epoch: 0, Step: 161, Rank: 25, loss = 1.318767317570746e-10
c621-132: Epoch: 0, Step: 161, Rank: 37, loss = 2.2649765014648438e-06
c613-111: Epoch: 0, Step: 161, Rank: 2, loss = 0.000911712646484375
c622-001: Epoch: 0, Step: 161, Rank: 42, loss = 3.0547380447387695e-07
c621-121: Epoch: 0, Step: 161, Rank: 34, loss = 1.3597309589385986e-07
c621-111: Epoch: 0, Step: 161, Rank: 32, loss = 2.4318695068359375e-05
c621-152: Epoch: 0, Step: 161, Rank: 41, loss = 4.798173904418945e-06
c621-142: Epoch: 0, Step: 161, Rank: 39, loss = 3.510081114654895e-12
c622-051: Epoch: 0, Step: 161, Rank: 52, loss = 0.00136566162109375
c619-011: Epoch: 0, Step: 161, Rank: 14, loss = 0.0
c619-031: Epoch: 0, Step: 161, Rank: 18, loss = 0.0
c621-081: Epoch: 0, Step: 161, Rank: 26, loss = 2.7008354663848877e-07
c622-012: Epoch: 0, Step: 161, Rank: 45, loss = 4.00543212890625e-05
c619-041: Epoch: 0, Step: 161, Rank: 20, loss = 0.0
c621-091: Epoch: 0, Step: 161, Rank: 28, loss = 8.859277744181285e-32
c621-061: Epoch: 0, Step: 161, Rank: 22, loss = 0.000392913818359375
c621-151: Epoch: 0, Step: 161, Rank: 40, loss = 0.00012302398681640625
c621-052: Epoch: 0, Step: 161, Rank: 21, loss = 5.817413330078125e-05
c619-022: Epoch: 0, Step: 161, Rank: 17, loss = 1.2218952178955078e-05
c621-112: Epoch: 0, Step: 161, Rank: 33, loss = 6.07222318649292e-07
c622-101: Epoch: 0, Step: 161, Rank: 62, loss = 2.2351741790771484e-07
c622-102: Epoch: 0, Step: 161, Rank: 63, loss = 5.424022674560547e-06
c622-081: Epoch: 0, Step: 161, Rank: 58, loss = 1.55717134475708e-06
c613-151: Epoch: 0, Step: 161, Rank: 10, loss = 0.0002956390380859375
c621-131: Epoch: 0, Step: 161, Rank: 36, loss = 0.0026397705078125
c622-032: Epoch: 0, Step: 161, Rank: 49, loss = 0.00433349609375
c621-122: Epoch: 0, Step: 161, Rank: 35, loss = 2.726912498474121e-06
c621-101: Epoch: 0, Step: 161, Rank: 30, loss = 3.0547380447387695e-07
c622-031: Epoch: 0, Step: 161, Rank: 48, loss = 5.029141902923584e-07
c621-141: Epoch: 0, Step: 161, Rank: 38, loss = 0.0005035400390625
c622-042: Epoch: 0, Step: 161, Rank: 51, loss = 1.126900315284729e-07
c619-032: Epoch: 0, Step: 161, Rank: 19, loss = 2.6056189295420372e-23
c619-012: Epoch: 0, Step: 161, Rank: 15, loss = 1.996755599975586e-06
c621-092: Epoch: 0, Step: 161, Rank: 29, loss = 8.307397365570068e-07
c613-102: Epoch: 0, Step: 161, Rank: 1, loss = 5.424022674560547e-06
c622-092: Epoch: 0, Step: 161, Rank: 61, loss = 5.44811591673966e-18
c619-001: Epoch: 0, Step: 161, Rank: 12, loss = 1.1399388313293457e-06
c622-011: Epoch: 0, Step: 161, Rank: 44, loss = 7.963180541992188e-05
c619-002: Epoch: 0, Step: 161, Rank: 13, loss = 6.891787052154541e-07
c622-022: Epoch: 0, Step: 161, Rank: 47, loss = 0.000553131103515625
c622-082: Epoch: 0, Step: 161, Rank: 59, loss = 0.00011587142944335938
c621-071: Epoch: 0, Step: 161, Rank: 24, loss = 0.0
c621-102: Epoch: 0, Step: 161, Rank: 31, loss = 5.617039278149605e-09
c613-152: Epoch: 0, Step: 161, Rank: 11, loss = 0.0002460479736328125
c622-072: Epoch: 0, Step: 161, Rank: 57, loss = 4.6798959374427795e-08
c621-062: Epoch: 0, Step: 161, Rank: 23, loss = 1.2218952178955078e-05
c613-132: Epoch: 0, Step: 161, Rank: 7, loss = 2.726912498474121e-06
c613-112: Epoch: 0, Step: 161, Rank: 3, loss = 3.46451997756958e-07
c613-121: Epoch: 0, Step: 161, Rank: 4, loss = 2.562999725341797e-06
c622-021: Epoch: 0, Step: 161, Rank: 46, loss = 8.307397365570068e-07
c622-041: Epoch: 0, Step: 161, Rank: 50, loss = 1.6391277313232422e-07
c613-141: Epoch: 0, Step: 161, Rank: 8, loss = 3.084540367126465e-06
c613-122: Epoch: 0, Step: 161, Rank: 5, loss = 6.198883056640625e-05
c622-061: Epoch: 0, Step: 161, Rank: 54, loss = 4.839897155761719e-05
c622-071: Epoch: 0, Step: 161, Rank: 56, loss = 3.293156623840332e-06
c613-142: Epoch: 0, Step: 161, Rank: 9, loss = 2.2351741790771484e-07
c613-131: Epoch: 0, Step: 161, Rank: 6, loss = 7.048583938740194e-11
c621-082: Epoch: 0, Step: 161, Rank: 27, loss = 0.000335693359375
c622-062: Epoch: 0, Step: 161, Rank: 55, loss = 0.0037078857421875
c622-091: Epoch: 0, Step: 161, Rank: 60, loss = 2.9976945370435715e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 162, Rank: 0, loss = 4.839897155761719e-05
c619-002: Epoch: 0, Step: 162, Rank: 13, loss = 8.940696716308594e-06
c619-021: Epoch: 0, Step: 162, Rank: 16, loss = 4.4517219066619873e-07
c619-001: Epoch: 0, Step: 162, Rank: 12, loss = 6.891787052154541e-07
c619-011: Epoch: 0, Step: 162, Rank: 14, loss = 6.007030606269836e-08
c613-131: Epoch: 0, Step: 162, Rank: 6, loss = 0.00083160400390625
c622-101: Epoch: 0, Step: 162, Rank: 62, loss = 2.726912498474121e-06
c613-132: Epoch: 0, Step: 162, Rank: 7, loss = 1.996755599975586e-06
c613-141: Epoch: 0, Step: 162, Rank: 8, loss = 0.000278472900390625
c613-152: Epoch: 0, Step: 162, Rank: 11, loss = 4.00543212890625e-05
c622-052: Epoch: 0, Step: 162, Rank: 53, loss = 8.940696716308594e-06
c619-012: Epoch: 0, Step: 162, Rank: 15, loss = 1.126900315284729e-07
c619-031: Epoch: 0, Step: 162, Rank: 18, loss = 1.418811734765768e-09
c613-122: Epoch: 0, Step: 162, Rank: 5, loss = 2.014636993408203e-05
c619-022: Epoch: 0, Step: 162, Rank: 17, loss = 1.150369644165039e-05
c622-002: Epoch: 0, Step: 162, Rank: 43, loss = 0.0
c613-151: Epoch: 0, Step: 162, Rank: 10, loss = 3.688037395477295e-07
c622-062: Epoch: 0, Step: 162, Rank: 55, loss = 3.688037395477295e-07
c622-092: Epoch: 0, Step: 162, Rank: 61, loss = 2.562999725341797e-06
c613-112: Epoch: 0, Step: 162, Rank: 3, loss = 2.9331204132176936e-11
c622-102: Epoch: 0, Step: 162, Rank: 63, loss = 6.845220923423767e-08
c622-081: Epoch: 0, Step: 162, Rank: 58, loss = 1.150369644165039e-05
c622-041: Epoch: 0, Step: 162, Rank: 50, loss = 3.9637088775634766e-06
c622-061: Epoch: 0, Step: 162, Rank: 54, loss = 4.231929779052734e-06
c613-121: Epoch: 0, Step: 162, Rank: 4, loss = 8.859277744181285e-32
c622-012: Epoch: 0, Step: 162, Rank: 45, loss = 0.69140625
c613-111: Epoch: 0, Step: 162, Rank: 2, loss = 0.06494140625
c622-051: Epoch: 0, Step: 162, Rank: 52, loss = 3.688037395477295e-07
c622-032: Epoch: 0, Step: 162, Rank: 49, loss = 2.726912498474121e-06
c622-071: Epoch: 0, Step: 162, Rank: 56, loss = 2.514570951461792e-08
c613-142: Epoch: 0, Step: 162, Rank: 9, loss = 0.00070953369140625
c621-111: Epoch: 0, Step: 162, Rank: 32, loss = 1.996755599975586e-06
c621-151: Epoch: 0, Step: 162, Rank: 40, loss = 9.424984455108643e-07
c622-031: Epoch: 0, Step: 162, Rank: 48, loss = 5.817413330078125e-05
c622-091: Epoch: 0, Step: 162, Rank: 60, loss = 1.8775463104248047e-06
c622-022: Epoch: 0, Step: 162, Rank: 47, loss = 4.267692565917969e-05
c621-082: Epoch: 0, Step: 162, Rank: 27, loss = 5.781650543212891e-06
c621-091: Epoch: 0, Step: 162, Rank: 28, loss = 0.69140625
c621-061: Epoch: 0, Step: 162, Rank: 22, loss = 3.7670135498046875e-05
c622-072: Epoch: 0, Step: 162, Rank: 57, loss = 2.562999725341797e-06
c622-001: Epoch: 0, Step: 162, Rank: 42, loss = 1.4722347259521484e-05
c621-081: Epoch: 0, Step: 162, Rank: 26, loss = 5.3085386753082275e-08
c622-042: Epoch: 0, Step: 162, Rank: 51, loss = 3.268496584496461e-13
c622-011: Epoch: 0, Step: 162, Rank: 44, loss = 3.688037395477295e-07
c622-082: Epoch: 0, Step: 162, Rank: 59, loss = 7.963180541992188e-05
c621-072: Epoch: 0, Step: 162, Rank: 25, loss = 0.0
c621-062: Epoch: 0, Step: 162, Rank: 23, loss = 3.694822225952521e-13
c621-121: Epoch: 0, Step: 162, Rank: 34, loss = 4.301339185275744e-23
c621-142: Epoch: 0, Step: 162, Rank: 39, loss = 0.00075531005859375
c619-032: Epoch: 0, Step: 162, Rank: 19, loss = 0.000148773193359375
c621-052: Epoch: 0, Step: 162, Rank: 21, loss = 2.4318695068359375e-05
c621-131: Epoch: 0, Step: 162, Rank: 36, loss = 1.6391277313232422e-07
c621-132: Epoch: 0, Step: 162, Rank: 37, loss = 0.0
c621-152: Epoch: 0, Step: 162, Rank: 41, loss = 1.2993812561035156e-05
c621-092: Epoch: 0, Step: 162, Rank: 29, loss = 0.00019073486328125
c621-112: Epoch: 0, Step: 162, Rank: 33, loss = 1.4722347259521484e-05
c622-021: Epoch: 0, Step: 162, Rank: 46, loss = 7.420778274536133e-06
c621-122: Epoch: 0, Step: 162, Rank: 35, loss = 1.4722347259521484e-05
c621-102: Epoch: 0, Step: 162, Rank: 31, loss = 2.455635694786906e-10
c619-041: Epoch: 0, Step: 162, Rank: 20, loss = 7.338821887969971e-07
c621-141: Epoch: 0, Step: 162, Rank: 38, loss = 0.69140625
c621-101: Epoch: 0, Step: 162, Rank: 30, loss = 0.00787353515625
c621-071: Epoch: 0, Step: 162, Rank: 24, loss = 0.000179290771484375
c613-102: Epoch: 0, Step: 162, Rank: 1, loss = 2.8405338525772095e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 163, Rank: 43, loss = 4.839897155761719e-05
c622-081: Epoch: 0, Step: 163, Rank: 58, loss = 5.4836273193359375e-05
c619-002: Epoch: 0, Step: 163, Rank: 13, loss = 0.000518798828125
c619-021: Epoch: 0, Step: 163, Rank: 16, loss = 4.405564747785802e-33
c613-101: Epoch: 0, Step: 163, Rank: 0, loss = 0.0002307891845703125
c621-111: Epoch: 0, Step: 163, Rank: 32, loss = 7.486343383789062e-05
c622-052: Epoch: 0, Step: 163, Rank: 53, loss = 7.729977369308472e-08
c622-012: Epoch: 0, Step: 163, Rank: 45, loss = 5.699694156646729e-07
c619-011: Epoch: 0, Step: 163, Rank: 14, loss = 3.688037395477295e-07
c621-082: Epoch: 0, Step: 163, Rank: 27, loss = 0.69140625
c621-091: Epoch: 0, Step: 163, Rank: 28, loss = 6.628036499023438e-05
c621-081: Epoch: 0, Step: 163, Rank: 26, loss = 6.891787052154541e-07
c613-121: Epoch: 0, Step: 163, Rank: 4, loss = 2.4318695068359375e-05
c613-122: Epoch: 0, Step: 163, Rank: 5, loss = 0.0052490234375
c619-001: Epoch: 0, Step: 163, Rank: 12, loss = 5.0182080713057076e-14
c621-052: Epoch: 0, Step: 163, Rank: 21, loss = 1.3213420162451948e-29
c613-111: Epoch: 0, Step: 163, Rank: 2, loss = 1.3597309589385986e-07
c622-102: Epoch: 0, Step: 163, Rank: 63, loss = 3.123283386230469e-05
c621-072: Epoch: 0, Step: 163, Rank: 25, loss = 5.14984130859375e-05
c622-051: Epoch: 0, Step: 163, Rank: 52, loss = 0.0
c621-092: Epoch: 0, Step: 163, Rank: 29, loss = 7.420778274536133e-06
c621-121: Epoch: 0, Step: 163, Rank: 34, loss = 5.14984130859375e-05
c622-041: Epoch: 0, Step: 163, Rank: 50, loss = 8.487701416015625e-05
c621-061: Epoch: 0, Step: 163, Rank: 22, loss = 1.3597309589385986e-07
c613-141: Epoch: 0, Step: 163, Rank: 8, loss = 0.01007080078125
c621-151: Epoch: 0, Step: 163, Rank: 40, loss = 4.267692565917969e-05
c622-001: Epoch: 0, Step: 163, Rank: 42, loss = 4.00543212890625e-05
c613-102: Epoch: 0, Step: 163, Rank: 1, loss = 5.3085386753082275e-08
c613-152: Epoch: 0, Step: 163, Rank: 11, loss = 2.9331204132176936e-11
c622-082: Epoch: 0, Step: 163, Rank: 59, loss = 4.00543212890625e-05
c619-041: Epoch: 0, Step: 163, Rank: 20, loss = 1.3445969671010971e-08
c619-012: Epoch: 0, Step: 163, Rank: 15, loss = 2.2118911147117615e-08
c619-032: Epoch: 0, Step: 163, Rank: 19, loss = 2.4318695068359375e-05
c622-101: Epoch: 0, Step: 163, Rank: 62, loss = 2.8405338525772095e-08
c622-021: Epoch: 0, Step: 163, Rank: 46, loss = 2.726912498474121e-06
c622-032: Epoch: 0, Step: 163, Rank: 49, loss = 3.3060778616876836e-37
c622-022: Epoch: 0, Step: 163, Rank: 47, loss = 0.0034942626953125
c619-022: Epoch: 0, Step: 163, Rank: 17, loss = 0.0003566741943359375
c621-071: Epoch: 0, Step: 163, Rank: 24, loss = 1.0132789611816406e-05
c621-102: Epoch: 0, Step: 163, Rank: 31, loss = 0.0
c613-112: Epoch: 0, Step: 163, Rank: 3, loss = 1.6689300537109375e-05
c621-101: Epoch: 0, Step: 163, Rank: 30, loss = 7.338821887969971e-07
c621-142: Epoch: 0, Step: 163, Rank: 39, loss = 2.8405338525772095e-08
c621-122: Epoch: 0, Step: 163, Rank: 35, loss = 1.2069940567016602e-06
c621-152: Epoch: 0, Step: 163, Rank: 41, loss = 0.000392913818359375
c621-132: Epoch: 0, Step: 163, Rank: 37, loss = 4.418687638008123e-14
c622-061: Epoch: 0, Step: 163, Rank: 54, loss = 0.00012302398681640625
c621-112: Epoch: 0, Step: 163, Rank: 33, loss = 1.44393100091654e-26
c621-141: Epoch: 0, Step: 163, Rank: 38, loss = 0.00010251998901367188
c622-042: Epoch: 0, Step: 163, Rank: 51, loss = 0.000179290771484375
c622-011: Epoch: 0, Step: 163, Rank: 44, loss = 0.0003147125244140625
c622-092: Epoch: 0, Step: 163, Rank: 61, loss = 0.00020313262939453125
c622-062: Epoch: 0, Step: 163, Rank: 55, loss = 7.009506225585938e-05
c613-142: Epoch: 0, Step: 163, Rank: 9, loss = 4.231929779052734e-06
c622-071: Epoch: 0, Step: 163, Rank: 56, loss = 9.632110595703125e-05
c613-151: Epoch: 0, Step: 163, Rank: 10, loss = 7.188646122813225e-09
c622-072: Epoch: 0, Step: 163, Rank: 57, loss = 0.000553131103515625
c622-091: Epoch: 0, Step: 163, Rank: 60, loss = 2.5331974029541016e-07
c621-062: Epoch: 0, Step: 163, Rank: 23, loss = 5.3085386753082275e-08
c613-131: Epoch: 0, Step: 163, Rank: 6, loss = 2.342858351767063e-09
c619-031: Epoch: 0, Step: 163, Rank: 18, loss = 2.2649765014648438e-06
c613-132: Epoch: 0, Step: 163, Rank: 7, loss = 8.307397365570068e-07
c622-031: Epoch: 0, Step: 163, Rank: 48, loss = 1.2218952178955078e-05
c621-131: Epoch: 0, Step: 163, Rank: 36, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 164, Rank: 0, loss = 1.6391277313232422e-07
c622-081: Epoch: 0, Step: 164, Rank: 58, loss = 1.5366822481155396e-07
c619-002: Epoch: 0, Step: 164, Rank: 13, loss = 7.987216665362745e-30
c621-111: Epoch: 0, Step: 164, Rank: 32, loss = 2.4158453015843406e-12
c622-002: Epoch: 0, Step: 164, Rank: 43, loss = 0.000278472900390625
c622-101: Epoch: 0, Step: 164, Rank: 62, loss = 2.4318695068359375e-05
c622-012: Epoch: 0, Step: 164, Rank: 45, loss = 1.1059455573558807e-09
c622-071: Epoch: 0, Step: 164, Rank: 56, loss = 0.0023193359375
c621-132: Epoch: 0, Step: 164, Rank: 37, loss = 0.00136566162109375
c621-072: Epoch: 0, Step: 164, Rank: 25, loss = 1.2218952178955078e-05
c621-131: Epoch: 0, Step: 164, Rank: 36, loss = 4.4517219066619873e-07
c621-121: Epoch: 0, Step: 164, Rank: 34, loss = 0.0
c622-052: Epoch: 0, Step: 164, Rank: 53, loss = 5.699694156646729e-07
c621-152: Epoch: 0, Step: 164, Rank: 41, loss = 8.754432201385498e-08
c619-011: Epoch: 0, Step: 164, Rank: 14, loss = 2.726912498474121e-06
c621-151: Epoch: 0, Step: 164, Rank: 40, loss = 8.083811398051921e-16
c613-132: Epoch: 0, Step: 164, Rank: 7, loss = 1.6689300537109375e-05
c622-022: Epoch: 0, Step: 164, Rank: 47, loss = 0.000148773193359375
c613-152: Epoch: 0, Step: 164, Rank: 11, loss = 0.69140625
c619-001: Epoch: 0, Step: 164, Rank: 12, loss = 3.213062882423401e-08
c622-082: Epoch: 0, Step: 164, Rank: 59, loss = 1.55717134475708e-06
c622-062: Epoch: 0, Step: 164, Rank: 55, loss = 9.424984455108643e-07
c619-032: Epoch: 0, Step: 164, Rank: 19, loss = 2.066371962428093e-09
c622-092: Epoch: 0, Step: 164, Rank: 61, loss = 1.5735626220703125e-05
c621-142: Epoch: 0, Step: 164, Rank: 39, loss = 2.4318695068359375e-05
c613-121: Epoch: 0, Step: 164, Rank: 4, loss = 0.0002956390380859375
c622-001: Epoch: 0, Step: 164, Rank: 42, loss = 9.632110595703125e-05
c622-091: Epoch: 0, Step: 164, Rank: 60, loss = 1.4722347259521484e-05
c622-102: Epoch: 0, Step: 164, Rank: 63, loss = 3.688037395477295e-07
c613-131: Epoch: 0, Step: 164, Rank: 6, loss = 8.149072527885437e-09
c621-081: Epoch: 0, Step: 164, Rank: 26, loss = 0.00012302398681640625
c613-122: Epoch: 0, Step: 164, Rank: 5, loss = 4.231929779052734e-06
c621-091: Epoch: 0, Step: 164, Rank: 28, loss = 3.144186300207963e-17
c622-072: Epoch: 0, Step: 164, Rank: 57, loss = 3.3921018924503508e-28
c613-151: Epoch: 0, Step: 164, Rank: 10, loss = 1.55717134475708e-06
c622-032: Epoch: 0, Step: 164, Rank: 49, loss = 0.1572265625
c619-022: Epoch: 0, Step: 164, Rank: 17, loss = 0.000431060791015625
c622-061: Epoch: 0, Step: 164, Rank: 54, loss = 0.0
c621-101: Epoch: 0, Step: 164, Rank: 30, loss = 2.2118911147117615e-08
c621-061: Epoch: 0, Step: 164, Rank: 22, loss = 8.754432201385498e-08
c621-052: Epoch: 0, Step: 164, Rank: 21, loss = 6.5267086029052734e-06
c621-112: Epoch: 0, Step: 164, Rank: 33, loss = 1.5366822481155396e-07
c621-122: Epoch: 0, Step: 164, Rank: 35, loss = 0.00180816650390625
c621-141: Epoch: 0, Step: 164, Rank: 38, loss = 1.150369644165039e-05
c613-141: Epoch: 0, Step: 164, Rank: 8, loss = 4.231929779052734e-06
c622-042: Epoch: 0, Step: 164, Rank: 51, loss = 9.424984455108643e-07
c613-111: Epoch: 0, Step: 164, Rank: 2, loss = 0.00186920166015625
c622-041: Epoch: 0, Step: 164, Rank: 50, loss = 0.0002307891845703125
c613-112: Epoch: 0, Step: 164, Rank: 3, loss = 0.0001087188720703125
c619-012: Epoch: 0, Step: 164, Rank: 15, loss = 3.213062882423401e-08
c622-051: Epoch: 0, Step: 164, Rank: 52, loss = 2.2649765014648438e-06
c613-102: Epoch: 0, Step: 164, Rank: 1, loss = 0.000667572021484375
c622-011: Epoch: 0, Step: 164, Rank: 44, loss = 8.487701416015625e-05
c621-102: Epoch: 0, Step: 164, Rank: 31, loss = 1.2993812561035156e-05
c621-062: Epoch: 0, Step: 164, Rank: 23, loss = 0.00225830078125
c619-041: Epoch: 0, Step: 164, Rank: 20, loss = 0.00070953369140625
c621-082: Epoch: 0, Step: 164, Rank: 27, loss = 1.1399388313293457e-06
c622-031: Epoch: 0, Step: 164, Rank: 48, loss = 0.0
c622-021: Epoch: 0, Step: 164, Rank: 46, loss = 0.05322265625
c619-031: Epoch: 0, Step: 164, Rank: 18, loss = 2.1457672119140625e-05
c621-071: Epoch: 0, Step: 164, Rank: 24, loss = 1.3709068298339844e-06
c613-142: Epoch: 0, Step: 164, Rank: 9, loss = 2.9976945370435715e-09
c621-092: Epoch: 0, Step: 164, Rank: 29, loss = 0.00021648406982421875
c619-021: Epoch: 0, Step: 164, Rank: 16, loss = 4.418687638008123e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 165, Rank: 43, loss = 0.000179290771484375
c621-132: Epoch: 0, Step: 165, Rank: 37, loss = 0.0
c621-151: Epoch: 0, Step: 165, Rank: 40, loss = 0.00058746337890625
c622-052: Epoch: 0, Step: 165, Rank: 53, loss = 5.699694156646729e-07
c621-142: Epoch: 0, Step: 165, Rank: 39, loss = 1.199040866595169e-13
c622-081: Epoch: 0, Step: 165, Rank: 58, loss = 0.006317138671875
c621-111: Epoch: 0, Step: 165, Rank: 32, loss = 4.500150680541992e-06
c621-131: Epoch: 0, Step: 165, Rank: 36, loss = 0.0002956390380859375
c613-101: Epoch: 0, Step: 165, Rank: 0, loss = 6.5267086029052734e-06
c622-012: Epoch: 0, Step: 165, Rank: 45, loss = 3.725290298461914e-06
c622-001: Epoch: 0, Step: 165, Rank: 42, loss = 1.3597309589385986e-07
c622-061: Epoch: 0, Step: 165, Rank: 54, loss = 2.726912498474121e-06
c621-081: Epoch: 0, Step: 165, Rank: 26, loss = 0.00811767578125
c621-152: Epoch: 0, Step: 165, Rank: 41, loss = 0.00075531005859375
c621-122: Epoch: 0, Step: 165, Rank: 35, loss = 3.123283386230469e-05
c621-072: Epoch: 0, Step: 165, Rank: 25, loss = 1.1399388313293457e-06
c621-121: Epoch: 0, Step: 165, Rank: 34, loss = 0.69140625
c622-051: Epoch: 0, Step: 165, Rank: 52, loss = 3.144186300207963e-17
c621-101: Epoch: 0, Step: 165, Rank: 30, loss = 0.0
c621-091: Epoch: 0, Step: 165, Rank: 28, loss = 5.617039278149605e-09
c613-132: Epoch: 0, Step: 165, Rank: 7, loss = 0.69140625
c622-032: Epoch: 0, Step: 165, Rank: 49, loss = 1.2069940567016602e-06
c621-112: Epoch: 0, Step: 165, Rank: 33, loss = 3.583409124985337e-10
c622-011: Epoch: 0, Step: 165, Rank: 44, loss = 2.7008354663848877e-07
c622-041: Epoch: 0, Step: 165, Rank: 50, loss = 0.0005035400390625
c622-062: Epoch: 0, Step: 165, Rank: 55, loss = 4.172325134277344e-07
c613-152: Epoch: 0, Step: 165, Rank: 11, loss = 1.2195753889781233e-37
c621-141: Epoch: 0, Step: 165, Rank: 38, loss = 5.699694156646729e-07
c622-071: Epoch: 0, Step: 165, Rank: 56, loss = 0.00020313262939453125
c613-121: Epoch: 0, Step: 165, Rank: 4, loss = 0.0001583099365234375
c622-092: Epoch: 0, Step: 165, Rank: 61, loss = 6.891787052154541e-07
c621-082: Epoch: 0, Step: 165, Rank: 27, loss = 8.307397365570068e-07
c613-111: Epoch: 0, Step: 165, Rank: 2, loss = 0.0026397705078125
c622-072: Epoch: 0, Step: 165, Rank: 57, loss = 4.1443854570388794e-08
c622-101: Epoch: 0, Step: 165, Rank: 62, loss = 7.338821887969971e-07
c622-042: Epoch: 0, Step: 165, Rank: 51, loss = 0.00124359130859375
c619-001: Epoch: 0, Step: 165, Rank: 12, loss = 0.0
c613-131: Epoch: 0, Step: 165, Rank: 6, loss = 4.1443854570388794e-08
c613-141: Epoch: 0, Step: 165, Rank: 8, loss = 0.00020313262939453125
c622-082: Epoch: 0, Step: 165, Rank: 59, loss = 0.0
c621-052: Epoch: 0, Step: 165, Rank: 21, loss = 0.0
c619-002: Epoch: 0, Step: 165, Rank: 13, loss = 9.424984455108643e-07
c613-142: Epoch: 0, Step: 165, Rank: 9, loss = 0.0002460479736328125
c621-061: Epoch: 0, Step: 165, Rank: 22, loss = 1.6391277313232422e-07
c613-151: Epoch: 0, Step: 165, Rank: 10, loss = 2.562999725341797e-06
c622-031: Epoch: 0, Step: 165, Rank: 48, loss = 0.0002460479736328125
c619-041: Epoch: 0, Step: 165, Rank: 20, loss = 1.1399388313293457e-06
c613-122: Epoch: 0, Step: 165, Rank: 5, loss = 5.995204332975845e-15
c613-112: Epoch: 0, Step: 165, Rank: 3, loss = 3.725290298461914e-06
c621-102: Epoch: 0, Step: 165, Rank: 31, loss = 0.00012302398681640625
c622-091: Epoch: 0, Step: 165, Rank: 60, loss = 3.46451997756958e-07
c619-012: Epoch: 0, Step: 165, Rank: 15, loss = 1.955777406692505e-08
c619-022: Epoch: 0, Step: 165, Rank: 17, loss = 7.048583938740194e-11
c619-011: Epoch: 0, Step: 165, Rank: 14, loss = 3.084540367126465e-06
c622-022: Epoch: 0, Step: 165, Rank: 47, loss = 0.00180816650390625
c622-021: Epoch: 0, Step: 165, Rank: 46, loss = 4.4517219066619873e-07
c613-102: Epoch: 0, Step: 165, Rank: 1, loss = 1.6391277313232422e-07
c621-071: Epoch: 0, Step: 165, Rank: 24, loss = 7.867813110351562e-06
c621-092: Epoch: 0, Step: 165, Rank: 29, loss = 0.000179290771484375
c619-032: Epoch: 0, Step: 165, Rank: 19, loss = 8.940696716308594e-06
c621-062: Epoch: 0, Step: 165, Rank: 23, loss = 4.267692565917969e-05
c619-021: Epoch: 0, Step: 165, Rank: 16, loss = 0.00136566162109375
c622-102: Epoch: 0, Step: 165, Rank: 63, loss = 0.0
c619-031: Epoch: 0, Step: 165, Rank: 18, loss = 1.5366822481155396e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 166, Rank: 43, loss = 6.5267086029052734e-06
c621-151: Epoch: 0, Step: 166, Rank: 40, loss = 0.00070953369140625
c621-111: Epoch: 0, Step: 166, Rank: 32, loss = 0.000457763671875
c621-131: Epoch: 0, Step: 166, Rank: 36, loss = 0.000392913818359375
c622-081: Epoch: 0, Step: 166, Rank: 58, loss = 3.5017728805541992e-06
c622-012: Epoch: 0, Step: 166, Rank: 45, loss = 1.3709068298339844e-06
c613-101: Epoch: 0, Step: 166, Rank: 0, loss = 0.0
c621-091: Epoch: 0, Step: 166, Rank: 28, loss = 4.3655745685100555e-09
c621-132: Epoch: 0, Step: 166, Rank: 37, loss = 6.891787052154541e-07
c621-121: Epoch: 0, Step: 166, Rank: 34, loss = 2.648448571562767e-09
c619-001: Epoch: 0, Step: 166, Rank: 12, loss = 6.07222318649292e-07
c622-101: Epoch: 0, Step: 166, Rank: 62, loss = 9.632110595703125e-05
c621-142: Epoch: 0, Step: 166, Rank: 39, loss = 2.4318695068359375e-05
c621-122: Epoch: 0, Step: 166, Rank: 35, loss = 6.007030606269836e-08
c622-051: Epoch: 0, Step: 166, Rank: 52, loss = 0.0003147125244140625
c622-041: Epoch: 0, Step: 166, Rank: 50, loss = 1.2069940567016602e-06
c622-082: Epoch: 0, Step: 166, Rank: 59, loss = 8.307397365570068e-07
c619-002: Epoch: 0, Step: 166, Rank: 13, loss = 0.69140625
c622-062: Epoch: 0, Step: 166, Rank: 55, loss = 3.5017728805541992e-06
c622-052: Epoch: 0, Step: 166, Rank: 53, loss = 8.087401133657338e-35
c619-021: Epoch: 0, Step: 166, Rank: 16, loss = 4.4517219066619873e-07
c621-101: Epoch: 0, Step: 166, Rank: 30, loss = 0.69140625
c622-032: Epoch: 0, Step: 166, Rank: 49, loss = 2.753734588623047e-05
c621-072: Epoch: 0, Step: 166, Rank: 25, loss = 9.424984455108643e-07
c621-082: Epoch: 0, Step: 166, Rank: 27, loss = 0.00408935546875
c621-141: Epoch: 0, Step: 166, Rank: 38, loss = 1.3213420162451948e-29
c622-031: Epoch: 0, Step: 166, Rank: 48, loss = 1.4722347259521484e-05
c621-081: Epoch: 0, Step: 166, Rank: 26, loss = 4.231929779052734e-06
c622-001: Epoch: 0, Step: 166, Rank: 42, loss = 3.725290298461914e-06
c621-102: Epoch: 0, Step: 166, Rank: 31, loss = 7.188646122813225e-09
c622-091: Epoch: 0, Step: 166, Rank: 60, loss = 0.00136566162109375
c621-152: Epoch: 0, Step: 166, Rank: 41, loss = 3.123283386230469e-05
c621-092: Epoch: 0, Step: 166, Rank: 29, loss = 3.5017728805541992e-06
c622-022: Epoch: 0, Step: 166, Rank: 47, loss = 0.00116729736328125
c613-152: Epoch: 0, Step: 166, Rank: 11, loss = 0.69140625
c622-042: Epoch: 0, Step: 166, Rank: 51, loss = 0.00299072265625
c613-121: Epoch: 0, Step: 166, Rank: 4, loss = 0.00020313262939453125
c622-071: Epoch: 0, Step: 166, Rank: 56, loss = 0.000335693359375
c613-151: Epoch: 0, Step: 166, Rank: 10, loss = 0.00058746337890625
c619-032: Epoch: 0, Step: 166, Rank: 19, loss = 0.0
c621-112: Epoch: 0, Step: 166, Rank: 33, loss = 1.5366822481155396e-07
c622-021: Epoch: 0, Step: 166, Rank: 46, loss = 9.918585419654846e-08
c619-031: Epoch: 0, Step: 166, Rank: 18, loss = 7.729977369308472e-08
c613-142: Epoch: 0, Step: 166, Rank: 9, loss = 1.6391277313232422e-07
c619-011: Epoch: 0, Step: 166, Rank: 14, loss = 5.424022674560547e-06
c613-131: Epoch: 0, Step: 166, Rank: 6, loss = 9.012222290039062e-05
c622-061: Epoch: 0, Step: 166, Rank: 54, loss = 0.004486083984375
c622-011: Epoch: 0, Step: 166, Rank: 44, loss = 7.486343383789062e-05
c621-052: Epoch: 0, Step: 166, Rank: 21, loss = 0.000179290771484375
c621-062: Epoch: 0, Step: 166, Rank: 23, loss = 0.0001583099365234375
c613-122: Epoch: 0, Step: 166, Rank: 5, loss = 0.00136566162109375
c621-061: Epoch: 0, Step: 166, Rank: 22, loss = 0.0
c613-112: Epoch: 0, Step: 166, Rank: 3, loss = 9.74978320300579e-10
c613-132: Epoch: 0, Step: 166, Rank: 7, loss = 3.688037395477295e-07
c619-022: Epoch: 0, Step: 166, Rank: 17, loss = 8.404254913330078e-06
c613-141: Epoch: 0, Step: 166, Rank: 8, loss = 3.123283386230469e-05
c619-041: Epoch: 0, Step: 166, Rank: 20, loss = 7.486343383789062e-05
c621-071: Epoch: 0, Step: 166, Rank: 24, loss = 5.3085386753082275e-08
c622-102: Epoch: 0, Step: 166, Rank: 63, loss = 1.3445969671010971e-08
c622-092: Epoch: 0, Step: 166, Rank: 61, loss = 1.996755599975586e-06
c622-072: Epoch: 0, Step: 166, Rank: 57, loss = 1.1988913903810543e-32
c613-111: Epoch: 0, Step: 166, Rank: 2, loss = 9.424984455108643e-07
c613-102: Epoch: 0, Step: 166, Rank: 1, loss = 0.0002956390380859375
c619-012: Epoch: 0, Step: 166, Rank: 15, loss = 2.7008354663848877e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.10s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.10s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 167, Rank: 58, loss = 2.2649765014648438e-06
c621-111: Epoch: 0, Step: 167, Rank: 32, loss = 0.00299072265625
c613-101: Epoch: 0, Step: 167, Rank: 0, loss = 1.895427703857422e-05
c619-021: Epoch: 0, Step: 167, Rank: 16, loss = 1.6432439176733427e-19
c622-002: Epoch: 0, Step: 167, Rank: 43, loss = 1.6689300537109375e-05
c622-012: Epoch: 0, Step: 167, Rank: 45, loss = 3.0547380447387695e-07
c622-101: Epoch: 0, Step: 167, Rank: 62, loss = 3.4051481634378433e-09
c619-022: Epoch: 0, Step: 167, Rank: 17, loss = 1.0277290130034089e-10
c619-031: Epoch: 0, Step: 167, Rank: 18, loss = 3.774403012357652e-11
c621-132: Epoch: 0, Step: 167, Rank: 37, loss = 5.0961971282958984e-06
c613-151: Epoch: 0, Step: 167, Rank: 10, loss = 8.404254913330078e-06
c622-052: Epoch: 0, Step: 167, Rank: 53, loss = 3.0547380447387695e-07
c621-091: Epoch: 0, Step: 167, Rank: 28, loss = 0.000457763671875
c621-081: Epoch: 0, Step: 167, Rank: 26, loss = 9.5367431640625e-06
c621-082: Epoch: 0, Step: 167, Rank: 27, loss = 4.00543212890625e-05
c619-002: Epoch: 0, Step: 167, Rank: 13, loss = 4.231929779052734e-06
c621-061: Epoch: 0, Step: 167, Rank: 22, loss = 5.893525667488575e-10
c619-032: Epoch: 0, Step: 167, Rank: 19, loss = 2.1047890186309814e-07
c619-012: Epoch: 0, Step: 167, Rank: 15, loss = 0.00011587142944335938
c619-001: Epoch: 0, Step: 167, Rank: 12, loss = 5.14984130859375e-05
c622-022: Epoch: 0, Step: 167, Rank: 47, loss = 1.601387637598654e-28
c621-151: Epoch: 0, Step: 167, Rank: 40, loss = 3.510081114654895e-12
c621-131: Epoch: 0, Step: 167, Rank: 36, loss = 1.5735626220703125e-05
c621-101: Epoch: 0, Step: 167, Rank: 30, loss = 7.566995918750763e-10
c619-041: Epoch: 0, Step: 167, Rank: 20, loss = 0.0005035400390625
c613-132: Epoch: 0, Step: 167, Rank: 7, loss = 3.46451997756958e-07
c622-062: Epoch: 0, Step: 167, Rank: 55, loss = 0.0
c621-072: Epoch: 0, Step: 167, Rank: 25, loss = 1.2993812561035156e-05
c613-152: Epoch: 0, Step: 167, Rank: 11, loss = 4.231929779052734e-06
c613-121: Epoch: 0, Step: 167, Rank: 4, loss = 6.891787052154541e-07
c621-142: Epoch: 0, Step: 167, Rank: 39, loss = 2.726912498474121e-06
c622-071: Epoch: 0, Step: 167, Rank: 56, loss = 4.782137916322191e-25
c622-072: Epoch: 0, Step: 167, Rank: 57, loss = 6.973743438720703e-06
c613-142: Epoch: 0, Step: 167, Rank: 9, loss = 9.5367431640625e-06
c621-052: Epoch: 0, Step: 167, Rank: 21, loss = 9.424984455108643e-07
c619-011: Epoch: 0, Step: 167, Rank: 14, loss = 8.940696716308594e-06
c621-122: Epoch: 0, Step: 167, Rank: 35, loss = 0.69140625
c622-032: Epoch: 0, Step: 167, Rank: 49, loss = 5.699694156646729e-07
c621-112: Epoch: 0, Step: 167, Rank: 33, loss = 3.5695955961250784e-29
c622-082: Epoch: 0, Step: 167, Rank: 59, loss = 8.307397365570068e-07
c613-131: Epoch: 0, Step: 167, Rank: 6, loss = 2.3245294578089215e-16
c621-121: Epoch: 0, Step: 167, Rank: 34, loss = 0.00011587142944335938
c622-041: Epoch: 0, Step: 167, Rank: 50, loss = 7.188646122813225e-09
c622-061: Epoch: 0, Step: 167, Rank: 54, loss = 2.7008354663848877e-07
c622-001: Epoch: 0, Step: 167, Rank: 42, loss = 2.648448571562767e-09
c621-071: Epoch: 0, Step: 167, Rank: 24, loss = 1.6079866327345371e-09
c621-141: Epoch: 0, Step: 167, Rank: 38, loss = 2.2351741790771484e-07
c622-102: Epoch: 0, Step: 167, Rank: 63, loss = 2.4318695068359375e-05
c622-011: Epoch: 0, Step: 167, Rank: 44, loss = 8.487701416015625e-05
c621-152: Epoch: 0, Step: 167, Rank: 41, loss = 1.955777406692505e-08
c613-122: Epoch: 0, Step: 167, Rank: 5, loss = 4.5299530029296875e-05
c621-062: Epoch: 0, Step: 167, Rank: 23, loss = 6.007030606269836e-08
c613-112: Epoch: 0, Step: 167, Rank: 3, loss = 3.314018249511719e-05
c621-092: Epoch: 0, Step: 167, Rank: 29, loss = 0.00360107421875
c622-092: Epoch: 0, Step: 167, Rank: 61, loss = 0.00141143798828125
c613-111: Epoch: 0, Step: 167, Rank: 2, loss = 2.5920599000528455e-11
c622-051: Epoch: 0, Step: 167, Rank: 52, loss = 0.00075531005859375
c622-091: Epoch: 0, Step: 167, Rank: 60, loss = 2.562999725341797e-06
c622-042: Epoch: 0, Step: 167, Rank: 51, loss = 1.2514647096395493e-09
c622-021: Epoch: 0, Step: 167, Rank: 46, loss = 7.188646122813225e-09
c613-141: Epoch: 0, Step: 167, Rank: 8, loss = 0.0
c622-031: Epoch: 0, Step: 167, Rank: 48, loss = 4.94765117764473e-09
c613-102: Epoch: 0, Step: 167, Rank: 1, loss = 1.3322676295501878e-15
c621-102: Epoch: 0, Step: 167, Rank: 31, loss = 1.6079866327345371e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 168, Rank: 58, loss = 3.688037395477295e-07
c613-101: Epoch: 0, Step: 168, Rank: 0, loss = 1.1399388313293457e-06
c622-002: Epoch: 0, Step: 168, Rank: 43, loss = 4.3655745685100555e-09
c621-111: Epoch: 0, Step: 168, Rank: 32, loss = 3.213062882423401e-08
c622-052: Epoch: 0, Step: 168, Rank: 53, loss = 5.14984130859375e-05
c621-072: Epoch: 0, Step: 168, Rank: 25, loss = 0.000553131103515625
c621-081: Epoch: 0, Step: 168, Rank: 26, loss = 6.628036499023438e-05
c621-142: Epoch: 0, Step: 168, Rank: 39, loss = 9.74978320300579e-10
c619-021: Epoch: 0, Step: 168, Rank: 16, loss = 3.084540367126465e-06
c622-051: Epoch: 0, Step: 168, Rank: 52, loss = 8.404254913330078e-06
c622-012: Epoch: 0, Step: 168, Rank: 45, loss = 0.69140625
c621-151: Epoch: 0, Step: 168, Rank: 40, loss = 1.2759119272232056e-07
c622-101: Epoch: 0, Step: 168, Rank: 62, loss = 1.525040715932846e-08
c622-082: Epoch: 0, Step: 168, Rank: 59, loss = 0.00040435791015625
c622-072: Epoch: 0, Step: 168, Rank: 57, loss = 5.0961971282958984e-06
c622-022: Epoch: 0, Step: 168, Rank: 47, loss = 3.725290298461914e-06
c622-001: Epoch: 0, Step: 168, Rank: 42, loss = 2.562999725341797e-06
c613-142: Epoch: 0, Step: 168, Rank: 9, loss = 0.005584716796875
c622-071: Epoch: 0, Step: 168, Rank: 56, loss = 1.0058283805847168e-06
c621-131: Epoch: 0, Step: 168, Rank: 36, loss = 0.000179290771484375
c621-121: Epoch: 0, Step: 168, Rank: 34, loss = 5.0182080713057076e-14
c619-001: Epoch: 0, Step: 168, Rank: 12, loss = 2.014636993408203e-05
c622-032: Epoch: 0, Step: 168, Rank: 49, loss = 3.293156623840332e-06
c613-151: Epoch: 0, Step: 168, Rank: 10, loss = 5.699694156646729e-07
c621-082: Epoch: 0, Step: 168, Rank: 27, loss = 3.7670135498046875e-05
c622-041: Epoch: 0, Step: 168, Rank: 50, loss = 3.293156623840332e-06
c619-022: Epoch: 0, Step: 168, Rank: 17, loss = 1.4227506535912076e-21
c621-091: Epoch: 0, Step: 168, Rank: 28, loss = 4.231929779052734e-06
c613-152: Epoch: 0, Step: 168, Rank: 11, loss = 0.0001583099365234375
c622-031: Epoch: 0, Step: 168, Rank: 48, loss = 1.7848833522293717e-11
c619-041: Epoch: 0, Step: 168, Rank: 20, loss = 6.139278411865234e-06
c622-092: Epoch: 0, Step: 168, Rank: 61, loss = 3.0547380447387695e-07
c622-062: Epoch: 0, Step: 168, Rank: 55, loss = 5.3085386753082275e-08
c619-011: Epoch: 0, Step: 168, Rank: 14, loss = 0.001068115234375
c619-032: Epoch: 0, Step: 168, Rank: 19, loss = 8.754432201385498e-08
c622-042: Epoch: 0, Step: 168, Rank: 51, loss = 4.5299530029296875e-05
c613-111: Epoch: 0, Step: 168, Rank: 2, loss = 1.6540288925170898e-06
c621-122: Epoch: 0, Step: 168, Rank: 35, loss = 1.895427703857422e-05
c622-102: Epoch: 0, Step: 168, Rank: 63, loss = 0.0
c621-092: Epoch: 0, Step: 168, Rank: 29, loss = 5.3085386753082275e-08
c621-112: Epoch: 0, Step: 168, Rank: 33, loss = 0.000457763671875
c613-121: Epoch: 0, Step: 168, Rank: 4, loss = 1.2514647096395493e-09
c622-011: Epoch: 0, Step: 168, Rank: 44, loss = 4.00543212890625e-05
c621-052: Epoch: 0, Step: 168, Rank: 21, loss = 2.4158453015843406e-12
c621-102: Epoch: 0, Step: 168, Rank: 31, loss = 0.0
c621-141: Epoch: 0, Step: 168, Rank: 38, loss = 0.003173828125
c622-061: Epoch: 0, Step: 168, Rank: 54, loss = 8.754432201385498e-08
c613-102: Epoch: 0, Step: 168, Rank: 1, loss = 0.000644683837890625
c613-132: Epoch: 0, Step: 168, Rank: 7, loss = 0.69140625
c621-071: Epoch: 0, Step: 168, Rank: 24, loss = 0.0
c613-141: Epoch: 0, Step: 168, Rank: 8, loss = 8.754432201385498e-08
c621-062: Epoch: 0, Step: 168, Rank: 23, loss = 1.1641532182693481e-10
c619-002: Epoch: 0, Step: 168, Rank: 13, loss = 4.231929779052734e-06
c621-152: Epoch: 0, Step: 168, Rank: 41, loss = 2.276897430419922e-05
c613-112: Epoch: 0, Step: 168, Rank: 3, loss = 0.0003147125244140625
c621-101: Epoch: 0, Step: 168, Rank: 30, loss = 0.00010251998901367188
c621-061: Epoch: 0, Step: 168, Rank: 22, loss = 8.940696716308594e-06
c619-031: Epoch: 0, Step: 168, Rank: 18, loss = 6.973743438720703e-06
c613-122: Epoch: 0, Step: 168, Rank: 5, loss = 0.0003566741943359375
c622-021: Epoch: 0, Step: 168, Rank: 46, loss = 3.293156623840332e-06
c622-091: Epoch: 0, Step: 168, Rank: 60, loss = 6.344635039567947e-09
c621-132: Epoch: 0, Step: 168, Rank: 37, loss = 3.725290298461914e-06
c619-012: Epoch: 0, Step: 168, Rank: 15, loss = 0.06689453125
c613-131: Epoch: 0, Step: 168, Rank: 6, loss = 0.0003566741943359375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 169, Rank: 26, loss = 6.891787052154541e-07
c621-072: Epoch: 0, Step: 169, Rank: 25, loss = 0.0
c621-082: Epoch: 0, Step: 169, Rank: 27, loss = 1.6079866327345371e-09
c621-071: Epoch: 0, Step: 169, Rank: 24, loss = 3.293156623840332e-06
c621-062: Epoch: 0, Step: 169, Rank: 23, loss = 9.424984455108643e-07
c621-061: Epoch: 0, Step: 169, Rank: 22, loss = 3.314018249511719e-05
c621-052: Epoch: 0, Step: 169, Rank: 21, loss = 0.0
c621-091: Epoch: 0, Step: 169, Rank: 28, loss = 8.265194654219209e-40
c619-041: Epoch: 0, Step: 169, Rank: 20, loss = 2.1457672119140625e-05
c619-031: Epoch: 0, Step: 169, Rank: 18, loss = 0.00408935546875
c619-032: Epoch: 0, Step: 169, Rank: 19, loss = 5.14984130859375e-05
c619-021: Epoch: 0, Step: 169, Rank: 16, loss = 1.150369644165039e-05
c619-022: Epoch: 0, Step: 169, Rank: 17, loss = 2.5331974029541016e-07
c619-012: Epoch: 0, Step: 169, Rank: 15, loss = 2.9325485229492188e-05
c621-092: Epoch: 0, Step: 169, Rank: 29, loss = 2.753734588623047e-05
c619-011: Epoch: 0, Step: 169, Rank: 14, loss = 8.940696716308594e-06
c619-002: Epoch: 0, Step: 169, Rank: 13, loss = 2.765625
c619-001: Epoch: 0, Step: 169, Rank: 12, loss = 1.955777406692505e-08
c613-152: Epoch: 0, Step: 169, Rank: 11, loss = 0.00021648406982421875
c621-101: Epoch: 0, Step: 169, Rank: 30, loss = 0.69140625
c613-151: Epoch: 0, Step: 169, Rank: 10, loss = 2.68426485998971e-33
c613-132: Epoch: 0, Step: 169, Rank: 7, loss = 8.940696716308594e-06
c613-142: Epoch: 0, Step: 169, Rank: 9, loss = 2.014636993408203e-05
c613-141: Epoch: 0, Step: 169, Rank: 8, loss = 0.0001316070556640625
c613-131: Epoch: 0, Step: 169, Rank: 6, loss = 1.6540288925170898e-06
c613-122: Epoch: 0, Step: 169, Rank: 5, loss = 0.004486083984375
c613-101: Epoch: 0, Step: 169, Rank: 0, loss = 9.370282327836321e-14
c613-121: Epoch: 0, Step: 169, Rank: 4, loss = 0.000553131103515625
c613-111: Epoch: 0, Step: 169, Rank: 2, loss = 0.0
c613-112: Epoch: 0, Step: 169, Rank: 3, loss = 1.150369644165039e-05
c621-102: Epoch: 0, Step: 169, Rank: 31, loss = 3.0547380447387695e-07
c613-102: Epoch: 0, Step: 169, Rank: 1, loss = 2.2649765014648438e-06
c621-111: Epoch: 0, Step: 169, Rank: 32, loss = 4.05634636990726e-10
c622-101: Epoch: 0, Step: 169, Rank: 62, loss = 1.8775463104248047e-06
c622-102: Epoch: 0, Step: 169, Rank: 63, loss = 3.084540367126465e-06
c622-092: Epoch: 0, Step: 169, Rank: 61, loss = 0.0008544921875
c622-082: Epoch: 0, Step: 169, Rank: 59, loss = 0.00083160400390625
c622-081: Epoch: 0, Step: 169, Rank: 58, loss = 0.0
c622-091: Epoch: 0, Step: 169, Rank: 60, loss = 27.375
c622-062: Epoch: 0, Step: 169, Rank: 55, loss = 7.420778274536133e-06
c622-072: Epoch: 0, Step: 169, Rank: 57, loss = 6.628036499023438e-05
c622-071: Epoch: 0, Step: 169, Rank: 56, loss = 3.46451997756958e-07
c622-052: Epoch: 0, Step: 169, Rank: 53, loss = 0.0
c621-112: Epoch: 0, Step: 169, Rank: 33, loss = 2.2351741790771484e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-061: Epoch: 0, Step: 169, Rank: 54, loss = 7.420778274536133e-06
c622-051: Epoch: 0, Step: 169, Rank: 52, loss = 0.0
c621-121: Epoch: 0, Step: 169, Rank: 34, loss = 4.172325134277344e-07
c622-042: Epoch: 0, Step: 169, Rank: 51, loss = 5.14984130859375e-05
c622-041: Epoch: 0, Step: 169, Rank: 50, loss = 3.655441105365753e-08
c622-032: Epoch: 0, Step: 169, Rank: 49, loss = 2.2851054382044822e-11
c622-002: Epoch: 0, Step: 169, Rank: 43, loss = 6.198883056640625e-05
c622-031: Epoch: 0, Step: 169, Rank: 48, loss = 1.1399388313293457e-06
c622-022: Epoch: 0, Step: 169, Rank: 47, loss = 1.7762184143066406e-05
c621-122: Epoch: 0, Step: 169, Rank: 35, loss = 0.0
c622-012: Epoch: 0, Step: 169, Rank: 45, loss = 2.726912498474121e-06
c621-131: Epoch: 0, Step: 169, Rank: 36, loss = 5.893525667488575e-10
c621-132: Epoch: 0, Step: 169, Rank: 37, loss = 4.1875
c622-011: Epoch: 0, Step: 169, Rank: 44, loss = 0.0
c622-021: Epoch: 0, Step: 169, Rank: 46, loss = 1.126900315284729e-07
c622-001: Epoch: 0, Step: 169, Rank: 42, loss = 1.4637180356658064e-12
c621-141: Epoch: 0, Step: 169, Rank: 38, loss = 5.029141902923584e-07
c621-151: Epoch: 0, Step: 169, Rank: 40, loss = 0.00016880035400390625
c621-142: Epoch: 0, Step: 169, Rank: 39, loss = 3.314018249511719e-05
c621-152: Epoch: 0, Step: 169, Rank: 41, loss = 0.73828125
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 170, Rank: 0, loss = 1.150369644165039e-05
c619-021: Epoch: 0, Step: 170, Rank: 16, loss = 0.00012302398681640625
c622-002: Epoch: 0, Step: 170, Rank: 43, loss = 1.525040715932846e-08
c621-072: Epoch: 0, Step: 170, Rank: 25, loss = 7.566995918750763e-10
c621-142: Epoch: 0, Step: 170, Rank: 39, loss = 5.14984130859375e-05
c622-081: Epoch: 0, Step: 170, Rank: 58, loss = 1.2069940567016602e-06
c622-012: Epoch: 0, Step: 170, Rank: 45, loss = 3.841705620288849e-09
c622-092: Epoch: 0, Step: 170, Rank: 61, loss = 0.01507568359375
c622-101: Epoch: 0, Step: 170, Rank: 62, loss = 0.0
c619-002: Epoch: 0, Step: 170, Rank: 13, loss = 0.0
c621-111: Epoch: 0, Step: 170, Rank: 32, loss = 0.000335693359375
c621-091: Epoch: 0, Step: 170, Rank: 28, loss = 8.307397365570068e-07
c621-131: Epoch: 0, Step: 170, Rank: 36, loss = 5.14984130859375e-05
c621-081: Epoch: 0, Step: 170, Rank: 26, loss = 0.0
c613-121: Epoch: 0, Step: 170, Rank: 4, loss = 0.0020599365234375
c619-011: Epoch: 0, Step: 170, Rank: 14, loss = 1.3828277587890625e-05
c622-052: Epoch: 0, Step: 170, Rank: 53, loss = 1.6916601452976465e-10
c613-111: Epoch: 0, Step: 170, Rank: 2, loss = 1.126900315284729e-07
c619-001: Epoch: 0, Step: 170, Rank: 12, loss = 2.9325485229492188e-05
c622-102: Epoch: 0, Step: 170, Rank: 63, loss = 1.5366822481155396e-07
c621-061: Epoch: 0, Step: 170, Rank: 22, loss = 0.69140625
c621-052: Epoch: 0, Step: 170, Rank: 21, loss = 4.5299530029296875e-05
c613-122: Epoch: 0, Step: 170, Rank: 5, loss = 7.009506225585938e-05
c613-112: Epoch: 0, Step: 170, Rank: 3, loss = 0.0016021728515625
c622-082: Epoch: 0, Step: 170, Rank: 59, loss = 0.0
c613-102: Epoch: 0, Step: 170, Rank: 1, loss = 4.1443854570388794e-08
c621-132: Epoch: 0, Step: 170, Rank: 37, loss = 4.267692565917969e-05
c621-121: Epoch: 0, Step: 170, Rank: 34, loss = 8.754432201385498e-08
c621-112: Epoch: 0, Step: 170, Rank: 33, loss = 4.231929779052734e-06
c622-001: Epoch: 0, Step: 170, Rank: 42, loss = 5.14984130859375e-05
c613-152: Epoch: 0, Step: 170, Rank: 11, loss = 0.0
c622-011: Epoch: 0, Step: 170, Rank: 44, loss = 0.0
c619-041: Epoch: 0, Step: 170, Rank: 20, loss = 1.166324663699769e-22
c613-151: Epoch: 0, Step: 170, Rank: 10, loss = 4.1443854570388794e-08
c622-071: Epoch: 0, Step: 170, Rank: 56, loss = 1.955777406692505e-08
c622-072: Epoch: 0, Step: 170, Rank: 57, loss = 4.3655745685100555e-09
c622-051: Epoch: 0, Step: 170, Rank: 52, loss = 1.150369644165039e-05
c622-031: Epoch: 0, Step: 170, Rank: 48, loss = 3.441691376337985e-14
c621-082: Epoch: 0, Step: 170, Rank: 27, loss = 6.891787052154541e-07
c621-141: Epoch: 0, Step: 170, Rank: 38, loss = 5.029141902923584e-07
c613-132: Epoch: 0, Step: 170, Rank: 7, loss = 0.0001583099365234375
c613-141: Epoch: 0, Step: 170, Rank: 8, loss = 1.0058283805847168e-06
c619-022: Epoch: 0, Step: 170, Rank: 17, loss = 1.6540288925170898e-06
c621-092: Epoch: 0, Step: 170, Rank: 29, loss = 2.753734588623047e-05
c621-101: Epoch: 0, Step: 170, Rank: 30, loss = 2.648448571562767e-09
c613-131: Epoch: 0, Step: 170, Rank: 6, loss = 1.7848833522293717e-11
c622-041: Epoch: 0, Step: 170, Rank: 50, loss = 0.000457763671875
c622-062: Epoch: 0, Step: 170, Rank: 55, loss = 1.857925203976527e-26
c621-062: Epoch: 0, Step: 170, Rank: 23, loss = 1.55717134475708e-06
c621-151: Epoch: 0, Step: 170, Rank: 40, loss = 9.424984455108643e-07
c619-032: Epoch: 0, Step: 170, Rank: 19, loss = 8.307397365570068e-07
c622-021: Epoch: 0, Step: 170, Rank: 46, loss = 7.338821887969971e-07
c622-091: Epoch: 0, Step: 170, Rank: 60, loss = 1.4722347259521484e-05
c622-032: Epoch: 0, Step: 170, Rank: 49, loss = 3.7670135498046875e-05
c621-152: Epoch: 0, Step: 170, Rank: 41, loss = 1.3597309589385986e-07
c621-071: Epoch: 0, Step: 170, Rank: 24, loss = 5.3085386753082275e-08
c622-042: Epoch: 0, Step: 170, Rank: 51, loss = 4.1443854570388794e-08
c621-102: Epoch: 0, Step: 170, Rank: 31, loss = 8.754432201385498e-08
c622-061: Epoch: 0, Step: 170, Rank: 54, loss = 0.0
c622-022: Epoch: 0, Step: 170, Rank: 47, loss = 1.7229467630386353e-08
c613-142: Epoch: 0, Step: 170, Rank: 9, loss = 9.918585419654846e-08
c619-031: Epoch: 0, Step: 170, Rank: 18, loss = 1.6689300537109375e-05
c621-122: Epoch: 0, Step: 170, Rank: 35, loss = 1.2069940567016602e-06
c619-012: Epoch: 0, Step: 170, Rank: 15, loss = 5.14984130859375e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 171, Rank: 16, loss = 0.0002307891845703125
c622-081: Epoch: 0, Step: 171, Rank: 58, loss = 2.753734588623047e-05
c622-002: Epoch: 0, Step: 171, Rank: 43, loss = 0.00014019012451171875
c613-101: Epoch: 0, Step: 171, Rank: 0, loss = 1.7762184143066406e-05
c621-111: Epoch: 0, Step: 171, Rank: 32, loss = 7.729977369308472e-08
c622-012: Epoch: 0, Step: 171, Rank: 45, loss = 0.00016880035400390625
c622-001: Epoch: 0, Step: 171, Rank: 42, loss = 4.6798959374427795e-08
c619-002: Epoch: 0, Step: 171, Rank: 13, loss = 8.940696716308594e-06
c622-031: Epoch: 0, Step: 171, Rank: 48, loss = 3.4051481634378433e-09
c622-101: Epoch: 0, Step: 171, Rank: 62, loss = 0.0002613067626953125
c622-062: Epoch: 0, Step: 171, Rank: 55, loss = 0.00016880035400390625
c622-032: Epoch: 0, Step: 171, Rank: 49, loss = 0.0002307891845703125
c622-052: Epoch: 0, Step: 171, Rank: 53, loss = 0.0001316070556640625
c619-001: Epoch: 0, Step: 171, Rank: 12, loss = 1.1641532182693481e-10
c622-082: Epoch: 0, Step: 171, Rank: 59, loss = 0.0002307891845703125
c621-151: Epoch: 0, Step: 171, Rank: 40, loss = 0.69140625
c613-111: Epoch: 0, Step: 171, Rank: 2, loss = 3.6261649734165925e-34
c613-132: Epoch: 0, Step: 171, Rank: 7, loss = 3.314018249511719e-05
c622-092: Epoch: 0, Step: 171, Rank: 61, loss = 3.293156623840332e-06
c622-072: Epoch: 0, Step: 171, Rank: 57, loss = 7.188646122813225e-09
c619-011: Epoch: 0, Step: 171, Rank: 14, loss = 0.69140625
c622-071: Epoch: 0, Step: 171, Rank: 56, loss = 1.2514647096395493e-09
c622-051: Epoch: 0, Step: 171, Rank: 52, loss = 1.7762184143066406e-05
c621-091: Epoch: 0, Step: 171, Rank: 28, loss = 2.7008354663848877e-07
c622-022: Epoch: 0, Step: 171, Rank: 47, loss = 5.893525667488575e-10
c613-121: Epoch: 0, Step: 171, Rank: 4, loss = 0.0026397705078125
c621-132: Epoch: 0, Step: 171, Rank: 37, loss = 4.231929779052734e-06
c621-142: Epoch: 0, Step: 171, Rank: 39, loss = 1.3322676295501878e-15
c621-101: Epoch: 0, Step: 171, Rank: 30, loss = 5.0961971282958984e-06
c613-152: Epoch: 0, Step: 171, Rank: 11, loss = 6.628036499023438e-05
c621-152: Epoch: 0, Step: 171, Rank: 41, loss = 4.172325134277344e-07
c619-022: Epoch: 0, Step: 171, Rank: 17, loss = 8.404254913330078e-06
c613-112: Epoch: 0, Step: 171, Rank: 3, loss = 5.3085386753082275e-08
c621-122: Epoch: 0, Step: 171, Rank: 35, loss = 1.6391277313232422e-07
c622-042: Epoch: 0, Step: 171, Rank: 51, loss = 0.00010251998901367188
c619-012: Epoch: 0, Step: 171, Rank: 15, loss = 0.000431060791015625
c621-121: Epoch: 0, Step: 171, Rank: 34, loss = 2.342858351767063e-09
c619-032: Epoch: 0, Step: 171, Rank: 19, loss = 0.000518798828125
c621-061: Epoch: 0, Step: 171, Rank: 22, loss = 1.1874362826347351e-08
c622-061: Epoch: 0, Step: 171, Rank: 54, loss = 1.3597309589385986e-07
c622-041: Epoch: 0, Step: 171, Rank: 50, loss = 9.632110595703125e-05
c613-122: Epoch: 0, Step: 171, Rank: 5, loss = 7.009506225585938e-05
c613-151: Epoch: 0, Step: 171, Rank: 10, loss = 0.00099945068359375
c621-102: Epoch: 0, Step: 171, Rank: 31, loss = 8.404254913330078e-06
c613-131: Epoch: 0, Step: 171, Rank: 6, loss = 2.586841583251953e-05
c622-102: Epoch: 0, Step: 171, Rank: 63, loss = 5.893525667488575e-10
c613-102: Epoch: 0, Step: 171, Rank: 1, loss = 2.7008354663848877e-07
c622-021: Epoch: 0, Step: 171, Rank: 46, loss = 7.987216665362745e-30
c621-112: Epoch: 0, Step: 171, Rank: 33, loss = 0.0
c619-041: Epoch: 0, Step: 171, Rank: 20, loss = 9.424984455108643e-07
c622-011: Epoch: 0, Step: 171, Rank: 44, loss = 5.14984130859375e-05
c613-142: Epoch: 0, Step: 171, Rank: 9, loss = 4.1443854570388794e-08
c621-082: Epoch: 0, Step: 171, Rank: 27, loss = 7.44648787076585e-12
c621-072: Epoch: 0, Step: 171, Rank: 25, loss = 4.231929779052734e-06
c621-052: Epoch: 0, Step: 171, Rank: 21, loss = 2.014636993408203e-05
c621-062: Epoch: 0, Step: 171, Rank: 23, loss = 1.0058283805847168e-06
c613-141: Epoch: 0, Step: 171, Rank: 8, loss = 0.000518798828125
c619-031: Epoch: 0, Step: 171, Rank: 18, loss = 9.74978320300579e-10
c621-131: Epoch: 0, Step: 171, Rank: 36, loss = 8.940696716308594e-06
c621-081: Epoch: 0, Step: 171, Rank: 26, loss = 2.0057740190981832e-18
c622-091: Epoch: 0, Step: 171, Rank: 60, loss = 0.003173828125
c621-141: Epoch: 0, Step: 171, Rank: 38, loss = 0.000179290771484375
c621-092: Epoch: 0, Step: 171, Rank: 29, loss = 0.00010251998901367188
c621-071: Epoch: 0, Step: 171, Rank: 24, loss = 1.3597309589385986e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75390625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 172, Rank: 43, loss = 1.4722347259521484e-05
c622-011: Epoch: 0, Step: 172, Rank: 44, loss = 4.4517219066619873e-07
c613-101: Epoch: 0, Step: 172, Rank: 0, loss = 1.150369644165039e-05
c622-001: Epoch: 0, Step: 172, Rank: 42, loss = 0.69140625
c622-052: Epoch: 0, Step: 172, Rank: 53, loss = 1.3732490638087374e-25
c622-012: Epoch: 0, Step: 172, Rank: 45, loss = 1.6540288925170898e-06
c621-151: Epoch: 0, Step: 172, Rank: 40, loss = 0.0
c622-051: Epoch: 0, Step: 172, Rank: 52, loss = 1.7229467630386353e-08
c621-142: Epoch: 0, Step: 172, Rank: 39, loss = 1.895427703857422e-05
c621-081: Epoch: 0, Step: 172, Rank: 26, loss = 6.693881005048752e-10
c621-132: Epoch: 0, Step: 172, Rank: 37, loss = 6.344635039567947e-09
c621-111: Epoch: 0, Step: 172, Rank: 32, loss = 1.6391277313232422e-07
c619-021: Epoch: 0, Step: 172, Rank: 16, loss = 6.198883056640625e-05
c621-091: Epoch: 0, Step: 172, Rank: 28, loss = 0.000431060791015625
c622-101: Epoch: 0, Step: 172, Rank: 62, loss = 2.905726432800293e-06
c621-121: Epoch: 0, Step: 172, Rank: 34, loss = 6.139278411865234e-06
c619-011: Epoch: 0, Step: 172, Rank: 14, loss = 0.00070953369140625
c621-131: Epoch: 0, Step: 172, Rank: 36, loss = 0.00075531005859375
c622-022: Epoch: 0, Step: 172, Rank: 47, loss = 8.149072527885437e-09
c622-041: Epoch: 0, Step: 172, Rank: 50, loss = 1.2069940567016602e-06
c621-122: Epoch: 0, Step: 172, Rank: 35, loss = 0.0002307891845703125
c621-152: Epoch: 0, Step: 172, Rank: 41, loss = 1.171875
c621-072: Epoch: 0, Step: 172, Rank: 25, loss = 1.6391277313232422e-07
c622-042: Epoch: 0, Step: 172, Rank: 51, loss = 8.487701416015625e-05
c621-141: Epoch: 0, Step: 172, Rank: 38, loss = 1.3589129821411916e-13
c621-052: Epoch: 0, Step: 172, Rank: 21, loss = 1.8533319234848022e-07
c622-031: Epoch: 0, Step: 172, Rank: 48, loss = 9.012222290039062e-05
c622-032: Epoch: 0, Step: 172, Rank: 49, loss = 0.00014019012451171875
c619-022: Epoch: 0, Step: 172, Rank: 17, loss = 2.831068712794149e-15
c613-151: Epoch: 0, Step: 172, Rank: 10, loss = 5.14984130859375e-05
c621-101: Epoch: 0, Step: 172, Rank: 30, loss = 0.0001087188720703125
c619-012: Epoch: 0, Step: 172, Rank: 15, loss = 4.231929779052734e-06
c621-092: Epoch: 0, Step: 172, Rank: 29, loss = 1.1874362826347351e-08
c619-041: Epoch: 0, Step: 172, Rank: 20, loss = 1.4227506535912076e-21
c619-002: Epoch: 0, Step: 172, Rank: 13, loss = 7.867813110351562e-06
c613-121: Epoch: 0, Step: 172, Rank: 4, loss = 1.2759119272232056e-07
c622-061: Epoch: 0, Step: 172, Rank: 54, loss = 7.566995918750763e-10
c621-061: Epoch: 0, Step: 172, Rank: 22, loss = 1.8189894035458565e-09
c619-032: Epoch: 0, Step: 172, Rank: 19, loss = 0.0001087188720703125
c621-062: Epoch: 0, Step: 172, Rank: 23, loss = 4.5917748078995606e-40
c613-132: Epoch: 0, Step: 172, Rank: 7, loss = 5.029141902923584e-07
c622-102: Epoch: 0, Step: 172, Rank: 63, loss = 0.01177978515625
c621-102: Epoch: 0, Step: 172, Rank: 31, loss = 0.00019073486328125
c613-142: Epoch: 0, Step: 172, Rank: 9, loss = 1.0788440704345703e-05
c622-021: Epoch: 0, Step: 172, Rank: 46, loss = 0.00058746337890625
c622-062: Epoch: 0, Step: 172, Rank: 55, loss = 8.754432201385498e-08
c613-122: Epoch: 0, Step: 172, Rank: 5, loss = 2.1047890186309814e-07
c621-112: Epoch: 0, Step: 172, Rank: 33, loss = 6.565414878423326e-12
c621-071: Epoch: 0, Step: 172, Rank: 24, loss = 4.4517219066619873e-07
c613-152: Epoch: 0, Step: 172, Rank: 11, loss = 0.0026397705078125
c622-081: Epoch: 0, Step: 172, Rank: 58, loss = 1.955777406692505e-08
c613-102: Epoch: 0, Step: 172, Rank: 1, loss = 0.69140625
c622-092: Epoch: 0, Step: 172, Rank: 61, loss = 0.69140625
c613-141: Epoch: 0, Step: 172, Rank: 8, loss = 5.14984130859375e-05
c613-111: Epoch: 0, Step: 172, Rank: 2, loss = 7.338821887969971e-07
c619-001: Epoch: 0, Step: 172, Rank: 12, loss = 1.895427703857422e-05
c621-082: Epoch: 0, Step: 172, Rank: 27, loss = 9.183549615799121e-41
c619-031: Epoch: 0, Step: 172, Rank: 18, loss = 1.3828277587890625e-05
c613-112: Epoch: 0, Step: 172, Rank: 3, loss = 1.7762184143066406e-05
c622-082: Epoch: 0, Step: 172, Rank: 59, loss = 0.00040435791015625
c613-131: Epoch: 0, Step: 172, Rank: 6, loss = 5.617039278149605e-09
c622-091: Epoch: 0, Step: 172, Rank: 60, loss = 3.725290298461914e-06
c622-071: Epoch: 0, Step: 172, Rank: 56, loss = 5.14984130859375e-05
c622-072: Epoch: 0, Step: 172, Rank: 57, loss = 0.004913330078125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 173, Rank: 16, loss = 3.7670135498046875e-05
c619-001: Epoch: 0, Step: 173, Rank: 12, loss = 0.0
c622-022: Epoch: 0, Step: 173, Rank: 47, loss = 0.00083160400390625
c622-031: Epoch: 0, Step: 173, Rank: 48, loss = 1.6391277313232422e-07
c622-002: Epoch: 0, Step: 173, Rank: 43, loss = 4.172325134277344e-07
c622-032: Epoch: 0, Step: 173, Rank: 49, loss = 8.585629984736443e-10
c622-081: Epoch: 0, Step: 173, Rank: 58, loss = 0.00014019012451171875
c619-002: Epoch: 0, Step: 173, Rank: 13, loss = 1.3709068298339844e-06
c613-101: Epoch: 0, Step: 173, Rank: 0, loss = 1.7762184143066406e-05
c619-031: Epoch: 0, Step: 173, Rank: 18, loss = 9.632110595703125e-05
c619-011: Epoch: 0, Step: 173, Rank: 14, loss = 1.0477378964424133e-08
c613-132: Epoch: 0, Step: 173, Rank: 7, loss = 4.3655745685100555e-09
c621-142: Epoch: 0, Step: 173, Rank: 39, loss = 1.895427703857422e-05
c621-151: Epoch: 0, Step: 173, Rank: 40, loss = 1.0089706847793423e-12
c622-012: Epoch: 0, Step: 173, Rank: 45, loss = 0.00020313262939453125
c622-041: Epoch: 0, Step: 173, Rank: 50, loss = 0.00021648406982421875
c619-022: Epoch: 0, Step: 173, Rank: 17, loss = 4.00543212890625e-05
c621-081: Epoch: 0, Step: 173, Rank: 26, loss = 3.123283386230469e-05
c621-132: Epoch: 0, Step: 173, Rank: 37, loss = 0.00040435791015625
c621-152: Epoch: 0, Step: 173, Rank: 41, loss = 3.017554874593445e-21
c619-012: Epoch: 0, Step: 173, Rank: 15, loss = 1.895427703857422e-05
c613-152: Epoch: 0, Step: 173, Rank: 11, loss = 0.00124359130859375
c621-111: Epoch: 0, Step: 173, Rank: 32, loss = 3.46451997756958e-07
c613-142: Epoch: 0, Step: 173, Rank: 9, loss = 6.628036499023438e-05
c613-121: Epoch: 0, Step: 173, Rank: 4, loss = 1.2759119272232056e-07
c621-082: Epoch: 0, Step: 173, Rank: 27, loss = 0.000148773193359375
c621-052: Epoch: 0, Step: 173, Rank: 21, loss = 2.8405338525772095e-08
c622-062: Epoch: 0, Step: 173, Rank: 55, loss = 0.69140625
c622-001: Epoch: 0, Step: 173, Rank: 42, loss = 0.003387451171875
c613-151: Epoch: 0, Step: 173, Rank: 10, loss = 0.0047607421875
c621-072: Epoch: 0, Step: 173, Rank: 25, loss = 0.0
c622-082: Epoch: 0, Step: 173, Rank: 59, loss = 0.000457763671875
c622-051: Epoch: 0, Step: 173, Rank: 52, loss = 0.0001087188720703125
c621-091: Epoch: 0, Step: 173, Rank: 28, loss = 0.00299072265625
c621-061: Epoch: 0, Step: 173, Rank: 22, loss = 2.8405338525772095e-08
c622-101: Epoch: 0, Step: 173, Rank: 62, loss = 1.0058283805847168e-06
c622-021: Epoch: 0, Step: 173, Rank: 46, loss = 2.4318695068359375e-05
c621-092: Epoch: 0, Step: 173, Rank: 29, loss = 0.000179290771484375
c622-102: Epoch: 0, Step: 173, Rank: 63, loss = 2.4318695068359375e-05
c621-122: Epoch: 0, Step: 173, Rank: 35, loss = 0.0
c619-041: Epoch: 0, Step: 173, Rank: 20, loss = 5.699694156646729e-07
c622-092: Epoch: 0, Step: 173, Rank: 61, loss = 1.2069940567016602e-06
c613-112: Epoch: 0, Step: 173, Rank: 3, loss = 0.0001316070556640625
c622-042: Epoch: 0, Step: 173, Rank: 51, loss = 2.2351741790771484e-07
c621-131: Epoch: 0, Step: 173, Rank: 36, loss = 2.8405338525772095e-08
c613-141: Epoch: 0, Step: 173, Rank: 8, loss = 2.5331974029541016e-07
c613-111: Epoch: 0, Step: 173, Rank: 2, loss = 3.213062882423401e-08
c622-052: Epoch: 0, Step: 173, Rank: 53, loss = 3.688037395477295e-07
c613-122: Epoch: 0, Step: 173, Rank: 5, loss = 1.4722347259521484e-05
c622-072: Epoch: 0, Step: 173, Rank: 57, loss = 5.0961971282958984e-06
c613-131: Epoch: 0, Step: 173, Rank: 6, loss = 2.8405338525772095e-08
c622-061: Epoch: 0, Step: 173, Rank: 54, loss = 0.00070953369140625
c622-011: Epoch: 0, Step: 173, Rank: 44, loss = 9.049472282640636e-11
c621-101: Epoch: 0, Step: 173, Rank: 30, loss = 1.953125
c621-071: Epoch: 0, Step: 173, Rank: 24, loss = 1.6689300537109375e-05
c621-141: Epoch: 0, Step: 173, Rank: 38, loss = 1.3828277587890625e-05
c621-102: Epoch: 0, Step: 173, Rank: 31, loss = 1.525040715932846e-08
c622-071: Epoch: 0, Step: 173, Rank: 56, loss = 1.6540288925170898e-06
c613-102: Epoch: 0, Step: 173, Rank: 1, loss = 1.150369644165039e-05
c621-062: Epoch: 0, Step: 173, Rank: 23, loss = 5.699694156646729e-07
c622-091: Epoch: 0, Step: 173, Rank: 60, loss = 0.0008544921875
c621-121: Epoch: 0, Step: 173, Rank: 34, loss = 0.00070953369140625
c621-112: Epoch: 0, Step: 173, Rank: 33, loss = 2.066371962428093e-09
c619-032: Epoch: 0, Step: 173, Rank: 19, loss = 5.43666137255308e-37
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 174, Rank: 16, loss = 1.55717134475708e-06
c619-002: Epoch: 0, Step: 174, Rank: 13, loss = 1.126900315284729e-07
c613-101: Epoch: 0, Step: 174, Rank: 0, loss = 6.628036499023438e-05
c622-002: Epoch: 0, Step: 174, Rank: 43, loss = 4.500150680541992e-06
c621-132: Epoch: 0, Step: 174, Rank: 37, loss = 0.0001087188720703125
c619-022: Epoch: 0, Step: 174, Rank: 17, loss = 4.3655745685100555e-09
c613-152: Epoch: 0, Step: 174, Rank: 11, loss = 1.0089706847793423e-12
c613-142: Epoch: 0, Step: 174, Rank: 9, loss = 0.00083160400390625
c619-011: Epoch: 0, Step: 174, Rank: 14, loss = 3.293156623840332e-06
c619-001: Epoch: 0, Step: 174, Rank: 12, loss = 0.00124359130859375
c619-031: Epoch: 0, Step: 174, Rank: 18, loss = 0.69140625
c613-151: Epoch: 0, Step: 174, Rank: 10, loss = 6.891787052154541e-07
c622-012: Epoch: 0, Step: 174, Rank: 45, loss = 1.2759119272232056e-07
c622-052: Epoch: 0, Step: 174, Rank: 53, loss = 1.996755599975586e-06
c613-121: Epoch: 0, Step: 174, Rank: 4, loss = 1.895427703857422e-05
c621-091: Epoch: 0, Step: 174, Rank: 28, loss = 1.895427703857422e-05
c621-082: Epoch: 0, Step: 174, Rank: 27, loss = 5.424022674560547e-06
c619-012: Epoch: 0, Step: 174, Rank: 15, loss = 4.231929779052734e-06
c622-081: Epoch: 0, Step: 174, Rank: 58, loss = 4.00543212890625e-05
c613-132: Epoch: 0, Step: 174, Rank: 7, loss = 0.0181884765625
c621-072: Epoch: 0, Step: 174, Rank: 25, loss = 6.891787052154541e-07
c621-151: Epoch: 0, Step: 174, Rank: 40, loss = 1.6689300537109375e-05
c613-141: Epoch: 0, Step: 174, Rank: 8, loss = 0.0001087188720703125
c622-031: Epoch: 0, Step: 174, Rank: 48, loss = 3.213062882423401e-08
c619-041: Epoch: 0, Step: 174, Rank: 20, loss = 1.2993812561035156e-05
c621-052: Epoch: 0, Step: 174, Rank: 21, loss = 3.213062882423401e-08
c621-081: Epoch: 0, Step: 174, Rank: 26, loss = 1.955777406692505e-08
c613-131: Epoch: 0, Step: 174, Rank: 6, loss = 0.00075531005859375
c621-061: Epoch: 0, Step: 174, Rank: 22, loss = 2.2649765014648438e-06
c613-122: Epoch: 0, Step: 174, Rank: 5, loss = 7.963180541992188e-05
c613-111: Epoch: 0, Step: 174, Rank: 2, loss = 3.293156623840332e-06
c621-121: Epoch: 0, Step: 174, Rank: 34, loss = 1.8758328224066645e-12
c622-001: Epoch: 0, Step: 174, Rank: 42, loss = 0.0025482177734375
c621-131: Epoch: 0, Step: 174, Rank: 36, loss = 0.0002307891845703125
c621-101: Epoch: 0, Step: 174, Rank: 30, loss = 4.00543212890625e-05
c613-112: Epoch: 0, Step: 174, Rank: 3, loss = 0.00014019012451171875
c622-051: Epoch: 0, Step: 174, Rank: 52, loss = 4.782137916322191e-25
c622-041: Epoch: 0, Step: 174, Rank: 50, loss = 5.0961971282958984e-06
c621-142: Epoch: 0, Step: 174, Rank: 39, loss = 6.891787052154541e-07
c621-122: Epoch: 0, Step: 174, Rank: 35, loss = 2.2649765014648438e-06
c613-102: Epoch: 0, Step: 174, Rank: 1, loss = 5.3085386753082275e-08
c622-011: Epoch: 0, Step: 174, Rank: 44, loss = 0.00010251998901367188
c622-101: Epoch: 0, Step: 174, Rank: 62, loss = 6.891787052154541e-07
c622-032: Epoch: 0, Step: 174, Rank: 49, loss = 0.002716064453125
c622-061: Epoch: 0, Step: 174, Rank: 54, loss = 0.000179290771484375
c622-092: Epoch: 0, Step: 174, Rank: 61, loss = 0.00014019012451171875
c621-102: Epoch: 0, Step: 174, Rank: 31, loss = 4.3655745685100555e-09
c621-112: Epoch: 0, Step: 174, Rank: 33, loss = 5.542110104105285e-23
c622-071: Epoch: 0, Step: 174, Rank: 56, loss = 0.00014019012451171875
c621-062: Epoch: 0, Step: 174, Rank: 23, loss = 9.918585419654846e-08
c619-032: Epoch: 0, Step: 174, Rank: 19, loss = 0.0
c622-022: Epoch: 0, Step: 174, Rank: 47, loss = 0.0
c622-082: Epoch: 0, Step: 174, Rank: 59, loss = 1.3828277587890625e-05
c621-092: Epoch: 0, Step: 174, Rank: 29, loss = 1.983707842718853e-32
c622-021: Epoch: 0, Step: 174, Rank: 46, loss = 3.213062882423401e-08
c622-102: Epoch: 0, Step: 174, Rank: 63, loss = 8.307397365570068e-07
c622-072: Epoch: 0, Step: 174, Rank: 57, loss = 0.036865234375
c622-042: Epoch: 0, Step: 174, Rank: 51, loss = 0.072265625
c622-062: Epoch: 0, Step: 174, Rank: 55, loss = 0.69140625
c621-071: Epoch: 0, Step: 174, Rank: 24, loss = 0.00058746337890625
c621-152: Epoch: 0, Step: 174, Rank: 41, loss = 0.0
c622-091: Epoch: 0, Step: 174, Rank: 60, loss = 2.562999725341797e-06
c621-141: Epoch: 0, Step: 174, Rank: 38, loss = 8.265194654219209e-40
c621-111: Epoch: 0, Step: 174, Rank: 32, loss = 0.0034942626953125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 175, Rank: 0, loss = 0.00070953369140625
c622-081: Epoch: 0, Step: 175, Rank: 58, loss = 6.891787052154541e-07
c613-152: Epoch: 0, Step: 175, Rank: 11, loss = 2.014636993408203e-05
c613-151: Epoch: 0, Step: 175, Rank: 10, loss = 0.69140625
c619-001: Epoch: 0, Step: 175, Rank: 12, loss = 6.314393452555578e-16
c621-101: Epoch: 0, Step: 175, Rank: 30, loss = 0.0002307891845703125
c622-002: Epoch: 0, Step: 175, Rank: 43, loss = 1.6540288925170898e-06
c613-112: Epoch: 0, Step: 175, Rank: 3, loss = 4.231929779052734e-06
c621-092: Epoch: 0, Step: 175, Rank: 29, loss = 6.891787052154541e-07
c619-021: Epoch: 0, Step: 175, Rank: 16, loss = 1.126900315284729e-07
c619-002: Epoch: 0, Step: 175, Rank: 13, loss = 8.404254913330078e-06
c621-102: Epoch: 0, Step: 175, Rank: 31, loss = 0.00058746337890625
c622-092: Epoch: 0, Step: 175, Rank: 61, loss = 1.3828277587890625e-05
c621-091: Epoch: 0, Step: 175, Rank: 28, loss = 0.000179290771484375
c621-082: Epoch: 0, Step: 175, Rank: 27, loss = 0.0007781982421875
c613-132: Epoch: 0, Step: 175, Rank: 7, loss = 7.420778274536133e-06
c622-052: Epoch: 0, Step: 175, Rank: 53, loss = 6.927791673660977e-13
c622-101: Epoch: 0, Step: 175, Rank: 62, loss = 1.955777406692505e-08
c621-151: Epoch: 0, Step: 175, Rank: 40, loss = 0.00083160400390625
c613-131: Epoch: 0, Step: 175, Rank: 6, loss = 0.000606536865234375
c621-132: Epoch: 0, Step: 175, Rank: 37, loss = 1.525040715932846e-08
c622-102: Epoch: 0, Step: 175, Rank: 63, loss = 9.424984455108643e-07
c613-142: Epoch: 0, Step: 175, Rank: 9, loss = 1.895427703857422e-05
c622-082: Epoch: 0, Step: 175, Rank: 59, loss = 4.231929779052734e-06
c622-071: Epoch: 0, Step: 175, Rank: 56, loss = 5.14984130859375e-05
c613-141: Epoch: 0, Step: 175, Rank: 8, loss = 6.3792168840089494e-21
c619-011: Epoch: 0, Step: 175, Rank: 14, loss = 3.528594970703125e-05
c622-012: Epoch: 0, Step: 175, Rank: 45, loss = 6.628036499023438e-05
c613-111: Epoch: 0, Step: 175, Rank: 2, loss = 0.00014019012451171875
c622-072: Epoch: 0, Step: 175, Rank: 57, loss = 4.267692565917969e-05
c622-001: Epoch: 0, Step: 175, Rank: 42, loss = 1.4722347259521484e-05
c621-141: Epoch: 0, Step: 175, Rank: 38, loss = 7.270385540061815e-33
c613-121: Epoch: 0, Step: 175, Rank: 4, loss = 1.7762184143066406e-05
c621-142: Epoch: 0, Step: 175, Rank: 39, loss = 0.0
c621-152: Epoch: 0, Step: 175, Rank: 41, loss = 3.9637088775634766e-06
c621-072: Epoch: 0, Step: 175, Rank: 25, loss = 0.000392913818359375
c622-062: Epoch: 0, Step: 175, Rank: 55, loss = 0.0002307891845703125
c622-051: Epoch: 0, Step: 175, Rank: 52, loss = 1.6689300537109375e-05
c622-061: Epoch: 0, Step: 175, Rank: 54, loss = 4.500150680541992e-06
c621-081: Epoch: 0, Step: 175, Rank: 26, loss = 1.150369644165039e-05
c621-131: Epoch: 0, Step: 175, Rank: 36, loss = 0.69140625
c622-091: Epoch: 0, Step: 175, Rank: 60, loss = 2.4318695068359375e-05
c619-012: Epoch: 0, Step: 175, Rank: 15, loss = 8.404254913330078e-06
c622-041: Epoch: 0, Step: 175, Rank: 50, loss = 0.0002956390380859375
c619-022: Epoch: 0, Step: 175, Rank: 17, loss = 5.424022674560547e-06
c622-042: Epoch: 0, Step: 175, Rank: 51, loss = 3.084540367126465e-06
c613-122: Epoch: 0, Step: 175, Rank: 5, loss = 5.029141902923584e-07
c622-031: Epoch: 0, Step: 175, Rank: 48, loss = 1.2759119272232056e-07
c613-102: Epoch: 0, Step: 175, Rank: 1, loss = 0.04443359375
c621-121: Epoch: 0, Step: 175, Rank: 34, loss = 1.2069940567016602e-06
c619-041: Epoch: 0, Step: 175, Rank: 20, loss = 3.0547380447387695e-07
c621-111: Epoch: 0, Step: 175, Rank: 32, loss = 8.149072527885437e-09
c619-032: Epoch: 0, Step: 175, Rank: 19, loss = 5.14984130859375e-05
c621-122: Epoch: 0, Step: 175, Rank: 35, loss = 4.500150680541992e-06
c621-061: Epoch: 0, Step: 175, Rank: 22, loss = 1.2069940567016602e-06
c619-031: Epoch: 0, Step: 175, Rank: 18, loss = 0.00083160400390625
c621-112: Epoch: 0, Step: 175, Rank: 33, loss = 8.149072527885437e-09
c622-011: Epoch: 0, Step: 175, Rank: 44, loss = 8.754432201385498e-08
c621-052: Epoch: 0, Step: 175, Rank: 21, loss = 0.00070953369140625
c622-032: Epoch: 0, Step: 175, Rank: 49, loss = 2.9325485229492188e-05
c622-022: Epoch: 0, Step: 175, Rank: 47, loss = 0.00299072265625
c621-062: Epoch: 0, Step: 175, Rank: 23, loss = 1.84297022087776e-14
c621-071: Epoch: 0, Step: 175, Rank: 24, loss = 1.3828277587890625e-05
c622-021: Epoch: 0, Step: 175, Rank: 46, loss = 3.583409124985337e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 176, Rank: 0, loss = 1.150369644165039e-05
c622-052: Epoch: 0, Step: 176, Rank: 53, loss = 2.5331974029541016e-07
c622-051: Epoch: 0, Step: 176, Rank: 52, loss = 7.566995918750763e-10
c622-002: Epoch: 0, Step: 176, Rank: 43, loss = 3.841705620288849e-09
c622-101: Epoch: 0, Step: 176, Rank: 62, loss = 8.940696716308594e-06
c619-002: Epoch: 0, Step: 176, Rank: 13, loss = 1.4811228820360823e-36
c622-012: Epoch: 0, Step: 176, Rank: 45, loss = 4.3655745685100555e-09
c622-081: Epoch: 0, Step: 176, Rank: 58, loss = 6.5267086029052734e-06
c619-001: Epoch: 0, Step: 176, Rank: 12, loss = 3.123283386230469e-05
c613-132: Epoch: 0, Step: 176, Rank: 7, loss = 4.6798959374427795e-08
c621-151: Epoch: 0, Step: 176, Rank: 40, loss = 6.07222318649292e-07
c613-151: Epoch: 0, Step: 176, Rank: 10, loss = 1.7497114868092467e-13
c622-041: Epoch: 0, Step: 176, Rank: 50, loss = 1.150369644165039e-05
c622-022: Epoch: 0, Step: 176, Rank: 47, loss = 2.648448571562767e-09
c622-042: Epoch: 0, Step: 176, Rank: 51, loss = 1.3709068298339844e-06
c619-021: Epoch: 0, Step: 176, Rank: 16, loss = 6.07222318649292e-07
c613-122: Epoch: 0, Step: 176, Rank: 5, loss = 1.1988913903810543e-32
c613-141: Epoch: 0, Step: 176, Rank: 8, loss = 8.940696716308594e-06
c622-031: Epoch: 0, Step: 176, Rank: 48, loss = 5.617039278149605e-09
c613-131: Epoch: 0, Step: 176, Rank: 6, loss = 8.754432201385498e-08
c622-032: Epoch: 0, Step: 176, Rank: 49, loss = 5.0961971282958984e-06
c619-031: Epoch: 0, Step: 176, Rank: 18, loss = 0.0186767578125
c613-152: Epoch: 0, Step: 176, Rank: 11, loss = 5.400124791776761e-13
c619-011: Epoch: 0, Step: 176, Rank: 14, loss = 1.166324663699769e-22
c613-121: Epoch: 0, Step: 176, Rank: 4, loss = 0.00433349609375
c622-062: Epoch: 0, Step: 176, Rank: 55, loss = 1.55717134475708e-06
c613-112: Epoch: 0, Step: 176, Rank: 3, loss = 5.502442945726216e-11
c622-001: Epoch: 0, Step: 176, Rank: 42, loss = 5.3085386753082275e-08
c621-142: Epoch: 0, Step: 176, Rank: 39, loss = 0.00040435791015625
c622-102: Epoch: 0, Step: 176, Rank: 63, loss = 0.00012302398681640625
c621-111: Epoch: 0, Step: 176, Rank: 32, loss = 1.0788440704345703e-05
c621-091: Epoch: 0, Step: 176, Rank: 28, loss = 7.338821887969971e-07
c621-152: Epoch: 0, Step: 176, Rank: 41, loss = 2.2649765014648438e-06
c613-111: Epoch: 0, Step: 176, Rank: 2, loss = 1.6391277313232422e-07
c622-072: Epoch: 0, Step: 176, Rank: 57, loss = 0.69140625
c621-121: Epoch: 0, Step: 176, Rank: 34, loss = 1.6916601452976465e-10
c621-132: Epoch: 0, Step: 176, Rank: 37, loss = 2.2851054382044822e-11
c622-011: Epoch: 0, Step: 176, Rank: 44, loss = 2.8405338525772095e-08
c613-142: Epoch: 0, Step: 176, Rank: 9, loss = 2.9325485229492188e-05
c621-112: Epoch: 0, Step: 176, Rank: 33, loss = 5.3085386753082275e-08
c622-021: Epoch: 0, Step: 176, Rank: 46, loss = 0.000431060791015625
c622-091: Epoch: 0, Step: 176, Rank: 60, loss = 0.00016880035400390625
c621-081: Epoch: 0, Step: 176, Rank: 26, loss = 3.841705620288849e-09
c613-102: Epoch: 0, Step: 176, Rank: 1, loss = 4.00543212890625e-05
c622-082: Epoch: 0, Step: 176, Rank: 59, loss = 0.000457763671875
c622-061: Epoch: 0, Step: 176, Rank: 54, loss = 0.00012302398681640625
c621-101: Epoch: 0, Step: 176, Rank: 30, loss = 8.404254913330078e-06
c621-072: Epoch: 0, Step: 176, Rank: 25, loss = 1.5366822481155396e-07
c622-071: Epoch: 0, Step: 176, Rank: 56, loss = 3.3921018924503508e-28
c619-022: Epoch: 0, Step: 176, Rank: 17, loss = 3.293156623840332e-06
c619-012: Epoch: 0, Step: 176, Rank: 15, loss = 4.05634636990726e-10
c621-102: Epoch: 0, Step: 176, Rank: 31, loss = 9.74978320300579e-10
c621-092: Epoch: 0, Step: 176, Rank: 29, loss = 3.213062882423401e-08
c621-131: Epoch: 0, Step: 176, Rank: 36, loss = 0.000148773193359375
c621-141: Epoch: 0, Step: 176, Rank: 38, loss = 1.8775463104248047e-06
c621-082: Epoch: 0, Step: 176, Rank: 27, loss = 8.149072527885437e-09
c622-092: Epoch: 0, Step: 176, Rank: 61, loss = 6.230038707144558e-11
c619-041: Epoch: 0, Step: 176, Rank: 20, loss = 1.955777406692505e-08
c621-061: Epoch: 0, Step: 176, Rank: 22, loss = 1.895427703857422e-05
c619-032: Epoch: 0, Step: 176, Rank: 19, loss = 2.276897430419922e-05
c621-122: Epoch: 0, Step: 176, Rank: 35, loss = 7.729977369308472e-08
c621-052: Epoch: 0, Step: 176, Rank: 21, loss = 3.5017728805541992e-06
c621-071: Epoch: 0, Step: 176, Rank: 24, loss = 5.3085386753082275e-08
c621-062: Epoch: 0, Step: 176, Rank: 23, loss = 4.1443854570388794e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 177, Rank: 43, loss = 5.43666137255308e-37
c621-132: Epoch: 0, Step: 177, Rank: 37, loss = 0.0001087188720703125
c622-012: Epoch: 0, Step: 177, Rank: 45, loss = 2.726912498474121e-06
c619-022: Epoch: 0, Step: 177, Rank: 17, loss = 3.123283386230469e-05
c621-111: Epoch: 0, Step: 177, Rank: 32, loss = 2.9976945370435715e-09
c622-052: Epoch: 0, Step: 177, Rank: 53, loss = 1.3828277587890625e-05
c619-021: Epoch: 0, Step: 177, Rank: 16, loss = 0.0002613067626953125
c619-012: Epoch: 0, Step: 177, Rank: 15, loss = 3.0547380447387695e-07
c613-101: Epoch: 0, Step: 177, Rank: 0, loss = 8.940696716308594e-06
c619-011: Epoch: 0, Step: 177, Rank: 14, loss = 9.918585419654846e-08
c613-152: Epoch: 0, Step: 177, Rank: 11, loss = 0.0
c622-001: Epoch: 0, Step: 177, Rank: 42, loss = 1.8775463104248047e-06
c619-001: Epoch: 0, Step: 177, Rank: 12, loss = 0.0003147125244140625
c619-002: Epoch: 0, Step: 177, Rank: 13, loss = 0.0
c622-011: Epoch: 0, Step: 177, Rank: 44, loss = 2.276897430419922e-05
c621-142: Epoch: 0, Step: 177, Rank: 39, loss = 8.881784197001252e-13
c621-082: Epoch: 0, Step: 177, Rank: 27, loss = 0.00099945068359375
c621-081: Epoch: 0, Step: 177, Rank: 26, loss = 6.007030606269836e-08
c622-042: Epoch: 0, Step: 177, Rank: 51, loss = 4.5299530029296875e-05
c622-051: Epoch: 0, Step: 177, Rank: 52, loss = 3.635980405647388e-15
c621-121: Epoch: 0, Step: 177, Rank: 34, loss = 0.69140625
c619-041: Epoch: 0, Step: 177, Rank: 20, loss = 1.4915713109076023e-10
c621-152: Epoch: 0, Step: 177, Rank: 41, loss = 1.84297022087776e-14
c621-112: Epoch: 0, Step: 177, Rank: 33, loss = 1.3709068298339844e-06
c622-031: Epoch: 0, Step: 177, Rank: 48, loss = 0.0
c619-031: Epoch: 0, Step: 177, Rank: 18, loss = 1.3597309589385986e-07
c622-041: Epoch: 0, Step: 177, Rank: 50, loss = 4.6798959374427795e-08
c622-032: Epoch: 0, Step: 177, Rank: 49, loss = 0.0
c622-022: Epoch: 0, Step: 177, Rank: 47, loss = 2.276897430419922e-05
c619-032: Epoch: 0, Step: 177, Rank: 19, loss = 5.893525667488575e-10
c621-131: Epoch: 0, Step: 177, Rank: 36, loss = 2.5331974029541016e-07
c613-151: Epoch: 0, Step: 177, Rank: 10, loss = 5.14984130859375e-05
c621-102: Epoch: 0, Step: 177, Rank: 31, loss = 5.781650543212891e-06
c621-052: Epoch: 0, Step: 177, Rank: 21, loss = 1.1641532182693481e-10
c621-141: Epoch: 0, Step: 177, Rank: 38, loss = 2.1047890186309814e-07
c621-151: Epoch: 0, Step: 177, Rank: 40, loss = 4.05634636990726e-10
c621-092: Epoch: 0, Step: 177, Rank: 29, loss = 0.0003566741943359375
c622-021: Epoch: 0, Step: 177, Rank: 46, loss = 0.00193023681640625
c621-122: Epoch: 0, Step: 177, Rank: 35, loss = 2.2118911147117615e-08
c621-091: Epoch: 0, Step: 177, Rank: 28, loss = 1.8775463104248047e-06
c613-111: Epoch: 0, Step: 177, Rank: 2, loss = 2.1736923372372985e-10
c613-132: Epoch: 0, Step: 177, Rank: 7, loss = 2.2649765014648438e-06
c621-101: Epoch: 0, Step: 177, Rank: 30, loss = 5.424022674560547e-06
c622-102: Epoch: 0, Step: 177, Rank: 63, loss = 3.46451997756958e-07
c622-061: Epoch: 0, Step: 177, Rank: 54, loss = 3.583409124985337e-10
c613-121: Epoch: 0, Step: 177, Rank: 4, loss = 0.0181884765625
c613-141: Epoch: 0, Step: 177, Rank: 8, loss = 4.472333961502706e-19
c622-081: Epoch: 0, Step: 177, Rank: 58, loss = 3.293156623840332e-06
c613-112: Epoch: 0, Step: 177, Rank: 3, loss = 4.843059286940843e-11
c621-061: Epoch: 0, Step: 177, Rank: 22, loss = 1.6079866327345371e-09
c613-102: Epoch: 0, Step: 177, Rank: 1, loss = 1.150369644165039e-05
c621-072: Epoch: 0, Step: 177, Rank: 25, loss = 8.487701416015625e-05
c622-101: Epoch: 0, Step: 177, Rank: 62, loss = 1.0972125985553305e-16
c613-131: Epoch: 0, Step: 177, Rank: 6, loss = 1.7848833522293717e-11
c622-092: Epoch: 0, Step: 177, Rank: 61, loss = 4.602043190971017e-10
c613-142: Epoch: 0, Step: 177, Rank: 9, loss = 6.845220923423767e-08
c621-071: Epoch: 0, Step: 177, Rank: 24, loss = 6.198883056640625e-05
c622-062: Epoch: 0, Step: 177, Rank: 55, loss = 0.000606536865234375
c613-122: Epoch: 0, Step: 177, Rank: 5, loss = 0.69140625
c622-071: Epoch: 0, Step: 177, Rank: 56, loss = 0.004913330078125
c622-072: Epoch: 0, Step: 177, Rank: 57, loss = 9.255018085241318e-09
c622-082: Epoch: 0, Step: 177, Rank: 59, loss = 9.255018085241318e-09
c621-062: Epoch: 0, Step: 177, Rank: 23, loss = 5.424022674560547e-06
c622-091: Epoch: 0, Step: 177, Rank: 60, loss = 1.4915713109076023e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.7548828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 178, Rank: 43, loss = 1.3597309589385986e-07
c621-132: Epoch: 0, Step: 178, Rank: 37, loss = 2.8405338525772095e-08
c621-142: Epoch: 0, Step: 178, Rank: 39, loss = 8.149072527885437e-09
c622-081: Epoch: 0, Step: 178, Rank: 58, loss = 1.418811734765768e-09
c613-101: Epoch: 0, Step: 178, Rank: 0, loss = 0.00811767578125
c621-151: Epoch: 0, Step: 178, Rank: 40, loss = 2.276897430419922e-05
c622-062: Epoch: 0, Step: 178, Rank: 55, loss = 0.69140625
c622-001: Epoch: 0, Step: 178, Rank: 42, loss = 1.2069940567016602e-06
c621-111: Epoch: 0, Step: 178, Rank: 32, loss = 1.0058283805847168e-06
c621-091: Epoch: 0, Step: 178, Rank: 28, loss = 5.14984130859375e-05
c621-081: Epoch: 0, Step: 178, Rank: 26, loss = 3.688037395477295e-07
c619-002: Epoch: 0, Step: 178, Rank: 13, loss = 2.8405338525772095e-08
c619-001: Epoch: 0, Step: 178, Rank: 12, loss = 1.55717134475708e-06
c622-052: Epoch: 0, Step: 178, Rank: 53, loss = 5.424022674560547e-06
c621-131: Epoch: 0, Step: 178, Rank: 36, loss = 6.693881005048752e-10
c621-121: Epoch: 0, Step: 178, Rank: 34, loss = 1.895427703857422e-05
c621-152: Epoch: 0, Step: 178, Rank: 41, loss = 1.5366822481155396e-07
c622-012: Epoch: 0, Step: 178, Rank: 45, loss = 5.14984130859375e-05
c619-021: Epoch: 0, Step: 178, Rank: 16, loss = 0.0001087188720703125
c621-101: Epoch: 0, Step: 178, Rank: 30, loss = 0.000179290771484375
c613-111: Epoch: 0, Step: 178, Rank: 2, loss = 0.0
c621-112: Epoch: 0, Step: 178, Rank: 33, loss = 4.00543212890625e-05
c622-092: Epoch: 0, Step: 178, Rank: 61, loss = 1.6391277313232422e-07
c622-071: Epoch: 0, Step: 178, Rank: 56, loss = 2.066371962428093e-09
c613-121: Epoch: 0, Step: 178, Rank: 4, loss = 3.688037395477295e-07
c622-072: Epoch: 0, Step: 178, Rank: 57, loss = 7.420778274536133e-06
c613-152: Epoch: 0, Step: 178, Rank: 11, loss = 1.955777406692505e-08
c622-101: Epoch: 0, Step: 178, Rank: 62, loss = 1.418811734765768e-09
c619-012: Epoch: 0, Step: 178, Rank: 15, loss = 1.3445969671010971e-08
c619-011: Epoch: 0, Step: 178, Rank: 14, loss = 0.0002307891845703125
c622-061: Epoch: 0, Step: 178, Rank: 54, loss = 2.7008354663848877e-07
c622-021: Epoch: 0, Step: 178, Rank: 46, loss = 6.07222318649292e-07
c613-112: Epoch: 0, Step: 178, Rank: 3, loss = 5.029141902923584e-07
c621-052: Epoch: 0, Step: 178, Rank: 21, loss = 1.0058283805847168e-06
c613-132: Epoch: 0, Step: 178, Rank: 7, loss = 5.3085386753082275e-08
c619-022: Epoch: 0, Step: 178, Rank: 17, loss = 1.1399388313293457e-06
c622-022: Epoch: 0, Step: 178, Rank: 47, loss = 1.6391277313232422e-07
c613-102: Epoch: 0, Step: 178, Rank: 1, loss = 0.0
c613-151: Epoch: 0, Step: 178, Rank: 10, loss = 1.126900315284729e-07
c622-051: Epoch: 0, Step: 178, Rank: 52, loss = 1.3597309589385986e-07
c621-122: Epoch: 0, Step: 178, Rank: 35, loss = 0.0
c621-102: Epoch: 0, Step: 178, Rank: 31, loss = 2.648448571562767e-09
c622-032: Epoch: 0, Step: 178, Rank: 49, loss = 4.4517219066619873e-07
c613-141: Epoch: 0, Step: 178, Rank: 8, loss = 2.1047890186309814e-07
c621-092: Epoch: 0, Step: 178, Rank: 29, loss = 3.213062882423401e-08
c621-071: Epoch: 0, Step: 178, Rank: 24, loss = 0.000179290771484375
c622-102: Epoch: 0, Step: 178, Rank: 63, loss = 0.00408935546875
c622-041: Epoch: 0, Step: 178, Rank: 50, loss = 0.00164794921875
c622-091: Epoch: 0, Step: 178, Rank: 60, loss = 0.69140625
c621-072: Epoch: 0, Step: 178, Rank: 25, loss = 0.0002613067626953125
c619-032: Epoch: 0, Step: 178, Rank: 19, loss = 0.00040435791015625
c621-062: Epoch: 0, Step: 178, Rank: 23, loss = 1.1399388313293457e-06
c621-141: Epoch: 0, Step: 178, Rank: 38, loss = 1.4637180356658064e-12
c613-142: Epoch: 0, Step: 178, Rank: 9, loss = 1.199040866595169e-13
c621-061: Epoch: 0, Step: 178, Rank: 22, loss = 5.115907697472721e-12
c619-041: Epoch: 0, Step: 178, Rank: 20, loss = 1.418811734765768e-09
c622-031: Epoch: 0, Step: 178, Rank: 48, loss = 2.2649765014648438e-06
c622-011: Epoch: 0, Step: 178, Rank: 44, loss = 2.905726432800293e-06
c621-082: Epoch: 0, Step: 178, Rank: 27, loss = 5.3085386753082275e-08
c619-031: Epoch: 0, Step: 178, Rank: 18, loss = 1.6689300537109375e-05
c613-131: Epoch: 0, Step: 178, Rank: 6, loss = 2.1457672119140625e-05
c622-042: Epoch: 0, Step: 178, Rank: 51, loss = 7.338821887969971e-07
c622-082: Epoch: 0, Step: 178, Rank: 59, loss = 0.0
c613-122: Epoch: 0, Step: 178, Rank: 5, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.98s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.98s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 179, Rank: 58, loss = 2.514570951461792e-08
c613-101: Epoch: 0, Step: 179, Rank: 0, loss = 2.014636993408203e-05
c622-062: Epoch: 0, Step: 179, Rank: 55, loss = 6.845220923423767e-08
c622-092: Epoch: 0, Step: 179, Rank: 61, loss = 4.13994203059987e-27
c622-002: Epoch: 0, Step: 179, Rank: 43, loss = 6.973743438720703e-06
c622-052: Epoch: 0, Step: 179, Rank: 53, loss = 4.798173904418945e-06
c622-101: Epoch: 0, Step: 179, Rank: 62, loss = 1.126900315284729e-07
c622-072: Epoch: 0, Step: 179, Rank: 57, loss = 0.0
c622-051: Epoch: 0, Step: 179, Rank: 52, loss = 0.0
c619-002: Epoch: 0, Step: 179, Rank: 13, loss = 7.729977369308472e-08
c622-061: Epoch: 0, Step: 179, Rank: 54, loss = 2.5920599000528455e-11
c619-011: Epoch: 0, Step: 179, Rank: 14, loss = 0.69140625
c622-031: Epoch: 0, Step: 179, Rank: 48, loss = 2.1047890186309814e-07
c622-022: Epoch: 0, Step: 179, Rank: 47, loss = 6.973743438720703e-06
c621-081: Epoch: 0, Step: 179, Rank: 26, loss = 1.4915713109076023e-10
c621-111: Epoch: 0, Step: 179, Rank: 32, loss = 0.0
c621-121: Epoch: 0, Step: 179, Rank: 34, loss = 1.2218952178955078e-05
c621-132: Epoch: 0, Step: 179, Rank: 37, loss = 0.0
c622-012: Epoch: 0, Step: 179, Rank: 45, loss = 5.3085386753082275e-08
c622-102: Epoch: 0, Step: 179, Rank: 63, loss = 2.2118911147117615e-08
c621-122: Epoch: 0, Step: 179, Rank: 35, loss = 3.213062882423401e-08
c619-032: Epoch: 0, Step: 179, Rank: 19, loss = 9.918585419654846e-08
c619-041: Epoch: 0, Step: 179, Rank: 20, loss = 0.0
c613-102: Epoch: 0, Step: 179, Rank: 1, loss = 2.0236257114447653e-11
c613-132: Epoch: 0, Step: 179, Rank: 7, loss = 2.5331974029541016e-07
c619-001: Epoch: 0, Step: 179, Rank: 12, loss = 0.69140625
c622-082: Epoch: 0, Step: 179, Rank: 59, loss = 4.4517219066619873e-07
c621-052: Epoch: 0, Step: 179, Rank: 21, loss = 1.996755599975586e-06
c613-111: Epoch: 0, Step: 179, Rank: 2, loss = 5.617039278149605e-09
c613-131: Epoch: 0, Step: 179, Rank: 6, loss = 2.753734588623047e-05
c613-122: Epoch: 0, Step: 179, Rank: 5, loss = 1.857925203976527e-26
c621-061: Epoch: 0, Step: 179, Rank: 22, loss = 3.293156623840332e-06
c613-142: Epoch: 0, Step: 179, Rank: 9, loss = 0.0002956390380859375
c621-112: Epoch: 0, Step: 179, Rank: 33, loss = 1.0788440704345703e-05
c621-131: Epoch: 0, Step: 179, Rank: 36, loss = 7.009506225585938e-05
c613-121: Epoch: 0, Step: 179, Rank: 4, loss = 5.893525667488575e-10
c621-082: Epoch: 0, Step: 179, Rank: 27, loss = 3.213062882423401e-08
c622-091: Epoch: 0, Step: 179, Rank: 60, loss = 0.022216796875
c622-071: Epoch: 0, Step: 179, Rank: 56, loss = 3.688037395477295e-07
c613-151: Epoch: 0, Step: 179, Rank: 10, loss = 3.293156623840332e-06
c613-152: Epoch: 0, Step: 179, Rank: 11, loss = 5.893525667488575e-10
c622-032: Epoch: 0, Step: 179, Rank: 49, loss = 2.753734588623047e-05
c613-112: Epoch: 0, Step: 179, Rank: 3, loss = 2.5920599000528455e-11
c621-151: Epoch: 0, Step: 179, Rank: 40, loss = 2.562999725341797e-06
c622-042: Epoch: 0, Step: 179, Rank: 51, loss = 2.2649765014648438e-06
c621-072: Epoch: 0, Step: 179, Rank: 25, loss = 4.4517219066619873e-07
c613-141: Epoch: 0, Step: 179, Rank: 8, loss = 1.150369644165039e-05
c619-012: Epoch: 0, Step: 179, Rank: 15, loss = 0.0003681182861328125
c621-152: Epoch: 0, Step: 179, Rank: 41, loss = 0.01104736328125
c621-141: Epoch: 0, Step: 179, Rank: 38, loss = 5.0961971282958984e-06
c621-091: Epoch: 0, Step: 179, Rank: 28, loss = 3.583409124985337e-10
c622-041: Epoch: 0, Step: 179, Rank: 50, loss = 9.74978320300579e-10
c621-142: Epoch: 0, Step: 179, Rank: 39, loss = 9.870390964984584e-34
c622-001: Epoch: 0, Step: 179, Rank: 42, loss = 8.754432201385498e-08
c619-021: Epoch: 0, Step: 179, Rank: 16, loss = 4.231929779052734e-06
c619-031: Epoch: 0, Step: 179, Rank: 18, loss = 6.693881005048752e-10
c622-021: Epoch: 0, Step: 179, Rank: 46, loss = 6.314393452555578e-16
c621-101: Epoch: 0, Step: 179, Rank: 30, loss = 2.753734588623047e-05
c621-092: Epoch: 0, Step: 179, Rank: 29, loss = 0.000278472900390625
c621-102: Epoch: 0, Step: 179, Rank: 31, loss = 3.0547380447387695e-07
c619-022: Epoch: 0, Step: 179, Rank: 17, loss = 5.182486384480711e-17
c622-011: Epoch: 0, Step: 179, Rank: 44, loss = 0.00083160400390625
c621-062: Epoch: 0, Step: 179, Rank: 23, loss = 1.6391277313232422e-07
c621-071: Epoch: 0, Step: 179, Rank: 24, loss = 9.012222290039062e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 180, Rank: 0, loss = 1.126900315284729e-07
c622-101: Epoch: 0, Step: 180, Rank: 62, loss = 7.729977369308472e-08
c613-132: Epoch: 0, Step: 180, Rank: 7, loss = 5.542110104105285e-23
c622-092: Epoch: 0, Step: 180, Rank: 61, loss = 7.188646122813225e-09
c613-141: Epoch: 0, Step: 180, Rank: 8, loss = 0.0
c613-131: Epoch: 0, Step: 180, Rank: 6, loss = 2.066371962428093e-09
c622-082: Epoch: 0, Step: 180, Rank: 59, loss = 0.0
c622-102: Epoch: 0, Step: 180, Rank: 63, loss = 7.987216665362745e-30
c613-121: Epoch: 0, Step: 180, Rank: 4, loss = 9.012222290039062e-05
c613-122: Epoch: 0, Step: 180, Rank: 5, loss = 0.00020313262939453125
c622-012: Epoch: 0, Step: 180, Rank: 45, loss = 0.69140625
c621-151: Epoch: 0, Step: 180, Rank: 40, loss = 2.671875
c621-132: Epoch: 0, Step: 180, Rank: 37, loss = 1.5735626220703125e-05
c619-021: Epoch: 0, Step: 180, Rank: 16, loss = 4.5299530029296875e-05
c613-112: Epoch: 0, Step: 180, Rank: 3, loss = 2.7830537874251604e-10
c622-002: Epoch: 0, Step: 180, Rank: 43, loss = 6.5635692504717e-31
c621-152: Epoch: 0, Step: 180, Rank: 41, loss = 1.6689300537109375e-05
c622-061: Epoch: 0, Step: 180, Rank: 54, loss = 7.420778274536133e-06
c622-052: Epoch: 0, Step: 180, Rank: 53, loss = 1.150369644165039e-05
c621-111: Epoch: 0, Step: 180, Rank: 32, loss = 1.9081958235744878e-17
c613-111: Epoch: 0, Step: 180, Rank: 2, loss = 4.231929779052734e-06
c621-142: Epoch: 0, Step: 180, Rank: 39, loss = 9.012222290039062e-05
c622-081: Epoch: 0, Step: 180, Rank: 58, loss = 1.3597309589385986e-07
c613-151: Epoch: 0, Step: 180, Rank: 10, loss = 1.1874362826347351e-08
c622-091: Epoch: 0, Step: 180, Rank: 60, loss = 0.69140625
c622-001: Epoch: 0, Step: 180, Rank: 42, loss = 0.0
c613-102: Epoch: 0, Step: 180, Rank: 1, loss = 9.255018085241318e-09
c622-022: Epoch: 0, Step: 180, Rank: 47, loss = 2.7284841053187847e-12
c621-131: Epoch: 0, Step: 180, Rank: 36, loss = 2.9325485229492188e-05
c622-072: Epoch: 0, Step: 180, Rank: 57, loss = 0.00010251998901367188
c621-112: Epoch: 0, Step: 180, Rank: 33, loss = 3.084540367126465e-06
c622-051: Epoch: 0, Step: 180, Rank: 52, loss = 0.0
c613-152: Epoch: 0, Step: 180, Rank: 11, loss = 3.655441105365753e-08
c619-012: Epoch: 0, Step: 180, Rank: 15, loss = 1.7229467630386353e-08
c621-121: Epoch: 0, Step: 180, Rank: 34, loss = 7.188646122813225e-09
c619-011: Epoch: 0, Step: 180, Rank: 14, loss = 4.231929779052734e-06
c619-001: Epoch: 0, Step: 180, Rank: 12, loss = 3.123283386230469e-05
c622-031: Epoch: 0, Step: 180, Rank: 48, loss = 3.123283386230469e-05
c619-002: Epoch: 0, Step: 180, Rank: 13, loss = 6.973743438720703e-06
c613-142: Epoch: 0, Step: 180, Rank: 9, loss = 2.2351741790771484e-07
c622-011: Epoch: 0, Step: 180, Rank: 44, loss = 6.007030606269836e-08
c622-062: Epoch: 0, Step: 180, Rank: 55, loss = 3.213062882423401e-08
c622-041: Epoch: 0, Step: 180, Rank: 50, loss = 0.000457763671875
c621-122: Epoch: 0, Step: 180, Rank: 35, loss = 1.6540288925170898e-06
c622-071: Epoch: 0, Step: 180, Rank: 56, loss = 2.1047890186309814e-07
c622-032: Epoch: 0, Step: 180, Rank: 49, loss = 0.0
c619-031: Epoch: 0, Step: 180, Rank: 18, loss = 1.0058283805847168e-06
c621-102: Epoch: 0, Step: 180, Rank: 31, loss = 7.009506225585938e-05
c622-021: Epoch: 0, Step: 180, Rank: 46, loss = 2.9331204132176936e-11
c622-042: Epoch: 0, Step: 180, Rank: 51, loss = 4.94765117764473e-09
c621-101: Epoch: 0, Step: 180, Rank: 30, loss = 2.9976945370435715e-09
c621-141: Epoch: 0, Step: 180, Rank: 38, loss = 1.6391277313232422e-07
c621-081: Epoch: 0, Step: 180, Rank: 26, loss = 2.726912498474121e-06
c621-082: Epoch: 0, Step: 180, Rank: 27, loss = 3.7670135498046875e-05
c621-092: Epoch: 0, Step: 180, Rank: 29, loss = 6.139278411865234e-06
c621-091: Epoch: 0, Step: 180, Rank: 28, loss = 1.1641532182693481e-10
c621-072: Epoch: 0, Step: 180, Rank: 25, loss = 8.754432201385498e-08
c619-032: Epoch: 0, Step: 180, Rank: 19, loss = 3.841705620288849e-09
c619-022: Epoch: 0, Step: 180, Rank: 17, loss = 2.905726432800293e-06
c621-071: Epoch: 0, Step: 180, Rank: 24, loss = 2.831068712794149e-15
c621-062: Epoch: 0, Step: 180, Rank: 23, loss = 5.029141902923584e-07
c621-061: Epoch: 0, Step: 180, Rank: 22, loss = 7.867813110351562e-06
c621-052: Epoch: 0, Step: 180, Rank: 21, loss = 1.2993812561035156e-05
c619-041: Epoch: 0, Step: 180, Rank: 20, loss = 2.5331974029541016e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 181, Rank: 0, loss = 0.00014019012451171875
c622-002: Epoch: 0, Step: 181, Rank: 43, loss = 2.9325485229492188e-05
c621-132: Epoch: 0, Step: 181, Rank: 37, loss = 0.000667572021484375
c621-111: Epoch: 0, Step: 181, Rank: 32, loss = 9.918585419654846e-08
c622-052: Epoch: 0, Step: 181, Rank: 53, loss = 0.0
c621-072: Epoch: 0, Step: 181, Rank: 25, loss = 2.276897430419922e-05
c621-082: Epoch: 0, Step: 181, Rank: 27, loss = 0.000911712646484375
c621-091: Epoch: 0, Step: 181, Rank: 28, loss = 0.0
c621-121: Epoch: 0, Step: 181, Rank: 34, loss = 8.487701416015625e-05
c622-081: Epoch: 0, Step: 181, Rank: 58, loss = 2.342858351767063e-09
c621-081: Epoch: 0, Step: 181, Rank: 26, loss = 8.307397365570068e-07
c621-142: Epoch: 0, Step: 181, Rank: 39, loss = 0.00040435791015625
c622-012: Epoch: 0, Step: 181, Rank: 45, loss = 3.841705620288849e-09
c622-011: Epoch: 0, Step: 181, Rank: 44, loss = 3.123283386230469e-05
c622-022: Epoch: 0, Step: 181, Rank: 47, loss = 0.0002613067626953125
c622-102: Epoch: 0, Step: 181, Rank: 63, loss = 0.00058746337890625
c622-051: Epoch: 0, Step: 181, Rank: 52, loss = 0.0380859375
c622-071: Epoch: 0, Step: 181, Rank: 56, loss = 4.4517219066619873e-07
c622-062: Epoch: 0, Step: 181, Rank: 55, loss = 4.94765117764473e-09
c621-112: Epoch: 0, Step: 181, Rank: 33, loss = 0.0
c619-001: Epoch: 0, Step: 181, Rank: 12, loss = 8.307397365570068e-07
c622-001: Epoch: 0, Step: 181, Rank: 42, loss = 1.895427703857422e-05
c619-021: Epoch: 0, Step: 181, Rank: 16, loss = 0.0002956390380859375
c622-101: Epoch: 0, Step: 181, Rank: 62, loss = 1.1399388313293457e-06
c621-131: Epoch: 0, Step: 181, Rank: 36, loss = 2.7008354663848877e-07
c621-061: Epoch: 0, Step: 181, Rank: 22, loss = 3.293156623840332e-06
c622-072: Epoch: 0, Step: 181, Rank: 57, loss = 8.585629984736443e-10
c613-152: Epoch: 0, Step: 181, Rank: 11, loss = 1.955777406692505e-08
c621-052: Epoch: 0, Step: 181, Rank: 21, loss = 0.0
c619-002: Epoch: 0, Step: 181, Rank: 13, loss = 8.003553375601768e-11
c622-061: Epoch: 0, Step: 181, Rank: 54, loss = 6.693881005048752e-10
c613-121: Epoch: 0, Step: 181, Rank: 4, loss = 3.725290298461914e-06
c613-131: Epoch: 0, Step: 181, Rank: 6, loss = 0.00075531005859375
c621-151: Epoch: 0, Step: 181, Rank: 40, loss = 1.996755599975586e-06
c622-042: Epoch: 0, Step: 181, Rank: 51, loss = 8.404254913330078e-06
c619-032: Epoch: 0, Step: 181, Rank: 19, loss = 3.293156623840332e-06
c622-031: Epoch: 0, Step: 181, Rank: 48, loss = 0.0
c621-122: Epoch: 0, Step: 181, Rank: 35, loss = 6.973743438720703e-06
c621-071: Epoch: 0, Step: 181, Rank: 24, loss = 7.188646122813225e-09
c613-151: Epoch: 0, Step: 181, Rank: 10, loss = 1.1059455573558807e-09
c621-101: Epoch: 0, Step: 181, Rank: 30, loss = 6.973743438720703e-06
c619-011: Epoch: 0, Step: 181, Rank: 14, loss = 3.213062882423401e-08
c622-082: Epoch: 0, Step: 181, Rank: 59, loss = 0.0
c619-022: Epoch: 0, Step: 181, Rank: 17, loss = 0.0
c622-032: Epoch: 0, Step: 181, Rank: 49, loss = 3.084540367126465e-06
c621-062: Epoch: 0, Step: 181, Rank: 23, loss = 0.69140625
c622-091: Epoch: 0, Step: 181, Rank: 60, loss = 8.754432201385498e-08
c621-102: Epoch: 0, Step: 181, Rank: 31, loss = 6.891787052154541e-07
c622-041: Epoch: 0, Step: 181, Rank: 50, loss = 6.439293542825908e-14
c613-112: Epoch: 0, Step: 181, Rank: 3, loss = 1.3597309589385986e-07
c613-132: Epoch: 0, Step: 181, Rank: 7, loss = 3.7670135498046875e-05
c613-111: Epoch: 0, Step: 181, Rank: 2, loss = 4.4517219066619873e-07
c622-021: Epoch: 0, Step: 181, Rank: 46, loss = 2.2649765014648438e-06
c619-041: Epoch: 0, Step: 181, Rank: 20, loss = 2.455635694786906e-10
c619-012: Epoch: 0, Step: 181, Rank: 15, loss = 1.0132789611816406e-05
c613-141: Epoch: 0, Step: 181, Rank: 8, loss = 0.000606536865234375
c621-092: Epoch: 0, Step: 181, Rank: 29, loss = 9.74978320300579e-10
c621-152: Epoch: 0, Step: 181, Rank: 41, loss = 1.1874362826347351e-08
c621-141: Epoch: 0, Step: 181, Rank: 38, loss = 7.420778274536133e-06
c613-102: Epoch: 0, Step: 181, Rank: 1, loss = 6.056285572868247e-20
c622-092: Epoch: 0, Step: 181, Rank: 61, loss = 2.562999725341797e-06
c613-142: Epoch: 0, Step: 181, Rank: 9, loss = 1.3709068298339844e-06
c613-122: Epoch: 0, Step: 181, Rank: 5, loss = 0.0
c619-031: Epoch: 0, Step: 181, Rank: 18, loss = 7.194411455615628e-28
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 182, Rank: 0, loss = 2.648448571562767e-09
c622-092: Epoch: 0, Step: 182, Rank: 61, loss = 0.0
c622-002: Epoch: 0, Step: 182, Rank: 43, loss = 8.404254913330078e-06
c622-101: Epoch: 0, Step: 182, Rank: 62, loss = 6.462348535570529e-26
c622-081: Epoch: 0, Step: 182, Rank: 58, loss = 1.166324663699769e-22
c619-021: Epoch: 0, Step: 182, Rank: 16, loss = 4.5917748078995606e-40
c622-071: Epoch: 0, Step: 182, Rank: 56, loss = 7.963180541992188e-05
c613-111: Epoch: 0, Step: 182, Rank: 2, loss = 0.0038299560546875
c619-002: Epoch: 0, Step: 182, Rank: 13, loss = 5.3085386753082275e-08
c613-131: Epoch: 0, Step: 182, Rank: 6, loss = 1.2759119272232056e-07
c621-081: Epoch: 0, Step: 182, Rank: 26, loss = 8.149072527885437e-09
c622-012: Epoch: 0, Step: 182, Rank: 45, loss = 2.7284841053187847e-12
c621-111: Epoch: 0, Step: 182, Rank: 32, loss = 0.0
c613-132: Epoch: 0, Step: 182, Rank: 7, loss = 6.628036499023438e-05
c613-121: Epoch: 0, Step: 182, Rank: 4, loss = 6.344635039567947e-09
c619-011: Epoch: 0, Step: 182, Rank: 14, loss = 6.007030606269836e-08
c621-132: Epoch: 0, Step: 182, Rank: 37, loss = 7.194411455615628e-28
c622-052: Epoch: 0, Step: 182, Rank: 53, loss = 2.5331974029541016e-07
c613-141: Epoch: 0, Step: 182, Rank: 8, loss = 2.905726432800293e-06
c613-102: Epoch: 0, Step: 182, Rank: 1, loss = 1.0800249583553523e-11
c622-102: Epoch: 0, Step: 182, Rank: 63, loss = 8.487701416015625e-05
c621-052: Epoch: 0, Step: 182, Rank: 21, loss = 6.993104012531504e-18
c622-062: Epoch: 0, Step: 182, Rank: 55, loss = 1.525040715932846e-08
c619-022: Epoch: 0, Step: 182, Rank: 17, loss = 0.0003566741943359375
c613-151: Epoch: 0, Step: 182, Rank: 10, loss = 1.1399388313293457e-06
c613-152: Epoch: 0, Step: 182, Rank: 11, loss = 0.0001583099365234375
c619-041: Epoch: 0, Step: 182, Rank: 20, loss = 6.3792168840089494e-21
c621-072: Epoch: 0, Step: 182, Rank: 25, loss = 2.5331974029541016e-07
c619-012: Epoch: 0, Step: 182, Rank: 15, loss = 3.293156623840332e-06
c621-101: Epoch: 0, Step: 182, Rank: 30, loss = 8.754432201385498e-08
c621-151: Epoch: 0, Step: 182, Rank: 40, loss = 8.265194654219209e-40
c622-001: Epoch: 0, Step: 182, Rank: 42, loss = 1.418811734765768e-09
c621-061: Epoch: 0, Step: 182, Rank: 22, loss = 2.6756374893466273e-14
c621-142: Epoch: 0, Step: 182, Rank: 39, loss = 3.3921018924503508e-28
c622-051: Epoch: 0, Step: 182, Rank: 52, loss = 9.74978320300579e-10
c613-122: Epoch: 0, Step: 182, Rank: 5, loss = 9.370282327836321e-14
c621-071: Epoch: 0, Step: 182, Rank: 24, loss = 1.6672859221771964e-24
c622-061: Epoch: 0, Step: 182, Rank: 54, loss = 2.014636993408203e-05
c621-091: Epoch: 0, Step: 182, Rank: 28, loss = 1.2069940567016602e-06
c613-142: Epoch: 0, Step: 182, Rank: 9, loss = 2.9325485229492188e-05
c622-032: Epoch: 0, Step: 182, Rank: 49, loss = 8.754432201385498e-08
c622-091: Epoch: 0, Step: 182, Rank: 60, loss = 4.6798959374427795e-08
c621-062: Epoch: 0, Step: 182, Rank: 23, loss = 2.1047890186309814e-07
c622-082: Epoch: 0, Step: 182, Rank: 59, loss = 2.586841583251953e-05
c621-152: Epoch: 0, Step: 182, Rank: 41, loss = 3.293156623840332e-06
c619-001: Epoch: 0, Step: 182, Rank: 12, loss = 5.14984130859375e-05
c621-082: Epoch: 0, Step: 182, Rank: 27, loss = 0.69140625
c621-131: Epoch: 0, Step: 182, Rank: 36, loss = 3.123283386230469e-05
c622-022: Epoch: 0, Step: 182, Rank: 47, loss = 3.213062882423401e-08
c622-072: Epoch: 0, Step: 182, Rank: 57, loss = 6.5267086029052734e-06
c621-121: Epoch: 0, Step: 182, Rank: 34, loss = 0.69140625
c622-031: Epoch: 0, Step: 182, Rank: 48, loss = 7.048583938740194e-11
c622-041: Epoch: 0, Step: 182, Rank: 50, loss = 6.007030606269836e-08
c621-112: Epoch: 0, Step: 182, Rank: 33, loss = 1.5366822481155396e-07
c621-141: Epoch: 0, Step: 182, Rank: 38, loss = 3.688037395477295e-07
c622-042: Epoch: 0, Step: 182, Rank: 51, loss = 1.55717134475708e-06
c619-032: Epoch: 0, Step: 182, Rank: 19, loss = 1.2069940567016602e-06
c622-011: Epoch: 0, Step: 182, Rank: 44, loss = 2.5920599000528455e-11
c622-021: Epoch: 0, Step: 182, Rank: 46, loss = 1.2759119272232056e-07
c621-092: Epoch: 0, Step: 182, Rank: 29, loss = 3.5017728805541992e-06
c621-122: Epoch: 0, Step: 182, Rank: 35, loss = 0.18359375
c613-112: Epoch: 0, Step: 182, Rank: 3, loss = 1.126900315284729e-07
c621-102: Epoch: 0, Step: 182, Rank: 31, loss = 5.14984130859375e-05
c619-031: Epoch: 0, Step: 182, Rank: 18, loss = 0.0003566741943359375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 183, Rank: 0, loss = 2.014636993408203e-05
c622-002: Epoch: 0, Step: 183, Rank: 43, loss = 6.07222318649292e-07
c621-132: Epoch: 0, Step: 183, Rank: 37, loss = 0.001068115234375
c622-001: Epoch: 0, Step: 183, Rank: 42, loss = 0.69140625
c622-101: Epoch: 0, Step: 183, Rank: 62, loss = 3.1650415621697903e-10
c621-151: Epoch: 0, Step: 183, Rank: 40, loss = 2.562999725341797e-06
c621-111: Epoch: 0, Step: 183, Rank: 32, loss = 3.46451997756958e-07
c622-081: Epoch: 0, Step: 183, Rank: 58, loss = 5.699694156646729e-07
c622-012: Epoch: 0, Step: 183, Rank: 45, loss = 0.0003681182861328125
c622-102: Epoch: 0, Step: 183, Rank: 63, loss = 5.3085386753082275e-08
c621-152: Epoch: 0, Step: 183, Rank: 41, loss = 6.628036499023438e-05
c622-092: Epoch: 0, Step: 183, Rank: 61, loss = 3.213062882423401e-08
c622-052: Epoch: 0, Step: 183, Rank: 53, loss = 9.424984455108643e-07
c621-142: Epoch: 0, Step: 183, Rank: 39, loss = 0.000518798828125
c613-121: Epoch: 0, Step: 183, Rank: 4, loss = 5.699694156646729e-07
c621-091: Epoch: 0, Step: 183, Rank: 28, loss = 4.839897155761719e-05
c621-081: Epoch: 0, Step: 183, Rank: 26, loss = 1.8775463104248047e-06
c621-121: Epoch: 0, Step: 183, Rank: 34, loss = 3.123283386230469e-05
c613-102: Epoch: 0, Step: 183, Rank: 1, loss = 1.5054687148465104e-22
c619-002: Epoch: 0, Step: 183, Rank: 13, loss = 1.0058283805847168e-06
c619-021: Epoch: 0, Step: 183, Rank: 16, loss = 6.007030606269836e-08
c613-122: Epoch: 0, Step: 183, Rank: 5, loss = 3.6734198463196485e-39
c613-112: Epoch: 0, Step: 183, Rank: 3, loss = 9.74978320300579e-10
c622-091: Epoch: 0, Step: 183, Rank: 60, loss = 5.3085386753082275e-08
c622-071: Epoch: 0, Step: 183, Rank: 56, loss = 1.3597309589385986e-07
c621-131: Epoch: 0, Step: 183, Rank: 36, loss = 0.0
c622-022: Epoch: 0, Step: 183, Rank: 47, loss = 5.617039278149605e-09
c622-041: Epoch: 0, Step: 183, Rank: 50, loss = 0.00040435791015625
c622-051: Epoch: 0, Step: 183, Rank: 52, loss = 0.0
c622-072: Epoch: 0, Step: 183, Rank: 57, loss = 1.418811734765768e-09
c621-082: Epoch: 0, Step: 183, Rank: 27, loss = 9.183549615799121e-41
c621-072: Epoch: 0, Step: 183, Rank: 25, loss = 1.955777406692505e-08
c621-102: Epoch: 0, Step: 183, Rank: 31, loss = 0.0
c613-152: Epoch: 0, Step: 183, Rank: 11, loss = 4.172325134277344e-07
c621-112: Epoch: 0, Step: 183, Rank: 33, loss = 0.01416015625
c622-031: Epoch: 0, Step: 183, Rank: 48, loss = 6.139278411865234e-06
c613-142: Epoch: 0, Step: 183, Rank: 9, loss = 1.3597309589385986e-07
c621-141: Epoch: 0, Step: 183, Rank: 38, loss = 2.7008354663848877e-07
c622-061: Epoch: 0, Step: 183, Rank: 54, loss = 7.188646122813225e-09
c622-032: Epoch: 0, Step: 183, Rank: 49, loss = 2.648448571562767e-09
c621-122: Epoch: 0, Step: 183, Rank: 35, loss = 0.0
c619-011: Epoch: 0, Step: 183, Rank: 14, loss = 5.3085386753082275e-08
c613-131: Epoch: 0, Step: 183, Rank: 6, loss = 1.3445969671010971e-08
c621-101: Epoch: 0, Step: 183, Rank: 30, loss = 5.14984130859375e-05
c619-041: Epoch: 0, Step: 183, Rank: 20, loss = 1.1059455573558807e-09
c622-062: Epoch: 0, Step: 183, Rank: 55, loss = 0.0002956390380859375
c613-151: Epoch: 0, Step: 183, Rank: 10, loss = 0.000278472900390625
c622-011: Epoch: 0, Step: 183, Rank: 44, loss = 1.8775463104248047e-06
c622-082: Epoch: 0, Step: 183, Rank: 59, loss = 9.424984455108643e-07
c622-021: Epoch: 0, Step: 183, Rank: 46, loss = 3.694822225952521e-13
c619-031: Epoch: 0, Step: 183, Rank: 18, loss = 0.69140625
c619-001: Epoch: 0, Step: 183, Rank: 12, loss = 0.000148773193359375
c619-022: Epoch: 0, Step: 183, Rank: 17, loss = 1.0662875083691372e-25
c621-061: Epoch: 0, Step: 183, Rank: 22, loss = 2.5331974029541016e-07
c619-012: Epoch: 0, Step: 183, Rank: 15, loss = 0.69140625
c619-032: Epoch: 0, Step: 183, Rank: 19, loss = 7.338821887969971e-07
c613-132: Epoch: 0, Step: 183, Rank: 7, loss = 5.699694156646729e-07
c621-071: Epoch: 0, Step: 183, Rank: 24, loss = 0.0
c622-042: Epoch: 0, Step: 183, Rank: 51, loss = 2.2649765014648438e-06
c621-052: Epoch: 0, Step: 183, Rank: 21, loss = 6.007030606269836e-08
c613-111: Epoch: 0, Step: 183, Rank: 2, loss = 2.9976945370435715e-09
c621-092: Epoch: 0, Step: 183, Rank: 29, loss = 1.955777406692505e-08
c613-141: Epoch: 0, Step: 183, Rank: 8, loss = 2.7830537874251604e-10
c621-062: Epoch: 0, Step: 183, Rank: 23, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 184, Rank: 16, loss = 1.318767317570746e-10
c619-022: Epoch: 0, Step: 184, Rank: 17, loss = 1.3828277587890625e-05
c619-031: Epoch: 0, Step: 184, Rank: 18, loss = 8.754432201385498e-08
c619-012: Epoch: 0, Step: 184, Rank: 15, loss = 3.213062882423401e-08
c613-151: Epoch: 0, Step: 184, Rank: 10, loss = 0.0002956390380859375
c619-011: Epoch: 0, Step: 184, Rank: 14, loss = 5.115907697472721e-12
c619-002: Epoch: 0, Step: 184, Rank: 13, loss = 4.05634636990726e-10
c619-001: Epoch: 0, Step: 184, Rank: 12, loss = 1.150369644165039e-05
c613-101: Epoch: 0, Step: 184, Rank: 0, loss = 6.198883056640625e-05
c613-132: Epoch: 0, Step: 184, Rank: 7, loss = 5.3085386753082275e-08
c613-152: Epoch: 0, Step: 184, Rank: 11, loss = 0.044921875
c622-081: Epoch: 0, Step: 184, Rank: 58, loss = 2.1047890186309814e-07
c622-092: Epoch: 0, Step: 184, Rank: 61, loss = 1.996755599975586e-06
c622-002: Epoch: 0, Step: 184, Rank: 43, loss = 2.905726432800293e-06
c622-101: Epoch: 0, Step: 184, Rank: 62, loss = 2.1736923372372985e-10
c613-141: Epoch: 0, Step: 184, Rank: 8, loss = 1.6540288925170898e-06
c613-131: Epoch: 0, Step: 184, Rank: 6, loss = 8.487701416015625e-05
c621-132: Epoch: 0, Step: 184, Rank: 37, loss = 1.1059455573558807e-09
c613-111: Epoch: 0, Step: 184, Rank: 2, loss = 0.00010251998901367188
c613-121: Epoch: 0, Step: 184, Rank: 4, loss = 2.276897430419922e-05
c613-122: Epoch: 0, Step: 184, Rank: 5, loss = 0.005584716796875
c613-142: Epoch: 0, Step: 184, Rank: 9, loss = 6.628036499023438e-05
c621-111: Epoch: 0, Step: 184, Rank: 32, loss = 5.617039278149605e-09
c622-102: Epoch: 0, Step: 184, Rank: 63, loss = 5.3085386753082275e-08
c621-101: Epoch: 0, Step: 184, Rank: 30, loss = 4.4517219066619873e-07
c621-091: Epoch: 0, Step: 184, Rank: 28, loss = 7.338821887969971e-07
c619-032: Epoch: 0, Step: 184, Rank: 19, loss = 4.1443854570388794e-08
c622-071: Epoch: 0, Step: 184, Rank: 56, loss = 2.1736923372372985e-10
c613-112: Epoch: 0, Step: 184, Rank: 3, loss = 1.0477378964424133e-08
c622-001: Epoch: 0, Step: 184, Rank: 42, loss = 0.0
c622-062: Epoch: 0, Step: 184, Rank: 55, loss = 1.955777406692505e-08
c622-052: Epoch: 0, Step: 184, Rank: 53, loss = 0.69140625
c613-102: Epoch: 0, Step: 184, Rank: 1, loss = 0.0
c622-072: Epoch: 0, Step: 184, Rank: 57, loss = 2.342858351767063e-09
c621-081: Epoch: 0, Step: 184, Rank: 26, loss = 2.1047890186309814e-07
c621-122: Epoch: 0, Step: 184, Rank: 35, loss = 1.8189894035458565e-09
c622-082: Epoch: 0, Step: 184, Rank: 59, loss = 0.0003681182861328125
c622-012: Epoch: 0, Step: 184, Rank: 45, loss = 1.55717134475708e-06
c622-061: Epoch: 0, Step: 184, Rank: 54, loss = 1.8775463104248047e-06
c621-121: Epoch: 0, Step: 184, Rank: 34, loss = 5.424022674560547e-06
c621-102: Epoch: 0, Step: 184, Rank: 31, loss = 1.0132789611816406e-05
c621-082: Epoch: 0, Step: 184, Rank: 27, loss = 5.893525667488575e-10
c621-151: Epoch: 0, Step: 184, Rank: 40, loss = 1.8758328224066645e-12
c621-131: Epoch: 0, Step: 184, Rank: 36, loss = 5.893525667488575e-10
c621-142: Epoch: 0, Step: 184, Rank: 39, loss = 1.6432439176733427e-19
c622-051: Epoch: 0, Step: 184, Rank: 52, loss = 2.648448571562767e-09
c621-152: Epoch: 0, Step: 184, Rank: 41, loss = 0.00075531005859375
c622-041: Epoch: 0, Step: 184, Rank: 50, loss = 6.891787052154541e-07
c622-032: Epoch: 0, Step: 184, Rank: 49, loss = 4.4517219066619873e-07
c621-112: Epoch: 0, Step: 184, Rank: 33, loss = 4.500150680541992e-06
c621-072: Epoch: 0, Step: 184, Rank: 25, loss = 6.07222318649292e-07
c621-092: Epoch: 0, Step: 184, Rank: 29, loss = 0.006103515625
c622-042: Epoch: 0, Step: 184, Rank: 51, loss = 4.3655745685100555e-09
c621-141: Epoch: 0, Step: 184, Rank: 38, loss = 1.895427703857422e-05
c619-041: Epoch: 0, Step: 184, Rank: 20, loss = 8.404254913330078e-06
c621-052: Epoch: 0, Step: 184, Rank: 21, loss = 6.973743438720703e-06
c622-022: Epoch: 0, Step: 184, Rank: 47, loss = 7.420778274536133e-06
c622-091: Epoch: 0, Step: 184, Rank: 60, loss = 1.3828277587890625e-05
c621-071: Epoch: 0, Step: 184, Rank: 24, loss = 1.6079866327345371e-09
c622-011: Epoch: 0, Step: 184, Rank: 44, loss = 0.69140625
c621-061: Epoch: 0, Step: 184, Rank: 22, loss = 2.2351741790771484e-07
c622-031: Epoch: 0, Step: 184, Rank: 48, loss = 3.725290298461914e-06
c621-062: Epoch: 0, Step: 184, Rank: 23, loss = 3.293156623840332e-06
c622-021: Epoch: 0, Step: 184, Rank: 46, loss = 3.774403012357652e-11
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 185, Rank: 58, loss = 7.729977369308472e-08
c622-002: Epoch: 0, Step: 185, Rank: 43, loss = 4.4517219066619873e-07
c613-101: Epoch: 0, Step: 185, Rank: 0, loss = 2.2351741790771484e-07
c621-151: Epoch: 0, Step: 185, Rank: 40, loss = 1.8533319234848022e-07
c622-092: Epoch: 0, Step: 185, Rank: 61, loss = 1.1399388313293457e-06
c619-021: Epoch: 0, Step: 185, Rank: 16, loss = 1.126900315284729e-07
c621-072: Epoch: 0, Step: 185, Rank: 25, loss = 4.839897155761719e-05
c622-052: Epoch: 0, Step: 185, Rank: 53, loss = 6.973743438720703e-06
c621-091: Epoch: 0, Step: 185, Rank: 28, loss = 2.2351741790771484e-07
c622-012: Epoch: 0, Step: 185, Rank: 45, loss = 0.0
c621-132: Epoch: 0, Step: 185, Rank: 37, loss = 9.5367431640625e-06
c619-032: Epoch: 0, Step: 185, Rank: 19, loss = 5.14984130859375e-05
c622-001: Epoch: 0, Step: 185, Rank: 42, loss = 3.268496584496461e-13
c622-102: Epoch: 0, Step: 185, Rank: 63, loss = 1.996755599975586e-06
c621-121: Epoch: 0, Step: 185, Rank: 34, loss = 6.628036499023438e-05
c621-142: Epoch: 0, Step: 185, Rank: 39, loss = 4.172325134277344e-07
c621-152: Epoch: 0, Step: 185, Rank: 41, loss = 7.66053886991358e-15
c622-101: Epoch: 0, Step: 185, Rank: 62, loss = 0.00020313262939453125
c613-151: Epoch: 0, Step: 185, Rank: 10, loss = 1.3597309589385986e-07
c622-051: Epoch: 0, Step: 185, Rank: 52, loss = 1.3445969671010971e-08
c622-071: Epoch: 0, Step: 185, Rank: 56, loss = 1.0058283805847168e-06
c621-111: Epoch: 0, Step: 185, Rank: 32, loss = 2.342858351767063e-09
c621-081: Epoch: 0, Step: 185, Rank: 26, loss = 7.486343383789062e-05
c619-012: Epoch: 0, Step: 185, Rank: 15, loss = 3.841705620288849e-09
c613-111: Epoch: 0, Step: 185, Rank: 2, loss = 2.726912498474121e-06
c619-011: Epoch: 0, Step: 185, Rank: 14, loss = 8.412825991399586e-12
c622-032: Epoch: 0, Step: 185, Rank: 49, loss = 1.3445969671010971e-08
c619-022: Epoch: 0, Step: 185, Rank: 17, loss = 6.973743438720703e-06
c619-002: Epoch: 0, Step: 185, Rank: 13, loss = 2.2649765014648438e-06
c622-061: Epoch: 0, Step: 185, Rank: 54, loss = 2.455635694786906e-10
c622-062: Epoch: 0, Step: 185, Rank: 55, loss = 3.583409124985337e-10
c619-031: Epoch: 0, Step: 185, Rank: 18, loss = 2.7008354663848877e-07
c621-082: Epoch: 0, Step: 185, Rank: 27, loss = 0.69140625
c622-031: Epoch: 0, Step: 185, Rank: 48, loss = 3.655441105365753e-08
c621-052: Epoch: 0, Step: 185, Rank: 21, loss = 6.628036499023438e-05
c613-102: Epoch: 0, Step: 185, Rank: 1, loss = 3.3068166260807885e-18
c622-082: Epoch: 0, Step: 185, Rank: 59, loss = 8.487701416015625e-05
c621-101: Epoch: 0, Step: 185, Rank: 30, loss = 0.69140625
c621-141: Epoch: 0, Step: 185, Rank: 38, loss = 0.00014019012451171875
c613-152: Epoch: 0, Step: 185, Rank: 11, loss = 1.525040715932846e-08
c613-122: Epoch: 0, Step: 185, Rank: 5, loss = 5.617039278149605e-09
c622-022: Epoch: 0, Step: 185, Rank: 47, loss = 7.188646122813225e-09
c619-041: Epoch: 0, Step: 185, Rank: 20, loss = 3.7670135498046875e-05
c621-112: Epoch: 0, Step: 185, Rank: 33, loss = 5.4836273193359375e-05
c622-011: Epoch: 0, Step: 185, Rank: 44, loss = 3.688037395477295e-07
c621-071: Epoch: 0, Step: 185, Rank: 24, loss = 9.918585419654846e-08
c613-121: Epoch: 0, Step: 185, Rank: 4, loss = 8.940696716308594e-06
c621-061: Epoch: 0, Step: 185, Rank: 22, loss = 1.2993812561035156e-05
c622-021: Epoch: 0, Step: 185, Rank: 46, loss = 4.3655745685100555e-09
c621-102: Epoch: 0, Step: 185, Rank: 31, loss = 5.502442945726216e-11
c613-131: Epoch: 0, Step: 185, Rank: 6, loss = 3.213062882423401e-08
c622-091: Epoch: 0, Step: 185, Rank: 60, loss = 2.3245294578089215e-16
c613-142: Epoch: 0, Step: 185, Rank: 9, loss = 3.46451997756958e-07
c613-112: Epoch: 0, Step: 185, Rank: 3, loss = 0.0
c619-001: Epoch: 0, Step: 185, Rank: 12, loss = 0.00014019012451171875
c613-132: Epoch: 0, Step: 185, Rank: 7, loss = 0.0
c621-131: Epoch: 0, Step: 185, Rank: 36, loss = 1.6689300537109375e-05
c613-141: Epoch: 0, Step: 185, Rank: 8, loss = 5.424022674560547e-06
c622-041: Epoch: 0, Step: 185, Rank: 50, loss = 3.5017728805541992e-06
c621-122: Epoch: 0, Step: 185, Rank: 35, loss = 0.00012302398681640625
c621-092: Epoch: 0, Step: 185, Rank: 29, loss = 3.213062882423401e-08
c621-062: Epoch: 0, Step: 185, Rank: 23, loss = 0.000278472900390625
c622-042: Epoch: 0, Step: 185, Rank: 51, loss = 0.00083160400390625
c622-072: Epoch: 0, Step: 185, Rank: 57, loss = 1.55717134475708e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 186, Rank: 0, loss = 6.5267086029052734e-06
c621-111: Epoch: 0, Step: 186, Rank: 32, loss = 1.6540288925170898e-06
c619-001: Epoch: 0, Step: 186, Rank: 12, loss = 7.188646122813225e-09
c619-021: Epoch: 0, Step: 186, Rank: 16, loss = 1.996755599975586e-06
c622-002: Epoch: 0, Step: 186, Rank: 43, loss = 3.841705620288849e-09
c613-151: Epoch: 0, Step: 186, Rank: 10, loss = 2.2649765014648438e-06
c622-101: Epoch: 0, Step: 186, Rank: 62, loss = 4.231929779052734e-06
c622-012: Epoch: 0, Step: 186, Rank: 45, loss = 2.514570951461792e-08
c621-101: Epoch: 0, Step: 186, Rank: 30, loss = 2.2351741790771484e-07
c613-152: Epoch: 0, Step: 186, Rank: 11, loss = 5.400124791776761e-13
c622-052: Epoch: 0, Step: 186, Rank: 53, loss = 2.8405338525772095e-08
c622-081: Epoch: 0, Step: 186, Rank: 58, loss = 1.8189894035458565e-09
c613-142: Epoch: 0, Step: 186, Rank: 9, loss = 1.2218952178955078e-05
c613-131: Epoch: 0, Step: 186, Rank: 6, loss = 4.3655745685100555e-09
c622-062: Epoch: 0, Step: 186, Rank: 55, loss = 2.5331974029541016e-07
c621-081: Epoch: 0, Step: 186, Rank: 26, loss = 3.7670135498046875e-05
c619-022: Epoch: 0, Step: 186, Rank: 17, loss = 3.293156623840332e-06
c621-091: Epoch: 0, Step: 186, Rank: 28, loss = 1.3589129821411916e-13
c622-102: Epoch: 0, Step: 186, Rank: 63, loss = 8.487701416015625e-05
c622-092: Epoch: 0, Step: 186, Rank: 61, loss = 4.5299530029296875e-05
c621-082: Epoch: 0, Step: 186, Rank: 27, loss = 4.00543212890625e-05
c621-132: Epoch: 0, Step: 186, Rank: 37, loss = 3.0547380447387695e-07
c621-061: Epoch: 0, Step: 186, Rank: 22, loss = 2.905726432800293e-06
c622-071: Epoch: 0, Step: 186, Rank: 56, loss = 1.2514647096395493e-09
c613-122: Epoch: 0, Step: 186, Rank: 5, loss = 1.8189894035458565e-09
c622-001: Epoch: 0, Step: 186, Rank: 42, loss = 1.996755599975586e-06
c619-031: Epoch: 0, Step: 186, Rank: 18, loss = 8.754432201385498e-08
c619-011: Epoch: 0, Step: 186, Rank: 14, loss = 4.4517219066619873e-07
c613-132: Epoch: 0, Step: 186, Rank: 7, loss = 3.123283386230469e-05
c613-111: Epoch: 0, Step: 186, Rank: 2, loss = 0.00083160400390625
c613-102: Epoch: 0, Step: 186, Rank: 1, loss = 2.6756374893466273e-14
c613-121: Epoch: 0, Step: 186, Rank: 4, loss = 1.8775463104248047e-06
c621-052: Epoch: 0, Step: 186, Rank: 21, loss = 0.000911712646484375
c621-121: Epoch: 0, Step: 186, Rank: 34, loss = 8.404254913330078e-06
c619-041: Epoch: 0, Step: 186, Rank: 20, loss = 1.0058283805847168e-06
c622-032: Epoch: 0, Step: 186, Rank: 49, loss = 8.307397365570068e-07
c621-151: Epoch: 0, Step: 186, Rank: 40, loss = 2.586841583251953e-05
c613-141: Epoch: 0, Step: 186, Rank: 8, loss = 8.003553375601768e-11
c622-031: Epoch: 0, Step: 186, Rank: 48, loss = 6.993104012531504e-18
c622-011: Epoch: 0, Step: 186, Rank: 44, loss = 8.754432201385498e-08
c619-002: Epoch: 0, Step: 186, Rank: 13, loss = 1.4722347259521484e-05
c622-072: Epoch: 0, Step: 186, Rank: 57, loss = 0.0
c622-082: Epoch: 0, Step: 186, Rank: 59, loss = 3.688037395477295e-07
c621-102: Epoch: 0, Step: 186, Rank: 31, loss = 5.14984130859375e-05
c622-061: Epoch: 0, Step: 186, Rank: 54, loss = 0.0
c619-012: Epoch: 0, Step: 186, Rank: 15, loss = 1.0132789611816406e-05
c621-141: Epoch: 0, Step: 186, Rank: 38, loss = 3.0547380447387695e-07
c622-041: Epoch: 0, Step: 186, Rank: 50, loss = 1.2197274440461925e-18
c613-112: Epoch: 0, Step: 186, Rank: 3, loss = 6.007030606269836e-08
c622-051: Epoch: 0, Step: 186, Rank: 52, loss = 5.699694156646729e-07
c621-112: Epoch: 0, Step: 186, Rank: 33, loss = 4.172325134277344e-07
c619-032: Epoch: 0, Step: 186, Rank: 19, loss = 5.14984130859375e-05
c621-122: Epoch: 0, Step: 186, Rank: 35, loss = 7.338821887969971e-07
c621-142: Epoch: 0, Step: 186, Rank: 39, loss = 3.213062882423401e-08
c622-042: Epoch: 0, Step: 186, Rank: 51, loss = 4.7222086809427244e-20
c621-092: Epoch: 0, Step: 186, Rank: 29, loss = 5.115907697472721e-12
c621-152: Epoch: 0, Step: 186, Rank: 41, loss = 2.586841583251953e-05
c621-131: Epoch: 0, Step: 186, Rank: 36, loss = 5.14984130859375e-05
c622-091: Epoch: 0, Step: 186, Rank: 60, loss = 0.0
c622-022: Epoch: 0, Step: 186, Rank: 47, loss = 7.188646122813225e-09
c621-071: Epoch: 0, Step: 186, Rank: 24, loss = 0.00075531005859375
c622-021: Epoch: 0, Step: 186, Rank: 46, loss = 3.725290298461914e-06
c621-062: Epoch: 0, Step: 186, Rank: 23, loss = 8.940696716308594e-06
c621-072: Epoch: 0, Step: 186, Rank: 25, loss = 2.342858351767063e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 187, Rank: 43, loss = 2.7008354663848877e-07
c621-111: Epoch: 0, Step: 187, Rank: 32, loss = 1.55717134475708e-06
c613-101: Epoch: 0, Step: 187, Rank: 0, loss = 4.301339185275744e-23
c622-052: Epoch: 0, Step: 187, Rank: 53, loss = 0.00164794921875
c621-132: Epoch: 0, Step: 187, Rank: 37, loss = 8.881784197001252e-13
c621-091: Epoch: 0, Step: 187, Rank: 28, loss = 1.1399388313293457e-06
c622-092: Epoch: 0, Step: 187, Rank: 61, loss = 1.4722347259521484e-05
c622-101: Epoch: 0, Step: 187, Rank: 62, loss = 3.213062882423401e-08
c619-021: Epoch: 0, Step: 187, Rank: 16, loss = 4.05634636990726e-10
c622-041: Epoch: 0, Step: 187, Rank: 50, loss = 0.0
c621-151: Epoch: 0, Step: 187, Rank: 40, loss = 1.2069940567016602e-06
c621-081: Epoch: 0, Step: 187, Rank: 26, loss = 8.585629984736443e-10
c621-122: Epoch: 0, Step: 187, Rank: 35, loss = 2.342858351767063e-09
c622-001: Epoch: 0, Step: 187, Rank: 42, loss = 0.0
c621-112: Epoch: 0, Step: 187, Rank: 33, loss = 8.734267942607043e-27
c619-002: Epoch: 0, Step: 187, Rank: 13, loss = 1.5688783605583012e-11
c621-061: Epoch: 0, Step: 187, Rank: 22, loss = 7.66053886991358e-15
c621-121: Epoch: 0, Step: 187, Rank: 34, loss = 1.3597309589385986e-07
c619-022: Epoch: 0, Step: 187, Rank: 17, loss = 8.404254913330078e-06
c622-051: Epoch: 0, Step: 187, Rank: 52, loss = 6.973743438720703e-06
c622-081: Epoch: 0, Step: 187, Rank: 58, loss = 6.230038707144558e-11
c622-042: Epoch: 0, Step: 187, Rank: 51, loss = 1.0058283805847168e-06
c622-012: Epoch: 0, Step: 187, Rank: 45, loss = 2.2649765014648438e-06
c621-152: Epoch: 0, Step: 187, Rank: 41, loss = 1.318767317570746e-10
c621-131: Epoch: 0, Step: 187, Rank: 36, loss = 1.3597309589385986e-07
c619-032: Epoch: 0, Step: 187, Rank: 19, loss = 5.3085386753082275e-08
c621-142: Epoch: 0, Step: 187, Rank: 39, loss = 8.754432201385498e-08
c621-072: Epoch: 0, Step: 187, Rank: 25, loss = 3.293156623840332e-06
c619-011: Epoch: 0, Step: 187, Rank: 14, loss = 1.7848833522293717e-11
c621-062: Epoch: 0, Step: 187, Rank: 23, loss = 5.3085386753082275e-08
c613-131: Epoch: 0, Step: 187, Rank: 6, loss = 3.5017728805541992e-06
c619-041: Epoch: 0, Step: 187, Rank: 20, loss = 1.6689300537109375e-05
c613-151: Epoch: 0, Step: 187, Rank: 10, loss = 0.000278472900390625
c613-102: Epoch: 0, Step: 187, Rank: 1, loss = 0.0
c621-082: Epoch: 0, Step: 187, Rank: 27, loss = 8.307397365570068e-07
c619-031: Epoch: 0, Step: 187, Rank: 18, loss = 5.14984130859375e-05
c621-071: Epoch: 0, Step: 187, Rank: 24, loss = 0.0
c613-152: Epoch: 0, Step: 187, Rank: 11, loss = 7.338821887969971e-07
c622-011: Epoch: 0, Step: 187, Rank: 44, loss = 1.1641532182693481e-10
c622-032: Epoch: 0, Step: 187, Rank: 49, loss = 8.404254913330078e-06
c613-111: Epoch: 0, Step: 187, Rank: 2, loss = 1.5366822481155396e-07
c613-112: Epoch: 0, Step: 187, Rank: 3, loss = 3.9637088775634766e-06
c622-072: Epoch: 0, Step: 187, Rank: 57, loss = 1.3869794202037156e-11
c621-141: Epoch: 0, Step: 187, Rank: 38, loss = 0.000335693359375
c622-102: Epoch: 0, Step: 187, Rank: 63, loss = 1.6689300537109375e-05
c613-121: Epoch: 0, Step: 187, Rank: 4, loss = 5.0961971282958984e-06
c622-022: Epoch: 0, Step: 187, Rank: 47, loss = 3.46451997756958e-07
c622-021: Epoch: 0, Step: 187, Rank: 46, loss = 0.00225830078125
c621-101: Epoch: 0, Step: 187, Rank: 30, loss = 1.126900315284729e-07
c619-012: Epoch: 0, Step: 187, Rank: 15, loss = 1.7762184143066406e-05
c613-132: Epoch: 0, Step: 187, Rank: 7, loss = 5.893525667488575e-10
c621-052: Epoch: 0, Step: 187, Rank: 21, loss = 0.0001087188720703125
c619-001: Epoch: 0, Step: 187, Rank: 12, loss = 7.338821887969971e-07
c613-142: Epoch: 0, Step: 187, Rank: 9, loss = 3.5017728805541992e-06
c622-071: Epoch: 0, Step: 187, Rank: 56, loss = 4.4517219066619873e-07
c622-091: Epoch: 0, Step: 187, Rank: 60, loss = 0.0
c622-082: Epoch: 0, Step: 187, Rank: 59, loss = 3.213062882423401e-08
c622-062: Epoch: 0, Step: 187, Rank: 55, loss = 6.198883056640625e-05
c622-031: Epoch: 0, Step: 187, Rank: 48, loss = 0.0
c622-061: Epoch: 0, Step: 187, Rank: 54, loss = 1.0132789611816406e-05
c621-092: Epoch: 0, Step: 187, Rank: 29, loss = 7.729977369308472e-08
c613-141: Epoch: 0, Step: 187, Rank: 8, loss = 1.418811734765768e-09
c613-122: Epoch: 0, Step: 187, Rank: 5, loss = 1.3869794202037156e-11
c621-102: Epoch: 0, Step: 187, Rank: 31, loss = 1.199040866595169e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 188, Rank: 43, loss = 0.00040435791015625
c619-021: Epoch: 0, Step: 188, Rank: 16, loss = 1.0058283805847168e-06
c621-061: Epoch: 0, Step: 188, Rank: 22, loss = 6.5267086029052734e-06
c622-052: Epoch: 0, Step: 188, Rank: 53, loss = 5.14984130859375e-05
c621-052: Epoch: 0, Step: 188, Rank: 21, loss = 7.338821887969971e-07
c622-061: Epoch: 0, Step: 188, Rank: 54, loss = 2.014636993408203e-05
c622-081: Epoch: 0, Step: 188, Rank: 58, loss = 0.000179290771484375
c619-041: Epoch: 0, Step: 188, Rank: 20, loss = 0.69140625
c621-132: Epoch: 0, Step: 188, Rank: 37, loss = 7.009506225585938e-05
c619-032: Epoch: 0, Step: 188, Rank: 19, loss = 0.00096893310546875
c621-111: Epoch: 0, Step: 188, Rank: 32, loss = 0.00021648406982421875
c619-022: Epoch: 0, Step: 188, Rank: 17, loss = 0.00014019012451171875
c622-012: Epoch: 0, Step: 188, Rank: 45, loss = 4.94765117764473e-09
c619-031: Epoch: 0, Step: 188, Rank: 18, loss = 0.0
c619-001: Epoch: 0, Step: 188, Rank: 12, loss = 5.424022674560547e-06
c622-022: Epoch: 0, Step: 188, Rank: 47, loss = 0.0001087188720703125
c619-011: Epoch: 0, Step: 188, Rank: 14, loss = 1.525040715932846e-08
c622-031: Epoch: 0, Step: 188, Rank: 48, loss = 2.1047890186309814e-07
c621-151: Epoch: 0, Step: 188, Rank: 40, loss = 0.0
c622-062: Epoch: 0, Step: 188, Rank: 55, loss = 7.867813110351562e-06
c621-091: Epoch: 0, Step: 188, Rank: 28, loss = 4.172325134277344e-07
c619-002: Epoch: 0, Step: 188, Rank: 13, loss = 3.510081114654895e-12
c622-042: Epoch: 0, Step: 188, Rank: 51, loss = 0.0003147125244140625
c622-001: Epoch: 0, Step: 188, Rank: 42, loss = 7.566995918750763e-10
c619-012: Epoch: 0, Step: 188, Rank: 15, loss = 5.029141902923584e-07
c613-131: Epoch: 0, Step: 188, Rank: 6, loss = 1.0058283805847168e-06
c622-101: Epoch: 0, Step: 188, Rank: 62, loss = 3.293156623840332e-06
c613-132: Epoch: 0, Step: 188, Rank: 7, loss = 0.00075531005859375
c621-141: Epoch: 0, Step: 188, Rank: 38, loss = 3.0547380447387695e-07
c622-041: Epoch: 0, Step: 188, Rank: 50, loss = 9.74978320300579e-10
c622-092: Epoch: 0, Step: 188, Rank: 61, loss = 1.3709068298339844e-06
c622-032: Epoch: 0, Step: 188, Rank: 49, loss = 2.066371962428093e-09
c613-101: Epoch: 0, Step: 188, Rank: 0, loss = 4.172325134277344e-07
c621-142: Epoch: 0, Step: 188, Rank: 39, loss = 6.06114274642742e-39
c622-051: Epoch: 0, Step: 188, Rank: 52, loss = 1.0788440704345703e-05
c621-081: Epoch: 0, Step: 188, Rank: 26, loss = 2.445028249710358e-36
c613-152: Epoch: 0, Step: 188, Rank: 11, loss = 4.839897155761719e-05
c621-072: Epoch: 0, Step: 188, Rank: 25, loss = 2.7008354663848877e-07
c622-071: Epoch: 0, Step: 188, Rank: 56, loss = 2.2351741790771484e-07
c621-112: Epoch: 0, Step: 188, Rank: 33, loss = 6.462348535570529e-26
c622-072: Epoch: 0, Step: 188, Rank: 57, loss = 2.2118911147117615e-08
c613-151: Epoch: 0, Step: 188, Rank: 10, loss = 0.69140625
c621-152: Epoch: 0, Step: 188, Rank: 41, loss = 1.5366822481155396e-07
c621-101: Epoch: 0, Step: 188, Rank: 30, loss = 0.0
c613-141: Epoch: 0, Step: 188, Rank: 8, loss = 1.3869794202037156e-11
c621-121: Epoch: 0, Step: 188, Rank: 34, loss = 9.918585419654846e-08
c613-121: Epoch: 0, Step: 188, Rank: 4, loss = 2.514570951461792e-08
c622-021: Epoch: 0, Step: 188, Rank: 46, loss = 0.000179290771484375
c622-011: Epoch: 0, Step: 188, Rank: 44, loss = 6.973743438720703e-06
c613-142: Epoch: 0, Step: 188, Rank: 9, loss = 5.699694156646729e-07
c621-082: Epoch: 0, Step: 188, Rank: 27, loss = 1.4722347259521484e-05
c621-122: Epoch: 0, Step: 188, Rank: 35, loss = 1.8041124150158794e-16
c622-082: Epoch: 0, Step: 188, Rank: 59, loss = 6.973743438720703e-06
c621-071: Epoch: 0, Step: 188, Rank: 24, loss = 7.729977369308472e-08
c613-122: Epoch: 0, Step: 188, Rank: 5, loss = 0.0003681182861328125
c621-102: Epoch: 0, Step: 188, Rank: 31, loss = 1.8758328224066645e-12
c621-092: Epoch: 0, Step: 188, Rank: 29, loss = 6.462348535570529e-26
c613-111: Epoch: 0, Step: 188, Rank: 2, loss = 5.502442945726216e-11
c613-112: Epoch: 0, Step: 188, Rank: 3, loss = 6.344635039567947e-09
c613-102: Epoch: 0, Step: 188, Rank: 1, loss = 2.586841583251953e-05
c622-091: Epoch: 0, Step: 188, Rank: 60, loss = 0.00506591796875
c622-102: Epoch: 0, Step: 188, Rank: 63, loss = 1.955777406692505e-08
c621-131: Epoch: 0, Step: 188, Rank: 36, loss = 8.307397365570068e-07
c621-062: Epoch: 0, Step: 188, Rank: 23, loss = 5.0961971282958984e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 189, Rank: 43, loss = 0.0181884765625
c621-091: Epoch: 0, Step: 189, Rank: 28, loss = 0.0
c621-102: Epoch: 0, Step: 189, Rank: 31, loss = 3.293156623840332e-06
c622-012: Epoch: 0, Step: 189, Rank: 45, loss = 3.725290298461914e-06
c621-081: Epoch: 0, Step: 189, Rank: 26, loss = 3.123283386230469e-05
c621-082: Epoch: 0, Step: 189, Rank: 27, loss = 1.1864468014524018e-27
c619-021: Epoch: 0, Step: 189, Rank: 16, loss = 6.07222318649292e-07
c622-011: Epoch: 0, Step: 189, Rank: 44, loss = 0.000911712646484375
c621-111: Epoch: 0, Step: 189, Rank: 32, loss = 0.0
c621-112: Epoch: 0, Step: 189, Rank: 33, loss = 9.424984455108643e-07
c621-132: Epoch: 0, Step: 189, Rank: 37, loss = 0.0
c622-001: Epoch: 0, Step: 189, Rank: 42, loss = 1.3732490638087374e-25
c621-152: Epoch: 0, Step: 189, Rank: 41, loss = 0.0002956390380859375
c621-142: Epoch: 0, Step: 189, Rank: 39, loss = 8.307397365570068e-07
c622-052: Epoch: 0, Step: 189, Rank: 53, loss = 0.000667572021484375
c621-151: Epoch: 0, Step: 189, Rank: 40, loss = 5.14984130859375e-05
c621-072: Epoch: 0, Step: 189, Rank: 25, loss = 6.993104012531504e-18
c613-101: Epoch: 0, Step: 189, Rank: 0, loss = 9.486769009248164e-19
c621-121: Epoch: 0, Step: 189, Rank: 34, loss = 0.0
c621-122: Epoch: 0, Step: 189, Rank: 35, loss = 2.753734588623047e-05
c621-101: Epoch: 0, Step: 189, Rank: 30, loss = 6.628036499023438e-05
c622-051: Epoch: 0, Step: 189, Rank: 52, loss = 0.000667572021484375
c619-032: Epoch: 0, Step: 189, Rank: 19, loss = 7.729977369308472e-08
c621-092: Epoch: 0, Step: 189, Rank: 29, loss = 3.084540367126465e-06
c621-131: Epoch: 0, Step: 189, Rank: 36, loss = 2.276897430419922e-05
c622-041: Epoch: 0, Step: 189, Rank: 50, loss = 2.342858351767063e-09
c621-052: Epoch: 0, Step: 189, Rank: 21, loss = 2.726912498474121e-06
c619-022: Epoch: 0, Step: 189, Rank: 17, loss = 2.342858351767063e-09
c619-011: Epoch: 0, Step: 189, Rank: 14, loss = 6.973743438720703e-06
c622-032: Epoch: 0, Step: 189, Rank: 49, loss = 4.00543212890625e-05
c622-031: Epoch: 0, Step: 189, Rank: 48, loss = 4.3655745685100555e-09
c621-062: Epoch: 0, Step: 189, Rank: 23, loss = 4.00543212890625e-05
c619-001: Epoch: 0, Step: 189, Rank: 12, loss = 0.00014019012451171875
c622-081: Epoch: 0, Step: 189, Rank: 58, loss = 0.0
c619-002: Epoch: 0, Step: 189, Rank: 13, loss = 3.084540367126465e-06
c613-121: Epoch: 0, Step: 189, Rank: 4, loss = 3.213062882423401e-08
c622-022: Epoch: 0, Step: 189, Rank: 47, loss = 1.996755599975586e-06
c621-061: Epoch: 0, Step: 189, Rank: 22, loss = 9.74978320300579e-10
c613-132: Epoch: 0, Step: 189, Rank: 7, loss = 2.2351741790771484e-07
c622-021: Epoch: 0, Step: 189, Rank: 46, loss = 2.5331974029541016e-07
c621-071: Epoch: 0, Step: 189, Rank: 24, loss = 6.07222318649292e-07
c622-061: Epoch: 0, Step: 189, Rank: 54, loss = 0.0038299560546875
c613-111: Epoch: 0, Step: 189, Rank: 2, loss = 0.201171875
c613-151: Epoch: 0, Step: 189, Rank: 10, loss = 7.188646122813225e-09
c619-041: Epoch: 0, Step: 189, Rank: 20, loss = 9.994988777600744e-20
c613-102: Epoch: 0, Step: 189, Rank: 1, loss = 1.3445969671010971e-08
c613-141: Epoch: 0, Step: 189, Rank: 8, loss = 2.9976945370435715e-09
c621-141: Epoch: 0, Step: 189, Rank: 38, loss = 4.231929779052734e-06
c622-092: Epoch: 0, Step: 189, Rank: 61, loss = 2.7008354663848877e-07
c622-062: Epoch: 0, Step: 189, Rank: 55, loss = 7.729977369308472e-08
c613-112: Epoch: 0, Step: 189, Rank: 3, loss = 3.841705620288849e-09
c619-012: Epoch: 0, Step: 189, Rank: 15, loss = 0.0002956390380859375
c613-152: Epoch: 0, Step: 189, Rank: 11, loss = 1.6391277313232422e-07
c613-122: Epoch: 0, Step: 189, Rank: 5, loss = 2.4318695068359375e-05
c613-131: Epoch: 0, Step: 189, Rank: 6, loss = 3.54136699749265e-24
c622-042: Epoch: 0, Step: 189, Rank: 51, loss = 4.5299530029296875e-05
c622-102: Epoch: 0, Step: 189, Rank: 63, loss = 0.69140625
c613-142: Epoch: 0, Step: 189, Rank: 9, loss = 1.0477378964424133e-08
c622-101: Epoch: 0, Step: 189, Rank: 62, loss = 4.9763185651190145e-21
c622-072: Epoch: 0, Step: 189, Rank: 57, loss = 6.891787052154541e-07
c622-071: Epoch: 0, Step: 189, Rank: 56, loss = 7.188646122813225e-09
c619-031: Epoch: 0, Step: 189, Rank: 18, loss = 5.9884384208290615e-34
c622-091: Epoch: 0, Step: 189, Rank: 60, loss = 0.0
c622-082: Epoch: 0, Step: 189, Rank: 59, loss = 2.5331974029541016e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2490234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 190, Rank: 43, loss = 2.753734588623047e-05
c621-132: Epoch: 0, Step: 190, Rank: 37, loss = 0.000335693359375
c622-012: Epoch: 0, Step: 190, Rank: 45, loss = 1.9806378759312793e-13
c621-081: Epoch: 0, Step: 190, Rank: 26, loss = 0.0
c622-052: Epoch: 0, Step: 190, Rank: 53, loss = 3.583409124985337e-10
c619-002: Epoch: 0, Step: 190, Rank: 13, loss = 4.05634636990726e-10
c622-051: Epoch: 0, Step: 190, Rank: 52, loss = 2.905726432800293e-06
c622-022: Epoch: 0, Step: 190, Rank: 47, loss = 2.2649765014648438e-06
c621-151: Epoch: 0, Step: 190, Rank: 40, loss = 3.314018249511719e-05
c621-091: Epoch: 0, Step: 190, Rank: 28, loss = 7.420778274536133e-06
c621-072: Epoch: 0, Step: 190, Rank: 25, loss = 1.1399388313293457e-06
c622-081: Epoch: 0, Step: 190, Rank: 58, loss = 9.255018085241318e-09
c621-111: Epoch: 0, Step: 190, Rank: 32, loss = 0.0002956390380859375
c622-001: Epoch: 0, Step: 190, Rank: 42, loss = 6.891787052154541e-07
c622-032: Epoch: 0, Step: 190, Rank: 49, loss = 1.150369644165039e-05
c619-021: Epoch: 0, Step: 190, Rank: 16, loss = 0.00021648406982421875
c621-152: Epoch: 0, Step: 190, Rank: 41, loss = 3.213062882423401e-08
c622-042: Epoch: 0, Step: 190, Rank: 51, loss = 3.213062882423401e-08
c621-121: Epoch: 0, Step: 190, Rank: 34, loss = 0.0
c622-021: Epoch: 0, Step: 190, Rank: 46, loss = 5.182486384480711e-17
c621-142: Epoch: 0, Step: 190, Rank: 39, loss = 5.699694156646729e-07
c622-041: Epoch: 0, Step: 190, Rank: 50, loss = 0.00360107421875
c621-141: Epoch: 0, Step: 190, Rank: 38, loss = 0.0
c621-131: Epoch: 0, Step: 190, Rank: 36, loss = 0.0
c622-011: Epoch: 0, Step: 190, Rank: 44, loss = 1.318767317570746e-10
c621-122: Epoch: 0, Step: 190, Rank: 35, loss = 3.084540367126465e-06
c621-052: Epoch: 0, Step: 190, Rank: 21, loss = 0.0
c621-101: Epoch: 0, Step: 190, Rank: 30, loss = 8.487701416015625e-05
c622-061: Epoch: 0, Step: 190, Rank: 54, loss = 7.048583938740194e-11
c622-062: Epoch: 0, Step: 190, Rank: 55, loss = 0.0
c621-061: Epoch: 0, Step: 190, Rank: 22, loss = 8.003553375601768e-11
c613-151: Epoch: 0, Step: 190, Rank: 10, loss = 4.94765117764473e-09
c613-152: Epoch: 0, Step: 190, Rank: 11, loss = 5.817413330078125e-05
c622-071: Epoch: 0, Step: 190, Rank: 56, loss = 5.115907697472721e-12
c619-001: Epoch: 0, Step: 190, Rank: 12, loss = 6.07222318649292e-07
c619-022: Epoch: 0, Step: 190, Rank: 17, loss = 5.3085386753082275e-08
c619-011: Epoch: 0, Step: 190, Rank: 14, loss = 0.00083160400390625
c619-041: Epoch: 0, Step: 190, Rank: 20, loss = 1.5366822481155396e-07
c621-112: Epoch: 0, Step: 190, Rank: 33, loss = 8.754432201385498e-08
c613-132: Epoch: 0, Step: 190, Rank: 7, loss = 0.00012302398681640625
c621-071: Epoch: 0, Step: 190, Rank: 24, loss = 3.0547380447387695e-07
c622-031: Epoch: 0, Step: 190, Rank: 48, loss = 5.14984130859375e-05
c619-031: Epoch: 0, Step: 190, Rank: 18, loss = 2.4035605705952703e-31
c622-072: Epoch: 0, Step: 190, Rank: 57, loss = 0.004486083984375
c613-121: Epoch: 0, Step: 190, Rank: 4, loss = 3.4897757426877174e-19
c613-131: Epoch: 0, Step: 190, Rank: 6, loss = 3.213062882423401e-08
c613-142: Epoch: 0, Step: 190, Rank: 9, loss = 3.510081114654895e-12
c621-102: Epoch: 0, Step: 190, Rank: 31, loss = 2.1457672119140625e-05
c613-101: Epoch: 0, Step: 190, Rank: 0, loss = 1.6916601452976465e-10
c622-092: Epoch: 0, Step: 190, Rank: 61, loss = 0.0003147125244140625
c619-032: Epoch: 0, Step: 190, Rank: 19, loss = 0.0002307891845703125
c621-092: Epoch: 0, Step: 190, Rank: 29, loss = 2.726912498474121e-06
c619-012: Epoch: 0, Step: 190, Rank: 15, loss = 2.4158453015843406e-12
c622-082: Epoch: 0, Step: 190, Rank: 59, loss = 0.0
c621-062: Epoch: 0, Step: 190, Rank: 23, loss = 0.01287841796875
c613-112: Epoch: 0, Step: 190, Rank: 3, loss = 0.0
c613-111: Epoch: 0, Step: 190, Rank: 2, loss = 8.307397365570068e-07
c613-141: Epoch: 0, Step: 190, Rank: 8, loss = 3.774403012357652e-11
c621-082: Epoch: 0, Step: 190, Rank: 27, loss = 3.864587821847745e-21
c622-101: Epoch: 0, Step: 190, Rank: 62, loss = 8.940696716308594e-06
c622-102: Epoch: 0, Step: 190, Rank: 63, loss = 0.0
c613-122: Epoch: 0, Step: 190, Rank: 5, loss = 1.996755599975586e-06
c622-091: Epoch: 0, Step: 190, Rank: 60, loss = 0.03271484375
c613-102: Epoch: 0, Step: 190, Rank: 1, loss = 0.0001087188720703125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.97s, TFLOPs: 0.96, Samples/sec: 0.51, Time/seq 1.97s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 191, Rank: 28, loss = 2.342858351767063e-09
c621-072: Epoch: 0, Step: 191, Rank: 25, loss = 2.753734588623047e-05
c621-081: Epoch: 0, Step: 191, Rank: 26, loss = 7.486343383789062e-05
c621-062: Epoch: 0, Step: 191, Rank: 23, loss = 1.5366822481155396e-07
c621-061: Epoch: 0, Step: 191, Rank: 22, loss = 8.940696716308594e-06
c621-082: Epoch: 0, Step: 191, Rank: 27, loss = 3.293156623840332e-06
c621-071: Epoch: 0, Step: 191, Rank: 24, loss = 3.123283386230469e-05
c621-101: Epoch: 0, Step: 191, Rank: 30, loss = 8.003553375601768e-11
c613-101: Epoch: 0, Step: 191, Rank: 0, loss = 0.0
c622-052: Epoch: 0, Step: 191, Rank: 53, loss = 4.94765117764473e-09
c621-052: Epoch: 0, Step: 191, Rank: 21, loss = 0.000179290771484375
c622-062: Epoch: 0, Step: 191, Rank: 55, loss = 1.6689300537109375e-05
c619-041: Epoch: 0, Step: 191, Rank: 20, loss = 0.0
c622-081: Epoch: 0, Step: 191, Rank: 58, loss = 2.8405338525772095e-08
c622-032: Epoch: 0, Step: 191, Rank: 49, loss = 7.792703114739563e-20
c622-002: Epoch: 0, Step: 191, Rank: 43, loss = 2.8405338525772095e-08
c622-041: Epoch: 0, Step: 191, Rank: 50, loss = 8.307397365570068e-07
c619-021: Epoch: 0, Step: 191, Rank: 16, loss = 1.126900315284729e-07
c619-032: Epoch: 0, Step: 191, Rank: 19, loss = 2.8405338525772095e-08
c619-001: Epoch: 0, Step: 191, Rank: 12, loss = 3.655441105365753e-08
c622-061: Epoch: 0, Step: 191, Rank: 54, loss = 0.0
c613-121: Epoch: 0, Step: 191, Rank: 4, loss = 1.1874362826347351e-08
c621-092: Epoch: 0, Step: 191, Rank: 29, loss = 0.0
c613-131: Epoch: 0, Step: 191, Rank: 6, loss = 7.566995918750763e-10
c622-051: Epoch: 0, Step: 191, Rank: 52, loss = 2.7008354663848877e-07
c613-132: Epoch: 0, Step: 191, Rank: 7, loss = 2.014636993408203e-05
c622-042: Epoch: 0, Step: 191, Rank: 51, loss = 0.0002460479736328125
c622-012: Epoch: 0, Step: 191, Rank: 45, loss = 1.7762184143066406e-05
c622-022: Epoch: 0, Step: 191, Rank: 47, loss = 9.424984455108643e-07
c613-122: Epoch: 0, Step: 191, Rank: 5, loss = 1.1399388313293457e-06
c613-111: Epoch: 0, Step: 191, Rank: 2, loss = 0.69140625
c621-111: Epoch: 0, Step: 191, Rank: 32, loss = 4.00543212890625e-05
c613-151: Epoch: 0, Step: 191, Rank: 10, loss = 4.172325134277344e-07
c619-002: Epoch: 0, Step: 191, Rank: 13, loss = 1.2759119272232056e-07
c622-101: Epoch: 0, Step: 191, Rank: 62, loss = 6.139278411865234e-06
c619-031: Epoch: 0, Step: 191, Rank: 18, loss = 3.46451997756958e-07
c613-102: Epoch: 0, Step: 191, Rank: 1, loss = 2.562999725341797e-06
c622-031: Epoch: 0, Step: 191, Rank: 48, loss = 2.514570951461792e-08
c613-142: Epoch: 0, Step: 191, Rank: 9, loss = 3.213062882423401e-08
c622-102: Epoch: 0, Step: 191, Rank: 63, loss = 1.8189894035458565e-09
c621-132: Epoch: 0, Step: 191, Rank: 37, loss = 0.0016021728515625
c621-142: Epoch: 0, Step: 191, Rank: 39, loss = 1.7762184143066406e-05
c622-072: Epoch: 0, Step: 191, Rank: 57, loss = 1.8775463104248047e-06
c622-092: Epoch: 0, Step: 191, Rank: 61, loss = 8.307397365570068e-07
c619-012: Epoch: 0, Step: 191, Rank: 15, loss = 0.000179290771484375
c622-001: Epoch: 0, Step: 191, Rank: 42, loss = 4.1443854570388794e-08
c613-141: Epoch: 0, Step: 191, Rank: 8, loss = 1.4915713109076023e-10
c619-022: Epoch: 0, Step: 191, Rank: 17, loss = 3.688037395477295e-07
c621-121: Epoch: 0, Step: 191, Rank: 34, loss = 2.726912498474121e-06
c621-102: Epoch: 0, Step: 191, Rank: 31, loss = 0.0
c621-151: Epoch: 0, Step: 191, Rank: 40, loss = 2.8405338525772095e-08
c619-011: Epoch: 0, Step: 191, Rank: 14, loss = 1.0058283805847168e-06
c613-152: Epoch: 0, Step: 191, Rank: 11, loss = 5.699694156646729e-07
c621-112: Epoch: 0, Step: 191, Rank: 33, loss = 0.0
c622-082: Epoch: 0, Step: 191, Rank: 59, loss = 7.188646122813225e-09
c622-071: Epoch: 0, Step: 191, Rank: 56, loss = 2.3245294578089215e-16
c622-021: Epoch: 0, Step: 191, Rank: 46, loss = 0.1669921875
c621-122: Epoch: 0, Step: 191, Rank: 35, loss = 0.0
c621-131: Epoch: 0, Step: 191, Rank: 36, loss = 4.13994203059987e-27
c621-152: Epoch: 0, Step: 191, Rank: 41, loss = 5.4836273193359375e-05
c621-141: Epoch: 0, Step: 191, Rank: 38, loss = 3.655441105365753e-08
c622-011: Epoch: 0, Step: 191, Rank: 44, loss = 1.7229467630386353e-08
c622-091: Epoch: 0, Step: 191, Rank: 60, loss = 2.831068712794149e-15
c613-112: Epoch: 0, Step: 191, Rank: 3, loss = 4.94765117764473e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.13s, TFLOPs: 0.88, Samples/sec: 0.47, Time/seq 2.13s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 192, Rank: 0, loss = 5.20230969414115e-10
c622-092: Epoch: 0, Step: 192, Rank: 61, loss = 8.940696716308594e-06
c622-081: Epoch: 0, Step: 192, Rank: 58, loss = 6.007030606269836e-08
c619-002: Epoch: 0, Step: 192, Rank: 13, loss = 5.424022674560547e-06
c622-002: Epoch: 0, Step: 192, Rank: 43, loss = 3.1650415621697903e-10
c622-101: Epoch: 0, Step: 192, Rank: 62, loss = 1.8775463104248047e-06
c613-151: Epoch: 0, Step: 192, Rank: 10, loss = 3.213062882423401e-08
c622-012: Epoch: 0, Step: 192, Rank: 45, loss = 4.3655745685100555e-09
c622-062: Epoch: 0, Step: 192, Rank: 55, loss = 4.5299530029296875e-05
c621-111: Epoch: 0, Step: 192, Rank: 32, loss = 2.2649765014648438e-06
c621-132: Epoch: 0, Step: 192, Rank: 37, loss = 6.693881005048752e-10
c622-052: Epoch: 0, Step: 192, Rank: 53, loss = 3.841705620288849e-09
c613-132: Epoch: 0, Step: 192, Rank: 7, loss = 9.918585419654846e-08
c613-111: Epoch: 0, Step: 192, Rank: 2, loss = 1.955777406692505e-08
c613-131: Epoch: 0, Step: 192, Rank: 6, loss = 1.4722347259521484e-05
c622-001: Epoch: 0, Step: 192, Rank: 42, loss = 6.139278411865234e-06
c613-102: Epoch: 0, Step: 192, Rank: 1, loss = 5.424022674560547e-06
c619-021: Epoch: 0, Step: 192, Rank: 16, loss = 3.5017728805541992e-06
c621-072: Epoch: 0, Step: 192, Rank: 25, loss = 1.7229467630386353e-08
c613-121: Epoch: 0, Step: 192, Rank: 4, loss = 1.55717134475708e-06
c619-001: Epoch: 0, Step: 192, Rank: 12, loss = 1.078520768856852e-30
c613-112: Epoch: 0, Step: 192, Rank: 3, loss = 1.7229467630386353e-08
c622-082: Epoch: 0, Step: 192, Rank: 59, loss = 3.084540367126465e-06
c621-081: Epoch: 0, Step: 192, Rank: 26, loss = 1.1874362826347351e-08
c621-131: Epoch: 0, Step: 192, Rank: 36, loss = 0.0038299560546875
c621-151: Epoch: 0, Step: 192, Rank: 40, loss = 5.699694156646729e-07
c613-152: Epoch: 0, Step: 192, Rank: 11, loss = 2.5331974029541016e-07
c621-061: Epoch: 0, Step: 192, Rank: 22, loss = 3.688037395477295e-07
c619-011: Epoch: 0, Step: 192, Rank: 14, loss = 3.293156623840332e-06
c613-141: Epoch: 0, Step: 192, Rank: 8, loss = 9.424984455108643e-07
c622-011: Epoch: 0, Step: 192, Rank: 44, loss = 1.418811734765768e-09
c619-041: Epoch: 0, Step: 192, Rank: 20, loss = 2.5331974029541016e-07
c622-051: Epoch: 0, Step: 192, Rank: 52, loss = 1.955777406692505e-08
c622-091: Epoch: 0, Step: 192, Rank: 60, loss = 3.0547380447387695e-07
c622-072: Epoch: 0, Step: 192, Rank: 57, loss = 2.1693674893577825e-29
c613-122: Epoch: 0, Step: 192, Rank: 5, loss = 1.8189894035458565e-09
c619-031: Epoch: 0, Step: 192, Rank: 18, loss = 1.6079866327345371e-09
c622-102: Epoch: 0, Step: 192, Rank: 63, loss = 2.2118911147117615e-08
c621-101: Epoch: 0, Step: 192, Rank: 30, loss = 1.0058283805847168e-06
c622-061: Epoch: 0, Step: 192, Rank: 54, loss = 0.00020313262939453125
c621-141: Epoch: 0, Step: 192, Rank: 38, loss = 8.940696716308594e-06
c621-052: Epoch: 0, Step: 192, Rank: 21, loss = 5.893525667488575e-10
c621-142: Epoch: 0, Step: 192, Rank: 39, loss = 0.0
c621-152: Epoch: 0, Step: 192, Rank: 41, loss = 0.0
c622-032: Epoch: 0, Step: 192, Rank: 49, loss = 3.8163916471489756e-16
c619-012: Epoch: 0, Step: 192, Rank: 15, loss = 1.2069940567016602e-06
c621-082: Epoch: 0, Step: 192, Rank: 27, loss = 5.424022674560547e-06
c613-142: Epoch: 0, Step: 192, Rank: 9, loss = 1.6689300537109375e-05
c621-091: Epoch: 0, Step: 192, Rank: 28, loss = 0.69140625
c621-092: Epoch: 0, Step: 192, Rank: 29, loss = 7.729977369308472e-08
c621-102: Epoch: 0, Step: 192, Rank: 31, loss = 0.69140625
c621-112: Epoch: 0, Step: 192, Rank: 33, loss = 3.54136699749265e-24
c622-041: Epoch: 0, Step: 192, Rank: 50, loss = 0.69140625
c622-021: Epoch: 0, Step: 192, Rank: 46, loss = 0.0002460479736328125
c619-032: Epoch: 0, Step: 192, Rank: 19, loss = 1.0662875083691372e-25
c621-062: Epoch: 0, Step: 192, Rank: 23, loss = 3.979039320256561e-12
c621-122: Epoch: 0, Step: 192, Rank: 35, loss = 1.8533319234848022e-07
c621-121: Epoch: 0, Step: 192, Rank: 34, loss = 3.213062882423401e-08
c621-071: Epoch: 0, Step: 192, Rank: 24, loss = 5.893525667488575e-10
c622-031: Epoch: 0, Step: 192, Rank: 48, loss = 4.172325134277344e-07
c619-022: Epoch: 0, Step: 192, Rank: 17, loss = 9.255018085241318e-09
c622-022: Epoch: 0, Step: 192, Rank: 47, loss = 6.716706573930585e-22
c622-042: Epoch: 0, Step: 192, Rank: 51, loss = 7.729977369308472e-08
c622-071: Epoch: 0, Step: 192, Rank: 56, loss = 1.7139067942650854e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 193, Rank: 43, loss = 2.196763842816691e-34
c619-002: Epoch: 0, Step: 193, Rank: 13, loss = 1.150369644165039e-05
c613-101: Epoch: 0, Step: 193, Rank: 0, loss = 0.00099945068359375
c621-091: Epoch: 0, Step: 193, Rank: 28, loss = 8.754432201385498e-08
c619-021: Epoch: 0, Step: 193, Rank: 16, loss = 1.1874362826347351e-08
c621-081: Epoch: 0, Step: 193, Rank: 26, loss = 8.585629984736443e-10
c622-101: Epoch: 0, Step: 193, Rank: 62, loss = 7.729977369308472e-08
c613-111: Epoch: 0, Step: 193, Rank: 2, loss = 1.895427703857422e-05
c613-131: Epoch: 0, Step: 193, Rank: 6, loss = 0.26171875
c619-001: Epoch: 0, Step: 193, Rank: 12, loss = 0.69140625
c613-132: Epoch: 0, Step: 193, Rank: 7, loss = 1.1059455573558807e-09
c621-111: Epoch: 0, Step: 193, Rank: 32, loss = 0.0
c622-012: Epoch: 0, Step: 193, Rank: 45, loss = 9.424984455108643e-07
c619-011: Epoch: 0, Step: 193, Rank: 14, loss = 1.6209256159527285e-14
c621-132: Epoch: 0, Step: 193, Rank: 37, loss = 6.139278411865234e-06
c622-092: Epoch: 0, Step: 193, Rank: 61, loss = 2.9976945370435715e-09
c622-001: Epoch: 0, Step: 193, Rank: 42, loss = 3.293156623840332e-06
c613-122: Epoch: 0, Step: 193, Rank: 5, loss = 9.255018085241318e-09
c613-121: Epoch: 0, Step: 193, Rank: 4, loss = 3.6734198463196485e-39
c621-082: Epoch: 0, Step: 193, Rank: 27, loss = 7.729977369308472e-08
c621-122: Epoch: 0, Step: 193, Rank: 35, loss = 5.4836273193359375e-05
c622-081: Epoch: 0, Step: 193, Rank: 58, loss = 4.6798959374427795e-08
c613-152: Epoch: 0, Step: 193, Rank: 11, loss = 1.5735626220703125e-05
c621-121: Epoch: 0, Step: 193, Rank: 34, loss = 2.2649765014648438e-06
c613-141: Epoch: 0, Step: 193, Rank: 8, loss = 4.1443854570388794e-08
c613-112: Epoch: 0, Step: 193, Rank: 3, loss = 4.94765117764473e-09
c621-131: Epoch: 0, Step: 193, Rank: 36, loss = 2.648448571562767e-09
c622-102: Epoch: 0, Step: 193, Rank: 63, loss = 4.500150680541992e-06
c613-151: Epoch: 0, Step: 193, Rank: 10, loss = 1.2759119272232056e-07
c613-142: Epoch: 0, Step: 193, Rank: 9, loss = 5.893525667488575e-10
c621-151: Epoch: 0, Step: 193, Rank: 40, loss = 5.817413330078125e-05
c613-102: Epoch: 0, Step: 193, Rank: 1, loss = 4.6798959374427795e-08
c621-142: Epoch: 0, Step: 193, Rank: 39, loss = 3.578125
c621-152: Epoch: 0, Step: 193, Rank: 41, loss = 2.6756374893466273e-14
c619-032: Epoch: 0, Step: 193, Rank: 19, loss = 5.3085386753082275e-08
c619-031: Epoch: 0, Step: 193, Rank: 18, loss = 2.6756374893466273e-14
c621-052: Epoch: 0, Step: 193, Rank: 21, loss = 4.843059286940843e-11
c621-112: Epoch: 0, Step: 193, Rank: 33, loss = 7.729977369308472e-08
c621-101: Epoch: 0, Step: 193, Rank: 30, loss = 1.2759119272232056e-07
c622-021: Epoch: 0, Step: 193, Rank: 46, loss = 0.69140625
c621-102: Epoch: 0, Step: 193, Rank: 31, loss = 2.831068712794149e-15
c619-022: Epoch: 0, Step: 193, Rank: 17, loss = 1.5366822481155396e-07
c619-041: Epoch: 0, Step: 193, Rank: 20, loss = 4.3655745685100555e-09
c621-061: Epoch: 0, Step: 193, Rank: 22, loss = 5.893525667488575e-10
c622-011: Epoch: 0, Step: 193, Rank: 44, loss = 3.917798799689633e-26
c621-072: Epoch: 0, Step: 193, Rank: 25, loss = 1.3709068298339844e-06
c621-071: Epoch: 0, Step: 193, Rank: 24, loss = 2.6056189295420372e-23
c621-062: Epoch: 0, Step: 193, Rank: 23, loss = 1.126900315284729e-07
c619-012: Epoch: 0, Step: 193, Rank: 15, loss = 0.0
c622-091: Epoch: 0, Step: 193, Rank: 60, loss = 3.5695955961250784e-29
c621-092: Epoch: 0, Step: 193, Rank: 29, loss = 8.543513119185775e-17
c621-141: Epoch: 0, Step: 193, Rank: 38, loss = 8.881784197001252e-13
c622-082: Epoch: 0, Step: 193, Rank: 59, loss = 7.420778274536133e-06
c622-071: Epoch: 0, Step: 193, Rank: 56, loss = 2.726912498474121e-06
c622-062: Epoch: 0, Step: 193, Rank: 55, loss = 1.8533319234848022e-07
c622-072: Epoch: 0, Step: 193, Rank: 57, loss = 1.6079866327345371e-09
c622-052: Epoch: 0, Step: 193, Rank: 53, loss = 1.1399388313293457e-06
c622-022: Epoch: 0, Step: 193, Rank: 47, loss = 3.213062882423401e-08
c622-041: Epoch: 0, Step: 193, Rank: 50, loss = 9.74978320300579e-10
c622-061: Epoch: 0, Step: 193, Rank: 54, loss = 7.338821887969971e-07
c622-032: Epoch: 0, Step: 193, Rank: 49, loss = 4.5299530029296875e-05
c622-051: Epoch: 0, Step: 193, Rank: 52, loss = 1.2278178473934531e-11
c622-031: Epoch: 0, Step: 193, Rank: 48, loss = 3.0547380447387695e-07
c622-042: Epoch: 0, Step: 193, Rank: 51, loss = 3.841705620288849e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 194, Rank: 43, loss = 0.0001316070556640625
c621-111: Epoch: 0, Step: 194, Rank: 32, loss = 4.274625098332763e-11
c613-101: Epoch: 0, Step: 194, Rank: 0, loss = 1.5366822481155396e-07
c622-081: Epoch: 0, Step: 194, Rank: 58, loss = 8.149072527885437e-09
c619-021: Epoch: 0, Step: 194, Rank: 16, loss = 5.115907697472721e-12
c622-012: Epoch: 0, Step: 194, Rank: 45, loss = 4.839897155761719e-05
c622-052: Epoch: 0, Step: 194, Rank: 53, loss = 4.843059286940843e-11
c621-081: Epoch: 0, Step: 194, Rank: 26, loss = 0.00010251998901367188
c619-002: Epoch: 0, Step: 194, Rank: 13, loss = 7.048583938740194e-11
c622-001: Epoch: 0, Step: 194, Rank: 42, loss = 0.0
c621-132: Epoch: 0, Step: 194, Rank: 37, loss = 0.000457763671875
c621-091: Epoch: 0, Step: 194, Rank: 28, loss = 0.0
c613-132: Epoch: 0, Step: 194, Rank: 7, loss = 1.9190338207408786e-10
c621-151: Epoch: 0, Step: 194, Rank: 40, loss = 0.07666015625
c613-142: Epoch: 0, Step: 194, Rank: 9, loss = 0.0
c622-062: Epoch: 0, Step: 194, Rank: 55, loss = 5.3085386753082275e-08
c621-131: Epoch: 0, Step: 194, Rank: 36, loss = 0.69140625
c613-122: Epoch: 0, Step: 194, Rank: 5, loss = 1.996755599975586e-06
c622-011: Epoch: 0, Step: 194, Rank: 44, loss = 5.0182080713057076e-14
c613-131: Epoch: 0, Step: 194, Rank: 6, loss = 8.149072527885437e-09
c621-112: Epoch: 0, Step: 194, Rank: 33, loss = 3.0547380447387695e-07
c613-141: Epoch: 0, Step: 194, Rank: 8, loss = 4.839897155761719e-05
c622-022: Epoch: 0, Step: 194, Rank: 47, loss = 2.5920599000528455e-11
c613-151: Epoch: 0, Step: 194, Rank: 10, loss = 5.4836273193359375e-05
c622-051: Epoch: 0, Step: 194, Rank: 52, loss = 4.255493527005605e-18
c622-092: Epoch: 0, Step: 194, Rank: 61, loss = 1.996755599975586e-06
c619-032: Epoch: 0, Step: 194, Rank: 19, loss = 1.5366822481155396e-07
c613-121: Epoch: 0, Step: 194, Rank: 4, loss = 6.198883056640625e-05
c622-102: Epoch: 0, Step: 194, Rank: 63, loss = 2.1047890186309814e-07
c621-121: Epoch: 0, Step: 194, Rank: 34, loss = 2.0236257114447653e-11
c621-122: Epoch: 0, Step: 194, Rank: 35, loss = 1.2759119272232056e-07
c622-101: Epoch: 0, Step: 194, Rank: 62, loss = 2.514570951461792e-08
c613-111: Epoch: 0, Step: 194, Rank: 2, loss = 1.4915713109076023e-10
c621-061: Epoch: 0, Step: 194, Rank: 22, loss = 1.6540288925170898e-06
c619-022: Epoch: 0, Step: 194, Rank: 17, loss = 4.760636329592671e-13
c621-082: Epoch: 0, Step: 194, Rank: 27, loss = 2.1736923372372985e-10
c613-152: Epoch: 0, Step: 194, Rank: 11, loss = 3.694822225952521e-13
c619-011: Epoch: 0, Step: 194, Rank: 14, loss = 6.635317295611287e-17
c622-071: Epoch: 0, Step: 194, Rank: 56, loss = 4.00543212890625e-05
c621-052: Epoch: 0, Step: 194, Rank: 21, loss = 1.2759119272232056e-07
c622-072: Epoch: 0, Step: 194, Rank: 57, loss = 1.955777406692505e-08
c621-101: Epoch: 0, Step: 194, Rank: 30, loss = 4.405564747785802e-33
c622-041: Epoch: 0, Step: 194, Rank: 50, loss = 3.7670135498046875e-05
c621-102: Epoch: 0, Step: 194, Rank: 31, loss = 1.7229467630386353e-08
c619-001: Epoch: 0, Step: 194, Rank: 12, loss = 1.199040866595169e-13
c621-142: Epoch: 0, Step: 194, Rank: 39, loss = 5.3085386753082275e-08
c621-141: Epoch: 0, Step: 194, Rank: 38, loss = 1.418811734765768e-09
c622-082: Epoch: 0, Step: 194, Rank: 59, loss = 4.1443854570388794e-08
c619-031: Epoch: 0, Step: 194, Rank: 18, loss = 3.841705620288849e-09
c613-102: Epoch: 0, Step: 194, Rank: 1, loss = 1.3597309589385986e-07
c621-152: Epoch: 0, Step: 194, Rank: 41, loss = 0.0002307891845703125
c622-031: Epoch: 0, Step: 194, Rank: 48, loss = 0.0
c622-032: Epoch: 0, Step: 194, Rank: 49, loss = 1.1874362826347351e-08
c622-021: Epoch: 0, Step: 194, Rank: 46, loss = 2.4158453015843406e-12
c619-012: Epoch: 0, Step: 194, Rank: 15, loss = 1.955777406692505e-08
c621-092: Epoch: 0, Step: 194, Rank: 29, loss = 1.8533319234848022e-07
c619-041: Epoch: 0, Step: 194, Rank: 20, loss = 1.0058283805847168e-06
c613-112: Epoch: 0, Step: 194, Rank: 3, loss = 2.1457672119140625e-05
c621-062: Epoch: 0, Step: 194, Rank: 23, loss = 8.307397365570068e-07
c622-042: Epoch: 0, Step: 194, Rank: 51, loss = 4.4517219066619873e-07
c622-061: Epoch: 0, Step: 194, Rank: 54, loss = 1.0132789611816406e-05
c621-071: Epoch: 0, Step: 194, Rank: 24, loss = 1.3828277587890625e-05
c622-091: Epoch: 0, Step: 194, Rank: 60, loss = 0.00180816650390625
c621-072: Epoch: 0, Step: 194, Rank: 25, loss = 3.688037395477295e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c621-081: Epoch: 0, Step: 195, Rank: 26, loss = 1.4915713109076023e-10
c621-082: Epoch: 0, Step: 195, Rank: 27, loss = 2.514570951461792e-08
c622-002: Epoch: 0, Step: 195, Rank: 43, loss = 1.5366822481155396e-07
c619-021: Epoch: 0, Step: 195, Rank: 16, loss = 1.955777406692505e-08
c621-072: Epoch: 0, Step: 195, Rank: 25, loss = 1.7848833522293717e-11
c621-091: Epoch: 0, Step: 195, Rank: 28, loss = 4.6798959374427795e-08
c613-101: Epoch: 0, Step: 195, Rank: 0, loss = 0.0
c619-002: Epoch: 0, Step: 195, Rank: 13, loss = 4.172325134277344e-07
c621-092: Epoch: 0, Step: 195, Rank: 29, loss = 0.000644683837890625
c621-151: Epoch: 0, Step: 195, Rank: 40, loss = 4.6798959374427795e-08
c621-111: Epoch: 0, Step: 195, Rank: 32, loss = 8.307397365570068e-07
c622-001: Epoch: 0, Step: 195, Rank: 42, loss = 3.213062882423401e-08
c619-011: Epoch: 0, Step: 195, Rank: 14, loss = 7.44648787076585e-12
c621-132: Epoch: 0, Step: 195, Rank: 37, loss = 0.69140625
c622-052: Epoch: 0, Step: 195, Rank: 53, loss = 8.487701416015625e-05
c621-101: Epoch: 0, Step: 195, Rank: 30, loss = 2.753734588623047e-05
c622-081: Epoch: 0, Step: 195, Rank: 58, loss = 2.2649765014648438e-06
c621-061: Epoch: 0, Step: 195, Rank: 22, loss = 2.9331204132176936e-11
c621-121: Epoch: 0, Step: 195, Rank: 34, loss = 5.502442945726216e-11
c622-101: Epoch: 0, Step: 195, Rank: 62, loss = 2.648448571562767e-09
c619-022: Epoch: 0, Step: 195, Rank: 17, loss = 1.4915713109076023e-10
c613-152: Epoch: 0, Step: 195, Rank: 11, loss = 3.655441105365753e-08
c613-132: Epoch: 0, Step: 195, Rank: 7, loss = 2.9331204132176936e-11
c621-071: Epoch: 0, Step: 195, Rank: 24, loss = 1.7848833522293717e-11
c621-152: Epoch: 0, Step: 195, Rank: 41, loss = 5.502442945726216e-11
c619-001: Epoch: 0, Step: 195, Rank: 12, loss = 1.4637180356658064e-12
c621-102: Epoch: 0, Step: 195, Rank: 31, loss = 1.3828277587890625e-05
c619-041: Epoch: 0, Step: 195, Rank: 20, loss = 1.8533319234848022e-07
c621-142: Epoch: 0, Step: 195, Rank: 39, loss = 0.0
c621-112: Epoch: 0, Step: 195, Rank: 33, loss = 0.0
c622-012: Epoch: 0, Step: 195, Rank: 45, loss = 0.0
c619-032: Epoch: 0, Step: 195, Rank: 19, loss = 8.307397365570068e-07
c619-012: Epoch: 0, Step: 195, Rank: 15, loss = 4.05634636990726e-10
c622-092: Epoch: 0, Step: 195, Rank: 61, loss = 1.150369644165039e-05
c622-051: Epoch: 0, Step: 195, Rank: 52, loss = 6.973743438720703e-06
c621-062: Epoch: 0, Step: 195, Rank: 23, loss = 8.754432201385498e-08
c613-131: Epoch: 0, Step: 195, Rank: 6, loss = 0.0
c613-111: Epoch: 0, Step: 195, Rank: 2, loss = 1.2931877790833823e-12
c622-021: Epoch: 0, Step: 195, Rank: 46, loss = 1.9190338207408786e-10
c613-142: Epoch: 0, Step: 195, Rank: 9, loss = 1.6391277313232422e-07
c621-052: Epoch: 0, Step: 195, Rank: 21, loss = 2.1736923372372985e-10
c621-131: Epoch: 0, Step: 195, Rank: 36, loss = 1.5688783605583012e-11
c622-031: Epoch: 0, Step: 195, Rank: 48, loss = 1.7229467630386353e-08
c613-151: Epoch: 0, Step: 195, Rank: 10, loss = 0.69140625
c622-032: Epoch: 0, Step: 195, Rank: 49, loss = 0.69140625
c619-031: Epoch: 0, Step: 195, Rank: 18, loss = 1.7848833522293717e-11
c622-022: Epoch: 0, Step: 195, Rank: 47, loss = 3.7670135498046875e-05
c613-102: Epoch: 0, Step: 195, Rank: 1, loss = 1.2790197503539935e-19
c622-041: Epoch: 0, Step: 195, Rank: 50, loss = 3.46451997756958e-07
c622-011: Epoch: 0, Step: 195, Rank: 44, loss = 2.5331974029541016e-07
c622-072: Epoch: 0, Step: 195, Rank: 57, loss = 1.525040715932846e-08
c613-122: Epoch: 0, Step: 195, Rank: 5, loss = 1.7762184143066406e-05
c622-042: Epoch: 0, Step: 195, Rank: 51, loss = 5.699694156646729e-07
c613-141: Epoch: 0, Step: 195, Rank: 8, loss = 1.955777406692505e-08
c621-141: Epoch: 0, Step: 195, Rank: 38, loss = 5.995204332975845e-15
c613-112: Epoch: 0, Step: 195, Rank: 3, loss = 0.0002956390380859375
c622-091: Epoch: 0, Step: 195, Rank: 60, loss = 1.0800249583553523e-11
c621-122: Epoch: 0, Step: 195, Rank: 35, loss = 3.688037395477295e-07
c622-102: Epoch: 0, Step: 195, Rank: 63, loss = 1.3828277587890625e-05
c622-061: Epoch: 0, Step: 195, Rank: 54, loss = 2.9976945370435715e-09
c622-082: Epoch: 0, Step: 195, Rank: 59, loss = 1.2304311611726287e-23
c622-071: Epoch: 0, Step: 195, Rank: 56, loss = 0.00128173828125
c613-121: Epoch: 0, Step: 195, Rank: 4, loss = 3.293156623840332e-06
c622-062: Epoch: 0, Step: 195, Rank: 55, loss = 1.3869794202037156e-11
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 196, Rank: 43, loss = 2.1736923372372985e-10
c613-101: Epoch: 0, Step: 196, Rank: 0, loss = 0.000278472900390625
c619-021: Epoch: 0, Step: 196, Rank: 16, loss = 8.940696716308594e-06
c619-002: Epoch: 0, Step: 196, Rank: 13, loss = 2.7830537874251604e-10
c621-132: Epoch: 0, Step: 196, Rank: 37, loss = 3.213062882423401e-08
c621-081: Epoch: 0, Step: 196, Rank: 26, loss = 4.3655745685100555e-09
c619-001: Epoch: 0, Step: 196, Rank: 12, loss = 5.3085386753082275e-08
c621-111: Epoch: 0, Step: 196, Rank: 32, loss = 8.754432201385498e-08
c622-012: Epoch: 0, Step: 196, Rank: 45, loss = 0.01507568359375
c621-142: Epoch: 0, Step: 196, Rank: 39, loss = 1.1399388313293457e-06
c619-011: Epoch: 0, Step: 196, Rank: 14, loss = 4.3655745685100555e-09
c621-091: Epoch: 0, Step: 196, Rank: 28, loss = 1.5819829215076654e-23
c622-001: Epoch: 0, Step: 196, Rank: 42, loss = 1.55717134475708e-06
c619-012: Epoch: 0, Step: 196, Rank: 15, loss = 2.342858351767063e-09
c613-151: Epoch: 0, Step: 196, Rank: 10, loss = 0.0
c621-131: Epoch: 0, Step: 196, Rank: 36, loss = 0.00012302398681640625
c619-032: Epoch: 0, Step: 196, Rank: 19, loss = 6.07222318649292e-07
c619-031: Epoch: 0, Step: 196, Rank: 18, loss = 1.7229467630386353e-08
c621-121: Epoch: 0, Step: 196, Rank: 34, loss = 3.293156623840332e-06
c621-122: Epoch: 0, Step: 196, Rank: 35, loss = 1.2195753889781233e-37
c622-092: Epoch: 0, Step: 196, Rank: 61, loss = 0.0
c621-151: Epoch: 0, Step: 196, Rank: 40, loss = 0.01104736328125
c621-152: Epoch: 0, Step: 196, Rank: 41, loss = 0.0
c621-092: Epoch: 0, Step: 196, Rank: 29, loss = 0.0
c621-112: Epoch: 0, Step: 196, Rank: 33, loss = 6.439293542825908e-14
c619-041: Epoch: 0, Step: 196, Rank: 20, loss = 9.632110595703125e-05
c613-152: Epoch: 0, Step: 196, Rank: 11, loss = 5.0961971282958984e-06
c621-052: Epoch: 0, Step: 196, Rank: 21, loss = 6.007030606269836e-08
c619-022: Epoch: 0, Step: 196, Rank: 17, loss = 1.5366822481155396e-07
c622-052: Epoch: 0, Step: 196, Rank: 53, loss = 1.1641532182693481e-10
c621-141: Epoch: 0, Step: 196, Rank: 38, loss = 1.9081958235744878e-17
c621-061: Epoch: 0, Step: 196, Rank: 22, loss = 4.00543212890625e-05
c621-101: Epoch: 0, Step: 196, Rank: 30, loss = 5.893525667488575e-10
c622-081: Epoch: 0, Step: 196, Rank: 58, loss = 2.2351741790771484e-07
c613-102: Epoch: 0, Step: 196, Rank: 1, loss = 5.617039278149605e-09
c622-022: Epoch: 0, Step: 196, Rank: 47, loss = 5.3085386753082275e-08
c622-011: Epoch: 0, Step: 196, Rank: 44, loss = 0.0
c613-111: Epoch: 0, Step: 196, Rank: 2, loss = 3.583409124985337e-10
c621-082: Epoch: 0, Step: 196, Rank: 27, loss = 2.2351741790771484e-07
c613-121: Epoch: 0, Step: 196, Rank: 4, loss = 1.0132789611816406e-05
c613-112: Epoch: 0, Step: 196, Rank: 3, loss = 5.893525667488575e-10
c621-102: Epoch: 0, Step: 196, Rank: 31, loss = 2.5331974029541016e-07
c622-071: Epoch: 0, Step: 196, Rank: 56, loss = 0.0
c622-021: Epoch: 0, Step: 196, Rank: 46, loss = 4.1443854570388794e-08
c622-032: Epoch: 0, Step: 196, Rank: 49, loss = 5.4836273193359375e-05
c622-051: Epoch: 0, Step: 196, Rank: 52, loss = 1.8533319234848022e-07
c622-061: Epoch: 0, Step: 196, Rank: 54, loss = 5.0961971282958984e-06
c622-062: Epoch: 0, Step: 196, Rank: 55, loss = 1.2759119272232056e-07
c622-101: Epoch: 0, Step: 196, Rank: 62, loss = 0.00075531005859375
c613-122: Epoch: 0, Step: 196, Rank: 5, loss = 3.688037395477295e-07
c621-062: Epoch: 0, Step: 196, Rank: 23, loss = 8.087401133657338e-35
c613-132: Epoch: 0, Step: 196, Rank: 7, loss = 1.0477378964424133e-08
c622-102: Epoch: 0, Step: 196, Rank: 63, loss = 1.126900315284729e-07
c622-031: Epoch: 0, Step: 196, Rank: 48, loss = 0.0005035400390625
c622-041: Epoch: 0, Step: 196, Rank: 50, loss = 4.4517219066619873e-07
c613-131: Epoch: 0, Step: 196, Rank: 6, loss = 5.3085386753082275e-08
c613-141: Epoch: 0, Step: 196, Rank: 8, loss = 1.0058283805847168e-06
c613-142: Epoch: 0, Step: 196, Rank: 9, loss = 1.3445969671010971e-08
c622-091: Epoch: 0, Step: 196, Rank: 60, loss = 2.4158453015843406e-12
c622-082: Epoch: 0, Step: 196, Rank: 59, loss = 1.9081958235744878e-17
c621-071: Epoch: 0, Step: 196, Rank: 24, loss = 3.123283386230469e-05
c622-072: Epoch: 0, Step: 196, Rank: 57, loss = 0.0091552734375
c621-072: Epoch: 0, Step: 196, Rank: 25, loss = 0.0002460479736328125
c622-042: Epoch: 0, Step: 196, Rank: 51, loss = 1.2069940567016602e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 197, Rank: 43, loss = 0.0
c613-101: Epoch: 0, Step: 197, Rank: 0, loss = 3.688037395477295e-07
c613-111: Epoch: 0, Step: 197, Rank: 2, loss = 5.699694156646729e-07
c619-021: Epoch: 0, Step: 197, Rank: 16, loss = 1.2759119272232056e-07
c622-062: Epoch: 0, Step: 197, Rank: 55, loss = 1.9563750449481093e-27
c622-012: Epoch: 0, Step: 197, Rank: 45, loss = 7.729977369308472e-08
c622-071: Epoch: 0, Step: 197, Rank: 56, loss = 1.1874362826347351e-08
c619-002: Epoch: 0, Step: 197, Rank: 13, loss = 0.0
c621-081: Epoch: 0, Step: 197, Rank: 26, loss = 1.55717134475708e-06
c622-061: Epoch: 0, Step: 197, Rank: 54, loss = 6.007030606269836e-08
c621-132: Epoch: 0, Step: 197, Rank: 37, loss = 5.4836273193359375e-05
c613-132: Epoch: 0, Step: 197, Rank: 7, loss = 3.583409124985337e-10
c622-052: Epoch: 0, Step: 197, Rank: 53, loss = 4.6798959374427795e-08
c619-011: Epoch: 0, Step: 197, Rank: 14, loss = 5.029141902923584e-07
c621-111: Epoch: 0, Step: 197, Rank: 32, loss = 0.00010251998901367188
c613-112: Epoch: 0, Step: 197, Rank: 3, loss = 0.69140625
c622-001: Epoch: 0, Step: 197, Rank: 42, loss = 8.881784197001252e-13
c621-151: Epoch: 0, Step: 197, Rank: 40, loss = 0.69140625
c621-142: Epoch: 0, Step: 197, Rank: 39, loss = 0.00860595703125
c621-082: Epoch: 0, Step: 197, Rank: 27, loss = 0.00040435791015625
c621-072: Epoch: 0, Step: 197, Rank: 25, loss = 2.3245294578089215e-16
c621-121: Epoch: 0, Step: 197, Rank: 34, loss = 4.4517219066619873e-07
c622-072: Epoch: 0, Step: 197, Rank: 57, loss = 2.8405338525772095e-08
c613-122: Epoch: 0, Step: 197, Rank: 5, loss = 1.2931877790833823e-12
c622-011: Epoch: 0, Step: 197, Rank: 44, loss = 2.648448571562767e-09
c622-041: Epoch: 0, Step: 197, Rank: 50, loss = 3.213062882423401e-08
c613-121: Epoch: 0, Step: 197, Rank: 4, loss = 0.0002613067626953125
c622-092: Epoch: 0, Step: 197, Rank: 61, loss = 8.940696716308594e-06
c613-152: Epoch: 0, Step: 197, Rank: 11, loss = 1.7229467630386353e-08
c619-041: Epoch: 0, Step: 197, Rank: 20, loss = 1.0972125985553305e-16
c613-141: Epoch: 0, Step: 197, Rank: 8, loss = 7.729977369308472e-08
c613-131: Epoch: 0, Step: 197, Rank: 6, loss = 2.8405338525772095e-08
c621-102: Epoch: 0, Step: 197, Rank: 31, loss = 0.69140625
c613-151: Epoch: 0, Step: 197, Rank: 10, loss = 0.0
c622-022: Epoch: 0, Step: 197, Rank: 47, loss = 1.1641532182693481e-10
c621-152: Epoch: 0, Step: 197, Rank: 41, loss = 2.5331974029541016e-07
c613-102: Epoch: 0, Step: 197, Rank: 1, loss = 5.617039278149605e-09
c621-122: Epoch: 0, Step: 197, Rank: 35, loss = 0.00164794921875
c622-101: Epoch: 0, Step: 197, Rank: 62, loss = 3.0547380447387695e-07
c622-051: Epoch: 0, Step: 197, Rank: 52, loss = 0.00014019012451171875
c621-131: Epoch: 0, Step: 197, Rank: 36, loss = 1.84297022087776e-14
c622-102: Epoch: 0, Step: 197, Rank: 63, loss = 1.4453125
c621-112: Epoch: 0, Step: 197, Rank: 33, loss = 2.8405338525772095e-08
c621-092: Epoch: 0, Step: 197, Rank: 29, loss = 6.230038707144558e-11
c619-012: Epoch: 0, Step: 197, Rank: 15, loss = 1.126900315284729e-07
c619-001: Epoch: 0, Step: 197, Rank: 12, loss = 0.0
c622-021: Epoch: 0, Step: 197, Rank: 46, loss = 0.0
c621-141: Epoch: 0, Step: 197, Rank: 38, loss = 1.3597309589385986e-07
c622-032: Epoch: 0, Step: 197, Rank: 49, loss = 1.1874362826347351e-08
c619-022: Epoch: 0, Step: 197, Rank: 17, loss = 0.69140625
c622-031: Epoch: 0, Step: 197, Rank: 48, loss = 3.841705620288849e-09
c621-101: Epoch: 0, Step: 197, Rank: 30, loss = 3.293156623840332e-06
c621-062: Epoch: 0, Step: 197, Rank: 23, loss = 2.2351741790771484e-07
c621-052: Epoch: 0, Step: 197, Rank: 21, loss = 8.307397365570068e-07
c622-042: Epoch: 0, Step: 197, Rank: 51, loss = 0.0002460479736328125
c619-031: Epoch: 0, Step: 197, Rank: 18, loss = 1.3597309589385986e-07
c613-142: Epoch: 0, Step: 197, Rank: 9, loss = 3.213062882423401e-08
c621-061: Epoch: 0, Step: 197, Rank: 22, loss = 3.688037395477295e-07
c619-032: Epoch: 0, Step: 197, Rank: 19, loss = 4.3655745685100555e-09
c622-091: Epoch: 0, Step: 197, Rank: 60, loss = 1.955777406692505e-08
c621-091: Epoch: 0, Step: 197, Rank: 28, loss = 2.196763842816691e-34
c621-071: Epoch: 0, Step: 197, Rank: 24, loss = 0.69140625
c622-081: Epoch: 0, Step: 197, Rank: 58, loss = 1.1641532182693481e-10
c622-082: Epoch: 0, Step: 197, Rank: 59, loss = 0.0133056640625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-151: Epoch: 0, Step: 198, Rank: 10, loss = 2.831068712794149e-15
c613-141: Epoch: 0, Step: 198, Rank: 8, loss = 9.098986738083304e-23
c619-001: Epoch: 0, Step: 198, Rank: 12, loss = 3.084540367126465e-06
c613-132: Epoch: 0, Step: 198, Rank: 7, loss = 2.2118911147117615e-08
c613-152: Epoch: 0, Step: 198, Rank: 11, loss = 5.916456789157589e-29
c613-131: Epoch: 0, Step: 198, Rank: 6, loss = 4.7222086809427244e-20
c613-122: Epoch: 0, Step: 198, Rank: 5, loss = 4.1443854570388794e-08
c619-002: Epoch: 0, Step: 198, Rank: 13, loss = 7.420778274536133e-06
c621-072: Epoch: 0, Step: 198, Rank: 25, loss = 5.4836273193359375e-05
c619-011: Epoch: 0, Step: 198, Rank: 14, loss = 4.231929779052734e-06
c621-061: Epoch: 0, Step: 198, Rank: 22, loss = 4.274625098332763e-11
c619-031: Epoch: 0, Step: 198, Rank: 18, loss = 1.4637067577342992e-31
c619-021: Epoch: 0, Step: 198, Rank: 16, loss = 7.566995918750763e-10
c613-142: Epoch: 0, Step: 198, Rank: 9, loss = 5.699694156646729e-07
c613-121: Epoch: 0, Step: 198, Rank: 4, loss = 0.69140625
c621-052: Epoch: 0, Step: 198, Rank: 21, loss = 5.39260384428426e-32
c619-041: Epoch: 0, Step: 198, Rank: 20, loss = 0.69140625
c621-081: Epoch: 0, Step: 198, Rank: 26, loss = 1.9983403963978888e-37
c619-012: Epoch: 0, Step: 198, Rank: 15, loss = 2.5920599000528455e-11
c613-112: Epoch: 0, Step: 198, Rank: 3, loss = 4.4517219066619873e-07
c619-032: Epoch: 0, Step: 198, Rank: 19, loss = 0.0
c621-062: Epoch: 0, Step: 198, Rank: 23, loss = 4.267692565917969e-05
c619-022: Epoch: 0, Step: 198, Rank: 17, loss = 1.3732490638087374e-25
c621-071: Epoch: 0, Step: 198, Rank: 24, loss = 7.66053886991358e-15
c613-101: Epoch: 0, Step: 198, Rank: 0, loss = 1.2218952178955078e-05
c613-111: Epoch: 0, Step: 198, Rank: 2, loss = 1.2759119272232056e-07
c613-102: Epoch: 0, Step: 198, Rank: 1, loss = 1.4915713109076023e-10
c622-102: Epoch: 0, Step: 198, Rank: 63, loss = 2.9331204132176936e-11
c621-082: Epoch: 0, Step: 198, Rank: 27, loss = 1.8758328224066645e-12
c622-092: Epoch: 0, Step: 198, Rank: 61, loss = 6.845220923423767e-08
c622-101: Epoch: 0, Step: 198, Rank: 62, loss = 7.048583938740194e-11
c621-151: Epoch: 0, Step: 198, Rank: 40, loss = 8.307397365570068e-07
c621-152: Epoch: 0, Step: 198, Rank: 41, loss = 4.1443854570388794e-08
c621-091: Epoch: 0, Step: 198, Rank: 28, loss = 3.0547380447387695e-07
c621-132: Epoch: 0, Step: 198, Rank: 37, loss = 1.0800249583553523e-11
c622-082: Epoch: 0, Step: 198, Rank: 59, loss = 5.182486384480711e-17
c622-052: Epoch: 0, Step: 198, Rank: 53, loss = 9.424984455108643e-07
c621-142: Epoch: 0, Step: 198, Rank: 39, loss = 8.083811398051921e-16
c622-001: Epoch: 0, Step: 198, Rank: 42, loss = 0.0001087188720703125
c622-081: Epoch: 0, Step: 198, Rank: 58, loss = 7.729977369308472e-08
c622-002: Epoch: 0, Step: 198, Rank: 43, loss = 1.2759119272232056e-07
c621-111: Epoch: 0, Step: 198, Rank: 32, loss = 3.841705620288849e-09
c622-072: Epoch: 0, Step: 198, Rank: 57, loss = 4.231929779052734e-06
c622-061: Epoch: 0, Step: 198, Rank: 54, loss = 3.841705620288849e-09
c622-051: Epoch: 0, Step: 198, Rank: 52, loss = 5.029141902923584e-07
c621-131: Epoch: 0, Step: 198, Rank: 36, loss = 2.355810384551023e-21
c622-031: Epoch: 0, Step: 198, Rank: 48, loss = 9.183549615799121e-41
c622-032: Epoch: 0, Step: 198, Rank: 49, loss = 5.0182080713057076e-14
c622-091: Epoch: 0, Step: 198, Rank: 60, loss = 7.729977369308472e-08
c621-101: Epoch: 0, Step: 198, Rank: 30, loss = 6.198883056640625e-05
c621-121: Epoch: 0, Step: 198, Rank: 34, loss = 2.71833068627654e-38
c622-012: Epoch: 0, Step: 198, Rank: 45, loss = 1.3597309589385986e-07
c621-141: Epoch: 0, Step: 198, Rank: 38, loss = 1.3597309589385986e-07
c621-122: Epoch: 0, Step: 198, Rank: 35, loss = 3.655441105365753e-08
c622-041: Epoch: 0, Step: 198, Rank: 50, loss = 4.6798959374427795e-08
c622-022: Epoch: 0, Step: 198, Rank: 47, loss = 6.344635039567947e-09
c622-042: Epoch: 0, Step: 198, Rank: 51, loss = 1.525040715932846e-08
c622-071: Epoch: 0, Step: 198, Rank: 56, loss = 8.083811398051921e-16
c621-102: Epoch: 0, Step: 198, Rank: 31, loss = 2.9331204132176936e-11
c621-112: Epoch: 0, Step: 198, Rank: 33, loss = 1.1874362826347351e-08
c622-062: Epoch: 0, Step: 198, Rank: 55, loss = 6.462348535570529e-26
c622-011: Epoch: 0, Step: 198, Rank: 44, loss = 5.3085386753082275e-08
c622-021: Epoch: 0, Step: 198, Rank: 46, loss = 2.753734588623047e-05
c621-092: Epoch: 0, Step: 198, Rank: 29, loss = 3.841705620288849e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 199, Rank: 32, loss = 0.0
c622-002: Epoch: 0, Step: 199, Rank: 43, loss = 6.07222318649292e-07
c622-052: Epoch: 0, Step: 199, Rank: 53, loss = 0.69140625
c621-081: Epoch: 0, Step: 199, Rank: 26, loss = 0.0
c621-072: Epoch: 0, Step: 199, Rank: 25, loss = 1.126900315284729e-07
c619-021: Epoch: 0, Step: 199, Rank: 16, loss = 5.699694156646729e-07
c621-091: Epoch: 0, Step: 199, Rank: 28, loss = 0.0
c613-132: Epoch: 0, Step: 199, Rank: 7, loss = 5.699694156646729e-07
c619-002: Epoch: 0, Step: 199, Rank: 13, loss = 2.540190280342358e-13
c613-101: Epoch: 0, Step: 199, Rank: 0, loss = 8.307397365570068e-07
c622-012: Epoch: 0, Step: 199, Rank: 45, loss = 3.084540367126465e-06
c619-041: Epoch: 0, Step: 199, Rank: 20, loss = 3.655441105365753e-08
c621-082: Epoch: 0, Step: 199, Rank: 27, loss = 2.455635694786906e-10
c613-131: Epoch: 0, Step: 199, Rank: 6, loss = 2.2351741790771484e-07
c613-141: Epoch: 0, Step: 199, Rank: 8, loss = 2.2118911147117615e-08
c621-121: Epoch: 0, Step: 199, Rank: 34, loss = 4.798173904418945e-06
c621-101: Epoch: 0, Step: 199, Rank: 30, loss = 6.927791673660977e-13
c621-131: Epoch: 0, Step: 199, Rank: 36, loss = 4.500150680541992e-06
c619-031: Epoch: 0, Step: 199, Rank: 18, loss = 0.69140625
c621-151: Epoch: 0, Step: 199, Rank: 40, loss = 6.06114274642742e-39
c622-031: Epoch: 0, Step: 199, Rank: 48, loss = 0.0
c621-052: Epoch: 0, Step: 199, Rank: 21, loss = 0.00019073486328125
c622-032: Epoch: 0, Step: 199, Rank: 49, loss = 0.0
c622-001: Epoch: 0, Step: 199, Rank: 42, loss = 8.881784197001252e-13
c622-022: Epoch: 0, Step: 199, Rank: 47, loss = 3.213062882423401e-08
c619-001: Epoch: 0, Step: 199, Rank: 12, loss = 2.1047890186309814e-07
c619-022: Epoch: 0, Step: 199, Rank: 17, loss = 1.1874362826347351e-08
c622-041: Epoch: 0, Step: 199, Rank: 50, loss = 4.890056499420716e-35
c613-152: Epoch: 0, Step: 199, Rank: 11, loss = 8.307397365570068e-07
c622-081: Epoch: 0, Step: 199, Rank: 58, loss = 3.583409124985337e-10
c622-061: Epoch: 0, Step: 199, Rank: 54, loss = 3.7670135498046875e-05
c613-151: Epoch: 0, Step: 199, Rank: 10, loss = 6.845220923423767e-08
c622-101: Epoch: 0, Step: 199, Rank: 62, loss = 9.74978320300579e-10
c621-122: Epoch: 0, Step: 199, Rank: 35, loss = 1.525040715932846e-08
c621-092: Epoch: 0, Step: 199, Rank: 29, loss = 4.602043190971017e-10
c621-102: Epoch: 0, Step: 199, Rank: 31, loss = 0.000392913818359375
c613-122: Epoch: 0, Step: 199, Rank: 5, loss = 8.998878031629687e-18
c621-132: Epoch: 0, Step: 199, Rank: 37, loss = 9.632110595703125e-05
c619-032: Epoch: 0, Step: 199, Rank: 19, loss = 2.7284841053187847e-12
c619-012: Epoch: 0, Step: 199, Rank: 15, loss = 3.917798799689633e-26
c621-141: Epoch: 0, Step: 199, Rank: 38, loss = 3.0547380447387695e-07
c621-142: Epoch: 0, Step: 199, Rank: 39, loss = 1.0058283805847168e-06
c621-152: Epoch: 0, Step: 199, Rank: 41, loss = 4.3655745685100555e-09
c622-051: Epoch: 0, Step: 199, Rank: 52, loss = 1.6689300537109375e-05
c622-011: Epoch: 0, Step: 199, Rank: 44, loss = 6.344635039567947e-09
c621-071: Epoch: 0, Step: 199, Rank: 24, loss = 6.007030606269836e-08
c613-102: Epoch: 0, Step: 199, Rank: 1, loss = 2.2118911147117615e-08
c619-011: Epoch: 0, Step: 199, Rank: 14, loss = 6.07222318649292e-07
c622-082: Epoch: 0, Step: 199, Rank: 59, loss = 5.3085386753082275e-08
c622-042: Epoch: 0, Step: 199, Rank: 51, loss = 2.2649765014648438e-06
c622-092: Epoch: 0, Step: 199, Rank: 61, loss = 4.172325134277344e-07
c613-121: Epoch: 0, Step: 199, Rank: 4, loss = 2.726912498474121e-06
c622-021: Epoch: 0, Step: 199, Rank: 46, loss = 0.0
c621-061: Epoch: 0, Step: 199, Rank: 22, loss = 7.729977369308472e-08
c613-142: Epoch: 0, Step: 199, Rank: 9, loss = 1.2197274440461925e-18
c621-062: Epoch: 0, Step: 199, Rank: 23, loss = 1.1874362826347351e-08
c613-111: Epoch: 0, Step: 199, Rank: 2, loss = 0.00016880035400390625
c622-102: Epoch: 0, Step: 199, Rank: 63, loss = 8.754432201385498e-08
c622-072: Epoch: 0, Step: 199, Rank: 57, loss = 0.69140625
c613-112: Epoch: 0, Step: 199, Rank: 3, loss = 3.213062882423401e-08
c622-071: Epoch: 0, Step: 199, Rank: 56, loss = 2.1736923372372985e-10
c621-112: Epoch: 0, Step: 199, Rank: 33, loss = 2.753734588623047e-05
c622-062: Epoch: 0, Step: 199, Rank: 55, loss = 8.754432201385498e-08
c622-091: Epoch: 0, Step: 199, Rank: 60, loss = 1.6391277313232422e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 200, Rank: 53, loss = 2.342858351767063e-09
c622-002: Epoch: 0, Step: 200, Rank: 43, loss = 1.1874362826347351e-08
c621-132: Epoch: 0, Step: 200, Rank: 37, loss = 1.150369644165039e-05
c622-051: Epoch: 0, Step: 200, Rank: 52, loss = 3.3921018924503508e-28
c613-101: Epoch: 0, Step: 200, Rank: 0, loss = 7.987216665362745e-30
c621-081: Epoch: 0, Step: 200, Rank: 26, loss = 3.4051481634378433e-09
c621-142: Epoch: 0, Step: 200, Rank: 39, loss = 0.0005035400390625
c622-012: Epoch: 0, Step: 200, Rank: 45, loss = 2.2118911147117615e-08
c621-072: Epoch: 0, Step: 200, Rank: 25, loss = 1.5366822481155396e-07
c622-081: Epoch: 0, Step: 200, Rank: 58, loss = 3.314018249511719e-05
c622-001: Epoch: 0, Step: 200, Rank: 42, loss = 1.2069940567016602e-06
c621-111: Epoch: 0, Step: 200, Rank: 32, loss = 0.0
c621-151: Epoch: 0, Step: 200, Rank: 40, loss = 2.9976945370435715e-09
c621-131: Epoch: 0, Step: 200, Rank: 36, loss = 0.000606536865234375
c619-021: Epoch: 0, Step: 200, Rank: 16, loss = 2.0236257114447653e-11
c621-082: Epoch: 0, Step: 200, Rank: 27, loss = 5.424022674560547e-06
c622-041: Epoch: 0, Step: 200, Rank: 50, loss = 6.314393452555578e-16
c621-141: Epoch: 0, Step: 200, Rank: 38, loss = 0.001983642578125
c621-152: Epoch: 0, Step: 200, Rank: 41, loss = 7.338821887969971e-07
c622-061: Epoch: 0, Step: 200, Rank: 54, loss = 1.55717134475708e-06
c622-032: Epoch: 0, Step: 200, Rank: 49, loss = 4.782137916322191e-25
c619-031: Epoch: 0, Step: 200, Rank: 18, loss = 9.918585419654846e-08
c622-031: Epoch: 0, Step: 200, Rank: 48, loss = 2.1047890186309814e-07
c621-101: Epoch: 0, Step: 200, Rank: 30, loss = 1.1399388313293457e-06
c621-061: Epoch: 0, Step: 200, Rank: 22, loss = 1.8758328224066645e-12
c622-092: Epoch: 0, Step: 200, Rank: 61, loss = 0.0001087188720703125
c621-122: Epoch: 0, Step: 200, Rank: 35, loss = 7.867813110351562e-06
c613-151: Epoch: 0, Step: 200, Rank: 10, loss = 2.0236257114447653e-11
c621-102: Epoch: 0, Step: 200, Rank: 31, loss = 2.726912498474121e-06
c619-032: Epoch: 0, Step: 200, Rank: 19, loss = 1.3709068298339844e-06
c621-092: Epoch: 0, Step: 200, Rank: 29, loss = 0.000553131103515625
c613-152: Epoch: 0, Step: 200, Rank: 11, loss = 5.14984130859375e-05
c622-011: Epoch: 0, Step: 200, Rank: 44, loss = 6.973743438720703e-06
c619-041: Epoch: 0, Step: 200, Rank: 20, loss = 9.549694368615746e-12
c622-021: Epoch: 0, Step: 200, Rank: 46, loss = 0.0002460479736328125
c619-011: Epoch: 0, Step: 200, Rank: 14, loss = 5.3085386753082275e-08
c622-022: Epoch: 0, Step: 200, Rank: 47, loss = 2.5331974029541016e-07
c613-132: Epoch: 0, Step: 200, Rank: 7, loss = 1.525040715932846e-08
c619-001: Epoch: 0, Step: 200, Rank: 12, loss = 0.000179290771484375
c621-052: Epoch: 0, Step: 200, Rank: 21, loss = 1.7848833522293717e-11
c613-131: Epoch: 0, Step: 200, Rank: 6, loss = 0.0242919921875
c622-101: Epoch: 0, Step: 200, Rank: 62, loss = 0.69140625
c622-071: Epoch: 0, Step: 200, Rank: 56, loss = 0.0
c619-002: Epoch: 0, Step: 200, Rank: 13, loss = 8.307397365570068e-07
c613-142: Epoch: 0, Step: 200, Rank: 9, loss = 2.7830537874251604e-10
c622-062: Epoch: 0, Step: 200, Rank: 55, loss = 1.5366822481155396e-07
c621-121: Epoch: 0, Step: 200, Rank: 34, loss = 3.841705620288849e-09
c619-022: Epoch: 0, Step: 200, Rank: 17, loss = 8.404254913330078e-06
c622-042: Epoch: 0, Step: 200, Rank: 51, loss = 8.487701416015625e-05
c622-082: Epoch: 0, Step: 200, Rank: 59, loss = 5.699694156646729e-07
c619-012: Epoch: 0, Step: 200, Rank: 15, loss = 0.0
c613-122: Epoch: 0, Step: 200, Rank: 5, loss = 4.4517219066619873e-07
c613-121: Epoch: 0, Step: 200, Rank: 4, loss = 0.0001583099365234375
c621-112: Epoch: 0, Step: 200, Rank: 33, loss = 3.293156623840332e-06
c622-102: Epoch: 0, Step: 200, Rank: 63, loss = 4.94765117764473e-09
c613-102: Epoch: 0, Step: 200, Rank: 1, loss = 3.213062882423401e-08
c613-141: Epoch: 0, Step: 200, Rank: 8, loss = 4.301339185275744e-23
c613-111: Epoch: 0, Step: 200, Rank: 2, loss = 7.188646122813225e-09
c621-062: Epoch: 0, Step: 200, Rank: 23, loss = 1.6689300537109375e-05
c621-071: Epoch: 0, Step: 200, Rank: 24, loss = 0.000335693359375
c613-112: Epoch: 0, Step: 200, Rank: 3, loss = 6.973743438720703e-06
c622-072: Epoch: 0, Step: 200, Rank: 57, loss = 3.655441105365753e-08
c621-091: Epoch: 0, Step: 200, Rank: 28, loss = 0.0
c622-091: Epoch: 0, Step: 200, Rank: 60, loss = 1.2197274440461925e-18
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24609375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 201, Rank: 43, loss = 9.74978320300579e-10
c621-111: Epoch: 0, Step: 201, Rank: 32, loss = 1.0132789611816406e-05
c621-081: Epoch: 0, Step: 201, Rank: 26, loss = 1.2069940567016602e-06
c621-052: Epoch: 0, Step: 201, Rank: 21, loss = 0.0
c621-131: Epoch: 0, Step: 201, Rank: 36, loss = 4.843059286940843e-11
c613-101: Epoch: 0, Step: 201, Rank: 0, loss = 1.3589129821411916e-13
c619-021: Epoch: 0, Step: 201, Rank: 16, loss = 7.188646122813225e-09
c621-121: Epoch: 0, Step: 201, Rank: 34, loss = 9.74978320300579e-10
c621-101: Epoch: 0, Step: 201, Rank: 30, loss = 0.0
c619-002: Epoch: 0, Step: 201, Rank: 13, loss = 2.2649765014648438e-06
c621-102: Epoch: 0, Step: 201, Rank: 31, loss = 0.0
c621-122: Epoch: 0, Step: 201, Rank: 35, loss = 0.0
c622-052: Epoch: 0, Step: 201, Rank: 53, loss = 0.0
c621-061: Epoch: 0, Step: 201, Rank: 22, loss = 5.617039278149605e-09
c621-082: Epoch: 0, Step: 201, Rank: 27, loss = 2.7830537874251604e-10
c622-051: Epoch: 0, Step: 201, Rank: 52, loss = 0.00058746337890625
c619-031: Epoch: 0, Step: 201, Rank: 18, loss = 1.8189894035458565e-09
c622-081: Epoch: 0, Step: 201, Rank: 58, loss = 0.69140625
c621-132: Epoch: 0, Step: 201, Rank: 37, loss = 1.3828277587890625e-05
c621-151: Epoch: 0, Step: 201, Rank: 40, loss = 2.342858351767063e-09
c621-142: Epoch: 0, Step: 201, Rank: 39, loss = 8.940696716308594e-06
c621-072: Epoch: 0, Step: 201, Rank: 25, loss = 1.2514647096395493e-09
c622-041: Epoch: 0, Step: 201, Rank: 50, loss = 3.6845933205562065e-20
c621-112: Epoch: 0, Step: 201, Rank: 33, loss = 7.420778274536133e-06
c619-011: Epoch: 0, Step: 201, Rank: 14, loss = 2.726912498474121e-06
c621-152: Epoch: 0, Step: 201, Rank: 41, loss = 2.726912498474121e-06
c622-012: Epoch: 0, Step: 201, Rank: 45, loss = 8.585629984736443e-10
c619-032: Epoch: 0, Step: 201, Rank: 19, loss = 5.029141902923584e-07
c613-111: Epoch: 0, Step: 201, Rank: 2, loss = 1.2069940567016602e-06
c621-092: Epoch: 0, Step: 201, Rank: 29, loss = 9.370282327836321e-14
c622-092: Epoch: 0, Step: 201, Rank: 61, loss = 6.565414878423326e-12
c621-062: Epoch: 0, Step: 201, Rank: 23, loss = 2.5331974029541016e-07
c622-101: Epoch: 0, Step: 201, Rank: 62, loss = 3.213062882423401e-08
c622-032: Epoch: 0, Step: 201, Rank: 49, loss = 2.9976945370435715e-09
c619-022: Epoch: 0, Step: 201, Rank: 17, loss = 9.918585419654846e-08
c619-041: Epoch: 0, Step: 201, Rank: 20, loss = 6.230038707144558e-11
c613-141: Epoch: 0, Step: 201, Rank: 8, loss = 0.0
c622-001: Epoch: 0, Step: 201, Rank: 42, loss = 4.6798959374427795e-08
c621-141: Epoch: 0, Step: 201, Rank: 38, loss = 3.46451997756958e-07
c619-001: Epoch: 0, Step: 201, Rank: 12, loss = 5.400124791776761e-13
c613-112: Epoch: 0, Step: 201, Rank: 3, loss = 5.20230969414115e-10
c621-071: Epoch: 0, Step: 201, Rank: 24, loss = 3.293156623840332e-06
c622-021: Epoch: 0, Step: 201, Rank: 46, loss = 3.0547380447387695e-07
c613-122: Epoch: 0, Step: 201, Rank: 5, loss = 3.655441105365753e-08
c613-151: Epoch: 0, Step: 201, Rank: 10, loss = 1.8758328224066645e-12
c622-042: Epoch: 0, Step: 201, Rank: 51, loss = 5.227781471335135e-22
c613-152: Epoch: 0, Step: 201, Rank: 11, loss = 2.455635694786906e-10
c613-131: Epoch: 0, Step: 201, Rank: 6, loss = 1.318767317570746e-10
c622-011: Epoch: 0, Step: 201, Rank: 44, loss = 6.628036499023438e-05
c622-102: Epoch: 0, Step: 201, Rank: 63, loss = 2.455635694786906e-10
c622-022: Epoch: 0, Step: 201, Rank: 47, loss = 0.0002460479736328125
c613-142: Epoch: 0, Step: 201, Rank: 9, loss = 1.8533319234848022e-07
c619-012: Epoch: 0, Step: 201, Rank: 15, loss = 5.182486384480711e-17
c613-132: Epoch: 0, Step: 201, Rank: 7, loss = 4.3655745685100555e-09
c622-071: Epoch: 0, Step: 201, Rank: 56, loss = 7.566995918750763e-10
c622-061: Epoch: 0, Step: 201, Rank: 54, loss = 4.843059286940843e-11
c622-031: Epoch: 0, Step: 201, Rank: 48, loss = 7.66053886991358e-15
c621-091: Epoch: 0, Step: 201, Rank: 28, loss = 4.4517219066619873e-07
c613-102: Epoch: 0, Step: 201, Rank: 1, loss = 3.314018249511719e-05
c613-121: Epoch: 0, Step: 201, Rank: 4, loss = 7.188646122813225e-09
c622-072: Epoch: 0, Step: 201, Rank: 57, loss = 0.000179290771484375
c622-062: Epoch: 0, Step: 201, Rank: 55, loss = 8.003553375601768e-11
c622-082: Epoch: 0, Step: 201, Rank: 59, loss = 5.3085386753082275e-08
c622-091: Epoch: 0, Step: 201, Rank: 60, loss = 2.1457672119140625e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 202, Rank: 0, loss = 2.3647750424515834e-14
c619-021: Epoch: 0, Step: 202, Rank: 16, loss = 1.1059455573558807e-09
c622-012: Epoch: 0, Step: 202, Rank: 45, loss = 1.8189894035458565e-09
c622-002: Epoch: 0, Step: 202, Rank: 43, loss = 1.8775463104248047e-06
c619-002: Epoch: 0, Step: 202, Rank: 13, loss = 3.655441105365753e-08
c621-132: Epoch: 0, Step: 202, Rank: 37, loss = 2.648448571562767e-09
c622-052: Epoch: 0, Step: 202, Rank: 53, loss = 8.307397365570068e-07
c622-081: Epoch: 0, Step: 202, Rank: 58, loss = 1.1059455573558807e-09
c621-151: Epoch: 0, Step: 202, Rank: 40, loss = 0.0
c613-152: Epoch: 0, Step: 202, Rank: 11, loss = 4.1443854570388794e-08
c622-001: Epoch: 0, Step: 202, Rank: 42, loss = 0.0
c613-142: Epoch: 0, Step: 202, Rank: 9, loss = 3.144186300207963e-17
c613-111: Epoch: 0, Step: 202, Rank: 2, loss = 2.7008354663848877e-07
c621-111: Epoch: 0, Step: 202, Rank: 32, loss = 5.424022674560547e-06
c622-051: Epoch: 0, Step: 202, Rank: 52, loss = 3.583409124985337e-10
c619-012: Epoch: 0, Step: 202, Rank: 15, loss = 5.4836273193359375e-05
c613-132: Epoch: 0, Step: 202, Rank: 7, loss = 0.0003147125244140625
c621-082: Epoch: 0, Step: 202, Rank: 27, loss = 3.688037395477295e-07
c621-142: Epoch: 0, Step: 202, Rank: 39, loss = 4.94765117764473e-09
c621-141: Epoch: 0, Step: 202, Rank: 38, loss = 0.0005035400390625
c613-151: Epoch: 0, Step: 202, Rank: 10, loss = 5.0961971282958984e-06
c619-022: Epoch: 0, Step: 202, Rank: 17, loss = 9.424984455108643e-07
c621-052: Epoch: 0, Step: 202, Rank: 21, loss = 3.917798799689633e-26
c622-092: Epoch: 0, Step: 202, Rank: 61, loss = 7.188646122813225e-09
c621-071: Epoch: 0, Step: 202, Rank: 24, loss = 1.55717134475708e-06
c621-091: Epoch: 0, Step: 202, Rank: 28, loss = 1.2218952178955078e-05
c621-101: Epoch: 0, Step: 202, Rank: 30, loss = 5.115907697472721e-12
c613-122: Epoch: 0, Step: 202, Rank: 5, loss = 5.3085386753082275e-08
c622-102: Epoch: 0, Step: 202, Rank: 63, loss = 6.198883056640625e-05
c613-141: Epoch: 0, Step: 202, Rank: 8, loss = 4.760636329592671e-13
c619-032: Epoch: 0, Step: 202, Rank: 19, loss = 1.053497228147536e-20
c622-101: Epoch: 0, Step: 202, Rank: 62, loss = 2.9325485229492188e-05
c619-001: Epoch: 0, Step: 202, Rank: 12, loss = 0.00075531005859375
c621-152: Epoch: 0, Step: 202, Rank: 41, loss = 1.1874362826347351e-08
c619-031: Epoch: 0, Step: 202, Rank: 18, loss = 3.213062882423401e-08
c621-081: Epoch: 0, Step: 202, Rank: 26, loss = 3.583409124985337e-10
c613-131: Epoch: 0, Step: 202, Rank: 6, loss = 1.6391277313232422e-07
c622-011: Epoch: 0, Step: 202, Rank: 44, loss = 1.3828277587890625e-05
c622-061: Epoch: 0, Step: 202, Rank: 54, loss = 7.729977369308472e-08
c613-121: Epoch: 0, Step: 202, Rank: 4, loss = 1.318767317570746e-10
c613-112: Epoch: 0, Step: 202, Rank: 3, loss = 3.655441105365753e-08
c621-121: Epoch: 0, Step: 202, Rank: 34, loss = 7.338821887969971e-07
c619-041: Epoch: 0, Step: 202, Rank: 20, loss = 2.9331204132176936e-11
c619-011: Epoch: 0, Step: 202, Rank: 14, loss = 6.693881005048752e-10
c621-122: Epoch: 0, Step: 202, Rank: 35, loss = 6.628036499023438e-05
c622-041: Epoch: 0, Step: 202, Rank: 50, loss = 1.2759119272232056e-07
c621-061: Epoch: 0, Step: 202, Rank: 22, loss = 8.543513119185775e-17
c622-031: Epoch: 0, Step: 202, Rank: 48, loss = 0.05322265625
c622-032: Epoch: 0, Step: 202, Rank: 49, loss = 0.0002956390380859375
c622-042: Epoch: 0, Step: 202, Rank: 51, loss = 5.3085386753082275e-08
c621-131: Epoch: 0, Step: 202, Rank: 36, loss = 1.8041124150158794e-16
c622-022: Epoch: 0, Step: 202, Rank: 47, loss = 5.14984130859375e-05
c621-112: Epoch: 0, Step: 202, Rank: 33, loss = 2.648448571562767e-09
c622-071: Epoch: 0, Step: 202, Rank: 56, loss = 3.314018249511719e-05
c613-102: Epoch: 0, Step: 202, Rank: 1, loss = 8.754432201385498e-08
c622-021: Epoch: 0, Step: 202, Rank: 46, loss = 2.1047890186309814e-07
c622-082: Epoch: 0, Step: 202, Rank: 59, loss = 3.841705620288849e-09
c621-102: Epoch: 0, Step: 202, Rank: 31, loss = 2.1457672119140625e-05
c621-062: Epoch: 0, Step: 202, Rank: 23, loss = 5.699694156646729e-07
c621-092: Epoch: 0, Step: 202, Rank: 29, loss = 1.318767317570746e-10
c622-091: Epoch: 0, Step: 202, Rank: 60, loss = 2.2851054382044822e-11
c622-062: Epoch: 0, Step: 202, Rank: 55, loss = 3.293156623840332e-06
c622-072: Epoch: 0, Step: 202, Rank: 57, loss = 3.213062882423401e-08
c621-072: Epoch: 0, Step: 202, Rank: 25, loss = 5.115907697472721e-12
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2451171875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 203, Rank: 16, loss = 2.905726432800293e-06
c622-052: Epoch: 0, Step: 203, Rank: 53, loss = 2.234049398383217e-20
c622-081: Epoch: 0, Step: 203, Rank: 58, loss = 0.69140625
c613-101: Epoch: 0, Step: 203, Rank: 0, loss = 7.338821887969971e-07
c622-002: Epoch: 0, Step: 203, Rank: 43, loss = 7.729977369308472e-08
c619-032: Epoch: 0, Step: 203, Rank: 19, loss = 1.126900315284729e-07
c619-001: Epoch: 0, Step: 203, Rank: 12, loss = 0.0
c619-022: Epoch: 0, Step: 203, Rank: 17, loss = 0.69140625
c619-031: Epoch: 0, Step: 203, Rank: 18, loss = 0.0
c621-142: Epoch: 0, Step: 203, Rank: 39, loss = 0.0
c613-151: Epoch: 0, Step: 203, Rank: 10, loss = 4.301339185275744e-23
c621-091: Epoch: 0, Step: 203, Rank: 28, loss = 2.455635694786906e-10
c622-092: Epoch: 0, Step: 203, Rank: 61, loss = 5.115907697472721e-12
c619-041: Epoch: 0, Step: 203, Rank: 20, loss = 2.648448571562767e-09
c621-151: Epoch: 0, Step: 203, Rank: 40, loss = 5.781650543212891e-06
c613-132: Epoch: 0, Step: 203, Rank: 7, loss = 8.003553375601768e-11
c622-001: Epoch: 0, Step: 203, Rank: 42, loss = 1.150369644165039e-05
c613-121: Epoch: 0, Step: 203, Rank: 4, loss = 1.2759119272232056e-07
c621-111: Epoch: 0, Step: 203, Rank: 32, loss = 6.973743438720703e-06
c621-072: Epoch: 0, Step: 203, Rank: 25, loss = 0.0
c613-141: Epoch: 0, Step: 203, Rank: 8, loss = 3.688037395477295e-07
c621-081: Epoch: 0, Step: 203, Rank: 26, loss = 5.4836273193359375e-05
c619-002: Epoch: 0, Step: 203, Rank: 13, loss = 2.7008354663848877e-07
c621-152: Epoch: 0, Step: 203, Rank: 41, loss = 0.0002956390380859375
c613-142: Epoch: 0, Step: 203, Rank: 9, loss = 2.5331974029541016e-07
c621-132: Epoch: 0, Step: 203, Rank: 37, loss = 0.0002460479736328125
c622-071: Epoch: 0, Step: 203, Rank: 56, loss = 8.307397365570068e-07
c622-051: Epoch: 0, Step: 203, Rank: 52, loss = 1.2656542480726785e-14
c622-012: Epoch: 0, Step: 203, Rank: 45, loss = 8.487701416015625e-05
c613-102: Epoch: 0, Step: 203, Rank: 1, loss = 1.3869794202037156e-11
c622-032: Epoch: 0, Step: 203, Rank: 49, loss = 1.55717134475708e-06
c621-121: Epoch: 0, Step: 203, Rank: 34, loss = 2.648448571562767e-09
c613-152: Epoch: 0, Step: 203, Rank: 11, loss = 1.9806378759312793e-13
c622-041: Epoch: 0, Step: 203, Rank: 50, loss = 7.566995918750763e-10
c622-031: Epoch: 0, Step: 203, Rank: 48, loss = 3.7670135498046875e-05
c619-011: Epoch: 0, Step: 203, Rank: 14, loss = 1.8775463104248047e-06
c622-061: Epoch: 0, Step: 203, Rank: 54, loss = 0.00014019012451171875
c621-082: Epoch: 0, Step: 203, Rank: 27, loss = 4.1443854570388794e-08
c622-101: Epoch: 0, Step: 203, Rank: 62, loss = 1.525040715932846e-08
c619-012: Epoch: 0, Step: 203, Rank: 15, loss = 1.1874362826347351e-08
c613-111: Epoch: 0, Step: 203, Rank: 2, loss = 1.0058283805847168e-06
c622-011: Epoch: 0, Step: 203, Rank: 44, loss = 0.0
c621-071: Epoch: 0, Step: 203, Rank: 24, loss = 8.940696716308594e-06
c622-021: Epoch: 0, Step: 203, Rank: 46, loss = 0.000179290771484375
c613-131: Epoch: 0, Step: 203, Rank: 6, loss = 3.293156623840332e-06
c621-061: Epoch: 0, Step: 203, Rank: 22, loss = 0.0
c621-112: Epoch: 0, Step: 203, Rank: 33, loss = 2.1047890186309814e-07
c622-062: Epoch: 0, Step: 203, Rank: 55, loss = 1.55717134475708e-06
c621-101: Epoch: 0, Step: 203, Rank: 30, loss = 0.00083160400390625
c621-122: Epoch: 0, Step: 203, Rank: 35, loss = 3.774403012357652e-11
c622-022: Epoch: 0, Step: 203, Rank: 47, loss = 1.3828277587890625e-05
c621-102: Epoch: 0, Step: 203, Rank: 31, loss = 3.0547380447387695e-07
c621-052: Epoch: 0, Step: 203, Rank: 21, loss = 1.55717134475708e-06
c622-042: Epoch: 0, Step: 203, Rank: 51, loss = 0.69140625
c622-102: Epoch: 0, Step: 203, Rank: 63, loss = 0.0
c621-062: Epoch: 0, Step: 203, Rank: 23, loss = 1.318767317570746e-10
c613-112: Epoch: 0, Step: 203, Rank: 3, loss = 1.1641532182693481e-10
c622-091: Epoch: 0, Step: 203, Rank: 60, loss = 5.502442945726216e-11
c621-141: Epoch: 0, Step: 203, Rank: 38, loss = 2.648448571562767e-09
c622-072: Epoch: 0, Step: 203, Rank: 57, loss = 7.566995918750763e-10
c621-092: Epoch: 0, Step: 203, Rank: 29, loss = 0.00010251998901367188
c613-122: Epoch: 0, Step: 203, Rank: 5, loss = 2.514570951461792e-08
c621-131: Epoch: 0, Step: 203, Rank: 36, loss = 2.4035605705952703e-31
c622-082: Epoch: 0, Step: 203, Rank: 59, loss = 0.0004730224609375
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.10s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.10s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 204, Rank: 28, loss = 0.0
c621-081: Epoch: 0, Step: 204, Rank: 26, loss = 4.839897155761719e-05
c621-072: Epoch: 0, Step: 204, Rank: 25, loss = 1.996755599975586e-06
c621-082: Epoch: 0, Step: 204, Rank: 27, loss = 8.149072527885437e-09
c621-092: Epoch: 0, Step: 204, Rank: 29, loss = 8.003553375601768e-11
c621-101: Epoch: 0, Step: 204, Rank: 30, loss = 2.2649765014648438e-06
c621-111: Epoch: 0, Step: 204, Rank: 32, loss = 2.6056189295420372e-23
c621-142: Epoch: 0, Step: 204, Rank: 39, loss = 2.540190280342358e-13
c621-071: Epoch: 0, Step: 204, Rank: 24, loss = 5.617039278149605e-09
c621-102: Epoch: 0, Step: 204, Rank: 31, loss = 1.1059455573558807e-09
c622-002: Epoch: 0, Step: 204, Rank: 43, loss = 1.0800249583553523e-11
c621-062: Epoch: 0, Step: 204, Rank: 23, loss = 0.0
c621-052: Epoch: 0, Step: 204, Rank: 21, loss = 4.172325134277344e-07
c621-132: Epoch: 0, Step: 204, Rank: 37, loss = 1.150369644165039e-05
c621-121: Epoch: 0, Step: 204, Rank: 34, loss = 2.4318695068359375e-05
c621-061: Epoch: 0, Step: 204, Rank: 22, loss = 2.1047890186309814e-07
c621-131: Epoch: 0, Step: 204, Rank: 36, loss = 1.150369644165039e-05
c622-001: Epoch: 0, Step: 204, Rank: 42, loss = 2.648448571562767e-09
c621-151: Epoch: 0, Step: 204, Rank: 40, loss = 0.0
c622-012: Epoch: 0, Step: 204, Rank: 45, loss = 1.6916601452976465e-10
c621-112: Epoch: 0, Step: 204, Rank: 33, loss = 4.926614671774132e-16
c621-122: Epoch: 0, Step: 204, Rank: 35, loss = 0.0
c619-041: Epoch: 0, Step: 204, Rank: 20, loss = 5.3085386753082275e-08
c621-141: Epoch: 0, Step: 204, Rank: 38, loss = 2.9325485229492188e-05
c621-152: Epoch: 0, Step: 204, Rank: 41, loss = 3.841705620288849e-09
c622-011: Epoch: 0, Step: 204, Rank: 44, loss = 4.05634636990726e-10
c619-032: Epoch: 0, Step: 204, Rank: 19, loss = 5.14984130859375e-05
c622-021: Epoch: 0, Step: 204, Rank: 46, loss = 0.0002956390380859375
c619-021: Epoch: 0, Step: 204, Rank: 16, loss = 1.150369644165039e-05
c619-031: Epoch: 0, Step: 204, Rank: 18, loss = 3.213062882423401e-08
c619-022: Epoch: 0, Step: 204, Rank: 17, loss = 4.00543212890625e-05
c619-012: Epoch: 0, Step: 204, Rank: 15, loss = 0.0
c619-002: Epoch: 0, Step: 204, Rank: 13, loss = 3.1650415621697903e-10
c622-022: Epoch: 0, Step: 204, Rank: 47, loss = 4.4517219066619873e-07
c619-011: Epoch: 0, Step: 204, Rank: 14, loss = 7.486343383789062e-05
c619-001: Epoch: 0, Step: 204, Rank: 12, loss = 0.69140625
c613-152: Epoch: 0, Step: 204, Rank: 11, loss = 5.115907697472721e-12
c613-101: Epoch: 0, Step: 204, Rank: 0, loss = 1.3445969671010971e-08
c613-151: Epoch: 0, Step: 204, Rank: 10, loss = 9.74978320300579e-10
c613-142: Epoch: 0, Step: 204, Rank: 9, loss = 3.6734198463196485e-39
c622-031: Epoch: 0, Step: 204, Rank: 48, loss = 4.760636329592671e-13
c622-081: Epoch: 0, Step: 204, Rank: 58, loss = 0.0
c613-141: Epoch: 0, Step: 204, Rank: 8, loss = 5.699694156646729e-07
c622-041: Epoch: 0, Step: 204, Rank: 50, loss = 4.05634636990726e-10
c613-132: Epoch: 0, Step: 204, Rank: 7, loss = 1.418811734765768e-09
c613-131: Epoch: 0, Step: 204, Rank: 6, loss = 1.5366822481155396e-07
c622-071: Epoch: 0, Step: 204, Rank: 56, loss = 0.69140625
c622-101: Epoch: 0, Step: 204, Rank: 62, loss = 3.583409124985337e-10
c622-092: Epoch: 0, Step: 204, Rank: 61, loss = 0.002899169921875
c613-111: Epoch: 0, Step: 204, Rank: 2, loss = 2.1047890186309814e-07
c622-062: Epoch: 0, Step: 204, Rank: 55, loss = 0.0
c622-102: Epoch: 0, Step: 204, Rank: 63, loss = 5.3085386753082275e-08
c622-051: Epoch: 0, Step: 204, Rank: 52, loss = 8.149072527885437e-09
c613-122: Epoch: 0, Step: 204, Rank: 5, loss = 9.632110595703125e-05
c622-061: Epoch: 0, Step: 204, Rank: 54, loss = 2.726912498474121e-06
c622-052: Epoch: 0, Step: 204, Rank: 53, loss = 0.00083160400390625
c613-112: Epoch: 0, Step: 204, Rank: 3, loss = 1.2514647096395493e-09
c613-102: Epoch: 0, Step: 204, Rank: 1, loss = 7.188646122813225e-09
c622-091: Epoch: 0, Step: 204, Rank: 60, loss = 8.754432201385498e-08
c622-032: Epoch: 0, Step: 204, Rank: 49, loss = 4.5299530029296875e-05
c622-072: Epoch: 0, Step: 204, Rank: 57, loss = 1.6079866327345371e-09
c622-042: Epoch: 0, Step: 204, Rank: 51, loss = 1.1874362826347351e-08
c613-121: Epoch: 0, Step: 204, Rank: 4, loss = 6.845220923423767e-08
c622-082: Epoch: 0, Step: 204, Rank: 59, loss = 2.1047890186309814e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 205, Rank: 0, loss = 3.688037395477295e-07
c622-081: Epoch: 0, Step: 205, Rank: 58, loss = 0.000518798828125
c622-101: Epoch: 0, Step: 205, Rank: 62, loss = 3.583409124985337e-10
c622-092: Epoch: 0, Step: 205, Rank: 61, loss = 2.066371962428093e-09
c619-021: Epoch: 0, Step: 205, Rank: 16, loss = 8.003553375601768e-11
c613-121: Epoch: 0, Step: 205, Rank: 4, loss = 2.2118911147117615e-08
c613-112: Epoch: 0, Step: 205, Rank: 3, loss = 4.231929779052734e-06
c613-151: Epoch: 0, Step: 205, Rank: 10, loss = 9.74978320300579e-10
c619-002: Epoch: 0, Step: 205, Rank: 13, loss = 6.891787052154541e-07
c619-011: Epoch: 0, Step: 205, Rank: 14, loss = 1.0800249583553523e-11
c619-001: Epoch: 0, Step: 205, Rank: 12, loss = 1.2069940567016602e-06
c613-132: Epoch: 0, Step: 205, Rank: 7, loss = 0.0
c613-131: Epoch: 0, Step: 205, Rank: 6, loss = 9.424984455108643e-07
c622-082: Epoch: 0, Step: 205, Rank: 59, loss = 0.69140625
c613-152: Epoch: 0, Step: 205, Rank: 11, loss = 6.07222318649292e-07
c619-022: Epoch: 0, Step: 205, Rank: 17, loss = 2.2118911147117615e-08
c613-111: Epoch: 0, Step: 205, Rank: 2, loss = 6.973743438720703e-06
c622-102: Epoch: 0, Step: 205, Rank: 63, loss = 6.230038707144558e-11
c622-062: Epoch: 0, Step: 205, Rank: 55, loss = 0.0
c613-142: Epoch: 0, Step: 205, Rank: 9, loss = 1.8533319234848022e-07
c622-091: Epoch: 0, Step: 205, Rank: 60, loss = 7.66053886991358e-15
c613-122: Epoch: 0, Step: 205, Rank: 5, loss = 0.0
c621-151: Epoch: 0, Step: 205, Rank: 40, loss = 1.1874362826347351e-08
c613-141: Epoch: 0, Step: 205, Rank: 8, loss = 2.4318695068359375e-05
c613-102: Epoch: 0, Step: 205, Rank: 1, loss = 5.699694156646729e-07
c619-031: Epoch: 0, Step: 205, Rank: 18, loss = 7.729977369308472e-08
c621-072: Epoch: 0, Step: 205, Rank: 25, loss = 0.265625
c621-122: Epoch: 0, Step: 205, Rank: 35, loss = 1.9806378759312793e-13
c621-081: Epoch: 0, Step: 205, Rank: 26, loss = 1.6079866327345371e-09
c621-112: Epoch: 0, Step: 205, Rank: 33, loss = 8.585629984736443e-10
c621-152: Epoch: 0, Step: 205, Rank: 41, loss = 0.0001583099365234375
c622-071: Epoch: 0, Step: 205, Rank: 56, loss = 2.905726432800293e-06
c622-001: Epoch: 0, Step: 205, Rank: 42, loss = 8.754432201385498e-08
c622-002: Epoch: 0, Step: 205, Rank: 43, loss = 9.74978320300579e-10
c621-131: Epoch: 0, Step: 205, Rank: 36, loss = 0.000457763671875
c621-121: Epoch: 0, Step: 205, Rank: 34, loss = 0.69140625
c619-012: Epoch: 0, Step: 205, Rank: 15, loss = 1.6079866327345371e-09
c622-072: Epoch: 0, Step: 205, Rank: 57, loss = 0.0
c621-111: Epoch: 0, Step: 205, Rank: 32, loss = 8.754432201385498e-08
c621-101: Epoch: 0, Step: 205, Rank: 30, loss = 0.003173828125
c622-012: Epoch: 0, Step: 205, Rank: 45, loss = 7.338821887969971e-07
c621-082: Epoch: 0, Step: 205, Rank: 27, loss = 0.0002956390380859375
c621-132: Epoch: 0, Step: 205, Rank: 37, loss = 3.979039320256561e-12
c622-051: Epoch: 0, Step: 205, Rank: 52, loss = 5.20230969414115e-10
c622-061: Epoch: 0, Step: 205, Rank: 54, loss = 9.918585419654846e-08
c621-142: Epoch: 0, Step: 205, Rank: 39, loss = 0.0052490234375
c621-091: Epoch: 0, Step: 205, Rank: 28, loss = 1.2759119272232056e-07
c621-061: Epoch: 0, Step: 205, Rank: 22, loss = 2.9976945370435715e-09
c622-041: Epoch: 0, Step: 205, Rank: 50, loss = 1.126900315284729e-07
c621-141: Epoch: 0, Step: 205, Rank: 38, loss = 2.7008354663848877e-07
c621-102: Epoch: 0, Step: 205, Rank: 31, loss = 2.905726432800293e-06
c622-052: Epoch: 0, Step: 205, Rank: 53, loss = 2.0057740190981832e-18
c621-052: Epoch: 0, Step: 205, Rank: 21, loss = 4.4517219066619873e-07
c619-041: Epoch: 0, Step: 205, Rank: 20, loss = 1.4227506535912076e-21
c622-032: Epoch: 0, Step: 205, Rank: 49, loss = 2.342858351767063e-09
c621-071: Epoch: 0, Step: 205, Rank: 24, loss = 3.0547380447387695e-07
c621-062: Epoch: 0, Step: 205, Rank: 23, loss = 7.188646122813225e-09
c622-021: Epoch: 0, Step: 205, Rank: 46, loss = 1.6391277313232422e-07
c622-042: Epoch: 0, Step: 205, Rank: 51, loss = 0.0
c622-022: Epoch: 0, Step: 205, Rank: 47, loss = 1.1059455573558807e-09
c619-032: Epoch: 0, Step: 205, Rank: 19, loss = 0.0
c621-092: Epoch: 0, Step: 205, Rank: 29, loss = 0.0002613067626953125
c622-031: Epoch: 0, Step: 205, Rank: 48, loss = 3.213062882423401e-08
c622-011: Epoch: 0, Step: 205, Rank: 44, loss = 1.3597309589385986e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.08s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.08s, Batch Size: 1, Sequence Length: 2048
c621-132: Epoch: 0, Step: 206, Rank: 37, loss = 8.734267942607043e-27
c622-002: Epoch: 0, Step: 206, Rank: 43, loss = 1.150369644165039e-05
c621-111: Epoch: 0, Step: 206, Rank: 32, loss = 0.00180816650390625
c621-121: Epoch: 0, Step: 206, Rank: 34, loss = 6.5635692504717e-31
c621-142: Epoch: 0, Step: 206, Rank: 39, loss = 1.3597309589385986e-07
c621-091: Epoch: 0, Step: 206, Rank: 28, loss = 2.066371962428093e-09
c621-131: Epoch: 0, Step: 206, Rank: 36, loss = 5.424022674560547e-06
c621-081: Epoch: 0, Step: 206, Rank: 26, loss = 8.307397365570068e-07
c622-001: Epoch: 0, Step: 206, Rank: 42, loss = 7.188646122813225e-09
c621-151: Epoch: 0, Step: 206, Rank: 40, loss = 4.4517219066619873e-07
c621-101: Epoch: 0, Step: 206, Rank: 30, loss = 9.74978320300579e-10
c619-022: Epoch: 0, Step: 206, Rank: 17, loss = 3.293156623840332e-06
c613-101: Epoch: 0, Step: 206, Rank: 0, loss = 0.0002307891845703125
c622-012: Epoch: 0, Step: 206, Rank: 45, loss = 1.1874362826347351e-08
c621-112: Epoch: 0, Step: 206, Rank: 33, loss = 4.6798959374427795e-08
c619-021: Epoch: 0, Step: 206, Rank: 16, loss = 6.07222318649292e-07
c621-122: Epoch: 0, Step: 206, Rank: 35, loss = 8.585629984736443e-10
c621-092: Epoch: 0, Step: 206, Rank: 29, loss = 1.0408340855860843e-15
c622-011: Epoch: 0, Step: 206, Rank: 44, loss = 0.69140625
c621-102: Epoch: 0, Step: 206, Rank: 31, loss = 2.2351741790771484e-07
c621-152: Epoch: 0, Step: 206, Rank: 41, loss = 5.893525667488575e-10
c621-141: Epoch: 0, Step: 206, Rank: 38, loss = 2.648448571562767e-09
c622-081: Epoch: 0, Step: 206, Rank: 58, loss = 3.084540367126465e-06
c619-032: Epoch: 0, Step: 206, Rank: 19, loss = 4.94765117764473e-09
c621-072: Epoch: 0, Step: 206, Rank: 25, loss = 3.0547380447387695e-07
c619-031: Epoch: 0, Step: 206, Rank: 18, loss = 0.0002956390380859375
c619-012: Epoch: 0, Step: 206, Rank: 15, loss = 1.7848833522293717e-11
c613-111: Epoch: 0, Step: 206, Rank: 2, loss = 4.3655745685100555e-09
c622-032: Epoch: 0, Step: 206, Rank: 49, loss = 4.05634636990726e-10
c622-041: Epoch: 0, Step: 206, Rank: 50, loss = 1.84297022087776e-14
c622-051: Epoch: 0, Step: 206, Rank: 52, loss = 5.502442945726216e-11
c622-071: Epoch: 0, Step: 206, Rank: 56, loss = 6.139278411865234e-06
c619-001: Epoch: 0, Step: 206, Rank: 12, loss = 4.172325134277344e-07
c622-092: Epoch: 0, Step: 206, Rank: 61, loss = 3.293156623840332e-06
c622-022: Epoch: 0, Step: 206, Rank: 47, loss = 2.4158453015843406e-12
c613-132: Epoch: 0, Step: 206, Rank: 7, loss = 1.3828277587890625e-05
c613-112: Epoch: 0, Step: 206, Rank: 3, loss = 0.000392913818359375
c621-071: Epoch: 0, Step: 206, Rank: 24, loss = 5.617039278149605e-09
c619-002: Epoch: 0, Step: 206, Rank: 13, loss = 7.009506225585938e-05
c613-152: Epoch: 0, Step: 206, Rank: 11, loss = 4.231929779052734e-06
c621-061: Epoch: 0, Step: 206, Rank: 22, loss = 4.6798959374427795e-08
c613-121: Epoch: 0, Step: 206, Rank: 4, loss = 3.123283386230469e-05
c622-031: Epoch: 0, Step: 206, Rank: 48, loss = 2.014636993408203e-05
c619-041: Epoch: 0, Step: 206, Rank: 20, loss = 8.487701416015625e-05
c613-131: Epoch: 0, Step: 206, Rank: 6, loss = 1.6391277313232422e-07
c613-151: Epoch: 0, Step: 206, Rank: 10, loss = 8.881784197001252e-13
c613-141: Epoch: 0, Step: 206, Rank: 8, loss = 5.3085386753082275e-08
c622-042: Epoch: 0, Step: 206, Rank: 51, loss = 0.69140625
c619-011: Epoch: 0, Step: 206, Rank: 14, loss = 2.648448571562767e-09
c613-142: Epoch: 0, Step: 206, Rank: 9, loss = 1.525040715932846e-08
c621-082: Epoch: 0, Step: 206, Rank: 27, loss = 3.46451997756958e-07
c621-052: Epoch: 0, Step: 206, Rank: 21, loss = 0.0
c613-122: Epoch: 0, Step: 206, Rank: 5, loss = 8.940696716308594e-06
c622-102: Epoch: 0, Step: 206, Rank: 63, loss = 1.996755599975586e-06
c622-061: Epoch: 0, Step: 206, Rank: 54, loss = 1.0058283805847168e-06
c622-101: Epoch: 0, Step: 206, Rank: 62, loss = 1.955777406692505e-08
c621-062: Epoch: 0, Step: 206, Rank: 23, loss = 4.1443854570388794e-08
c622-052: Epoch: 0, Step: 206, Rank: 53, loss = 5.699694156646729e-07
c622-062: Epoch: 0, Step: 206, Rank: 55, loss = 2.9325485229492188e-05
c622-021: Epoch: 0, Step: 206, Rank: 46, loss = 0.00083160400390625
c613-102: Epoch: 0, Step: 206, Rank: 1, loss = 4.05634636990726e-10
c622-072: Epoch: 0, Step: 206, Rank: 57, loss = 5.20230969414115e-10
c622-091: Epoch: 0, Step: 206, Rank: 60, loss = 1.0408340855860843e-15
c622-082: Epoch: 0, Step: 206, Rank: 59, loss = 4.1443854570388794e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-002: Epoch: 0, Step: 207, Rank: 13, loss = 0.0
c619-011: Epoch: 0, Step: 207, Rank: 14, loss = 5.424022674560547e-06
c619-021: Epoch: 0, Step: 207, Rank: 16, loss = 7.566995918750763e-10
c619-001: Epoch: 0, Step: 207, Rank: 12, loss = 3.4897757426877174e-19
c619-012: Epoch: 0, Step: 207, Rank: 15, loss = 5.0182080713057076e-14
c621-091: Epoch: 0, Step: 207, Rank: 28, loss = 2.2649765014648438e-06
c621-081: Epoch: 0, Step: 207, Rank: 26, loss = 6.344635039567947e-09
c621-082: Epoch: 0, Step: 207, Rank: 27, loss = 4.172325134277344e-07
c619-022: Epoch: 0, Step: 207, Rank: 17, loss = 5.3085386753082275e-08
c613-131: Epoch: 0, Step: 207, Rank: 6, loss = 1.0058283805847168e-06
c613-122: Epoch: 0, Step: 207, Rank: 5, loss = 2.9976945370435715e-09
c613-132: Epoch: 0, Step: 207, Rank: 7, loss = 5.115907697472721e-12
c619-031: Epoch: 0, Step: 207, Rank: 18, loss = 4.4517219066619873e-07
c613-141: Epoch: 0, Step: 207, Rank: 8, loss = 9.549694368615746e-12
c613-152: Epoch: 0, Step: 207, Rank: 11, loss = 3.655441105365753e-08
c613-142: Epoch: 0, Step: 207, Rank: 9, loss = 0.0
c613-121: Epoch: 0, Step: 207, Rank: 4, loss = 8.404254913330078e-06
c613-151: Epoch: 0, Step: 207, Rank: 10, loss = 1.0477378964424133e-08
c621-052: Epoch: 0, Step: 207, Rank: 21, loss = 7.420778274536133e-06
c621-061: Epoch: 0, Step: 207, Rank: 22, loss = 0.00014019012451171875
c621-072: Epoch: 0, Step: 207, Rank: 25, loss = 1.0089706847793423e-12
c613-112: Epoch: 0, Step: 207, Rank: 3, loss = 1.150369644165039e-05
c619-041: Epoch: 0, Step: 207, Rank: 20, loss = 1.6540288925170898e-06
c613-101: Epoch: 0, Step: 207, Rank: 0, loss = 3.688037395477295e-07
c619-032: Epoch: 0, Step: 207, Rank: 19, loss = 1.6391277313232422e-07
c613-111: Epoch: 0, Step: 207, Rank: 2, loss = 8.66885281955573e-22
c621-071: Epoch: 0, Step: 207, Rank: 24, loss = 6.973743438720703e-06
c613-102: Epoch: 0, Step: 207, Rank: 1, loss = 8.149072527885437e-09
c621-062: Epoch: 0, Step: 207, Rank: 23, loss = 5.617039278149605e-09
c622-092: Epoch: 0, Step: 207, Rank: 61, loss = 0.0001316070556640625
c622-101: Epoch: 0, Step: 207, Rank: 62, loss = 1.6540288925170898e-06
c622-102: Epoch: 0, Step: 207, Rank: 63, loss = 7.566995918750763e-10
c621-092: Epoch: 0, Step: 207, Rank: 29, loss = 7.729977369308472e-08
c622-081: Epoch: 0, Step: 207, Rank: 58, loss = 0.0
c622-082: Epoch: 0, Step: 207, Rank: 59, loss = 2.0236257114447653e-11
c622-091: Epoch: 0, Step: 207, Rank: 60, loss = 1.3869794202037156e-11
c622-071: Epoch: 0, Step: 207, Rank: 56, loss = 1.6391277313232422e-07
c622-072: Epoch: 0, Step: 207, Rank: 57, loss = 0.00299072265625
c622-062: Epoch: 0, Step: 207, Rank: 55, loss = 6.07222318649292e-07
c621-101: Epoch: 0, Step: 207, Rank: 30, loss = 1.0662875083691372e-25
c622-052: Epoch: 0, Step: 207, Rank: 53, loss = 1.0058283805847168e-06
c622-061: Epoch: 0, Step: 207, Rank: 54, loss = 2.2065682614424986e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-051: Epoch: 0, Step: 207, Rank: 52, loss = 3.1650415621697903e-10
c622-041: Epoch: 0, Step: 207, Rank: 50, loss = 0.0
c622-042: Epoch: 0, Step: 207, Rank: 51, loss = 7.048583938740194e-11
c622-032: Epoch: 0, Step: 207, Rank: 49, loss = 9.74978320300579e-10
c621-111: Epoch: 0, Step: 207, Rank: 32, loss = 1.0058283805847168e-06
c621-102: Epoch: 0, Step: 207, Rank: 31, loss = 2.9325485229492188e-05
c622-031: Epoch: 0, Step: 207, Rank: 48, loss = 6.565414878423326e-12
c622-022: Epoch: 0, Step: 207, Rank: 47, loss = 1.1399388313293457e-06
c622-012: Epoch: 0, Step: 207, Rank: 45, loss = 4.7222086809427244e-20
c622-002: Epoch: 0, Step: 207, Rank: 43, loss = 0.000518798828125
c622-021: Epoch: 0, Step: 207, Rank: 46, loss = 4.4517219066619873e-07
c622-011: Epoch: 0, Step: 207, Rank: 44, loss = 3.688037395477295e-07
c622-001: Epoch: 0, Step: 207, Rank: 42, loss = 2.1736923372372985e-10
c621-152: Epoch: 0, Step: 207, Rank: 41, loss = 1.895427703857422e-05
c621-112: Epoch: 0, Step: 207, Rank: 33, loss = 0.341796875
c621-121: Epoch: 0, Step: 207, Rank: 34, loss = 1.8775463104248047e-06
c621-151: Epoch: 0, Step: 207, Rank: 40, loss = 1.485356976305141e-17
c621-142: Epoch: 0, Step: 207, Rank: 39, loss = 1.2759119272232056e-07
c621-132: Epoch: 0, Step: 207, Rank: 37, loss = 9.5367431640625e-06
c621-141: Epoch: 0, Step: 207, Rank: 38, loss = 2.514570951461792e-08
c621-131: Epoch: 0, Step: 207, Rank: 36, loss = 0.0
c621-122: Epoch: 0, Step: 207, Rank: 35, loss = 5.20230969414115e-10
c613-101: Model Parameters: 7.505 B, Latency: 2.09s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.09s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 208, Rank: 43, loss = 7.188646122813225e-09
c622-012: Epoch: 0, Step: 208, Rank: 45, loss = 3.213062882423401e-08
c621-091: Epoch: 0, Step: 208, Rank: 28, loss = 0.0
c622-032: Epoch: 0, Step: 208, Rank: 49, loss = 6.565414878423326e-12
c622-042: Epoch: 0, Step: 208, Rank: 51, loss = 6.891787052154541e-07
c613-101: Epoch: 0, Step: 208, Rank: 0, loss = 1.418811734765768e-09
c622-051: Epoch: 0, Step: 208, Rank: 52, loss = 2.355810384551023e-21
c622-041: Epoch: 0, Step: 208, Rank: 50, loss = 3.293156623840332e-06
c621-142: Epoch: 0, Step: 208, Rank: 39, loss = 2.9976945370435715e-09
c621-052: Epoch: 0, Step: 208, Rank: 21, loss = 1.7229467630386353e-08
c621-151: Epoch: 0, Step: 208, Rank: 40, loss = 0.0
c622-061: Epoch: 0, Step: 208, Rank: 54, loss = 4.6629367034256575e-15
c619-021: Epoch: 0, Step: 208, Rank: 16, loss = 1.1059455573558807e-09
c621-082: Epoch: 0, Step: 208, Rank: 27, loss = 4.1443854570388794e-08
c621-132: Epoch: 0, Step: 208, Rank: 37, loss = 4.3655745685100555e-09
c621-061: Epoch: 0, Step: 208, Rank: 22, loss = 1.1874362826347351e-08
c621-111: Epoch: 0, Step: 208, Rank: 32, loss = 0.0
c621-081: Epoch: 0, Step: 208, Rank: 26, loss = 1.0477378964424133e-08
c621-072: Epoch: 0, Step: 208, Rank: 25, loss = 8.881784197001252e-13
c619-022: Epoch: 0, Step: 208, Rank: 17, loss = 0.0
c621-062: Epoch: 0, Step: 208, Rank: 23, loss = 6.439293542825908e-14
c622-081: Epoch: 0, Step: 208, Rank: 58, loss = 1.1399388313293457e-06
c621-131: Epoch: 0, Step: 208, Rank: 36, loss = 1.0058283805847168e-06
c621-121: Epoch: 0, Step: 208, Rank: 34, loss = 3.688037395477295e-07
c622-001: Epoch: 0, Step: 208, Rank: 42, loss = 7.338821887969971e-07
c621-141: Epoch: 0, Step: 208, Rank: 38, loss = 8.9925317837905e-37
c622-031: Epoch: 0, Step: 208, Rank: 48, loss = 0.69140625
c621-071: Epoch: 0, Step: 208, Rank: 24, loss = 0.0002956390380859375
c613-122: Epoch: 0, Step: 208, Rank: 5, loss = 0.0
c621-152: Epoch: 0, Step: 208, Rank: 41, loss = 8.404254913330078e-06
c622-022: Epoch: 0, Step: 208, Rank: 47, loss = 0.00020313262939453125
c621-092: Epoch: 0, Step: 208, Rank: 29, loss = 2.4158453015843406e-12
c619-031: Epoch: 0, Step: 208, Rank: 18, loss = 3.774403012357652e-11
c619-002: Epoch: 0, Step: 208, Rank: 13, loss = 3.583409124985337e-10
c622-021: Epoch: 0, Step: 208, Rank: 46, loss = 2.648448571562767e-09
c619-011: Epoch: 0, Step: 208, Rank: 14, loss = 3.841705620288849e-09
c619-001: Epoch: 0, Step: 208, Rank: 12, loss = 8.149072527885437e-09
c619-012: Epoch: 0, Step: 208, Rank: 15, loss = 8.307397365570068e-07
c621-101: Epoch: 0, Step: 208, Rank: 30, loss = 1.525040715932846e-08
c622-011: Epoch: 0, Step: 208, Rank: 44, loss = 5.617039278149605e-09
c622-101: Epoch: 0, Step: 208, Rank: 62, loss = 0.0
c619-041: Epoch: 0, Step: 208, Rank: 20, loss = 2.1047890186309814e-07
c621-112: Epoch: 0, Step: 208, Rank: 33, loss = 1.0058283805847168e-06
c613-151: Epoch: 0, Step: 208, Rank: 10, loss = 9.255018085241318e-09
c622-052: Epoch: 0, Step: 208, Rank: 53, loss = 2.1736923372372985e-10
c613-121: Epoch: 0, Step: 208, Rank: 4, loss = 7.44648787076585e-12
c613-142: Epoch: 0, Step: 208, Rank: 9, loss = 8.149072527885437e-09
c613-111: Epoch: 0, Step: 208, Rank: 2, loss = 3.774403012357652e-11
c619-032: Epoch: 0, Step: 208, Rank: 19, loss = 9.486769009248164e-19
c622-102: Epoch: 0, Step: 208, Rank: 63, loss = 5.699694156646729e-07
c622-092: Epoch: 0, Step: 208, Rank: 61, loss = 0.69140625
c622-091: Epoch: 0, Step: 208, Rank: 60, loss = 8.754432201385498e-08
c621-102: Epoch: 0, Step: 208, Rank: 31, loss = 4.418687638008123e-14
c621-122: Epoch: 0, Step: 208, Rank: 35, loss = 1.1059455573558807e-09
c613-112: Epoch: 0, Step: 208, Rank: 3, loss = 4.843059286940843e-11
c622-062: Epoch: 0, Step: 208, Rank: 55, loss = 0.69140625
c622-082: Epoch: 0, Step: 208, Rank: 59, loss = 1.1874362826347351e-08
c613-152: Epoch: 0, Step: 208, Rank: 11, loss = 1.4915713109076023e-10
c613-132: Epoch: 0, Step: 208, Rank: 7, loss = 6.07222318649292e-07
c613-141: Epoch: 0, Step: 208, Rank: 8, loss = 1.4915713109076023e-10
c613-131: Epoch: 0, Step: 208, Rank: 6, loss = 7.729977369308472e-08
c613-102: Epoch: 0, Step: 208, Rank: 1, loss = 9.012222290039062e-05
c622-071: Epoch: 0, Step: 208, Rank: 56, loss = 2.5331974029541016e-07
c622-072: Epoch: 0, Step: 208, Rank: 57, loss = 0.0003681182861328125
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-121: Epoch: 0, Step: 209, Rank: 4, loss = 0.0
c613-112: Epoch: 0, Step: 209, Rank: 3, loss = 2.514570951461792e-08
c613-101: Epoch: 0, Step: 209, Rank: 0, loss = 3.5017728805541992e-06
c613-111: Epoch: 0, Step: 209, Rank: 2, loss = 7.66053886991358e-15
c613-131: Epoch: 0, Step: 209, Rank: 6, loss = 3.0547380447387695e-07
c613-122: Epoch: 0, Step: 209, Rank: 5, loss = 1.014588720084573e-24
c613-102: Epoch: 0, Step: 209, Rank: 1, loss = 0.0
c622-102: Epoch: 0, Step: 209, Rank: 63, loss = 6.439293542825908e-14
c622-081: Epoch: 0, Step: 209, Rank: 58, loss = 1.8189894035458565e-09
c619-002: Epoch: 0, Step: 209, Rank: 13, loss = 1.955777406692505e-08
c619-001: Epoch: 0, Step: 209, Rank: 12, loss = 0.0
c619-021: Epoch: 0, Step: 209, Rank: 16, loss = 6.628036499023438e-05
c613-141: Epoch: 0, Step: 209, Rank: 8, loss = 0.0
c613-151: Epoch: 0, Step: 209, Rank: 10, loss = 1.525040715932846e-08
c613-132: Epoch: 0, Step: 209, Rank: 7, loss = 2.1175823681357508e-19
c622-092: Epoch: 0, Step: 209, Rank: 61, loss = 1.3597309589385986e-07
c613-152: Epoch: 0, Step: 209, Rank: 11, loss = 6.5267086029052734e-06
c613-142: Epoch: 0, Step: 209, Rank: 9, loss = 8.940696716308594e-06
c622-101: Epoch: 0, Step: 209, Rank: 62, loss = 3.5017728805541992e-06
c619-011: Epoch: 0, Step: 209, Rank: 14, loss = 1.126900315284729e-07
c619-031: Epoch: 0, Step: 209, Rank: 18, loss = 6.344635039567947e-09
c619-022: Epoch: 0, Step: 209, Rank: 17, loss = 9.370282327836321e-14
c622-071: Epoch: 0, Step: 209, Rank: 56, loss = 3.655441105365753e-08
c619-012: Epoch: 0, Step: 209, Rank: 15, loss = 7.566995918750763e-10
c622-082: Epoch: 0, Step: 209, Rank: 59, loss = 4.472333961502706e-19
c622-052: Epoch: 0, Step: 209, Rank: 53, loss = 2.6756374893466273e-14
c622-061: Epoch: 0, Step: 209, Rank: 54, loss = 0.69140625
c621-072: Epoch: 0, Step: 209, Rank: 25, loss = 1.0089706847793423e-12
c622-062: Epoch: 0, Step: 209, Rank: 55, loss = 3.084540367126465e-06
c622-091: Epoch: 0, Step: 209, Rank: 60, loss = 1.1874362826347351e-08
c622-072: Epoch: 0, Step: 209, Rank: 57, loss = 1.8775463104248047e-06
c622-002: Epoch: 0, Step: 209, Rank: 43, loss = 2.648448571562767e-09
c621-081: Epoch: 0, Step: 209, Rank: 26, loss = 0.000518798828125
c621-132: Epoch: 0, Step: 209, Rank: 37, loss = 0.0
c622-041: Epoch: 0, Step: 209, Rank: 50, loss = 0.0
c619-032: Epoch: 0, Step: 209, Rank: 19, loss = 9.255018085241318e-09
c622-012: Epoch: 0, Step: 209, Rank: 45, loss = 0.000278472900390625
c622-001: Epoch: 0, Step: 209, Rank: 42, loss = 3.213062882423401e-08
c619-041: Epoch: 0, Step: 209, Rank: 20, loss = 5.4836273193359375e-05
c621-052: Epoch: 0, Step: 209, Rank: 21, loss = 8.307397365570068e-07
c621-111: Epoch: 0, Step: 209, Rank: 32, loss = 0.0
c621-151: Epoch: 0, Step: 209, Rank: 40, loss = 1.525040715932846e-08
c621-142: Epoch: 0, Step: 209, Rank: 39, loss = 4.1443854570388794e-08
c622-051: Epoch: 0, Step: 209, Rank: 52, loss = 0.0
c621-071: Epoch: 0, Step: 209, Rank: 24, loss = 9.5367431640625e-06
c621-152: Epoch: 0, Step: 209, Rank: 41, loss = 9.098986738083304e-23
c621-112: Epoch: 0, Step: 209, Rank: 33, loss = 2.648448571562767e-09
c621-061: Epoch: 0, Step: 209, Rank: 22, loss = 1.6540288925170898e-06
c622-011: Epoch: 0, Step: 209, Rank: 44, loss = 1.3445969671010971e-08
c622-031: Epoch: 0, Step: 209, Rank: 48, loss = 3.084540367126465e-06
c622-021: Epoch: 0, Step: 209, Rank: 46, loss = 7.420778274536133e-06
c621-091: Epoch: 0, Step: 209, Rank: 28, loss = 7.66053886991358e-15
c622-022: Epoch: 0, Step: 209, Rank: 47, loss = 7.420778274536133e-06
c622-042: Epoch: 0, Step: 209, Rank: 51, loss = 0.69140625
c622-032: Epoch: 0, Step: 209, Rank: 49, loss = 0.69140625
c621-101: Epoch: 0, Step: 209, Rank: 30, loss = 4.1443854570388794e-08
c621-102: Epoch: 0, Step: 209, Rank: 31, loss = 1.0477378964424133e-08
c621-122: Epoch: 0, Step: 209, Rank: 35, loss = 4.4517219066619873e-07
c621-062: Epoch: 0, Step: 209, Rank: 23, loss = 2.514570951461792e-08
c621-082: Epoch: 0, Step: 209, Rank: 27, loss = 5.3085386753082275e-08
c621-131: Epoch: 0, Step: 209, Rank: 36, loss = 4.6798959374427795e-08
c621-092: Epoch: 0, Step: 209, Rank: 29, loss = 4.782137916322191e-25
c621-121: Epoch: 0, Step: 209, Rank: 34, loss = 2.9331204132176936e-11
c621-141: Epoch: 0, Step: 209, Rank: 38, loss = 6.693881005048752e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 210, Rank: 43, loss = 2.905726432800293e-06
c613-101: Epoch: 0, Step: 210, Rank: 0, loss = 2.1454997138094155e-24
c622-071: Epoch: 0, Step: 210, Rank: 56, loss = 5.542110104105285e-23
c619-021: Epoch: 0, Step: 210, Rank: 16, loss = 6.845220923423767e-08
c619-002: Epoch: 0, Step: 210, Rank: 13, loss = 0.0
c619-001: Epoch: 0, Step: 210, Rank: 12, loss = 2.562999725341797e-06
c622-081: Epoch: 0, Step: 210, Rank: 58, loss = 5.400124791776761e-13
c622-101: Epoch: 0, Step: 210, Rank: 62, loss = 0.69140625
c621-111: Epoch: 0, Step: 210, Rank: 32, loss = 5.617039278149605e-09
c613-111: Epoch: 0, Step: 210, Rank: 2, loss = 1.6171875
c622-012: Epoch: 0, Step: 210, Rank: 45, loss = 2.648448571562767e-09
c613-152: Epoch: 0, Step: 210, Rank: 11, loss = 0.095703125
c613-151: Epoch: 0, Step: 210, Rank: 10, loss = 3.841705620288849e-09
c613-112: Epoch: 0, Step: 210, Rank: 3, loss = 4.782137916322191e-25
c613-132: Epoch: 0, Step: 210, Rank: 7, loss = 4.05634636990726e-10
c613-121: Epoch: 0, Step: 210, Rank: 4, loss = 3.213062882423401e-08
c621-131: Epoch: 0, Step: 210, Rank: 36, loss = 2.2851054382044822e-11
c613-122: Epoch: 0, Step: 210, Rank: 5, loss = 2.5331974029541016e-07
c621-151: Epoch: 0, Step: 210, Rank: 40, loss = 0.69140625
c613-131: Epoch: 0, Step: 210, Rank: 6, loss = 1.3589129821411916e-13
c622-052: Epoch: 0, Step: 210, Rank: 53, loss = 1.7848833522293717e-11
c622-062: Epoch: 0, Step: 210, Rank: 55, loss = 3.0547380447387695e-07
c619-011: Epoch: 0, Step: 210, Rank: 14, loss = 0.0
c619-012: Epoch: 0, Step: 210, Rank: 15, loss = 3.293156623840332e-06
c621-091: Epoch: 0, Step: 210, Rank: 28, loss = 1.7762184143066406e-05
c621-132: Epoch: 0, Step: 210, Rank: 37, loss = 4.05634636990726e-10
c621-112: Epoch: 0, Step: 210, Rank: 33, loss = 2.671875
c621-081: Epoch: 0, Step: 210, Rank: 26, loss = 8.404254913330078e-06
c621-052: Epoch: 0, Step: 210, Rank: 21, loss = 1.983707842718853e-32
c619-022: Epoch: 0, Step: 210, Rank: 17, loss = 1.150369644165039e-05
c621-142: Epoch: 0, Step: 210, Rank: 39, loss = 8.754432201385498e-08
c622-102: Epoch: 0, Step: 210, Rank: 63, loss = 4.602043190971017e-10
c621-102: Epoch: 0, Step: 210, Rank: 31, loss = 3.979039320256561e-12
c622-061: Epoch: 0, Step: 210, Rank: 54, loss = 0.0
c621-141: Epoch: 0, Step: 210, Rank: 38, loss = 1.1059455573558807e-09
c621-122: Epoch: 0, Step: 210, Rank: 35, loss = 1.3709068298339844e-06
c621-072: Epoch: 0, Step: 210, Rank: 25, loss = 4.1443854570388794e-08
c621-082: Epoch: 0, Step: 210, Rank: 27, loss = 3.655441105365753e-08
c621-152: Epoch: 0, Step: 210, Rank: 41, loss = 3.510081114654895e-12
c621-101: Epoch: 0, Step: 210, Rank: 30, loss = 0.0
c622-001: Epoch: 0, Step: 210, Rank: 42, loss = 4.3655745685100555e-09
c619-041: Epoch: 0, Step: 210, Rank: 20, loss = 6.07222318649292e-07
c622-051: Epoch: 0, Step: 210, Rank: 52, loss = 8.487701416015625e-05
c619-031: Epoch: 0, Step: 210, Rank: 18, loss = 4.798173904418945e-06
c622-031: Epoch: 0, Step: 210, Rank: 48, loss = 5.617039278149605e-09
c621-092: Epoch: 0, Step: 210, Rank: 29, loss = 0.0
c621-061: Epoch: 0, Step: 210, Rank: 22, loss = 7.188646122813225e-09
c622-022: Epoch: 0, Step: 210, Rank: 47, loss = 1.2278178473934531e-11
c622-011: Epoch: 0, Step: 210, Rank: 44, loss = 1.6079866327345371e-09
c622-041: Epoch: 0, Step: 210, Rank: 50, loss = 8.754432201385498e-08
c622-032: Epoch: 0, Step: 210, Rank: 49, loss = 6.891787052154541e-07
c622-072: Epoch: 0, Step: 210, Rank: 57, loss = 1.4722347259521484e-05
c621-121: Epoch: 0, Step: 210, Rank: 34, loss = 2.9976945370435715e-09
c613-102: Epoch: 0, Step: 210, Rank: 1, loss = 4.602043190971017e-10
c622-042: Epoch: 0, Step: 210, Rank: 51, loss = 9.549694368615746e-12
c622-091: Epoch: 0, Step: 210, Rank: 60, loss = 3.688037395477295e-07
c621-071: Epoch: 0, Step: 210, Rank: 24, loss = 1.1641532182693481e-10
c619-032: Epoch: 0, Step: 210, Rank: 19, loss = 1.895427703857422e-05
c613-142: Epoch: 0, Step: 210, Rank: 9, loss = 4.0332320816460765e-17
c621-062: Epoch: 0, Step: 210, Rank: 23, loss = 1.955777406692505e-08
c613-141: Epoch: 0, Step: 210, Rank: 8, loss = 1.5366822481155396e-07
c622-021: Epoch: 0, Step: 210, Rank: 46, loss = 2.5920599000528455e-11
c622-082: Epoch: 0, Step: 210, Rank: 59, loss = 3.841705620288849e-09
c622-092: Epoch: 0, Step: 210, Rank: 61, loss = 5.699694156646729e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c621-142: Epoch: 0, Step: 211, Rank: 39, loss = 2.4158453015843406e-12
c621-151: Epoch: 0, Step: 211, Rank: 40, loss = 7.729977369308472e-08
c622-002: Epoch: 0, Step: 211, Rank: 43, loss = 2.1047890186309814e-07
c621-121: Epoch: 0, Step: 211, Rank: 34, loss = 9.549694368615746e-12
c622-001: Epoch: 0, Step: 211, Rank: 42, loss = 3.510081114654895e-12
c621-141: Epoch: 0, Step: 211, Rank: 38, loss = 2.6756374893466273e-14
c613-101: Epoch: 0, Step: 211, Rank: 0, loss = 1.7848833522293717e-11
c621-152: Epoch: 0, Step: 211, Rank: 41, loss = 1.7848833522293717e-11
c621-111: Epoch: 0, Step: 211, Rank: 32, loss = 1.2931877790833823e-12
c621-132: Epoch: 0, Step: 211, Rank: 37, loss = 4.6798959374427795e-08
c621-131: Epoch: 0, Step: 211, Rank: 36, loss = 7.283063041541027e-14
c621-112: Epoch: 0, Step: 211, Rank: 33, loss = 1.1641532182693481e-10
c621-091: Epoch: 0, Step: 211, Rank: 28, loss = 4.602043190971017e-10
c622-012: Epoch: 0, Step: 211, Rank: 45, loss = 2.5920599000528455e-11
c621-081: Epoch: 0, Step: 211, Rank: 26, loss = 0.000148773193359375
c619-021: Epoch: 0, Step: 211, Rank: 16, loss = 1.525040715932846e-08
c622-102: Epoch: 0, Step: 211, Rank: 63, loss = 4.418687638008123e-14
c622-081: Epoch: 0, Step: 211, Rank: 58, loss = 2.2351741790771484e-07
c621-122: Epoch: 0, Step: 211, Rank: 35, loss = 4.926614671774132e-16
c622-022: Epoch: 0, Step: 211, Rank: 47, loss = 1.3828277587890625e-05
c613-121: Epoch: 0, Step: 211, Rank: 4, loss = 4.6629367034256575e-15
c619-022: Epoch: 0, Step: 211, Rank: 17, loss = 1.1399388313293457e-06
c621-082: Epoch: 0, Step: 211, Rank: 27, loss = 4.231929779052734e-06
c613-131: Epoch: 0, Step: 211, Rank: 6, loss = 6.007030606269836e-08
c613-151: Epoch: 0, Step: 211, Rank: 10, loss = 0.69140625
c619-001: Epoch: 0, Step: 211, Rank: 12, loss = 0.69140625
c621-102: Epoch: 0, Step: 211, Rank: 31, loss = 7.048583938740194e-11
c622-101: Epoch: 0, Step: 211, Rank: 62, loss = 1.6391277313232422e-07
c613-152: Epoch: 0, Step: 211, Rank: 11, loss = 4.890056499420716e-35
c622-052: Epoch: 0, Step: 211, Rank: 53, loss = 0.0
c613-112: Epoch: 0, Step: 211, Rank: 3, loss = 4.172325134277344e-07
c621-092: Epoch: 0, Step: 211, Rank: 29, loss = 1.0089706847793423e-12
c613-111: Epoch: 0, Step: 211, Rank: 2, loss = 3.213062882423401e-08
c613-102: Epoch: 0, Step: 211, Rank: 1, loss = 1.1874362826347351e-08
c613-142: Epoch: 0, Step: 211, Rank: 9, loss = 0.0
c622-092: Epoch: 0, Step: 211, Rank: 61, loss = 3.4051481634378433e-09
c619-002: Epoch: 0, Step: 211, Rank: 13, loss = 2.4318695068359375e-05
c613-141: Epoch: 0, Step: 211, Rank: 8, loss = 1.5366822481155396e-07
c622-062: Epoch: 0, Step: 211, Rank: 55, loss = 2.540190280342358e-13
c622-011: Epoch: 0, Step: 211, Rank: 44, loss = 1.6689300537109375e-05
c621-072: Epoch: 0, Step: 211, Rank: 25, loss = 0.0
c613-122: Epoch: 0, Step: 211, Rank: 5, loss = 8.754432201385498e-08
c619-032: Epoch: 0, Step: 211, Rank: 19, loss = 3.5695955961250784e-29
c621-101: Epoch: 0, Step: 211, Rank: 30, loss = 0.69140625
c613-132: Epoch: 0, Step: 211, Rank: 7, loss = 2.2649765014648438e-06
c622-021: Epoch: 0, Step: 211, Rank: 46, loss = 0.0001087188720703125
c622-082: Epoch: 0, Step: 211, Rank: 59, loss = 2.1736923372372985e-10
c622-031: Epoch: 0, Step: 211, Rank: 48, loss = 2.2851054382044822e-11
c619-012: Epoch: 0, Step: 211, Rank: 15, loss = 2.9331204132176936e-11
c622-041: Epoch: 0, Step: 211, Rank: 50, loss = 8.149072527885437e-09
c622-071: Epoch: 0, Step: 211, Rank: 56, loss = 2.648448571562767e-09
c622-061: Epoch: 0, Step: 211, Rank: 54, loss = 4.94765117764473e-09
c622-051: Epoch: 0, Step: 211, Rank: 52, loss = 2.066371962428093e-09
c622-091: Epoch: 0, Step: 211, Rank: 60, loss = 2.514570951461792e-08
c621-071: Epoch: 0, Step: 211, Rank: 24, loss = 0.69140625
c622-072: Epoch: 0, Step: 211, Rank: 57, loss = 0.0
c619-011: Epoch: 0, Step: 211, Rank: 14, loss = 5.617039278149605e-09
c622-032: Epoch: 0, Step: 211, Rank: 49, loss = 2.3647750424515834e-14
c621-052: Epoch: 0, Step: 211, Rank: 21, loss = 6.927791673660977e-13
c622-042: Epoch: 0, Step: 211, Rank: 51, loss = 1.4051260155412137e-16
c621-062: Epoch: 0, Step: 211, Rank: 23, loss = 3.144186300207963e-17
c619-031: Epoch: 0, Step: 211, Rank: 18, loss = 8.307397365570068e-07
c619-041: Epoch: 0, Step: 211, Rank: 20, loss = 1.6689300537109375e-05
c621-061: Epoch: 0, Step: 211, Rank: 22, loss = 2.7008354663848877e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 212, Rank: 16, loss = 0.0
c619-002: Epoch: 0, Step: 212, Rank: 13, loss = 3.864587821847745e-21
c621-111: Epoch: 0, Step: 212, Rank: 32, loss = 1.2195753889781233e-37
c622-002: Epoch: 0, Step: 212, Rank: 43, loss = 1.126900315284729e-07
c619-031: Epoch: 0, Step: 212, Rank: 18, loss = 1.6209256159527285e-14
c621-072: Epoch: 0, Step: 212, Rank: 25, loss = 5.617039278149605e-09
c621-082: Epoch: 0, Step: 212, Rank: 27, loss = 0.0164794921875
c621-081: Epoch: 0, Step: 212, Rank: 26, loss = 8.585629984736443e-10
c613-101: Epoch: 0, Step: 212, Rank: 0, loss = 4.94765117764473e-09
c619-022: Epoch: 0, Step: 212, Rank: 17, loss = 1.1117307432712692e-21
c621-142: Epoch: 0, Step: 212, Rank: 39, loss = 1.4051260155412137e-16
c621-091: Epoch: 0, Step: 212, Rank: 28, loss = 3.4051481634378433e-09
c619-011: Epoch: 0, Step: 212, Rank: 14, loss = 3.314018249511719e-05
c619-041: Epoch: 0, Step: 212, Rank: 20, loss = 1.2514647096395493e-09
c613-142: Epoch: 0, Step: 212, Rank: 9, loss = 1.6079866327345371e-09
c621-132: Epoch: 0, Step: 212, Rank: 37, loss = 1.014588720084573e-24
c613-152: Epoch: 0, Step: 212, Rank: 11, loss = 2.9976945370435715e-09
c613-151: Epoch: 0, Step: 212, Rank: 10, loss = 1.485356976305141e-17
c621-151: Epoch: 0, Step: 212, Rank: 40, loss = 7.729977369308472e-08
c621-121: Epoch: 0, Step: 212, Rank: 34, loss = 1.6079866327345371e-09
c622-052: Epoch: 0, Step: 212, Rank: 53, loss = 2.8405338525772095e-08
c619-032: Epoch: 0, Step: 212, Rank: 19, loss = 8.754432201385498e-08
c621-131: Epoch: 0, Step: 212, Rank: 36, loss = 2.8405338525772095e-08
c613-132: Epoch: 0, Step: 212, Rank: 7, loss = 4.1443854570388794e-08
c622-012: Epoch: 0, Step: 212, Rank: 45, loss = 1.3589129821411916e-13
c622-051: Epoch: 0, Step: 212, Rank: 52, loss = 1.0477378964424133e-08
c621-112: Epoch: 0, Step: 212, Rank: 33, loss = 9.74978320300579e-10
c622-001: Epoch: 0, Step: 212, Rank: 42, loss = 3.4051481634378433e-09
c622-081: Epoch: 0, Step: 212, Rank: 58, loss = 3.510081114654895e-12
c622-031: Epoch: 0, Step: 212, Rank: 48, loss = 6.891787052154541e-07
c613-131: Epoch: 0, Step: 212, Rank: 6, loss = 6.693881005048752e-10
c621-092: Epoch: 0, Step: 212, Rank: 29, loss = 0.056640625
c621-062: Epoch: 0, Step: 212, Rank: 23, loss = 0.0
c622-041: Epoch: 0, Step: 212, Rank: 50, loss = 9.549694368615746e-12
c621-061: Epoch: 0, Step: 212, Rank: 22, loss = 6.007030606269836e-08
c621-102: Epoch: 0, Step: 212, Rank: 31, loss = 1.8758328224066645e-12
c622-101: Epoch: 0, Step: 212, Rank: 62, loss = 7.44648787076585e-12
c622-102: Epoch: 0, Step: 212, Rank: 63, loss = 0.01177978515625
c621-071: Epoch: 0, Step: 212, Rank: 24, loss = 1.7229467630386353e-08
c613-122: Epoch: 0, Step: 212, Rank: 5, loss = 5.3085386753082275e-08
c622-022: Epoch: 0, Step: 212, Rank: 47, loss = 1.0800249583553523e-11
c622-032: Epoch: 0, Step: 212, Rank: 49, loss = 3.144186300207963e-17
c621-101: Epoch: 0, Step: 212, Rank: 30, loss = 8.003553375601768e-11
c613-111: Epoch: 0, Step: 212, Rank: 2, loss = 2.2851054382044822e-11
c622-092: Epoch: 0, Step: 212, Rank: 61, loss = 2.4158453015843406e-12
c621-122: Epoch: 0, Step: 212, Rank: 35, loss = 0.0002460479736328125
c622-042: Epoch: 0, Step: 212, Rank: 51, loss = 5.893525667488575e-10
c621-152: Epoch: 0, Step: 212, Rank: 41, loss = 1.126900315284729e-07
c613-112: Epoch: 0, Step: 212, Rank: 3, loss = 3.841705620288849e-09
c621-052: Epoch: 0, Step: 212, Rank: 21, loss = 1.565316886525947e-18
c613-121: Epoch: 0, Step: 212, Rank: 4, loss = 1.3732490638087374e-25
c619-001: Epoch: 0, Step: 212, Rank: 12, loss = 1.6391277313232422e-07
c622-011: Epoch: 0, Step: 212, Rank: 44, loss = 5.893525667488575e-10
c621-141: Epoch: 0, Step: 212, Rank: 38, loss = 8.003553375601768e-11
c613-141: Epoch: 0, Step: 212, Rank: 8, loss = 6.693881005048752e-10
c622-061: Epoch: 0, Step: 212, Rank: 54, loss = 6.891787052154541e-07
c619-012: Epoch: 0, Step: 212, Rank: 15, loss = 2.455635694786906e-10
c622-091: Epoch: 0, Step: 212, Rank: 60, loss = 2.7284841053187847e-12
c622-062: Epoch: 0, Step: 212, Rank: 55, loss = 4.602043190971017e-10
c613-102: Epoch: 0, Step: 212, Rank: 1, loss = 2.8405338525772095e-08
c622-021: Epoch: 0, Step: 212, Rank: 46, loss = 3.510081114654895e-12
c622-071: Epoch: 0, Step: 212, Rank: 56, loss = 9.049472282640636e-11
c622-082: Epoch: 0, Step: 212, Rank: 59, loss = 4.274625098332763e-11
c622-072: Epoch: 0, Step: 212, Rank: 57, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 213, Rank: 0, loss = 2.2851054382044822e-11
c619-021: Epoch: 0, Step: 213, Rank: 16, loss = 8.149072527885437e-09
c621-132: Epoch: 0, Step: 213, Rank: 37, loss = 2.066371962428093e-09
c622-002: Epoch: 0, Step: 213, Rank: 43, loss = 6.007030606269836e-08
c622-012: Epoch: 0, Step: 213, Rank: 45, loss = 2.7008354663848877e-07
c613-132: Epoch: 0, Step: 213, Rank: 7, loss = 6.693881005048752e-10
c622-052: Epoch: 0, Step: 213, Rank: 53, loss = 2.5331974029541016e-07
c621-151: Epoch: 0, Step: 213, Rank: 40, loss = 6.344635039567947e-09
c622-062: Epoch: 0, Step: 213, Rank: 55, loss = 2.066371962428093e-09
c622-001: Epoch: 0, Step: 213, Rank: 42, loss = 1.418811734765768e-09
c622-101: Epoch: 0, Step: 213, Rank: 62, loss = 0.0
c621-091: Epoch: 0, Step: 213, Rank: 28, loss = 8.940696716308594e-06
c621-081: Epoch: 0, Step: 213, Rank: 26, loss = 2.710505431213761e-19
c621-142: Epoch: 0, Step: 213, Rank: 39, loss = 0.0002460479736328125
c621-111: Epoch: 0, Step: 213, Rank: 32, loss = 2.562999725341797e-06
c619-031: Epoch: 0, Step: 213, Rank: 18, loss = 5.115907697472721e-12
c619-002: Epoch: 0, Step: 213, Rank: 13, loss = 8.003553375601768e-11
c621-101: Epoch: 0, Step: 213, Rank: 30, loss = 4.94765117764473e-09
c613-141: Epoch: 0, Step: 213, Rank: 8, loss = 9.74978320300579e-10
c619-011: Epoch: 0, Step: 213, Rank: 14, loss = 5.400124791776761e-13
c622-092: Epoch: 0, Step: 213, Rank: 61, loss = 6.314393452555578e-16
c621-092: Epoch: 0, Step: 213, Rank: 29, loss = 4.00543212890625e-05
c622-081: Epoch: 0, Step: 213, Rank: 58, loss = 2.514570951461792e-08
c621-141: Epoch: 0, Step: 213, Rank: 38, loss = 1.8758328224066645e-12
c621-072: Epoch: 0, Step: 213, Rank: 25, loss = 5.4836273193359375e-05
c621-152: Epoch: 0, Step: 213, Rank: 41, loss = 8.754432201385498e-08
c621-112: Epoch: 0, Step: 213, Rank: 33, loss = 2.648448571562767e-09
c619-022: Epoch: 0, Step: 213, Rank: 17, loss = 1.9081958235744878e-17
c613-151: Epoch: 0, Step: 213, Rank: 10, loss = 1.3709068298339844e-06
c619-032: Epoch: 0, Step: 213, Rank: 19, loss = 3.583409124985337e-10
c622-061: Epoch: 0, Step: 213, Rank: 54, loss = 9.74978320300579e-10
c621-122: Epoch: 0, Step: 213, Rank: 35, loss = 0.0
c621-061: Epoch: 0, Step: 213, Rank: 22, loss = 1.2069940567016602e-06
c621-102: Epoch: 0, Step: 213, Rank: 31, loss = 1.199040866595169e-13
c621-121: Epoch: 0, Step: 213, Rank: 34, loss = 6.927791673660977e-13
c613-142: Epoch: 0, Step: 213, Rank: 9, loss = 1.1874362826347351e-08
c613-111: Epoch: 0, Step: 213, Rank: 2, loss = 7.270385540061815e-33
c622-102: Epoch: 0, Step: 213, Rank: 63, loss = 2.4158453015843406e-12
c622-022: Epoch: 0, Step: 213, Rank: 47, loss = 3.3068166260807885e-18
c622-031: Epoch: 0, Step: 213, Rank: 48, loss = 1.7848833522293717e-11
c622-072: Epoch: 0, Step: 213, Rank: 57, loss = 0.69140625
c622-051: Epoch: 0, Step: 213, Rank: 52, loss = 5.699694156646729e-07
c613-131: Epoch: 0, Step: 213, Rank: 6, loss = 0.0
c621-071: Epoch: 0, Step: 213, Rank: 24, loss = 1.7229467630386353e-08
c622-041: Epoch: 0, Step: 213, Rank: 50, loss = 0.00019073486328125
c621-082: Epoch: 0, Step: 213, Rank: 27, loss = 2.7830537874251604e-10
c613-122: Epoch: 0, Step: 213, Rank: 5, loss = 0.69140625
c619-012: Epoch: 0, Step: 213, Rank: 15, loss = 0.00083160400390625
c619-001: Epoch: 0, Step: 213, Rank: 12, loss = 1.8758328224066645e-12
c619-041: Epoch: 0, Step: 213, Rank: 20, loss = 0.0
c613-152: Epoch: 0, Step: 213, Rank: 11, loss = 6.314393452555578e-16
c622-011: Epoch: 0, Step: 213, Rank: 44, loss = 1.6079866327345371e-09
c621-062: Epoch: 0, Step: 213, Rank: 23, loss = 0.0
c613-112: Epoch: 0, Step: 213, Rank: 3, loss = 3.3068166260807885e-18
c622-082: Epoch: 0, Step: 213, Rank: 59, loss = 2.0236257114447653e-11
c622-091: Epoch: 0, Step: 213, Rank: 60, loss = 3.655441105365753e-08
c621-131: Epoch: 0, Step: 213, Rank: 36, loss = 0.0
c621-052: Epoch: 0, Step: 213, Rank: 21, loss = 5.699694156646729e-07
c622-021: Epoch: 0, Step: 213, Rank: 46, loss = 5.20230969414115e-10
c622-042: Epoch: 0, Step: 213, Rank: 51, loss = 1.2759119272232056e-07
c622-032: Epoch: 0, Step: 213, Rank: 49, loss = 1.0477378964424133e-08
c613-121: Epoch: 0, Step: 213, Rank: 4, loss = 5.893525667488575e-10
c613-102: Epoch: 0, Step: 213, Rank: 1, loss = 4.00543212890625e-05
c622-071: Epoch: 0, Step: 213, Rank: 56, loss = 2.1047890186309814e-07
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 214, Rank: 43, loss = 3.841705620288849e-09
c621-142: Epoch: 0, Step: 214, Rank: 39, loss = 1.1874362826347351e-08
c621-132: Epoch: 0, Step: 214, Rank: 37, loss = 0.69140625
c622-052: Epoch: 0, Step: 214, Rank: 53, loss = 1.1399388313293457e-06
c622-081: Epoch: 0, Step: 214, Rank: 58, loss = 1.55717134475708e-06
c613-101: Epoch: 0, Step: 214, Rank: 0, loss = 2.8405338525772095e-08
c622-062: Epoch: 0, Step: 214, Rank: 55, loss = 0.036865234375
c621-081: Epoch: 0, Step: 214, Rank: 26, loss = 0.69140625
c622-012: Epoch: 0, Step: 214, Rank: 45, loss = 6.344635039567947e-09
c621-111: Epoch: 0, Step: 214, Rank: 32, loss = 2.3647750424515834e-14
c621-091: Epoch: 0, Step: 214, Rank: 28, loss = 1.7497114868092467e-13
c619-002: Epoch: 0, Step: 214, Rank: 13, loss = 9.549694368615746e-12
c619-022: Epoch: 0, Step: 214, Rank: 17, loss = 9.74978320300579e-10
c621-131: Epoch: 0, Step: 214, Rank: 36, loss = 0.0
c621-141: Epoch: 0, Step: 214, Rank: 38, loss = 7.566995918750763e-10
c619-001: Epoch: 0, Step: 214, Rank: 12, loss = 8.404254913330078e-06
c622-092: Epoch: 0, Step: 214, Rank: 61, loss = 2.6756374893466273e-14
c622-051: Epoch: 0, Step: 214, Rank: 52, loss = 5.029141902923584e-07
c622-101: Epoch: 0, Step: 214, Rank: 62, loss = 4.405564747785802e-33
c619-011: Epoch: 0, Step: 214, Rank: 14, loss = 3.583409124985337e-10
c621-151: Epoch: 0, Step: 214, Rank: 40, loss = 0.0
c619-031: Epoch: 0, Step: 214, Rank: 18, loss = 0.69140625
c619-021: Epoch: 0, Step: 214, Rank: 16, loss = 6.07222318649292e-07
c613-131: Epoch: 0, Step: 214, Rank: 6, loss = 6.007030606269836e-08
c622-001: Epoch: 0, Step: 214, Rank: 42, loss = 4.1443854570388794e-08
c621-082: Epoch: 0, Step: 214, Rank: 27, loss = 0.0
c621-102: Epoch: 0, Step: 214, Rank: 31, loss = 1.7229467630386353e-08
c621-092: Epoch: 0, Step: 214, Rank: 29, loss = 0.0
c619-012: Epoch: 0, Step: 214, Rank: 15, loss = 9.74978320300579e-10
c621-121: Epoch: 0, Step: 214, Rank: 34, loss = 0.0
c621-152: Epoch: 0, Step: 214, Rank: 41, loss = 4.301339185275744e-23
c621-122: Epoch: 0, Step: 214, Rank: 35, loss = 6.007030606269836e-08
c622-011: Epoch: 0, Step: 214, Rank: 44, loss = 9.918585419654846e-08
c621-072: Epoch: 0, Step: 214, Rank: 25, loss = 2.1457672119140625e-05
c622-082: Epoch: 0, Step: 214, Rank: 59, loss = 3.510081114654895e-12
c622-061: Epoch: 0, Step: 214, Rank: 54, loss = 5.3085386753082275e-08
c613-141: Epoch: 0, Step: 214, Rank: 8, loss = 1.0058283805847168e-06
c622-031: Epoch: 0, Step: 214, Rank: 48, loss = 1.3869794202037156e-11
c619-032: Epoch: 0, Step: 214, Rank: 19, loss = 4.418687638008123e-14
c613-112: Epoch: 0, Step: 214, Rank: 3, loss = 1.318767317570746e-10
c613-132: Epoch: 0, Step: 214, Rank: 7, loss = 1.7848833522293717e-11
c613-122: Epoch: 0, Step: 214, Rank: 5, loss = 3.510081114654895e-12
c622-022: Epoch: 0, Step: 214, Rank: 47, loss = 0.00164794921875
c622-032: Epoch: 0, Step: 214, Rank: 49, loss = 0.0
c613-142: Epoch: 0, Step: 214, Rank: 9, loss = 1.1399388313293457e-06
c622-041: Epoch: 0, Step: 214, Rank: 50, loss = 6.973743438720703e-06
c621-101: Epoch: 0, Step: 214, Rank: 30, loss = 3.841705620288849e-09
c622-042: Epoch: 0, Step: 214, Rank: 51, loss = 5.893525667488575e-10
c613-121: Epoch: 0, Step: 214, Rank: 4, loss = 3.46451997756958e-07
c621-061: Epoch: 0, Step: 214, Rank: 22, loss = 1.7848833522293717e-11
c613-152: Epoch: 0, Step: 214, Rank: 11, loss = 4.6798959374427795e-08
c619-041: Epoch: 0, Step: 214, Rank: 20, loss = 5.3085386753082275e-08
c613-151: Epoch: 0, Step: 214, Rank: 10, loss = 2.5920599000528455e-11
c621-052: Epoch: 0, Step: 214, Rank: 21, loss = 7.66053886991358e-15
c622-072: Epoch: 0, Step: 214, Rank: 57, loss = 1.2514647096395493e-09
c622-021: Epoch: 0, Step: 214, Rank: 46, loss = 1.6916601452976465e-10
c621-112: Epoch: 0, Step: 214, Rank: 33, loss = 5.424022674560547e-06
c613-102: Epoch: 0, Step: 214, Rank: 1, loss = 7.048583938740194e-11
c613-111: Epoch: 0, Step: 214, Rank: 2, loss = 1.0277290130034089e-10
c622-091: Epoch: 0, Step: 214, Rank: 60, loss = 1.4915713109076023e-10
c621-062: Epoch: 0, Step: 214, Rank: 23, loss = 1.0058283805847168e-06
c622-102: Epoch: 0, Step: 214, Rank: 63, loss = 1.0477378964424133e-08
c621-071: Epoch: 0, Step: 214, Rank: 24, loss = 1.8758328224066645e-12
c622-071: Epoch: 0, Step: 214, Rank: 56, loss = 2.726912498474121e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.10s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.10s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 215, Rank: 58, loss = 6.007030606269836e-08
c613-101: Epoch: 0, Step: 215, Rank: 0, loss = 4.418687638008123e-14
c619-002: Epoch: 0, Step: 215, Rank: 13, loss = 3.921875
c622-002: Epoch: 0, Step: 215, Rank: 43, loss = 5.893525667488575e-10
c622-092: Epoch: 0, Step: 215, Rank: 61, loss = 8.940696716308594e-06
c622-101: Epoch: 0, Step: 215, Rank: 62, loss = 6.845220923423767e-08
c613-131: Epoch: 0, Step: 215, Rank: 6, loss = 0.69140625
c622-012: Epoch: 0, Step: 215, Rank: 45, loss = 1.2656542480726785e-14
c622-052: Epoch: 0, Step: 215, Rank: 53, loss = 0.0
c621-132: Epoch: 0, Step: 215, Rank: 37, loss = 3.725290298461914e-06
c619-021: Epoch: 0, Step: 215, Rank: 16, loss = 1.6391277313232422e-07
c613-132: Epoch: 0, Step: 215, Rank: 7, loss = 1.3445969671010971e-08
c621-111: Epoch: 0, Step: 215, Rank: 32, loss = 8.412825991399586e-12
c622-001: Epoch: 0, Step: 215, Rank: 42, loss = 0.69140625
c621-072: Epoch: 0, Step: 215, Rank: 25, loss = 0.0
c613-122: Epoch: 0, Step: 215, Rank: 5, loss = 5.424022674560547e-06
c622-062: Epoch: 0, Step: 215, Rank: 55, loss = 2.7284841053187847e-12
c622-041: Epoch: 0, Step: 215, Rank: 50, loss = 9.74978320300579e-10
c622-072: Epoch: 0, Step: 215, Rank: 57, loss = 9.370282327836321e-14
c619-022: Epoch: 0, Step: 215, Rank: 17, loss = 2.1047890186309814e-07
c613-111: Epoch: 0, Step: 215, Rank: 2, loss = 1.418811734765768e-09
c621-151: Epoch: 0, Step: 215, Rank: 40, loss = 2.7830537874251604e-10
c613-141: Epoch: 0, Step: 215, Rank: 8, loss = 3.293156623840332e-06
c621-081: Epoch: 0, Step: 215, Rank: 26, loss = 0.059326171875
c622-032: Epoch: 0, Step: 215, Rank: 49, loss = 6.891787052154541e-07
c613-152: Epoch: 0, Step: 215, Rank: 11, loss = 6.007030606269836e-08
c619-001: Epoch: 0, Step: 215, Rank: 12, loss = 3.979039320256561e-12
c613-112: Epoch: 0, Step: 215, Rank: 3, loss = 1.3322676295501878e-15
c621-091: Epoch: 0, Step: 215, Rank: 28, loss = 4.05634636990726e-10
c621-082: Epoch: 0, Step: 215, Rank: 27, loss = 0.0001087188720703125
c622-102: Epoch: 0, Step: 215, Rank: 63, loss = 4.1443854570388794e-08
c613-121: Epoch: 0, Step: 215, Rank: 4, loss = 2.1047890186309814e-07
c621-121: Epoch: 0, Step: 215, Rank: 34, loss = 5.115907697472721e-12
c613-151: Epoch: 0, Step: 215, Rank: 10, loss = 1.7229467630386353e-08
c621-142: Epoch: 0, Step: 215, Rank: 39, loss = 1.0089706847793423e-12
c622-051: Epoch: 0, Step: 215, Rank: 52, loss = 0.0181884765625
c621-131: Epoch: 0, Step: 215, Rank: 36, loss = 0.0
c619-031: Epoch: 0, Step: 215, Rank: 18, loss = 0.0001087188720703125
c613-142: Epoch: 0, Step: 215, Rank: 9, loss = 1.0089706847793423e-12
c622-031: Epoch: 0, Step: 215, Rank: 48, loss = 0.69140625
c622-022: Epoch: 0, Step: 215, Rank: 47, loss = 6.845220923423767e-08
c619-011: Epoch: 0, Step: 215, Rank: 14, loss = 3.583409124985337e-10
c621-112: Epoch: 0, Step: 215, Rank: 33, loss = 3.213062882423401e-08
c621-092: Epoch: 0, Step: 215, Rank: 29, loss = 0.0
c622-091: Epoch: 0, Step: 215, Rank: 60, loss = 1.5688783605583012e-11
c621-061: Epoch: 0, Step: 215, Rank: 22, loss = 0.0
c621-122: Epoch: 0, Step: 215, Rank: 35, loss = 1.3597309589385986e-07
c622-061: Epoch: 0, Step: 215, Rank: 54, loss = 1.3775324423698682e-39
c622-082: Epoch: 0, Step: 215, Rank: 59, loss = 4.843059286940843e-11
c621-102: Epoch: 0, Step: 215, Rank: 31, loss = 9.549694368615746e-12
c621-071: Epoch: 0, Step: 215, Rank: 24, loss = 5.699694156646729e-07
c621-052: Epoch: 0, Step: 215, Rank: 21, loss = 0.0
c619-041: Epoch: 0, Step: 215, Rank: 20, loss = 5.3085386753082275e-08
c622-042: Epoch: 0, Step: 215, Rank: 51, loss = 9.549694368615746e-12
c621-141: Epoch: 0, Step: 215, Rank: 38, loss = 2.066371962428093e-09
c619-012: Epoch: 0, Step: 215, Rank: 15, loss = 1.7848833522293717e-11
c622-011: Epoch: 0, Step: 215, Rank: 44, loss = 3.688037395477295e-07
c613-102: Epoch: 0, Step: 215, Rank: 1, loss = 5.699694156646729e-07
c621-152: Epoch: 0, Step: 215, Rank: 41, loss = 8.754432201385498e-08
c619-032: Epoch: 0, Step: 215, Rank: 19, loss = 1.126900315284729e-07
c621-062: Epoch: 0, Step: 215, Rank: 23, loss = 8.881784197001252e-13
c622-021: Epoch: 0, Step: 215, Rank: 46, loss = 3.841705620288849e-09
c621-101: Epoch: 0, Step: 215, Rank: 30, loss = 4.6798959374427795e-08
c622-071: Epoch: 0, Step: 215, Rank: 56, loss = 6.693881005048752e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c613-121: Epoch: 0, Step: 216, Rank: 4, loss = 2.514570951461792e-08
c613-112: Epoch: 0, Step: 216, Rank: 3, loss = 7.188646122813225e-09
c613-122: Epoch: 0, Step: 216, Rank: 5, loss = 1.2069940567016602e-06
c613-101: Epoch: 0, Step: 216, Rank: 0, loss = 0.69140625
c613-111: Epoch: 0, Step: 216, Rank: 2, loss = 5.3085386753082275e-08
c613-102: Epoch: 0, Step: 216, Rank: 1, loss = 3.342393029015511e-11
c613-131: Epoch: 0, Step: 216, Rank: 6, loss = 0.69140625
c622-102: Epoch: 0, Step: 216, Rank: 63, loss = 1.3499587596865412e-20
c622-092: Epoch: 0, Step: 216, Rank: 61, loss = 4.1443854570388794e-08
c622-101: Epoch: 0, Step: 216, Rank: 62, loss = 1.955777406692505e-08
c622-091: Epoch: 0, Step: 216, Rank: 60, loss = 3.342393029015511e-11
c622-081: Epoch: 0, Step: 216, Rank: 58, loss = 7.44648787076585e-12
c622-082: Epoch: 0, Step: 216, Rank: 59, loss = 0.0
c613-132: Epoch: 0, Step: 216, Rank: 7, loss = 8.754432201385498e-08
c622-052: Epoch: 0, Step: 216, Rank: 53, loss = 1.1874362826347351e-08
c622-071: Epoch: 0, Step: 216, Rank: 56, loss = 2.514570951461792e-08
c622-061: Epoch: 0, Step: 216, Rank: 54, loss = 1.6079866327345371e-09
c622-072: Epoch: 0, Step: 216, Rank: 57, loss = 2.9976945370435715e-09
c622-062: Epoch: 0, Step: 216, Rank: 55, loss = 1.150369644165039e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-051: Epoch: 0, Step: 216, Rank: 52, loss = 0.0
c613-141: Epoch: 0, Step: 216, Rank: 8, loss = 1.4051260155412137e-16
c622-042: Epoch: 0, Step: 216, Rank: 51, loss = 0.004486083984375
c622-041: Epoch: 0, Step: 216, Rank: 50, loss = 9.255018085241318e-09
c622-032: Epoch: 0, Step: 216, Rank: 49, loss = 9.255018085241318e-09
c622-031: Epoch: 0, Step: 216, Rank: 48, loss = 3.975119405215255e-31
c622-022: Epoch: 0, Step: 216, Rank: 47, loss = 7.188646122813225e-09
c622-012: Epoch: 0, Step: 216, Rank: 45, loss = 3.1650415621697903e-10
c622-002: Epoch: 0, Step: 216, Rank: 43, loss = 6.007030606269836e-08
c613-142: Epoch: 0, Step: 216, Rank: 9, loss = 0.000457763671875
c622-021: Epoch: 0, Step: 216, Rank: 46, loss = 5.699694156646729e-07
c622-011: Epoch: 0, Step: 216, Rank: 44, loss = 1.4915713109076023e-10
c622-001: Epoch: 0, Step: 216, Rank: 42, loss = 1.6079866327345371e-09
c621-152: Epoch: 0, Step: 216, Rank: 41, loss = 1.7848833522293717e-11
c613-151: Epoch: 0, Step: 216, Rank: 10, loss = 1.0089706847793423e-12
c621-151: Epoch: 0, Step: 216, Rank: 40, loss = 5.699694156646729e-07
c621-142: Epoch: 0, Step: 216, Rank: 39, loss = 1.55717134475708e-06
c621-132: Epoch: 0, Step: 216, Rank: 37, loss = 0.69140625
c621-141: Epoch: 0, Step: 216, Rank: 38, loss = 1.4637180356658064e-12
c621-131: Epoch: 0, Step: 216, Rank: 36, loss = 5.029141902923584e-07
c621-122: Epoch: 0, Step: 216, Rank: 35, loss = 7.66053886991358e-15
c621-112: Epoch: 0, Step: 216, Rank: 33, loss = 9.012222290039062e-05
c621-121: Epoch: 0, Step: 216, Rank: 34, loss = 0.0016021728515625
c621-111: Epoch: 0, Step: 216, Rank: 32, loss = 1.8758328224066645e-12
c613-152: Epoch: 0, Step: 216, Rank: 11, loss = 1.7848833522293717e-11
c621-102: Epoch: 0, Step: 216, Rank: 31, loss = 1.126900315284729e-07
c621-101: Epoch: 0, Step: 216, Rank: 30, loss = 4.843059286940843e-11
c621-092: Epoch: 0, Step: 216, Rank: 29, loss = 0.0
c621-082: Epoch: 0, Step: 216, Rank: 27, loss = 1.4915713109076023e-10
c621-091: Epoch: 0, Step: 216, Rank: 28, loss = 1.8775463104248047e-06
c621-072: Epoch: 0, Step: 216, Rank: 25, loss = 1.1874362826347351e-08
c621-081: Epoch: 0, Step: 216, Rank: 26, loss = 5.400124791776761e-13
c619-001: Epoch: 0, Step: 216, Rank: 12, loss = 1.0477378964424133e-08
c621-071: Epoch: 0, Step: 216, Rank: 24, loss = 1.9190338207408786e-10
c621-061: Epoch: 0, Step: 216, Rank: 22, loss = 2.5331974029541016e-07
c621-062: Epoch: 0, Step: 216, Rank: 23, loss = 3.774403012357652e-11
c619-041: Epoch: 0, Step: 216, Rank: 20, loss = 2.983724378680108e-16
c621-052: Epoch: 0, Step: 216, Rank: 21, loss = 1.4722347259521484e-05
c619-002: Epoch: 0, Step: 216, Rank: 13, loss = 0.69140625
c619-031: Epoch: 0, Step: 216, Rank: 18, loss = 8.003553375601768e-11
c619-032: Epoch: 0, Step: 216, Rank: 19, loss = 0.0
c619-022: Epoch: 0, Step: 216, Rank: 17, loss = 1.3869794202037156e-11
c619-021: Epoch: 0, Step: 216, Rank: 16, loss = 1.126900315284729e-07
c619-011: Epoch: 0, Step: 216, Rank: 14, loss = 1.55717134475708e-06
c619-012: Epoch: 0, Step: 216, Rank: 15, loss = 2.1736923372372985e-10
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 217, Rank: 43, loss = 5.182486384480711e-17
c619-002: Epoch: 0, Step: 217, Rank: 13, loss = 1.4637180356658064e-12
c621-111: Epoch: 0, Step: 217, Rank: 32, loss = 8.003553375601768e-11
c622-081: Epoch: 0, Step: 217, Rank: 58, loss = 0.0
c621-131: Epoch: 0, Step: 217, Rank: 36, loss = 0.0
c619-021: Epoch: 0, Step: 217, Rank: 16, loss = 1.1059455573558807e-09
c621-081: Epoch: 0, Step: 217, Rank: 26, loss = 3.268496584496461e-13
c613-101: Epoch: 0, Step: 217, Rank: 0, loss = 6.007030606269836e-08
c622-052: Epoch: 0, Step: 217, Rank: 53, loss = 2.9331204132176936e-11
c613-151: Epoch: 0, Step: 217, Rank: 10, loss = 5.699694156646729e-07
c622-001: Epoch: 0, Step: 217, Rank: 42, loss = 1.8189894035458565e-09
c622-012: Epoch: 0, Step: 217, Rank: 45, loss = 1.8189894035458565e-09
c619-001: Epoch: 0, Step: 217, Rank: 12, loss = 1.0408340855860843e-15
c621-072: Epoch: 0, Step: 217, Rank: 25, loss = 6.230038707144558e-11
c621-052: Epoch: 0, Step: 217, Rank: 21, loss = 3.9637088775634766e-06
c613-152: Epoch: 0, Step: 217, Rank: 11, loss = 2.2065682614424986e-15
c613-132: Epoch: 0, Step: 217, Rank: 7, loss = 3.4897757426877174e-19
c613-111: Epoch: 0, Step: 217, Rank: 2, loss = 1.2759119272232056e-07
c622-032: Epoch: 0, Step: 217, Rank: 49, loss = 1.955777406692505e-08
c621-082: Epoch: 0, Step: 217, Rank: 27, loss = 1.2656542480726785e-14
c621-091: Epoch: 0, Step: 217, Rank: 28, loss = 8.487701416015625e-05
c622-092: Epoch: 0, Step: 217, Rank: 61, loss = 2.455635694786906e-10
c613-121: Epoch: 0, Step: 217, Rank: 4, loss = 0.69140625
c621-121: Epoch: 0, Step: 217, Rank: 34, loss = 2.540190280342358e-13
c622-051: Epoch: 0, Step: 217, Rank: 52, loss = 9.918585419654846e-08
c619-011: Epoch: 0, Step: 217, Rank: 14, loss = 7.566995918750763e-10
c622-101: Epoch: 0, Step: 217, Rank: 62, loss = 2.0236257114447653e-11
c622-062: Epoch: 0, Step: 217, Rank: 55, loss = 6.628036499023438e-05
c621-151: Epoch: 0, Step: 217, Rank: 40, loss = 1.1874362826347351e-08
c622-102: Epoch: 0, Step: 217, Rank: 63, loss = 3.583409124985337e-10
c621-122: Epoch: 0, Step: 217, Rank: 35, loss = 5.617039278149605e-09
c613-141: Epoch: 0, Step: 217, Rank: 8, loss = 0.0
c613-131: Epoch: 0, Step: 217, Rank: 6, loss = 1.0132789611816406e-05
c621-101: Epoch: 0, Step: 217, Rank: 30, loss = 1.1874362826347351e-08
c613-142: Epoch: 0, Step: 217, Rank: 9, loss = 2.648448571562767e-09
c622-071: Epoch: 0, Step: 217, Rank: 56, loss = 2.648448571562767e-09
c621-142: Epoch: 0, Step: 217, Rank: 39, loss = 1.418811734765768e-09
c621-112: Epoch: 0, Step: 217, Rank: 33, loss = 5.893525667488575e-10
c619-032: Epoch: 0, Step: 217, Rank: 19, loss = 4.760636329592671e-13
c621-071: Epoch: 0, Step: 217, Rank: 24, loss = 0.0
c613-102: Epoch: 0, Step: 217, Rank: 1, loss = 1.6672859221771964e-24
c621-102: Epoch: 0, Step: 217, Rank: 31, loss = 2.276897430419922e-05
c622-061: Epoch: 0, Step: 217, Rank: 54, loss = 2.1457672119140625e-05
c621-061: Epoch: 0, Step: 217, Rank: 22, loss = 1.1059455573558807e-09
c622-021: Epoch: 0, Step: 217, Rank: 46, loss = 3.583409124985337e-10
c619-022: Epoch: 0, Step: 217, Rank: 17, loss = 2.2118911147117615e-08
c613-122: Epoch: 0, Step: 217, Rank: 5, loss = 3.841705620288849e-09
c613-112: Epoch: 0, Step: 217, Rank: 3, loss = 1.8758328224066645e-12
c619-031: Epoch: 0, Step: 217, Rank: 18, loss = 1.3589129821411916e-13
c621-152: Epoch: 0, Step: 217, Rank: 41, loss = 5.424022674560547e-06
c622-072: Epoch: 0, Step: 217, Rank: 57, loss = 2.2118911147117615e-08
c622-031: Epoch: 0, Step: 217, Rank: 48, loss = 2.5331974029541016e-07
c619-012: Epoch: 0, Step: 217, Rank: 15, loss = 2.831068712794149e-15
c621-092: Epoch: 0, Step: 217, Rank: 29, loss = 2.5920599000528455e-11
c621-132: Epoch: 0, Step: 217, Rank: 37, loss = 1.3709068298339844e-06
c619-041: Epoch: 0, Step: 217, Rank: 20, loss = 2.5331974029541016e-07
c622-091: Epoch: 0, Step: 217, Rank: 60, loss = 5.617039278149605e-09
c622-011: Epoch: 0, Step: 217, Rank: 44, loss = 5.893525667488575e-10
c622-022: Epoch: 0, Step: 217, Rank: 47, loss = 1.1059455573558807e-09
c621-062: Epoch: 0, Step: 217, Rank: 23, loss = 2.2351741790771484e-07
c622-041: Epoch: 0, Step: 217, Rank: 50, loss = 2.6505726415425997e-28
c622-042: Epoch: 0, Step: 217, Rank: 51, loss = 8.881784197001252e-13
c622-082: Epoch: 0, Step: 217, Rank: 59, loss = 2.066371962428093e-09
c621-141: Epoch: 0, Step: 217, Rank: 38, loss = 2.4318695068359375e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 218, Rank: 16, loss = 0.0
c622-002: Epoch: 0, Step: 218, Rank: 43, loss = 5.617039278149605e-09
c621-111: Epoch: 0, Step: 218, Rank: 32, loss = 8.307397365570068e-07
c622-052: Epoch: 0, Step: 218, Rank: 53, loss = 1.3597309589385986e-07
c613-101: Epoch: 0, Step: 218, Rank: 0, loss = 5.182486384480711e-17
c619-022: Epoch: 0, Step: 218, Rank: 17, loss = 2.7008354663848877e-07
c622-071: Epoch: 0, Step: 218, Rank: 56, loss = 1.2069940567016602e-06
c622-092: Epoch: 0, Step: 218, Rank: 61, loss = 2.445028249710358e-36
c621-081: Epoch: 0, Step: 218, Rank: 26, loss = 1.0277290130034089e-10
c621-082: Epoch: 0, Step: 218, Rank: 27, loss = 0.69140625
c622-012: Epoch: 0, Step: 218, Rank: 45, loss = 1.2304311611726287e-23
c622-062: Epoch: 0, Step: 218, Rank: 55, loss = 1.9806378759312793e-13
c622-081: Epoch: 0, Step: 218, Rank: 58, loss = 3.655441105365753e-08
c622-101: Epoch: 0, Step: 218, Rank: 62, loss = 3.841705620288849e-09
c622-001: Epoch: 0, Step: 218, Rank: 42, loss = 1.3709068298339844e-06
c619-002: Epoch: 0, Step: 218, Rank: 13, loss = 0.0
c621-052: Epoch: 0, Step: 218, Rank: 21, loss = 0.69140625
c622-061: Epoch: 0, Step: 218, Rank: 54, loss = 0.0
c619-011: Epoch: 0, Step: 218, Rank: 14, loss = 6.845220923423767e-08
c619-041: Epoch: 0, Step: 218, Rank: 20, loss = 2.9976945370435715e-09
c619-031: Epoch: 0, Step: 218, Rank: 18, loss = 4.3655745685100555e-09
c622-051: Epoch: 0, Step: 218, Rank: 52, loss = 2.7284841053187847e-12
c613-152: Epoch: 0, Step: 218, Rank: 11, loss = 5.029141902923584e-07
c613-151: Epoch: 0, Step: 218, Rank: 10, loss = 9.74978320300579e-10
c622-022: Epoch: 0, Step: 218, Rank: 47, loss = 2.514570951461792e-08
c613-132: Epoch: 0, Step: 218, Rank: 7, loss = 0.00116729736328125
c619-001: Epoch: 0, Step: 218, Rank: 12, loss = 1.955777406692505e-08
c621-151: Epoch: 0, Step: 218, Rank: 40, loss = 4.418687638008123e-14
c613-122: Epoch: 0, Step: 218, Rank: 5, loss = 3.583409124985337e-10
c619-012: Epoch: 0, Step: 218, Rank: 15, loss = 3.213062882423401e-08
c613-121: Epoch: 0, Step: 218, Rank: 4, loss = 2.2649765014648438e-06
c613-111: Epoch: 0, Step: 218, Rank: 2, loss = 9.549694368615746e-12
c621-131: Epoch: 0, Step: 218, Rank: 36, loss = 3.655441105365753e-08
c613-142: Epoch: 0, Step: 218, Rank: 9, loss = 3.688037395477295e-07
c621-142: Epoch: 0, Step: 218, Rank: 39, loss = 1.8775463104248047e-06
c621-101: Epoch: 0, Step: 218, Rank: 30, loss = 3.583409124985337e-10
c621-141: Epoch: 0, Step: 218, Rank: 38, loss = 6.565414878423326e-12
c619-032: Epoch: 0, Step: 218, Rank: 19, loss = 0.69140625
c622-011: Epoch: 0, Step: 218, Rank: 44, loss = 7.188646122813225e-09
c622-072: Epoch: 0, Step: 218, Rank: 57, loss = 1.4051260155412137e-16
c621-061: Epoch: 0, Step: 218, Rank: 22, loss = 1.6079866327345371e-09
c621-091: Epoch: 0, Step: 218, Rank: 28, loss = 2.276897430419922e-05
c613-112: Epoch: 0, Step: 218, Rank: 3, loss = 0.00299072265625
c622-082: Epoch: 0, Step: 218, Rank: 59, loss = 1.2790197503539935e-19
c622-031: Epoch: 0, Step: 218, Rank: 48, loss = 0.0
c621-121: Epoch: 0, Step: 218, Rank: 34, loss = 0.0
c622-041: Epoch: 0, Step: 218, Rank: 50, loss = 0.0732421875
c621-092: Epoch: 0, Step: 218, Rank: 29, loss = 2.540190280342358e-13
c622-032: Epoch: 0, Step: 218, Rank: 49, loss = 0.00012302398681640625
c621-072: Epoch: 0, Step: 218, Rank: 25, loss = 7.188646122813225e-09
c621-112: Epoch: 0, Step: 218, Rank: 33, loss = 8.149072527885437e-09
c622-042: Epoch: 0, Step: 218, Rank: 51, loss = 1.2278178473934531e-11
c622-021: Epoch: 0, Step: 218, Rank: 46, loss = 1.4637180356658064e-12
c622-102: Epoch: 0, Step: 218, Rank: 63, loss = 8.754432201385498e-08
c621-152: Epoch: 0, Step: 218, Rank: 41, loss = 2.8405338525772095e-08
c622-091: Epoch: 0, Step: 218, Rank: 60, loss = 8.998878031629687e-18
c613-141: Epoch: 0, Step: 218, Rank: 8, loss = 2.342858351767063e-09
c621-062: Epoch: 0, Step: 218, Rank: 23, loss = 7.486343383789062e-05
c613-131: Epoch: 0, Step: 218, Rank: 6, loss = 4.267692565917969e-05
c621-122: Epoch: 0, Step: 218, Rank: 35, loss = 2.2118911147117615e-08
c621-102: Epoch: 0, Step: 218, Rank: 31, loss = 1.6079866327345371e-09
c621-071: Epoch: 0, Step: 218, Rank: 24, loss = 7.386127300057499e-19
c613-102: Epoch: 0, Step: 218, Rank: 1, loss = 1.2759119272232056e-07
c621-132: Epoch: 0, Step: 218, Rank: 37, loss = 8.003553375601768e-11
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24658203125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.10s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.10s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 219, Rank: 43, loss = 3.688037395477295e-07
c613-101: Epoch: 0, Step: 219, Rank: 0, loss = 7.338821887969971e-07
c622-012: Epoch: 0, Step: 219, Rank: 45, loss = 1.0277290130034089e-10
c619-021: Epoch: 0, Step: 219, Rank: 16, loss = 1.0058283805847168e-06
c621-132: Epoch: 0, Step: 219, Rank: 37, loss = 1.4051260155412137e-16
c621-081: Epoch: 0, Step: 219, Rank: 26, loss = 3.213062882423401e-08
c621-072: Epoch: 0, Step: 219, Rank: 25, loss = 0.0
c622-081: Epoch: 0, Step: 219, Rank: 58, loss = 4.4517219066619873e-07
c619-002: Epoch: 0, Step: 219, Rank: 13, loss = 7.729977369308472e-08
c622-052: Epoch: 0, Step: 219, Rank: 53, loss = 1.418811734765768e-09
c621-151: Epoch: 0, Step: 219, Rank: 40, loss = 2.6756374893466273e-14
c622-001: Epoch: 0, Step: 219, Rank: 42, loss = 5.893525667488575e-10
c622-101: Epoch: 0, Step: 219, Rank: 62, loss = 9.370282327836321e-14
c613-151: Epoch: 0, Step: 219, Rank: 10, loss = 1.9190338207408786e-10
c622-071: Epoch: 0, Step: 219, Rank: 56, loss = 4.926614671774132e-16
c613-132: Epoch: 0, Step: 219, Rank: 7, loss = 3.268496584496461e-13
c621-091: Epoch: 0, Step: 219, Rank: 28, loss = 1.1874362826347351e-08
c613-111: Epoch: 0, Step: 219, Rank: 2, loss = 1.1546753136970622e-17
c613-121: Epoch: 0, Step: 219, Rank: 4, loss = 9.255018085241318e-09
c613-131: Epoch: 0, Step: 219, Rank: 6, loss = 9.74978320300579e-10
c622-092: Epoch: 0, Step: 219, Rank: 61, loss = 0.69140625
c621-082: Epoch: 0, Step: 219, Rank: 27, loss = 0.0
c619-031: Epoch: 0, Step: 219, Rank: 18, loss = 1.6391277313232422e-07
c621-061: Epoch: 0, Step: 219, Rank: 22, loss = 4.05634636990726e-10
c619-022: Epoch: 0, Step: 219, Rank: 17, loss = 4.3655745685100555e-09
c621-152: Epoch: 0, Step: 219, Rank: 41, loss = 1.0408340855860843e-15
c622-072: Epoch: 0, Step: 219, Rank: 57, loss = 2.355810384551023e-21
c613-152: Epoch: 0, Step: 219, Rank: 11, loss = 0.0023193359375
c622-051: Epoch: 0, Step: 219, Rank: 52, loss = 2.9976945370435715e-09
c621-052: Epoch: 0, Step: 219, Rank: 21, loss = 1.0800249583553523e-11
c622-062: Epoch: 0, Step: 219, Rank: 55, loss = 5.115907697472721e-12
c621-131: Epoch: 0, Step: 219, Rank: 36, loss = 1.2759119272232056e-07
c619-011: Epoch: 0, Step: 219, Rank: 14, loss = 7.188646122813225e-09
c621-111: Epoch: 0, Step: 219, Rank: 32, loss = 6.845220923423767e-08
c613-142: Epoch: 0, Step: 219, Rank: 9, loss = 2.1736923372372985e-10
c613-141: Epoch: 0, Step: 219, Rank: 8, loss = 5.115907697472721e-12
c622-011: Epoch: 0, Step: 219, Rank: 44, loss = 0.69140625
c621-101: Epoch: 0, Step: 219, Rank: 30, loss = 0.69140625
c622-022: Epoch: 0, Step: 219, Rank: 47, loss = 0.0
c621-112: Epoch: 0, Step: 219, Rank: 33, loss = 0.0
c613-112: Epoch: 0, Step: 219, Rank: 3, loss = 0.69140625
c613-122: Epoch: 0, Step: 219, Rank: 5, loss = 4.843059286940843e-11
c619-001: Epoch: 0, Step: 219, Rank: 12, loss = 6.344635039567947e-09
c622-082: Epoch: 0, Step: 219, Rank: 59, loss = 0.00010251998901367188
c622-021: Epoch: 0, Step: 219, Rank: 46, loss = 5.115907697472721e-12
c613-102: Epoch: 0, Step: 219, Rank: 1, loss = 3.7670135498046875e-05
c621-141: Epoch: 0, Step: 219, Rank: 38, loss = 5.893525667488575e-10
c622-031: Epoch: 0, Step: 219, Rank: 48, loss = 0.0002460479736328125
c622-061: Epoch: 0, Step: 219, Rank: 54, loss = 0.0
c621-142: Epoch: 0, Step: 219, Rank: 39, loss = 6.845220923423767e-08
c621-062: Epoch: 0, Step: 219, Rank: 23, loss = 3.979039320256561e-12
c622-032: Epoch: 0, Step: 219, Rank: 49, loss = 2.1047890186309814e-07
c622-102: Epoch: 0, Step: 219, Rank: 63, loss = 4.602043190971017e-10
c622-091: Epoch: 0, Step: 219, Rank: 60, loss = 4.798173904418945e-06
c619-012: Epoch: 0, Step: 219, Rank: 15, loss = 1.126900315284729e-07
c621-121: Epoch: 0, Step: 219, Rank: 34, loss = 2.2851054382044822e-11
c619-032: Epoch: 0, Step: 219, Rank: 19, loss = 9.486769009248164e-19
c621-122: Epoch: 0, Step: 219, Rank: 35, loss = 2.9976945370435715e-09
c622-042: Epoch: 0, Step: 219, Rank: 51, loss = 2.753734588623047e-05
c621-092: Epoch: 0, Step: 219, Rank: 29, loss = 1.8533319234848022e-07
c621-102: Epoch: 0, Step: 219, Rank: 31, loss = 0.0
c621-071: Epoch: 0, Step: 219, Rank: 24, loss = 3.6261649734165925e-34
c619-041: Epoch: 0, Step: 219, Rank: 20, loss = 1.996755599975586e-06
c622-041: Epoch: 0, Step: 219, Rank: 50, loss = 8.881784197001252e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.7548828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 220, Rank: 16, loss = 9.049472282640636e-11
c622-002: Epoch: 0, Step: 220, Rank: 43, loss = 7.66053886991358e-15
c621-081: Epoch: 0, Step: 220, Rank: 26, loss = 8.412825991399586e-12
c621-072: Epoch: 0, Step: 220, Rank: 25, loss = 2.586841583251953e-05
c621-082: Epoch: 0, Step: 220, Rank: 27, loss = 5.585135208964764e-28
c622-052: Epoch: 0, Step: 220, Rank: 53, loss = 8.149072527885437e-09
c621-111: Epoch: 0, Step: 220, Rank: 32, loss = 1.6079866327345371e-09
c622-041: Epoch: 0, Step: 220, Rank: 50, loss = 1.955777406692505e-08
c622-032: Epoch: 0, Step: 220, Rank: 49, loss = 7.66053886991358e-15
c613-101: Epoch: 0, Step: 220, Rank: 0, loss = 0.0
c622-012: Epoch: 0, Step: 220, Rank: 45, loss = 6.845220923423767e-08
c621-091: Epoch: 0, Step: 220, Rank: 28, loss = 2.1736923372372985e-10
c619-011: Epoch: 0, Step: 220, Rank: 14, loss = 7.66053886991358e-15
c622-031: Epoch: 0, Step: 220, Rank: 48, loss = 0.0
c622-051: Epoch: 0, Step: 220, Rank: 52, loss = 4.1443854570388794e-08
c619-002: Epoch: 0, Step: 220, Rank: 13, loss = 8.881784197001252e-13
c621-151: Epoch: 0, Step: 220, Rank: 40, loss = 0.0
c621-071: Epoch: 0, Step: 220, Rank: 24, loss = 1.2931877790833823e-12
c622-042: Epoch: 0, Step: 220, Rank: 51, loss = 1.7762184143066406e-05
c621-101: Epoch: 0, Step: 220, Rank: 30, loss = 0.0
c621-102: Epoch: 0, Step: 220, Rank: 31, loss = 3.0547380447387695e-07
c613-151: Epoch: 0, Step: 220, Rank: 10, loss = 7.566995918750763e-10
c622-001: Epoch: 0, Step: 220, Rank: 42, loss = 2.1736923372372985e-10
c613-152: Epoch: 0, Step: 220, Rank: 11, loss = 2.514570951461792e-08
c621-142: Epoch: 0, Step: 220, Rank: 39, loss = 0.000457763671875
c621-131: Epoch: 0, Step: 220, Rank: 36, loss = 8.307397365570068e-07
c613-132: Epoch: 0, Step: 220, Rank: 7, loss = 1.0078125
c622-081: Epoch: 0, Step: 220, Rank: 58, loss = 3.583409124985337e-10
c621-132: Epoch: 0, Step: 220, Rank: 37, loss = 7.729977369308472e-08
c622-022: Epoch: 0, Step: 220, Rank: 47, loss = 9.880984919163893e-15
c619-012: Epoch: 0, Step: 220, Rank: 15, loss = 2.9976945370435715e-09
c613-121: Epoch: 0, Step: 220, Rank: 4, loss = 2.586841583251953e-05
c613-131: Epoch: 0, Step: 220, Rank: 6, loss = 0.0010986328125
c619-001: Epoch: 0, Step: 220, Rank: 12, loss = 1.126900315284729e-07
c622-092: Epoch: 0, Step: 220, Rank: 61, loss = 1.4637180356658064e-12
c621-061: Epoch: 0, Step: 220, Rank: 22, loss = 0.69140625
c619-031: Epoch: 0, Step: 220, Rank: 18, loss = 3.213062882423401e-08
c621-121: Epoch: 0, Step: 220, Rank: 34, loss = 1.2931877790833823e-12
c622-061: Epoch: 0, Step: 220, Rank: 54, loss = 0.0
c613-111: Epoch: 0, Step: 220, Rank: 2, loss = 0.0
c621-052: Epoch: 0, Step: 220, Rank: 21, loss = 1.0477378964424133e-08
c621-092: Epoch: 0, Step: 220, Rank: 29, loss = 2.2351741790771484e-07
c621-122: Epoch: 0, Step: 220, Rank: 35, loss = 9.918585419654846e-08
c613-141: Epoch: 0, Step: 220, Rank: 8, loss = 1.9806378759312793e-13
c621-141: Epoch: 0, Step: 220, Rank: 38, loss = 0.2099609375
c622-101: Epoch: 0, Step: 220, Rank: 62, loss = 8.881784197001252e-13
c622-011: Epoch: 0, Step: 220, Rank: 44, loss = 1.8775463104248047e-06
c619-041: Epoch: 0, Step: 220, Rank: 20, loss = 7.44648787076585e-12
c622-102: Epoch: 0, Step: 220, Rank: 63, loss = 4.05634636990726e-10
c613-122: Epoch: 0, Step: 220, Rank: 5, loss = 0.0
c622-071: Epoch: 0, Step: 220, Rank: 56, loss = 1.0800249583553523e-11
c613-142: Epoch: 0, Step: 220, Rank: 9, loss = 3.975119405215255e-31
c621-062: Epoch: 0, Step: 220, Rank: 23, loss = 1.318767317570746e-10
c621-152: Epoch: 0, Step: 220, Rank: 41, loss = 2.7830537874251604e-10
c622-062: Epoch: 0, Step: 220, Rank: 55, loss = 2.2118911147117615e-08
c613-112: Epoch: 0, Step: 220, Rank: 3, loss = 1.9806378759312793e-13
c619-022: Epoch: 0, Step: 220, Rank: 17, loss = 5.995204332975845e-15
c622-021: Epoch: 0, Step: 220, Rank: 46, loss = 4.4517219066619873e-07
c622-091: Epoch: 0, Step: 220, Rank: 60, loss = 1.6391277313232422e-07
c619-032: Epoch: 0, Step: 220, Rank: 19, loss = 6.891787052154541e-07
c622-072: Epoch: 0, Step: 220, Rank: 57, loss = 4.760636329592671e-13
c622-082: Epoch: 0, Step: 220, Rank: 59, loss = 6.891787052154541e-07
c613-102: Epoch: 0, Step: 220, Rank: 1, loss = 3.979039320256561e-12
c621-112: Epoch: 0, Step: 220, Rank: 33, loss = 1.8775463104248047e-06
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.248046875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c621-142: Epoch: 0, Step: 221, Rank: 39, loss = 1.3445969671010971e-08
c621-132: Epoch: 0, Step: 221, Rank: 37, loss = 1.525040715932846e-08
c621-141: Epoch: 0, Step: 221, Rank: 38, loss = 3.54136699749265e-24
c621-131: Epoch: 0, Step: 221, Rank: 36, loss = 1.150369644165039e-05
c621-151: Epoch: 0, Step: 221, Rank: 40, loss = 1.0089706847793423e-12
c621-122: Epoch: 0, Step: 221, Rank: 35, loss = 3.314018249511719e-05
c621-111: Epoch: 0, Step: 221, Rank: 32, loss = 6.07222318649292e-07
c621-121: Epoch: 0, Step: 221, Rank: 34, loss = 0.0
c621-102: Epoch: 0, Step: 221, Rank: 31, loss = 1.150369644165039e-05
c621-112: Epoch: 0, Step: 221, Rank: 33, loss = 2.4158453015843406e-12
c621-152: Epoch: 0, Step: 221, Rank: 41, loss = 0.0021209716796875
c621-101: Epoch: 0, Step: 221, Rank: 30, loss = 5.3085386753082275e-08
c621-091: Epoch: 0, Step: 221, Rank: 28, loss = 7.048583938740194e-11
c621-092: Epoch: 0, Step: 221, Rank: 29, loss = 2.5331974029541016e-07
c621-082: Epoch: 0, Step: 221, Rank: 27, loss = 1.8758328224066645e-12
c622-001: Epoch: 0, Step: 221, Rank: 42, loss = 0.0
c621-072: Epoch: 0, Step: 221, Rank: 25, loss = 9.255018085241318e-09
c621-081: Epoch: 0, Step: 221, Rank: 26, loss = 4.843059286940843e-11
c621-061: Epoch: 0, Step: 221, Rank: 22, loss = 0.0001316070556640625
c622-002: Epoch: 0, Step: 221, Rank: 43, loss = 0.00136566162109375
c621-052: Epoch: 0, Step: 221, Rank: 21, loss = 4.843059286940843e-11
c621-062: Epoch: 0, Step: 221, Rank: 23, loss = 1.7229467630386353e-08
c621-071: Epoch: 0, Step: 221, Rank: 24, loss = 2.562999725341797e-06
c619-041: Epoch: 0, Step: 221, Rank: 20, loss = 4.1443854570388794e-08
c619-022: Epoch: 0, Step: 221, Rank: 17, loss = 2.276897430419922e-05
c619-032: Epoch: 0, Step: 221, Rank: 19, loss = 1.0277290130034089e-10
c619-031: Epoch: 0, Step: 221, Rank: 18, loss = 3.725290298461914e-06
c619-021: Epoch: 0, Step: 221, Rank: 16, loss = 8.881784197001252e-13
c619-002: Epoch: 0, Step: 221, Rank: 13, loss = 8.003553375601768e-11
c619-012: Epoch: 0, Step: 221, Rank: 15, loss = 1.6079866327345371e-09
c619-011: Epoch: 0, Step: 221, Rank: 14, loss = 4.760636329592671e-13
c613-152: Epoch: 0, Step: 221, Rank: 11, loss = 0.0
c622-011: Epoch: 0, Step: 221, Rank: 44, loss = 8.585629984736443e-10
c619-001: Epoch: 0, Step: 221, Rank: 12, loss = 0.005584716796875
c613-151: Epoch: 0, Step: 221, Rank: 10, loss = 8.754432201385498e-08
c622-012: Epoch: 0, Step: 221, Rank: 45, loss = 3.0547380447387695e-07
c613-142: Epoch: 0, Step: 221, Rank: 9, loss = 1.6689300537109375e-05
c613-132: Epoch: 0, Step: 221, Rank: 7, loss = 1.0662875083691372e-25
c613-141: Epoch: 0, Step: 221, Rank: 8, loss = 0.0
c613-131: Epoch: 0, Step: 221, Rank: 6, loss = 2.514570951461792e-08
c613-121: Epoch: 0, Step: 221, Rank: 4, loss = 5.3085386753082275e-08
c613-122: Epoch: 0, Step: 221, Rank: 5, loss = 5.617039278149605e-09
c613-112: Epoch: 0, Step: 221, Rank: 3, loss = 4.05634636990726e-10
c613-101: Epoch: 0, Step: 221, Rank: 0, loss = 7.188646122813225e-09
c613-111: Epoch: 0, Step: 221, Rank: 2, loss = 0.69140625
c622-021: Epoch: 0, Step: 221, Rank: 46, loss = 7.188646122813225e-09
c613-102: Epoch: 0, Step: 221, Rank: 1, loss = 7.729977369308472e-08
c622-102: Epoch: 0, Step: 221, Rank: 63, loss = 6.007030606269836e-08
c622-092: Epoch: 0, Step: 221, Rank: 61, loss = 6.693881005048752e-10
c622-101: Epoch: 0, Step: 221, Rank: 62, loss = 2.9325485229492188e-05
c622-052: Epoch: 0, Step: 221, Rank: 53, loss = 5.893525667488575e-10
c622-022: Epoch: 0, Step: 221, Rank: 47, loss = 4.172325134277344e-07
c622-031: Epoch: 0, Step: 221, Rank: 48, loss = 5.3085386753082275e-08
c622-081: Epoch: 0, Step: 221, Rank: 58, loss = 1.55717134475708e-06
c622-051: Epoch: 0, Step: 221, Rank: 52, loss = 2.2118911147117615e-08
c622-091: Epoch: 0, Step: 221, Rank: 60, loss = 6.344635039567947e-09
c622-032: Epoch: 0, Step: 221, Rank: 49, loss = 1.857925203976527e-26
c622-041: Epoch: 0, Step: 221, Rank: 50, loss = 5.4836273193359375e-05
c622-082: Epoch: 0, Step: 221, Rank: 59, loss = 5.0182080713057076e-14
c622-061: Epoch: 0, Step: 221, Rank: 54, loss = 4.94765117764473e-09
c622-042: Epoch: 0, Step: 221, Rank: 51, loss = 1.8189894035458565e-09
c622-072: Epoch: 0, Step: 221, Rank: 57, loss = 1.55717134475708e-06
c622-071: Epoch: 0, Step: 221, Rank: 56, loss = 0.0
c622-062: Epoch: 0, Step: 221, Rank: 55, loss = 7.729977369308472e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 222, Rank: 0, loss = 1.8533319234848022e-07
c622-002: Epoch: 0, Step: 222, Rank: 43, loss = 1.955777406692505e-08
c622-052: Epoch: 0, Step: 222, Rank: 53, loss = 3.583409124985337e-10
c619-001: Epoch: 0, Step: 222, Rank: 12, loss = 4.1443854570388794e-08
c619-002: Epoch: 0, Step: 222, Rank: 13, loss = 2.1736923372372985e-10
c622-081: Epoch: 0, Step: 222, Rank: 58, loss = 4.798173904418945e-06
c621-091: Epoch: 0, Step: 222, Rank: 28, loss = 1.9190338207408786e-10
c621-132: Epoch: 0, Step: 222, Rank: 37, loss = 2.8405338525772095e-08
c613-152: Epoch: 0, Step: 222, Rank: 11, loss = 2.726912498474121e-06
c621-111: Epoch: 0, Step: 222, Rank: 32, loss = 0.0
c622-001: Epoch: 0, Step: 222, Rank: 42, loss = 1.6079866327345371e-09
c619-021: Epoch: 0, Step: 222, Rank: 16, loss = 0.00096893310546875
c619-031: Epoch: 0, Step: 222, Rank: 18, loss = 0.69140625
c622-061: Epoch: 0, Step: 222, Rank: 54, loss = 0.69140625
c622-012: Epoch: 0, Step: 222, Rank: 45, loss = 2.5331974029541016e-07
c622-062: Epoch: 0, Step: 222, Rank: 55, loss = 0.69140625
c622-072: Epoch: 0, Step: 222, Rank: 57, loss = 5.3085386753082275e-08
c622-032: Epoch: 0, Step: 222, Rank: 49, loss = 5.3085386753082275e-08
c621-081: Epoch: 0, Step: 222, Rank: 26, loss = 0.0
c622-031: Epoch: 0, Step: 222, Rank: 48, loss = 4.843059286940843e-11
c619-022: Epoch: 0, Step: 222, Rank: 17, loss = 0.0
c613-151: Epoch: 0, Step: 222, Rank: 10, loss = 0.0
c622-102: Epoch: 0, Step: 222, Rank: 63, loss = 2.455635694786906e-10
c621-151: Epoch: 0, Step: 222, Rank: 40, loss = 1.955777406692505e-08
c613-121: Epoch: 0, Step: 222, Rank: 4, loss = 1.955777406692505e-08
c613-131: Epoch: 0, Step: 222, Rank: 6, loss = 0.69140625
c613-102: Epoch: 0, Step: 222, Rank: 1, loss = 7.566995918750763e-10
c621-072: Epoch: 0, Step: 222, Rank: 25, loss = 4.602043190971017e-10
c621-082: Epoch: 0, Step: 222, Rank: 27, loss = 4.1443854570388794e-08
c613-122: Epoch: 0, Step: 222, Rank: 5, loss = 4.05634636990726e-10
c622-022: Epoch: 0, Step: 222, Rank: 47, loss = 1.525040715932846e-08
c619-011: Epoch: 0, Step: 222, Rank: 14, loss = 5.400124791776761e-13
c622-101: Epoch: 0, Step: 222, Rank: 62, loss = 2.066371962428093e-09
c621-101: Epoch: 0, Step: 222, Rank: 30, loss = 0.0002956390380859375
c613-132: Epoch: 0, Step: 222, Rank: 7, loss = 5.617039278149605e-09
c622-051: Epoch: 0, Step: 222, Rank: 52, loss = 1.0800249583553523e-11
c622-011: Epoch: 0, Step: 222, Rank: 44, loss = 1.525040715932846e-08
c613-112: Epoch: 0, Step: 222, Rank: 3, loss = 0.0
c622-092: Epoch: 0, Step: 222, Rank: 61, loss = 4.760636329592671e-13
c621-142: Epoch: 0, Step: 222, Rank: 39, loss = 2.234049398383217e-20
c621-131: Epoch: 0, Step: 222, Rank: 36, loss = 3.46451997756958e-07
c621-152: Epoch: 0, Step: 222, Rank: 41, loss = 7.566995918750763e-10
c621-141: Epoch: 0, Step: 222, Rank: 38, loss = 1.2931877790833823e-12
c613-142: Epoch: 0, Step: 222, Rank: 9, loss = 5.699694156646729e-07
c621-102: Epoch: 0, Step: 222, Rank: 31, loss = 3.213062882423401e-08
c621-112: Epoch: 0, Step: 222, Rank: 33, loss = 1.1874362826347351e-08
c622-082: Epoch: 0, Step: 222, Rank: 59, loss = 1.8775463104248047e-06
c619-041: Epoch: 0, Step: 222, Rank: 20, loss = 9.255018085241318e-09
c622-091: Epoch: 0, Step: 222, Rank: 60, loss = 1.2993812561035156e-05
c613-141: Epoch: 0, Step: 222, Rank: 8, loss = 3.635980405647388e-15
c621-092: Epoch: 0, Step: 222, Rank: 29, loss = 3.841705620288849e-09
c613-111: Epoch: 0, Step: 222, Rank: 2, loss = 1.3597309589385986e-07
c622-021: Epoch: 0, Step: 222, Rank: 46, loss = 5.20230969414115e-10
c619-012: Epoch: 0, Step: 222, Rank: 15, loss = 4.760636329592671e-13
c619-032: Epoch: 0, Step: 222, Rank: 19, loss = 3.213062882423401e-08
c622-042: Epoch: 0, Step: 222, Rank: 51, loss = 6.344635039567947e-09
c621-121: Epoch: 0, Step: 222, Rank: 34, loss = 4.4517219066619873e-07
c621-122: Epoch: 0, Step: 222, Rank: 35, loss = 4.4517219066619873e-07
c622-041: Epoch: 0, Step: 222, Rank: 50, loss = 7.048583938740194e-11
c621-071: Epoch: 0, Step: 222, Rank: 24, loss = 2.7008354663848877e-07
c622-071: Epoch: 0, Step: 222, Rank: 56, loss = 4.1443854570388794e-08
c621-052: Epoch: 0, Step: 222, Rank: 21, loss = 2.2351741790771484e-07
c621-062: Epoch: 0, Step: 222, Rank: 23, loss = 1.7848833522293717e-11
c621-061: Epoch: 0, Step: 222, Rank: 22, loss = 3.635980405647388e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2451171875 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 223, Rank: 43, loss = 0.0
c619-021: Epoch: 0, Step: 223, Rank: 16, loss = 1.053497228147536e-20
c621-111: Epoch: 0, Step: 223, Rank: 32, loss = 3.841705620288849e-09
c621-142: Epoch: 0, Step: 223, Rank: 39, loss = 5.617039278149605e-09
c622-012: Epoch: 0, Step: 223, Rank: 45, loss = 0.0
c622-051: Epoch: 0, Step: 223, Rank: 52, loss = 5.3085386753082275e-08
c613-101: Epoch: 0, Step: 223, Rank: 0, loss = 1.318767317570746e-10
c622-081: Epoch: 0, Step: 223, Rank: 58, loss = 2.014636993408203e-05
c621-132: Epoch: 0, Step: 223, Rank: 37, loss = 3.841705620288849e-09
c619-002: Epoch: 0, Step: 223, Rank: 13, loss = 5.699694156646729e-07
c622-032: Epoch: 0, Step: 223, Rank: 49, loss = 0.0
c619-001: Epoch: 0, Step: 223, Rank: 12, loss = 0.0
c622-022: Epoch: 0, Step: 223, Rank: 47, loss = 5.029141902923584e-07
c621-131: Epoch: 0, Step: 223, Rank: 36, loss = 3.979039320256561e-12
c622-001: Epoch: 0, Step: 223, Rank: 42, loss = 3.583409124985337e-10
c619-031: Epoch: 0, Step: 223, Rank: 18, loss = 2.342858351767063e-09
c621-151: Epoch: 0, Step: 223, Rank: 40, loss = 0.0
c619-011: Epoch: 0, Step: 223, Rank: 14, loss = 2.8405338525772095e-08
c622-031: Epoch: 0, Step: 223, Rank: 48, loss = 0.0
c621-081: Epoch: 0, Step: 223, Rank: 26, loss = 1.1399388313293457e-06
c622-041: Epoch: 0, Step: 223, Rank: 50, loss = 4.760636329592671e-13
c613-152: Epoch: 0, Step: 223, Rank: 11, loss = 5.893525667488575e-10
c613-132: Epoch: 0, Step: 223, Rank: 7, loss = 4.267692565917969e-05
c622-101: Epoch: 0, Step: 223, Rank: 62, loss = 0.0
c613-122: Epoch: 0, Step: 223, Rank: 5, loss = 0.0
c622-052: Epoch: 0, Step: 223, Rank: 53, loss = 1.126900315284729e-07
c619-012: Epoch: 0, Step: 223, Rank: 15, loss = 4.6798959374427795e-08
c621-152: Epoch: 0, Step: 223, Rank: 41, loss = 6.891787052154541e-07
c613-141: Epoch: 0, Step: 223, Rank: 8, loss = 1.955777406692505e-08
c621-052: Epoch: 0, Step: 223, Rank: 21, loss = 6.565414878423326e-12
c621-091: Epoch: 0, Step: 223, Rank: 28, loss = 6.891787052154541e-07
c619-022: Epoch: 0, Step: 223, Rank: 17, loss = 2.0236257114447653e-11
c613-131: Epoch: 0, Step: 223, Rank: 6, loss = 6.845220923423767e-08
c621-101: Epoch: 0, Step: 223, Rank: 30, loss = 0.0
c622-011: Epoch: 0, Step: 223, Rank: 44, loss = 2.1454997138094155e-24
c621-072: Epoch: 0, Step: 223, Rank: 25, loss = 1.1874362826347351e-08
c621-141: Epoch: 0, Step: 223, Rank: 38, loss = 2.1047890186309814e-07
c622-061: Epoch: 0, Step: 223, Rank: 54, loss = 0.69140625
c622-042: Epoch: 0, Step: 223, Rank: 51, loss = 4.231929779052734e-06
c622-062: Epoch: 0, Step: 223, Rank: 55, loss = 1.955777406692505e-08
c622-092: Epoch: 0, Step: 223, Rank: 61, loss = 0.01104736328125
c613-142: Epoch: 0, Step: 223, Rank: 9, loss = 1.318767317570746e-10
c613-151: Epoch: 0, Step: 223, Rank: 10, loss = 3.510081114654895e-12
c613-121: Epoch: 0, Step: 223, Rank: 4, loss = 2.5331974029541016e-07
c621-112: Epoch: 0, Step: 223, Rank: 33, loss = 0.0
c621-082: Epoch: 0, Step: 223, Rank: 27, loss = 3.084540367126465e-06
c613-111: Epoch: 0, Step: 223, Rank: 2, loss = 0.0
c622-021: Epoch: 0, Step: 223, Rank: 46, loss = 1.8775463104248047e-06
c622-082: Epoch: 0, Step: 223, Rank: 59, loss = 3.655441105365753e-08
c619-041: Epoch: 0, Step: 223, Rank: 20, loss = 1.6540288925170898e-06
c622-072: Epoch: 0, Step: 223, Rank: 57, loss = 1.126900315284729e-07
c621-061: Epoch: 0, Step: 223, Rank: 22, loss = 5.781650543212891e-06
c619-032: Epoch: 0, Step: 223, Rank: 19, loss = 4.4517219066619873e-07
c621-122: Epoch: 0, Step: 223, Rank: 35, loss = 3.144186300207963e-17
c621-102: Epoch: 0, Step: 223, Rank: 31, loss = 1.0477378964424133e-08
c622-102: Epoch: 0, Step: 223, Rank: 63, loss = 3.213062882423401e-08
c621-121: Epoch: 0, Step: 223, Rank: 34, loss = 6.845220923423767e-08
c621-062: Epoch: 0, Step: 223, Rank: 23, loss = 8.881784197001252e-13
c621-092: Epoch: 0, Step: 223, Rank: 29, loss = 1.0972125985553305e-16
c622-091: Epoch: 0, Step: 223, Rank: 60, loss = 3.583409124985337e-10
c613-112: Epoch: 0, Step: 223, Rank: 3, loss = 1.0132789611816406e-05
c613-102: Epoch: 0, Step: 223, Rank: 1, loss = 3.123283386230469e-05
c621-071: Epoch: 0, Step: 223, Rank: 24, loss = 1.318767317570746e-10
c622-071: Epoch: 0, Step: 223, Rank: 56, loss = 1.0408340855860843e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24609375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.14s, TFLOPs: 0.88, Samples/sec: 0.47, Time/seq 2.14s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 224, Rank: 0, loss = 1.1874362826347351e-08
c622-002: Epoch: 0, Step: 224, Rank: 43, loss = 2.9331204132176936e-11
c621-111: Epoch: 0, Step: 224, Rank: 32, loss = 0.0
c613-121: Epoch: 0, Step: 224, Rank: 4, loss = 1.318767317570746e-10
c613-122: Epoch: 0, Step: 224, Rank: 5, loss = 2.7284841053187847e-12
c622-081: Epoch: 0, Step: 224, Rank: 58, loss = 6.344635039567947e-09
c613-151: Epoch: 0, Step: 224, Rank: 10, loss = 5.20230969414115e-10
c622-012: Epoch: 0, Step: 224, Rank: 45, loss = 8.585629984736443e-10
c613-152: Epoch: 0, Step: 224, Rank: 11, loss = 2.8405338525772095e-08
c622-001: Epoch: 0, Step: 224, Rank: 42, loss = 2.6756374893466273e-14
c621-132: Epoch: 0, Step: 224, Rank: 37, loss = 6.007030606269836e-08
c613-142: Epoch: 0, Step: 224, Rank: 9, loss = 6.439293542825908e-14
c613-131: Epoch: 0, Step: 224, Rank: 6, loss = 0.0
c621-112: Epoch: 0, Step: 224, Rank: 33, loss = 0.0
c622-052: Epoch: 0, Step: 224, Rank: 53, loss = 3.144186300207963e-17
c613-111: Epoch: 0, Step: 224, Rank: 2, loss = 2.455635694786906e-10
c622-062: Epoch: 0, Step: 224, Rank: 55, loss = 1.0089706847793423e-12
c613-132: Epoch: 0, Step: 224, Rank: 7, loss = 1.0972125985553305e-16
c621-121: Epoch: 0, Step: 224, Rank: 34, loss = 1.5688783605583012e-11
c619-002: Epoch: 0, Step: 224, Rank: 13, loss = 0.0
c619-021: Epoch: 0, Step: 224, Rank: 16, loss = 0.69140625
c622-071: Epoch: 0, Step: 224, Rank: 56, loss = 2.261821987449685e-25
c621-131: Epoch: 0, Step: 224, Rank: 36, loss = 1.4915713109076023e-10
c621-061: Epoch: 0, Step: 224, Rank: 22, loss = 2.1736923372372985e-10
c619-031: Epoch: 0, Step: 224, Rank: 18, loss = 1.2931877790833823e-12
c621-142: Epoch: 0, Step: 224, Rank: 39, loss = 1.1059455573558807e-09
c621-072: Epoch: 0, Step: 224, Rank: 25, loss = 7.283063041541027e-14
c619-012: Epoch: 0, Step: 224, Rank: 15, loss = 6.439293542825908e-14
c621-122: Epoch: 0, Step: 224, Rank: 35, loss = 1.318767317570746e-10
c613-102: Epoch: 0, Step: 224, Rank: 1, loss = 4.3655745685100555e-09
c619-011: Epoch: 0, Step: 224, Rank: 14, loss = 0.0
c613-141: Epoch: 0, Step: 224, Rank: 8, loss = 2.540190280342358e-13
c622-061: Epoch: 0, Step: 224, Rank: 54, loss = 1.318767317570746e-10
c619-032: Epoch: 0, Step: 224, Rank: 19, loss = 2.7284841053187847e-12
c619-001: Epoch: 0, Step: 224, Rank: 12, loss = 0.3203125
c622-011: Epoch: 0, Step: 224, Rank: 44, loss = 1.0800249583553523e-11
c621-052: Epoch: 0, Step: 224, Rank: 21, loss = 2.439454888092385e-17
c622-102: Epoch: 0, Step: 224, Rank: 63, loss = 9.486769009248164e-19
c621-101: Epoch: 0, Step: 224, Rank: 30, loss = 6.891787052154541e-07
c621-081: Epoch: 0, Step: 224, Rank: 26, loss = 2.2065682614424986e-15
c621-151: Epoch: 0, Step: 224, Rank: 40, loss = 9.549694368615746e-12
c621-152: Epoch: 0, Step: 224, Rank: 41, loss = 1.9806378759312793e-13
c621-102: Epoch: 0, Step: 224, Rank: 31, loss = 2.7830537874251604e-10
c622-041: Epoch: 0, Step: 224, Rank: 50, loss = 5.995204332975845e-15
c619-041: Epoch: 0, Step: 224, Rank: 20, loss = 7.188646122813225e-09
c621-091: Epoch: 0, Step: 224, Rank: 28, loss = 9.74978320300579e-10
c621-141: Epoch: 0, Step: 224, Rank: 38, loss = 8.003553375601768e-11
c621-082: Epoch: 0, Step: 224, Rank: 27, loss = 3.0547380447387695e-07
c619-022: Epoch: 0, Step: 224, Rank: 17, loss = 1.3589129821411916e-13
c622-072: Epoch: 0, Step: 224, Rank: 57, loss = 1.9806378759312793e-13
c621-092: Epoch: 0, Step: 224, Rank: 29, loss = 3.268496584496461e-13
c622-092: Epoch: 0, Step: 224, Rank: 61, loss = 1.0089706847793423e-12
c621-071: Epoch: 0, Step: 224, Rank: 24, loss = 5.115907697472721e-12
c622-051: Epoch: 0, Step: 224, Rank: 52, loss = 9.74978320300579e-10
c622-091: Epoch: 0, Step: 224, Rank: 60, loss = 1.84297022087776e-14
c622-032: Epoch: 0, Step: 224, Rank: 49, loss = 7.44648787076585e-12
c622-022: Epoch: 0, Step: 224, Rank: 47, loss = 5.115907697472721e-12
c622-042: Epoch: 0, Step: 224, Rank: 51, loss = 5.759824041329242e-19
c622-031: Epoch: 0, Step: 224, Rank: 48, loss = 1.6079866327345371e-09
c622-082: Epoch: 0, Step: 224, Rank: 59, loss = 1.0089706847793423e-12
c622-101: Epoch: 0, Step: 224, Rank: 62, loss = 1.7848833522293717e-11
c622-021: Epoch: 0, Step: 224, Rank: 46, loss = 2.7284841053187847e-12
c613-112: Epoch: 0, Step: 224, Rank: 3, loss = 6.628036499023438e-05
c621-062: Epoch: 0, Step: 224, Rank: 23, loss = 3.268496584496461e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-151: Epoch: 0, Step: 225, Rank: 10, loss = 4.3655745685100555e-09
c613-152: Epoch: 0, Step: 225, Rank: 11, loss = 2.648448571562767e-09
c613-142: Epoch: 0, Step: 225, Rank: 9, loss = 5.20230969414115e-10
c619-001: Epoch: 0, Step: 225, Rank: 12, loss = 3.4897757426877174e-19
c613-141: Epoch: 0, Step: 225, Rank: 8, loss = 0.69140625
c613-132: Epoch: 0, Step: 225, Rank: 7, loss = 2.9335764912906377e-30
c613-131: Epoch: 0, Step: 225, Rank: 6, loss = 0.0
c613-122: Epoch: 0, Step: 225, Rank: 5, loss = 3.46451997756958e-07
c613-112: Epoch: 0, Step: 225, Rank: 3, loss = 2.648448571562767e-09
c613-121: Epoch: 0, Step: 225, Rank: 4, loss = 5.115907697472721e-12
c619-002: Epoch: 0, Step: 225, Rank: 13, loss = 3.342393029015511e-11
c613-111: Epoch: 0, Step: 225, Rank: 2, loss = 3.688037395477295e-07
c613-101: Epoch: 0, Step: 225, Rank: 0, loss = 1.0089706847793423e-12
c613-102: Epoch: 0, Step: 225, Rank: 1, loss = 5.182486384480711e-17
c619-011: Epoch: 0, Step: 225, Rank: 14, loss = 2.4158453015843406e-12
c622-102: Epoch: 0, Step: 225, Rank: 63, loss = 1.4637180356658064e-12
c622-101: Epoch: 0, Step: 225, Rank: 62, loss = 8.881784197001252e-13
c622-092: Epoch: 0, Step: 225, Rank: 61, loss = 5.502442945726216e-11
c622-081: Epoch: 0, Step: 225, Rank: 58, loss = 7.44648787076585e-12
c622-072: Epoch: 0, Step: 225, Rank: 57, loss = 3.774403012357652e-11
c622-091: Epoch: 0, Step: 225, Rank: 60, loss = 3.8163916471489756e-16
c622-062: Epoch: 0, Step: 225, Rank: 55, loss = 2.562999725341797e-06
c619-012: Epoch: 0, Step: 225, Rank: 15, loss = 3.694822225952521e-13
c622-082: Epoch: 0, Step: 225, Rank: 59, loss = 0.00014019012451171875
c622-071: Epoch: 0, Step: 225, Rank: 56, loss = 2.5920599000528455e-11
c622-052: Epoch: 0, Step: 225, Rank: 53, loss = 0.1796875
c622-061: Epoch: 0, Step: 225, Rank: 54, loss = 2.4158453015843406e-12
c619-021: Epoch: 0, Step: 225, Rank: 16, loss = 3.078125
c622-051: Epoch: 0, Step: 225, Rank: 52, loss = 2.066371962428093e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-042: Epoch: 0, Step: 225, Rank: 51, loss = 4.3655745685100555e-09
c622-032: Epoch: 0, Step: 225, Rank: 49, loss = 0.0
c622-041: Epoch: 0, Step: 225, Rank: 50, loss = 2.261821987449685e-25
c622-031: Epoch: 0, Step: 225, Rank: 48, loss = 8.003553375601768e-11
c622-022: Epoch: 0, Step: 225, Rank: 47, loss = 0.0
c622-002: Epoch: 0, Step: 225, Rank: 43, loss = 2.1175823681357508e-19
c622-012: Epoch: 0, Step: 225, Rank: 45, loss = 1.199040866595169e-13
c622-021: Epoch: 0, Step: 225, Rank: 46, loss = 3.213062882423401e-08
c619-022: Epoch: 0, Step: 225, Rank: 17, loss = 1.2304311611726287e-23
c622-001: Epoch: 0, Step: 225, Rank: 42, loss = 5.115907697472721e-12
c622-011: Epoch: 0, Step: 225, Rank: 44, loss = 1.4915713109076023e-10
c621-152: Epoch: 0, Step: 225, Rank: 41, loss = 1.7139067942650854e-15
c621-151: Epoch: 0, Step: 225, Rank: 40, loss = 0.0
c621-142: Epoch: 0, Step: 225, Rank: 39, loss = 5.3085386753082275e-08
c621-132: Epoch: 0, Step: 225, Rank: 37, loss = 5.893525667488575e-10
c619-031: Epoch: 0, Step: 225, Rank: 18, loss = 3.6845933205562065e-20
c621-131: Epoch: 0, Step: 225, Rank: 36, loss = 3.510081114654895e-12
c621-141: Epoch: 0, Step: 225, Rank: 38, loss = 9.049472282640636e-11
c621-111: Epoch: 0, Step: 225, Rank: 32, loss = 3.510081114654895e-12
c621-122: Epoch: 0, Step: 225, Rank: 35, loss = 4.843059286940843e-11
c621-112: Epoch: 0, Step: 225, Rank: 33, loss = 2.2065682614424986e-15
c621-121: Epoch: 0, Step: 225, Rank: 34, loss = 2.4158453015843406e-12
c619-032: Epoch: 0, Step: 225, Rank: 19, loss = 6.845220923423767e-08
c621-091: Epoch: 0, Step: 225, Rank: 28, loss = 3.213062882423401e-08
c621-102: Epoch: 0, Step: 225, Rank: 31, loss = 1.0132789611816406e-05
c621-101: Epoch: 0, Step: 225, Rank: 30, loss = 0.0
c621-082: Epoch: 0, Step: 225, Rank: 27, loss = 2.7284841053187847e-12
c621-081: Epoch: 0, Step: 225, Rank: 26, loss = 5.115907697472721e-12
c621-072: Epoch: 0, Step: 225, Rank: 25, loss = 7.44648787076585e-12
c621-092: Epoch: 0, Step: 225, Rank: 29, loss = 0.0
c619-041: Epoch: 0, Step: 225, Rank: 20, loss = 7.66053886991358e-15
c621-061: Epoch: 0, Step: 225, Rank: 22, loss = 1.199040866595169e-13
c621-052: Epoch: 0, Step: 225, Rank: 21, loss = 3.510081114654895e-12
c621-062: Epoch: 0, Step: 225, Rank: 23, loss = 6.629788138637702e-36
c621-071: Epoch: 0, Step: 225, Rank: 24, loss = 4.3655745685100555e-09
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 226, Rank: 0, loss = 2.905726432800293e-06
c622-101: Epoch: 0, Step: 226, Rank: 62, loss = 5.182486384480711e-17
c622-092: Epoch: 0, Step: 226, Rank: 61, loss = 6.565414878423326e-12
c622-102: Epoch: 0, Step: 226, Rank: 63, loss = 2.1736923372372985e-10
c622-081: Epoch: 0, Step: 226, Rank: 58, loss = 5.115907697472721e-12
c622-062: Epoch: 0, Step: 226, Rank: 55, loss = 3.694822225952521e-13
c622-091: Epoch: 0, Step: 226, Rank: 60, loss = 1.0800249583553523e-11
c622-082: Epoch: 0, Step: 226, Rank: 59, loss = 3.635980405647388e-15
c622-052: Epoch: 0, Step: 226, Rank: 53, loss = 4.760636329592671e-13
c613-121: Epoch: 0, Step: 226, Rank: 4, loss = 0.0
c613-111: Epoch: 0, Step: 226, Rank: 2, loss = 1.6079866327345371e-09
c613-112: Epoch: 0, Step: 226, Rank: 3, loss = 7.048583938740194e-11
c613-122: Epoch: 0, Step: 226, Rank: 5, loss = 3.342393029015511e-11
c613-132: Epoch: 0, Step: 226, Rank: 7, loss = 6.344635039567947e-09
c622-061: Epoch: 0, Step: 226, Rank: 54, loss = 1.0089706847793423e-12
c622-002: Epoch: 0, Step: 226, Rank: 43, loss = 4.255493527005605e-18
c613-102: Epoch: 0, Step: 226, Rank: 1, loss = 4.760636329592671e-13
c622-071: Epoch: 0, Step: 226, Rank: 56, loss = 1.199040866595169e-13
c613-152: Epoch: 0, Step: 226, Rank: 11, loss = 1.199040866595169e-13
c619-001: Epoch: 0, Step: 226, Rank: 12, loss = 1.1546753136970622e-17
c622-032: Epoch: 0, Step: 226, Rank: 49, loss = 8.66885281955573e-22
c621-081: Epoch: 0, Step: 226, Rank: 26, loss = 9.880984919163893e-15
c622-012: Epoch: 0, Step: 226, Rank: 45, loss = 3.841705620288849e-09
c619-002: Epoch: 0, Step: 226, Rank: 13, loss = 1.3589129821411916e-13
c622-051: Epoch: 0, Step: 226, Rank: 52, loss = 1.7848833522293717e-11
c613-131: Epoch: 0, Step: 226, Rank: 6, loss = 5.400124791776761e-13
c622-041: Epoch: 0, Step: 226, Rank: 50, loss = 6.198883056640625e-05
c613-142: Epoch: 0, Step: 226, Rank: 9, loss = 1.9190338207408786e-10
c613-151: Epoch: 0, Step: 226, Rank: 10, loss = 3.841705620288849e-09
c622-042: Epoch: 0, Step: 226, Rank: 51, loss = 2.5920599000528455e-11
c622-072: Epoch: 0, Step: 226, Rank: 57, loss = 6.693881005048752e-10
c621-091: Epoch: 0, Step: 226, Rank: 28, loss = 0.69140625
c619-021: Epoch: 0, Step: 226, Rank: 16, loss = 2.9976945370435715e-09
c619-011: Epoch: 0, Step: 226, Rank: 14, loss = 3.5695955961250784e-29
c621-111: Epoch: 0, Step: 226, Rank: 32, loss = 1.4051260155412137e-16
c622-022: Epoch: 0, Step: 226, Rank: 47, loss = 3.583409124985337e-10
c622-031: Epoch: 0, Step: 226, Rank: 48, loss = 2.1047890186309814e-07
c621-082: Epoch: 0, Step: 226, Rank: 27, loss = 1.1641532182693481e-10
c621-151: Epoch: 0, Step: 226, Rank: 40, loss = 5.816113682013476e-24
c621-072: Epoch: 0, Step: 226, Rank: 25, loss = 6.007030606269836e-08
c621-142: Epoch: 0, Step: 226, Rank: 39, loss = 3.1650415621697903e-10
c613-141: Epoch: 0, Step: 226, Rank: 8, loss = 5.617039278149605e-09
c621-061: Epoch: 0, Step: 226, Rank: 22, loss = 5.301145283085199e-27
c621-131: Epoch: 0, Step: 226, Rank: 36, loss = 1.418811734765768e-09
c621-092: Epoch: 0, Step: 226, Rank: 29, loss = 2.5920599000528455e-11
c619-012: Epoch: 0, Step: 226, Rank: 15, loss = 1.3869794202037156e-11
c619-031: Epoch: 0, Step: 226, Rank: 18, loss = 1.7848833522293717e-11
c622-001: Epoch: 0, Step: 226, Rank: 42, loss = 7.66053886991358e-15
c621-112: Epoch: 0, Step: 226, Rank: 33, loss = 5.893525667488575e-10
c621-121: Epoch: 0, Step: 226, Rank: 34, loss = 7.44648787076585e-12
c621-052: Epoch: 0, Step: 226, Rank: 21, loss = 1.3828277587890625e-05
c621-122: Epoch: 0, Step: 226, Rank: 35, loss = 0.0
c621-141: Epoch: 0, Step: 226, Rank: 38, loss = 3.342393029015511e-11
c621-152: Epoch: 0, Step: 226, Rank: 41, loss = 9.370282327836321e-14
c619-022: Epoch: 0, Step: 226, Rank: 17, loss = 3.583409124985337e-10
c619-032: Epoch: 0, Step: 226, Rank: 19, loss = 4.274625098332763e-11
c622-011: Epoch: 0, Step: 226, Rank: 44, loss = 1.418811734765768e-09
c621-132: Epoch: 0, Step: 226, Rank: 37, loss = 0.0
c619-041: Epoch: 0, Step: 226, Rank: 20, loss = 72.0
c621-071: Epoch: 0, Step: 226, Rank: 24, loss = 2.4158453015843406e-12
c621-101: Epoch: 0, Step: 226, Rank: 30, loss = 4.4517219066619873e-07
c622-021: Epoch: 0, Step: 226, Rank: 46, loss = 7.188646122813225e-09
c621-102: Epoch: 0, Step: 226, Rank: 31, loss = 0.69140625
c621-062: Epoch: 0, Step: 226, Rank: 23, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 227, Rank: 32, loss = 5.400124791776761e-13
c619-021: Epoch: 0, Step: 227, Rank: 16, loss = 3.510081114654895e-12
c621-132: Epoch: 0, Step: 227, Rank: 37, loss = 2.204051907791789e-39
c622-002: Epoch: 0, Step: 227, Rank: 43, loss = 6.565414878423326e-12
c613-101: Epoch: 0, Step: 227, Rank: 0, loss = 3.979039320256561e-12
c619-002: Epoch: 0, Step: 227, Rank: 13, loss = 1.3213420162451948e-29
c619-001: Epoch: 0, Step: 227, Rank: 12, loss = 1.1641532182693481e-10
c622-001: Epoch: 0, Step: 227, Rank: 42, loss = 1.6079866327345371e-09
c621-082: Epoch: 0, Step: 227, Rank: 27, loss = 5.502442945726216e-11
c613-142: Epoch: 0, Step: 227, Rank: 9, loss = 1.9190338207408786e-10
c619-022: Epoch: 0, Step: 227, Rank: 17, loss = 1.2759119272232056e-07
c622-012: Epoch: 0, Step: 227, Rank: 45, loss = 1.0800249583553523e-11
c621-081: Epoch: 0, Step: 227, Rank: 26, loss = 6.565414878423326e-12
c619-011: Epoch: 0, Step: 227, Rank: 14, loss = 0.69140625
c622-081: Epoch: 0, Step: 227, Rank: 58, loss = 1.6209256159527285e-14
c622-052: Epoch: 0, Step: 227, Rank: 53, loss = 9.549694368615746e-12
c619-032: Epoch: 0, Step: 227, Rank: 19, loss = 2.9331204132176936e-11
c613-152: Epoch: 0, Step: 227, Rank: 11, loss = 1.0089706847793423e-12
c621-151: Epoch: 0, Step: 227, Rank: 40, loss = 3.841705620288849e-09
c621-061: Epoch: 0, Step: 227, Rank: 22, loss = 9.549694368615746e-12
c621-142: Epoch: 0, Step: 227, Rank: 39, loss = 0.0
c621-131: Epoch: 0, Step: 227, Rank: 36, loss = 1.1059455573558807e-09
c622-101: Epoch: 0, Step: 227, Rank: 62, loss = 3.4051481634378433e-09
c621-091: Epoch: 0, Step: 227, Rank: 28, loss = 1.9806378759312793e-13
c613-122: Epoch: 0, Step: 227, Rank: 5, loss = 2.7284841053187847e-12
c621-102: Epoch: 0, Step: 227, Rank: 31, loss = 6.07222318649292e-07
c621-072: Epoch: 0, Step: 227, Rank: 25, loss = 4.760636329592671e-13
c613-151: Epoch: 0, Step: 227, Rank: 10, loss = 1.7229467630386353e-08
c621-112: Epoch: 0, Step: 227, Rank: 33, loss = 2.9331204132176936e-11
c621-071: Epoch: 0, Step: 227, Rank: 24, loss = 3.864587821847745e-21
c621-101: Epoch: 0, Step: 227, Rank: 30, loss = 3.049420715222343e-26
c613-132: Epoch: 0, Step: 227, Rank: 7, loss = 1.857925203976527e-26
c613-141: Epoch: 0, Step: 227, Rank: 8, loss = 8.754432201385498e-08
c621-141: Epoch: 0, Step: 227, Rank: 38, loss = 5.617039278149605e-09
c621-052: Epoch: 0, Step: 227, Rank: 21, loss = 3.583409124985337e-10
c622-041: Epoch: 0, Step: 227, Rank: 50, loss = 1.1641532182693481e-10
c619-041: Epoch: 0, Step: 227, Rank: 20, loss = 2.1736923372372985e-10
c621-122: Epoch: 0, Step: 227, Rank: 35, loss = 1.2931877790833823e-12
c622-062: Epoch: 0, Step: 227, Rank: 55, loss = 2.5331974029541016e-07
c622-102: Epoch: 0, Step: 227, Rank: 63, loss = 8.003553375601768e-11
c621-121: Epoch: 0, Step: 227, Rank: 34, loss = 3.841705620288849e-09
c613-131: Epoch: 0, Step: 227, Rank: 6, loss = 4.3655745685100555e-09
c613-121: Epoch: 0, Step: 227, Rank: 4, loss = 6.973743438720703e-06
c619-012: Epoch: 0, Step: 227, Rank: 15, loss = 4.843059286940843e-11
c622-031: Epoch: 0, Step: 227, Rank: 48, loss = 6.07222318649292e-07
c622-011: Epoch: 0, Step: 227, Rank: 44, loss = 1.2514647096395493e-09
c622-032: Epoch: 0, Step: 227, Rank: 49, loss = 1.9190338207408786e-10
c622-042: Epoch: 0, Step: 227, Rank: 51, loss = 5.424022674560547e-06
c622-061: Epoch: 0, Step: 227, Rank: 54, loss = 4.760636329592671e-13
c621-062: Epoch: 0, Step: 227, Rank: 23, loss = 3.1650415621697903e-10
c622-092: Epoch: 0, Step: 227, Rank: 61, loss = 7.048583938740194e-11
c613-112: Epoch: 0, Step: 227, Rank: 3, loss = 5.4836273193359375e-05
c622-051: Epoch: 0, Step: 227, Rank: 52, loss = 1.0277290130034089e-10
c622-071: Epoch: 0, Step: 227, Rank: 56, loss = 4.1443854570388794e-08
c622-082: Epoch: 0, Step: 227, Rank: 59, loss = 3.8163916471489756e-16
c622-022: Epoch: 0, Step: 227, Rank: 47, loss = 0.0
c622-072: Epoch: 0, Step: 227, Rank: 57, loss = 1.955777406692505e-08
c622-091: Epoch: 0, Step: 227, Rank: 60, loss = 5.3085386753082275e-08
c613-102: Epoch: 0, Step: 227, Rank: 1, loss = 0.69140625
c621-152: Epoch: 0, Step: 227, Rank: 41, loss = 0.0
c621-092: Epoch: 0, Step: 227, Rank: 29, loss = 5.893525667488575e-10
c613-111: Epoch: 0, Step: 227, Rank: 2, loss = 2.8405338525772095e-08
c619-031: Epoch: 0, Step: 227, Rank: 18, loss = 1.6391277313232422e-07
c622-021: Epoch: 0, Step: 227, Rank: 46, loss = 3.841705620288849e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 228, Rank: 43, loss = 3.46451997756958e-07
c622-052: Epoch: 0, Step: 228, Rank: 53, loss = 6.439293542825908e-14
c621-131: Epoch: 0, Step: 228, Rank: 36, loss = 1.955777406692505e-08
c621-052: Epoch: 0, Step: 228, Rank: 21, loss = 6.439293542825908e-14
c622-012: Epoch: 0, Step: 228, Rank: 45, loss = 6.439293542825908e-14
c613-101: Epoch: 0, Step: 228, Rank: 0, loss = 1.150369644165039e-05
c619-021: Epoch: 0, Step: 228, Rank: 16, loss = 2.8919009696678116e-25
c619-022: Epoch: 0, Step: 228, Rank: 17, loss = 5.893525667488575e-10
c622-001: Epoch: 0, Step: 228, Rank: 42, loss = 4.3655745685100555e-09
c621-061: Epoch: 0, Step: 228, Rank: 22, loss = 9.549694368615746e-12
c621-142: Epoch: 0, Step: 228, Rank: 39, loss = 2.6756374893466273e-14
c621-101: Epoch: 0, Step: 228, Rank: 30, loss = 1.5688783605583012e-11
c621-062: Epoch: 0, Step: 228, Rank: 23, loss = 1.955777406692505e-08
c619-002: Epoch: 0, Step: 228, Rank: 13, loss = 7.188646122813225e-09
c621-122: Epoch: 0, Step: 228, Rank: 35, loss = 1.3869794202037156e-11
c622-031: Epoch: 0, Step: 228, Rank: 48, loss = 7.420778274536133e-06
c621-121: Epoch: 0, Step: 228, Rank: 34, loss = 0.0
c621-111: Epoch: 0, Step: 228, Rank: 32, loss = 4.05634636990726e-10
c619-031: Epoch: 0, Step: 228, Rank: 18, loss = 8.087401133657338e-35
c621-072: Epoch: 0, Step: 228, Rank: 25, loss = 1.9190338207408786e-10
c619-032: Epoch: 0, Step: 228, Rank: 19, loss = 8.881784197001252e-13
c621-082: Epoch: 0, Step: 228, Rank: 27, loss = 2.562999725341797e-06
c621-151: Epoch: 0, Step: 228, Rank: 40, loss = 0.0
c621-102: Epoch: 0, Step: 228, Rank: 31, loss = 6.230038707144558e-11
c621-132: Epoch: 0, Step: 228, Rank: 37, loss = 0.01416015625
c621-141: Epoch: 0, Step: 228, Rank: 38, loss = 9.74978320300579e-10
c622-051: Epoch: 0, Step: 228, Rank: 52, loss = 3.635980405647388e-15
c622-022: Epoch: 0, Step: 228, Rank: 47, loss = 0.0
c622-101: Epoch: 0, Step: 228, Rank: 62, loss = 0.0
c622-041: Epoch: 0, Step: 228, Rank: 50, loss = 1.0800249583553523e-11
c622-011: Epoch: 0, Step: 228, Rank: 44, loss = 2.2649765014648438e-06
c621-112: Epoch: 0, Step: 228, Rank: 33, loss = 1.2514647096395493e-09
c622-042: Epoch: 0, Step: 228, Rank: 51, loss = 0.0
c619-001: Epoch: 0, Step: 228, Rank: 12, loss = 3.917798799689633e-26
c621-152: Epoch: 0, Step: 228, Rank: 41, loss = 1.1641532182693481e-10
c619-041: Epoch: 0, Step: 228, Rank: 20, loss = 0.00019073486328125
c613-142: Epoch: 0, Step: 228, Rank: 9, loss = 1.6689300537109375e-05
c621-081: Epoch: 0, Step: 228, Rank: 26, loss = 1.4915713109076023e-10
c613-151: Epoch: 0, Step: 228, Rank: 10, loss = 6.628036499023438e-05
c621-091: Epoch: 0, Step: 228, Rank: 28, loss = 1.525040715932846e-08
c622-021: Epoch: 0, Step: 228, Rank: 46, loss = 6.565414878423326e-12
c613-121: Epoch: 0, Step: 228, Rank: 4, loss = 9.049472282640636e-11
c619-011: Epoch: 0, Step: 228, Rank: 14, loss = 1.4637180356658064e-12
c619-012: Epoch: 0, Step: 228, Rank: 15, loss = 7.66053886991358e-15
c613-131: Epoch: 0, Step: 228, Rank: 6, loss = 8.149072527885437e-09
c622-081: Epoch: 0, Step: 228, Rank: 58, loss = 1.3445969671010971e-08
c613-122: Epoch: 0, Step: 228, Rank: 5, loss = 3.841705620288849e-09
c621-071: Epoch: 0, Step: 228, Rank: 24, loss = 3.774403012357652e-11
c613-132: Epoch: 0, Step: 228, Rank: 7, loss = 6.927791673660977e-13
c621-092: Epoch: 0, Step: 228, Rank: 29, loss = 4.926614671774132e-16
c613-112: Epoch: 0, Step: 228, Rank: 3, loss = 0.0
c622-061: Epoch: 0, Step: 228, Rank: 54, loss = 1.1641532182693481e-10
c613-111: Epoch: 0, Step: 228, Rank: 2, loss = 0.095703125
c622-102: Epoch: 0, Step: 228, Rank: 63, loss = 3.979039320256561e-12
c613-152: Epoch: 0, Step: 228, Rank: 11, loss = 3.268496584496461e-13
c613-141: Epoch: 0, Step: 228, Rank: 8, loss = 8.881784197001252e-13
c622-092: Epoch: 0, Step: 228, Rank: 61, loss = 8.881784197001252e-13
c613-102: Epoch: 0, Step: 228, Rank: 1, loss = 4.7222086809427244e-20
c622-032: Epoch: 0, Step: 228, Rank: 49, loss = 0.0
c622-071: Epoch: 0, Step: 228, Rank: 56, loss = 2.5920599000528455e-11
c622-091: Epoch: 0, Step: 228, Rank: 60, loss = 3.583409124985337e-10
c622-082: Epoch: 0, Step: 228, Rank: 59, loss = 1.0089706847793423e-12
c622-062: Epoch: 0, Step: 228, Rank: 55, loss = 2.8405338525772095e-08
c622-072: Epoch: 0, Step: 228, Rank: 57, loss = 2.0236257114447653e-11
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.05s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.05s, Batch Size: 1, Sequence Length: 2048
c622-012: Epoch: 0, Step: 229, Rank: 45, loss = 9.880984919163893e-15
c622-002: Epoch: 0, Step: 229, Rank: 43, loss = 1.6391277313232422e-07
c622-021: Epoch: 0, Step: 229, Rank: 46, loss = 1.0408340855860843e-15
c622-011: Epoch: 0, Step: 229, Rank: 44, loss = 3.46451997756958e-07
c621-151: Epoch: 0, Step: 229, Rank: 40, loss = 5.115907697472721e-12
c622-001: Epoch: 0, Step: 229, Rank: 42, loss = 6.07222318649292e-07
c622-022: Epoch: 0, Step: 229, Rank: 47, loss = 0.0
c621-152: Epoch: 0, Step: 229, Rank: 41, loss = 2.648448571562767e-09
c622-031: Epoch: 0, Step: 229, Rank: 48, loss = 2.648448571562767e-09
c621-142: Epoch: 0, Step: 229, Rank: 39, loss = 7.44648787076585e-12
c621-132: Epoch: 0, Step: 229, Rank: 37, loss = 6.5635692504717e-31
c622-032: Epoch: 0, Step: 229, Rank: 49, loss = 2.9976945370435715e-09
c621-141: Epoch: 0, Step: 229, Rank: 38, loss = 1.3589129821411916e-13
c622-041: Epoch: 0, Step: 229, Rank: 50, loss = 1.318767317570746e-10
c621-131: Epoch: 0, Step: 229, Rank: 36, loss = 2.0236257114447653e-11
c622-051: Epoch: 0, Step: 229, Rank: 52, loss = 1.895427703857422e-05
c621-112: Epoch: 0, Step: 229, Rank: 33, loss = 0.0
c621-122: Epoch: 0, Step: 229, Rank: 35, loss = 1.955777406692505e-08
c622-081: Epoch: 0, Step: 229, Rank: 58, loss = 1.6079866327345371e-09
c621-111: Epoch: 0, Step: 229, Rank: 32, loss = 2.455635694786906e-10
c622-042: Epoch: 0, Step: 229, Rank: 51, loss = 0.0
c622-061: Epoch: 0, Step: 229, Rank: 54, loss = 1.0800249583553523e-11
c621-121: Epoch: 0, Step: 229, Rank: 34, loss = 6.693881005048752e-10
c613-101: Epoch: 0, Step: 229, Rank: 0, loss = 1.053497228147536e-20
c622-052: Epoch: 0, Step: 229, Rank: 53, loss = 2.7550648847397363e-40
c622-071: Epoch: 0, Step: 229, Rank: 56, loss = 0.62890625
c622-062: Epoch: 0, Step: 229, Rank: 55, loss = 1.3869794202037156e-11
c613-152: Epoch: 0, Step: 229, Rank: 11, loss = 1.2514647096395493e-09
c613-111: Epoch: 0, Step: 229, Rank: 2, loss = 2.1736923372372985e-10
c622-092: Epoch: 0, Step: 229, Rank: 61, loss = 1.0800249583553523e-11
c621-102: Epoch: 0, Step: 229, Rank: 31, loss = 1.1399388313293457e-06
c621-091: Epoch: 0, Step: 229, Rank: 28, loss = 2.514570951461792e-08
c622-072: Epoch: 0, Step: 229, Rank: 57, loss = 1.0277290130034089e-10
c621-101: Epoch: 0, Step: 229, Rank: 30, loss = 0.0
c619-031: Epoch: 0, Step: 229, Rank: 18, loss = 9.370282327836321e-14
c619-021: Epoch: 0, Step: 229, Rank: 16, loss = 2.4158453015843406e-12
c619-011: Epoch: 0, Step: 229, Rank: 14, loss = 3.8163916471489756e-16
c622-101: Epoch: 0, Step: 229, Rank: 62, loss = 0.69140625
c621-061: Epoch: 0, Step: 229, Rank: 22, loss = 2.3647750424515834e-14
c613-151: Epoch: 0, Step: 229, Rank: 10, loss = 1.0408340855860843e-15
c622-091: Epoch: 0, Step: 229, Rank: 60, loss = 3.123283386230469e-05
c613-121: Epoch: 0, Step: 229, Rank: 4, loss = 1.3869794202037156e-11
c622-102: Epoch: 0, Step: 229, Rank: 63, loss = 1.3869794202037156e-11
c613-131: Epoch: 0, Step: 229, Rank: 6, loss = 2.4158453015843406e-12
c619-001: Epoch: 0, Step: 229, Rank: 12, loss = 2.342858351767063e-09
c622-082: Epoch: 0, Step: 229, Rank: 59, loss = 2.5920599000528455e-11
c619-002: Epoch: 0, Step: 229, Rank: 13, loss = 4.00543212890625e-05
c613-122: Epoch: 0, Step: 229, Rank: 5, loss = 0.109375
c613-102: Epoch: 0, Step: 229, Rank: 1, loss = 0.69140625
c613-132: Epoch: 0, Step: 229, Rank: 7, loss = 2.7284841053187847e-12
c619-012: Epoch: 0, Step: 229, Rank: 15, loss = 1.1874362826347351e-08
c619-032: Epoch: 0, Step: 229, Rank: 19, loss = 7.66053886991358e-15
c613-112: Epoch: 0, Step: 229, Rank: 3, loss = 1.2931877790833823e-12
c613-141: Epoch: 0, Step: 229, Rank: 8, loss = 0.0001087188720703125
c619-041: Epoch: 0, Step: 229, Rank: 20, loss = 1.2656542480726785e-14
c621-052: Epoch: 0, Step: 229, Rank: 21, loss = 2.7550648847397363e-40
c619-022: Epoch: 0, Step: 229, Rank: 17, loss = 4.3655745685100555e-09
c621-081: Epoch: 0, Step: 229, Rank: 26, loss = 6.927791673660977e-13
c613-142: Epoch: 0, Step: 229, Rank: 9, loss = 5.893525667488575e-10
c621-062: Epoch: 0, Step: 229, Rank: 23, loss = 3.1650415621697903e-10
c621-082: Epoch: 0, Step: 229, Rank: 27, loss = 3.979039320256561e-12
c621-072: Epoch: 0, Step: 229, Rank: 25, loss = 4.05634636990726e-10
c621-092: Epoch: 0, Step: 229, Rank: 29, loss = 0.69140625
c621-071: Epoch: 0, Step: 229, Rank: 24, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24609375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c619-002: Epoch: 0, Step: 230, Rank: 13, loss = 6.973743438720703e-06
c619-021: Epoch: 0, Step: 230, Rank: 16, loss = 1.1864468014524018e-27
c613-101: Epoch: 0, Step: 230, Rank: 0, loss = 4.760636329592671e-13
c613-121: Epoch: 0, Step: 230, Rank: 4, loss = 4.500150680541992e-06
c622-101: Epoch: 0, Step: 230, Rank: 62, loss = 3.583409124985337e-10
c613-131: Epoch: 0, Step: 230, Rank: 6, loss = 0.000148773193359375
c619-011: Epoch: 0, Step: 230, Rank: 14, loss = 5.115907697472721e-12
c613-122: Epoch: 0, Step: 230, Rank: 5, loss = 0.0
c613-142: Epoch: 0, Step: 230, Rank: 9, loss = 1.3869794202037156e-11
c622-092: Epoch: 0, Step: 230, Rank: 61, loss = 1.3589129821411916e-13
c622-081: Epoch: 0, Step: 230, Rank: 58, loss = 7.44648787076585e-12
c622-102: Epoch: 0, Step: 230, Rank: 63, loss = 1.318767317570746e-10
c619-001: Epoch: 0, Step: 230, Rank: 12, loss = 5.20230969414115e-10
c619-012: Epoch: 0, Step: 230, Rank: 15, loss = 4.602043190971017e-10
c613-141: Epoch: 0, Step: 230, Rank: 8, loss = 7.188646122813225e-09
c613-151: Epoch: 0, Step: 230, Rank: 10, loss = 8.998878031629687e-18
c613-111: Epoch: 0, Step: 230, Rank: 2, loss = 4.3655745685100555e-09
c619-022: Epoch: 0, Step: 230, Rank: 17, loss = 0.0
c622-062: Epoch: 0, Step: 230, Rank: 55, loss = 2.0057740190981832e-18
c613-152: Epoch: 0, Step: 230, Rank: 11, loss = 7.048583938740194e-11
c622-052: Epoch: 0, Step: 230, Rank: 53, loss = 0.0
c619-031: Epoch: 0, Step: 230, Rank: 18, loss = 1.0800249583553523e-11
c622-061: Epoch: 0, Step: 230, Rank: 54, loss = 1.5688783605583012e-11
c622-071: Epoch: 0, Step: 230, Rank: 56, loss = 5.400124791776761e-13
c622-002: Epoch: 0, Step: 230, Rank: 43, loss = 5.9884384208290615e-34
c613-112: Epoch: 0, Step: 230, Rank: 3, loss = 8.003553375601768e-11
c622-032: Epoch: 0, Step: 230, Rank: 49, loss = 1.0089706847793423e-12
c622-091: Epoch: 0, Step: 230, Rank: 60, loss = 5.115907697472721e-12
c622-001: Epoch: 0, Step: 230, Rank: 42, loss = 2.1047890186309814e-07
c613-132: Epoch: 0, Step: 230, Rank: 7, loss = 5.893525667488575e-10
c622-041: Epoch: 0, Step: 230, Rank: 50, loss = 0.000278472900390625
c622-012: Epoch: 0, Step: 230, Rank: 45, loss = 7.048583938740194e-11
c621-151: Epoch: 0, Step: 230, Rank: 40, loss = 1.6079866327345371e-09
c622-051: Epoch: 0, Step: 230, Rank: 52, loss = 1.0800249583553523e-11
c622-011: Epoch: 0, Step: 230, Rank: 44, loss = 5.893525667488575e-10
c622-082: Epoch: 0, Step: 230, Rank: 59, loss = 5.400124791776761e-13
c622-022: Epoch: 0, Step: 230, Rank: 47, loss = 2.1736923372372985e-10
c621-121: Epoch: 0, Step: 230, Rank: 34, loss = 6.891787052154541e-07
c621-122: Epoch: 0, Step: 230, Rank: 35, loss = 0.69140625
c621-131: Epoch: 0, Step: 230, Rank: 36, loss = 7.44648787076585e-12
c622-031: Epoch: 0, Step: 230, Rank: 48, loss = 8.940696716308594e-06
c621-112: Epoch: 0, Step: 230, Rank: 33, loss = 8.487701416015625e-05
c613-102: Epoch: 0, Step: 230, Rank: 1, loss = 1.7848833522293717e-11
c619-032: Epoch: 0, Step: 230, Rank: 19, loss = 3.144186300207963e-17
c621-142: Epoch: 0, Step: 230, Rank: 39, loss = 7.113753267956038e-23
c622-042: Epoch: 0, Step: 230, Rank: 51, loss = 3.694822225952521e-13
c621-152: Epoch: 0, Step: 230, Rank: 41, loss = 1.6079866327345371e-09
c621-111: Epoch: 0, Step: 230, Rank: 32, loss = 7.566995918750763e-10
c622-021: Epoch: 0, Step: 230, Rank: 46, loss = 8.998878031629687e-18
c621-132: Epoch: 0, Step: 230, Rank: 37, loss = 1.525040715932846e-08
c621-072: Epoch: 0, Step: 230, Rank: 25, loss = 7.729977369308472e-08
c621-082: Epoch: 0, Step: 230, Rank: 27, loss = 2.8587361969832636e-20
c621-091: Epoch: 0, Step: 230, Rank: 28, loss = 4.94765117764473e-09
c621-081: Epoch: 0, Step: 230, Rank: 26, loss = 4.843059286940843e-11
c621-141: Epoch: 0, Step: 230, Rank: 38, loss = 8.083811398051921e-16
c622-072: Epoch: 0, Step: 230, Rank: 57, loss = 3.6845933205562065e-20
c621-102: Epoch: 0, Step: 230, Rank: 31, loss = 1.3589129821411916e-13
c621-092: Epoch: 0, Step: 230, Rank: 29, loss = 1.0800249583553523e-11
c621-101: Epoch: 0, Step: 230, Rank: 30, loss = 4.231929779052734e-06
c621-071: Epoch: 0, Step: 230, Rank: 24, loss = 1.9806378759312793e-13
c621-061: Epoch: 0, Step: 230, Rank: 22, loss = 0.0
c621-052: Epoch: 0, Step: 230, Rank: 21, loss = 1.1641532182693481e-10
c621-062: Epoch: 0, Step: 230, Rank: 23, loss = 9.549694368615746e-12
c619-041: Epoch: 0, Step: 230, Rank: 20, loss = 1.84297022087776e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 231, Rank: 0, loss = 0.022216796875
c613-141: Epoch: 0, Step: 231, Rank: 8, loss = 2.540190280342358e-13
c613-121: Epoch: 0, Step: 231, Rank: 4, loss = 1.1641532182693481e-10
c613-132: Epoch: 0, Step: 231, Rank: 7, loss = 3.864587821847745e-21
c613-151: Epoch: 0, Step: 231, Rank: 10, loss = 0.0
c619-002: Epoch: 0, Step: 231, Rank: 13, loss = 4.760636329592671e-13
c613-122: Epoch: 0, Step: 231, Rank: 5, loss = 1.6079866327345371e-09
c622-002: Epoch: 0, Step: 231, Rank: 43, loss = 2.648448571562767e-09
c613-131: Epoch: 0, Step: 231, Rank: 6, loss = 2.7830537874251604e-10
c613-111: Epoch: 0, Step: 231, Rank: 2, loss = 6.3792168840089494e-21
c622-052: Epoch: 0, Step: 231, Rank: 53, loss = 4.3655745685100555e-09
c619-001: Epoch: 0, Step: 231, Rank: 12, loss = 3.774403012357652e-11
c613-112: Epoch: 0, Step: 231, Rank: 3, loss = 3.4897757426877174e-19
c622-101: Epoch: 0, Step: 231, Rank: 62, loss = 2.0236257114447653e-11
c622-102: Epoch: 0, Step: 231, Rank: 63, loss = 1.4915713109076023e-10
c619-021: Epoch: 0, Step: 231, Rank: 16, loss = 2.2649765014648438e-06
c619-011: Epoch: 0, Step: 231, Rank: 14, loss = 5.115907697472721e-12
c619-032: Epoch: 0, Step: 231, Rank: 19, loss = 5.617039278149605e-09
c622-092: Epoch: 0, Step: 231, Rank: 61, loss = 1.418811734765768e-09
c613-102: Epoch: 0, Step: 231, Rank: 1, loss = 0.0
c622-012: Epoch: 0, Step: 231, Rank: 45, loss = 4.418687638008123e-14
c622-001: Epoch: 0, Step: 231, Rank: 42, loss = 1.6079866327345371e-09
c622-011: Epoch: 0, Step: 231, Rank: 44, loss = 3.268496584496461e-13
c613-152: Epoch: 0, Step: 231, Rank: 11, loss = 3.774403012357652e-11
c622-051: Epoch: 0, Step: 231, Rank: 52, loss = 8.412825991399586e-12
c622-062: Epoch: 0, Step: 231, Rank: 55, loss = 3.979039320256561e-12
c619-031: Epoch: 0, Step: 231, Rank: 18, loss = 2.8405338525772095e-08
c622-032: Epoch: 0, Step: 231, Rank: 49, loss = 6.993104012531504e-18
c621-111: Epoch: 0, Step: 231, Rank: 32, loss = 0.0
c621-132: Epoch: 0, Step: 231, Rank: 37, loss = 0.0
c622-041: Epoch: 0, Step: 231, Rank: 50, loss = 1.3869794202037156e-11
c622-081: Epoch: 0, Step: 231, Rank: 58, loss = 1.0089706847793423e-12
c619-022: Epoch: 0, Step: 231, Rank: 17, loss = 2.9976945370435715e-09
c622-071: Epoch: 0, Step: 231, Rank: 56, loss = 7.44648787076585e-12
c613-142: Epoch: 0, Step: 231, Rank: 9, loss = 7.729977369308472e-08
c622-022: Epoch: 0, Step: 231, Rank: 47, loss = 8.149072527885437e-09
c622-031: Epoch: 0, Step: 231, Rank: 48, loss = 1.0277290130034089e-10
c622-042: Epoch: 0, Step: 231, Rank: 51, loss = 2.9331204132176936e-11
c621-152: Epoch: 0, Step: 231, Rank: 41, loss = 1.6916601452976465e-10
c622-061: Epoch: 0, Step: 231, Rank: 54, loss = 1.8758328224066645e-12
c621-131: Epoch: 0, Step: 231, Rank: 36, loss = 5.893525667488575e-10
c621-081: Epoch: 0, Step: 231, Rank: 26, loss = 0.69140625
c619-012: Epoch: 0, Step: 231, Rank: 15, loss = 1.199040866595169e-13
c622-021: Epoch: 0, Step: 231, Rank: 46, loss = 2.6756374893466273e-14
c621-101: Epoch: 0, Step: 231, Rank: 30, loss = 1.4722347259521484e-05
c622-072: Epoch: 0, Step: 231, Rank: 57, loss = 1.4915713109076023e-10
c621-142: Epoch: 0, Step: 231, Rank: 39, loss = 0.0
c622-091: Epoch: 0, Step: 231, Rank: 60, loss = 1.4227506535912076e-21
c621-102: Epoch: 0, Step: 231, Rank: 31, loss = 1.4637180356658064e-12
c621-122: Epoch: 0, Step: 231, Rank: 35, loss = 2.7284841053187847e-12
c621-151: Epoch: 0, Step: 231, Rank: 40, loss = 5.20230969414115e-10
c621-091: Epoch: 0, Step: 231, Rank: 28, loss = 4.760636329592671e-13
c622-082: Epoch: 0, Step: 231, Rank: 59, loss = 5.893525667488575e-10
c621-072: Epoch: 0, Step: 231, Rank: 25, loss = 8.754432201385498e-08
c621-112: Epoch: 0, Step: 231, Rank: 33, loss = 1.1641532182693481e-10
c621-092: Epoch: 0, Step: 231, Rank: 29, loss = 3.213062882423401e-08
c621-082: Epoch: 0, Step: 231, Rank: 27, loss = 9.370282327836321e-14
c621-071: Epoch: 0, Step: 231, Rank: 24, loss = 1.318767317570746e-10
c621-121: Epoch: 0, Step: 231, Rank: 34, loss = 5.893525667488575e-10
c621-141: Epoch: 0, Step: 231, Rank: 38, loss = 6.230038707144558e-11
c619-041: Epoch: 0, Step: 231, Rank: 20, loss = 1.84297022087776e-14
c621-061: Epoch: 0, Step: 231, Rank: 22, loss = 6.344635039567947e-09
c621-052: Epoch: 0, Step: 231, Rank: 21, loss = 1.1059455573558807e-09
c621-062: Epoch: 0, Step: 231, Rank: 23, loss = 1.0788440704345703e-05
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 232, Rank: 58, loss = 1.3869794202037156e-11
c619-021: Epoch: 0, Step: 232, Rank: 16, loss = 4.6798959374427795e-08
c622-052: Epoch: 0, Step: 232, Rank: 53, loss = 0.0
c613-101: Epoch: 0, Step: 232, Rank: 0, loss = 5.617039278149605e-09
c622-002: Epoch: 0, Step: 232, Rank: 43, loss = 6.439293542825908e-14
c619-002: Epoch: 0, Step: 232, Rank: 13, loss = 8.859277744181285e-32
c622-092: Epoch: 0, Step: 232, Rank: 61, loss = 0.0001087188720703125
c622-071: Epoch: 0, Step: 232, Rank: 56, loss = 0.69140625
c613-141: Epoch: 0, Step: 232, Rank: 8, loss = 4.94765117764473e-09
c613-132: Epoch: 0, Step: 232, Rank: 7, loss = 7.188646122813225e-09
c621-081: Epoch: 0, Step: 232, Rank: 26, loss = 3.0547380447387695e-07
c613-131: Epoch: 0, Step: 232, Rank: 6, loss = 0.0
c622-072: Epoch: 0, Step: 232, Rank: 57, loss = 0.0
c613-111: Epoch: 0, Step: 232, Rank: 2, loss = 1.7848833522293717e-11
c619-001: Epoch: 0, Step: 232, Rank: 12, loss = 0.0
c619-011: Epoch: 0, Step: 232, Rank: 14, loss = 2.7008354663848877e-07
c613-151: Epoch: 0, Step: 232, Rank: 10, loss = 9.632110595703125e-05
c621-111: Epoch: 0, Step: 232, Rank: 32, loss = 3.213062882423401e-08
c622-101: Epoch: 0, Step: 232, Rank: 62, loss = 1.9081958235744878e-17
c619-012: Epoch: 0, Step: 232, Rank: 15, loss = 1.8758328224066645e-12
c621-151: Epoch: 0, Step: 232, Rank: 40, loss = 2.7284841053187847e-12
c622-062: Epoch: 0, Step: 232, Rank: 55, loss = 5.400124791776761e-13
c622-082: Epoch: 0, Step: 232, Rank: 59, loss = 2.066371962428093e-09
c613-122: Epoch: 0, Step: 232, Rank: 5, loss = 6.565414878423326e-12
c622-012: Epoch: 0, Step: 232, Rank: 45, loss = 6.439293542825908e-14
c622-051: Epoch: 0, Step: 232, Rank: 52, loss = 1.0277290130034089e-10
c622-001: Epoch: 0, Step: 232, Rank: 42, loss = 1.84297022087776e-14
c613-102: Epoch: 0, Step: 232, Rank: 1, loss = 1.1015625
c613-152: Epoch: 0, Step: 232, Rank: 11, loss = 1.7497114868092467e-13
c613-142: Epoch: 0, Step: 232, Rank: 9, loss = 2.1736923372372985e-10
c619-022: Epoch: 0, Step: 232, Rank: 17, loss = 1.3828277587890625e-05
c619-031: Epoch: 0, Step: 232, Rank: 18, loss = 8.003553375601768e-11
c621-131: Epoch: 0, Step: 232, Rank: 36, loss = 1.1059455573558807e-09
c622-042: Epoch: 0, Step: 232, Rank: 51, loss = 3.583409124985337e-10
c622-091: Epoch: 0, Step: 232, Rank: 60, loss = 0.0
c621-121: Epoch: 0, Step: 232, Rank: 34, loss = 7.283063041541027e-14
c613-121: Epoch: 0, Step: 232, Rank: 4, loss = 1.0058283805847168e-06
c621-072: Epoch: 0, Step: 232, Rank: 25, loss = 4.760636329592671e-13
c622-032: Epoch: 0, Step: 232, Rank: 49, loss = 4.843059286940843e-11
c621-142: Epoch: 0, Step: 232, Rank: 39, loss = 7.44648787076585e-12
c622-061: Epoch: 0, Step: 232, Rank: 54, loss = 8.881784197001252e-13
c622-011: Epoch: 0, Step: 232, Rank: 44, loss = 3.688037395477295e-07
c613-112: Epoch: 0, Step: 232, Rank: 3, loss = 1.1641532182693481e-10
c622-022: Epoch: 0, Step: 232, Rank: 47, loss = 6.927791673660977e-13
c622-031: Epoch: 0, Step: 232, Rank: 48, loss = 7.048583938740194e-11
c621-122: Epoch: 0, Step: 232, Rank: 35, loss = 1.2278178473934531e-11
c621-112: Epoch: 0, Step: 232, Rank: 33, loss = 1.0477378964424133e-08
c621-091: Epoch: 0, Step: 232, Rank: 28, loss = 2.2649765014648438e-06
c622-102: Epoch: 0, Step: 232, Rank: 63, loss = 9.370282327836321e-14
c621-102: Epoch: 0, Step: 232, Rank: 31, loss = 0.69140625
c621-082: Epoch: 0, Step: 232, Rank: 27, loss = 4.602043190971017e-10
c621-092: Epoch: 0, Step: 232, Rank: 29, loss = 7.44648787076585e-12
c621-132: Epoch: 0, Step: 232, Rank: 37, loss = 3.979039320256561e-12
c621-052: Epoch: 0, Step: 232, Rank: 21, loss = 8.940696716308594e-06
c621-061: Epoch: 0, Step: 232, Rank: 22, loss = 1.996755599975586e-06
c621-152: Epoch: 0, Step: 232, Rank: 41, loss = 1.9806378759312793e-13
c622-021: Epoch: 0, Step: 232, Rank: 46, loss = 3.583409124985337e-10
c621-071: Epoch: 0, Step: 232, Rank: 24, loss = 3.1650415621697903e-10
c621-101: Epoch: 0, Step: 232, Rank: 30, loss = 5.14984130859375e-05
c621-141: Epoch: 0, Step: 232, Rank: 38, loss = 5.893525667488575e-10
c622-041: Epoch: 0, Step: 232, Rank: 50, loss = 0.0
c619-041: Epoch: 0, Step: 232, Rank: 20, loss = 3.635980405647388e-15
c621-062: Epoch: 0, Step: 232, Rank: 23, loss = 0.072265625
c619-032: Epoch: 0, Step: 232, Rank: 19, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 233, Rank: 43, loss = 1.2656542480726785e-14
c622-052: Epoch: 0, Step: 233, Rank: 53, loss = 4.5917748078995606e-40
c613-101: Epoch: 0, Step: 233, Rank: 0, loss = 1.3597309589385986e-07
c622-051: Epoch: 0, Step: 233, Rank: 52, loss = 0.0
c622-001: Epoch: 0, Step: 233, Rank: 42, loss = 1.3597309589385986e-07
c622-012: Epoch: 0, Step: 233, Rank: 45, loss = 2.066371962428093e-09
c619-002: Epoch: 0, Step: 233, Rank: 13, loss = 2.648448571562767e-09
c619-021: Epoch: 0, Step: 233, Rank: 16, loss = 8.754432201385498e-08
c622-081: Epoch: 0, Step: 233, Rank: 58, loss = 2.5920599000528455e-11
c622-061: Epoch: 0, Step: 233, Rank: 54, loss = 0.0
c621-111: Epoch: 0, Step: 233, Rank: 32, loss = 1.5735626220703125e-05
c622-041: Epoch: 0, Step: 233, Rank: 50, loss = 4.926614671774132e-16
c622-021: Epoch: 0, Step: 233, Rank: 46, loss = 8.998878031629687e-18
c613-152: Epoch: 0, Step: 233, Rank: 11, loss = 1.955777406692505e-08
c619-001: Epoch: 0, Step: 233, Rank: 12, loss = 8.003553375601768e-11
c621-151: Epoch: 0, Step: 233, Rank: 40, loss = 1.199040866595169e-13
c622-071: Epoch: 0, Step: 233, Rank: 56, loss = 3.979039320256561e-12
c613-142: Epoch: 0, Step: 233, Rank: 9, loss = 3.0547380447387695e-07
c621-091: Epoch: 0, Step: 233, Rank: 28, loss = 5.542110104105285e-23
c622-062: Epoch: 0, Step: 233, Rank: 55, loss = 3.6261649734165925e-34
c622-072: Epoch: 0, Step: 233, Rank: 57, loss = 4.267692565917969e-05
c622-042: Epoch: 0, Step: 233, Rank: 51, loss = 5.0182080713057076e-14
c622-011: Epoch: 0, Step: 233, Rank: 44, loss = 5.893525667488575e-10
c621-101: Epoch: 0, Step: 233, Rank: 30, loss = 7.44648787076585e-12
c613-121: Epoch: 0, Step: 233, Rank: 4, loss = 5.3085386753082275e-08
c622-082: Epoch: 0, Step: 233, Rank: 59, loss = 1.6540288925170898e-06
c622-101: Epoch: 0, Step: 233, Rank: 62, loss = 9.049472282640636e-11
c613-111: Epoch: 0, Step: 233, Rank: 2, loss = 1.0972125985553305e-16
c619-011: Epoch: 0, Step: 233, Rank: 14, loss = 4.760636329592671e-13
c619-031: Epoch: 0, Step: 233, Rank: 18, loss = 4.172325134277344e-07
c619-041: Epoch: 0, Step: 233, Rank: 20, loss = 0.0
c621-061: Epoch: 0, Step: 233, Rank: 22, loss = 0.0
c622-031: Epoch: 0, Step: 233, Rank: 48, loss = 3.314018249511719e-05
c613-151: Epoch: 0, Step: 233, Rank: 10, loss = 1.525040715932846e-08
c621-122: Epoch: 0, Step: 233, Rank: 35, loss = 0.69140625
c621-112: Epoch: 0, Step: 233, Rank: 33, loss = 3.774403012357652e-11
c621-121: Epoch: 0, Step: 233, Rank: 34, loss = 1.6079866327345371e-09
c613-141: Epoch: 0, Step: 233, Rank: 8, loss = 1.6079866327345371e-09
c621-081: Epoch: 0, Step: 233, Rank: 26, loss = 4.760636329592671e-13
c621-082: Epoch: 0, Step: 233, Rank: 27, loss = 8.940696716308594e-06
c621-152: Epoch: 0, Step: 233, Rank: 41, loss = 2.9331204132176936e-11
c613-112: Epoch: 0, Step: 233, Rank: 3, loss = 8.881784197001252e-13
c613-122: Epoch: 0, Step: 233, Rank: 5, loss = 9.918585419654846e-08
c613-132: Epoch: 0, Step: 233, Rank: 7, loss = 1.7848833522293717e-11
c619-012: Epoch: 0, Step: 233, Rank: 15, loss = 3.979039320256561e-12
c622-032: Epoch: 0, Step: 233, Rank: 49, loss = 1.0800249583553523e-11
c613-131: Epoch: 0, Step: 233, Rank: 6, loss = 2.648448571562767e-09
c622-022: Epoch: 0, Step: 233, Rank: 47, loss = 9.549694368615746e-12
c621-132: Epoch: 0, Step: 233, Rank: 37, loss = 1.996755599975586e-06
c613-102: Epoch: 0, Step: 233, Rank: 1, loss = 4.843059286940843e-11
c621-131: Epoch: 0, Step: 233, Rank: 36, loss = 3.510081114654895e-12
c621-102: Epoch: 0, Step: 233, Rank: 31, loss = 9.870390964984584e-34
c621-092: Epoch: 0, Step: 233, Rank: 29, loss = 5.3085386753082275e-08
c622-091: Epoch: 0, Step: 233, Rank: 60, loss = 0.69140625
c621-141: Epoch: 0, Step: 233, Rank: 38, loss = 6.693881005048752e-10
c621-072: Epoch: 0, Step: 233, Rank: 25, loss = 4.3655745685100555e-09
c619-022: Epoch: 0, Step: 233, Rank: 17, loss = 3.774403012357652e-11
c621-052: Epoch: 0, Step: 233, Rank: 21, loss = 2.6505726415425997e-28
c619-032: Epoch: 0, Step: 233, Rank: 19, loss = 8.754432201385498e-08
c621-142: Epoch: 0, Step: 233, Rank: 39, loss = 0.0
c621-071: Epoch: 0, Step: 233, Rank: 24, loss = 1.7497114868092467e-13
c621-062: Epoch: 0, Step: 233, Rank: 23, loss = 5.3085386753082275e-08
c622-102: Epoch: 0, Step: 233, Rank: 63, loss = 2.276897430419922e-05
c622-092: Epoch: 0, Step: 233, Rank: 61, loss = 3.4051481634378433e-09
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.10s, TFLOPs: 0.90, Samples/sec: 0.48, Time/seq 2.10s, Batch Size: 1, Sequence Length: 2048
c622-052: Epoch: 0, Step: 234, Rank: 53, loss = 0.01507568359375
c613-101: Epoch: 0, Step: 234, Rank: 0, loss = 5.995204332975845e-15
c619-021: Epoch: 0, Step: 234, Rank: 16, loss = 0.0
c622-002: Epoch: 0, Step: 234, Rank: 43, loss = 1.8758328224066645e-12
c619-002: Epoch: 0, Step: 234, Rank: 13, loss = 1.2197274440461925e-18
c613-142: Epoch: 0, Step: 234, Rank: 9, loss = 5.893525667488575e-10
c613-151: Epoch: 0, Step: 234, Rank: 10, loss = 0.0
c622-051: Epoch: 0, Step: 234, Rank: 52, loss = 0.0
c622-081: Epoch: 0, Step: 234, Rank: 58, loss = 6.927791673660977e-13
c621-081: Epoch: 0, Step: 234, Rank: 26, loss = 2.3647750424515834e-14
c619-001: Epoch: 0, Step: 234, Rank: 12, loss = 1.955777406692505e-08
c613-132: Epoch: 0, Step: 234, Rank: 7, loss = 1.4051260155412137e-16
c622-101: Epoch: 0, Step: 234, Rank: 62, loss = 3.694822225952521e-13
c619-011: Epoch: 0, Step: 234, Rank: 14, loss = 1.565316886525947e-18
c622-012: Epoch: 0, Step: 234, Rank: 45, loss = 0.0
c621-061: Epoch: 0, Step: 234, Rank: 22, loss = 0.0007781982421875
c619-012: Epoch: 0, Step: 234, Rank: 15, loss = 3.268496584496461e-13
c621-082: Epoch: 0, Step: 234, Rank: 27, loss = 1.2790197503539935e-19
c621-111: Epoch: 0, Step: 234, Rank: 32, loss = 1.9806378759312793e-13
c613-152: Epoch: 0, Step: 234, Rank: 11, loss = 1.318767317570746e-10
c613-111: Epoch: 0, Step: 234, Rank: 2, loss = 1.2656542480726785e-14
c622-041: Epoch: 0, Step: 234, Rank: 50, loss = 1.6250033905767303e-33
c621-121: Epoch: 0, Step: 234, Rank: 34, loss = 1.2931877790833823e-12
c621-151: Epoch: 0, Step: 234, Rank: 40, loss = 3.084540367126465e-06
c622-102: Epoch: 0, Step: 234, Rank: 63, loss = 4.472333961502706e-19
c613-122: Epoch: 0, Step: 234, Rank: 5, loss = 3.268496584496461e-13
c621-152: Epoch: 0, Step: 234, Rank: 41, loss = 0.0
c613-121: Epoch: 0, Step: 234, Rank: 4, loss = 3.979039320256561e-12
c622-001: Epoch: 0, Step: 234, Rank: 42, loss = 5.400124791776761e-13
c619-022: Epoch: 0, Step: 234, Rank: 17, loss = 4.418687638008123e-14
c622-011: Epoch: 0, Step: 234, Rank: 44, loss = 2.540190280342358e-13
c619-031: Epoch: 0, Step: 234, Rank: 18, loss = 6.565414878423326e-12
c621-131: Epoch: 0, Step: 234, Rank: 36, loss = 0.0
c622-042: Epoch: 0, Step: 234, Rank: 51, loss = 1.4915713109076023e-10
c613-141: Epoch: 0, Step: 234, Rank: 8, loss = 1.2931877790833823e-12
c621-122: Epoch: 0, Step: 234, Rank: 35, loss = 1.7848833522293717e-11
c621-091: Epoch: 0, Step: 234, Rank: 28, loss = 4.6798959374427795e-08
c622-062: Epoch: 0, Step: 234, Rank: 55, loss = 1.6916601452976465e-10
c619-041: Epoch: 0, Step: 234, Rank: 20, loss = 0.69140625
c621-052: Epoch: 0, Step: 234, Rank: 21, loss = 6.439293542825908e-14
c622-032: Epoch: 0, Step: 234, Rank: 49, loss = 6.693881005048752e-10
c622-082: Epoch: 0, Step: 234, Rank: 59, loss = 3.268496584496461e-13
c613-131: Epoch: 0, Step: 234, Rank: 6, loss = 2.1736923372372985e-10
c622-061: Epoch: 0, Step: 234, Rank: 54, loss = 7.048583938740194e-11
c621-072: Epoch: 0, Step: 234, Rank: 25, loss = 3.1650415621697903e-10
c622-031: Epoch: 0, Step: 234, Rank: 48, loss = 2.204051907791789e-39
c621-112: Epoch: 0, Step: 234, Rank: 33, loss = 0.0
c621-101: Epoch: 0, Step: 234, Rank: 30, loss = 2.7830537874251604e-10
c621-132: Epoch: 0, Step: 234, Rank: 37, loss = 4.6629367034256575e-15
c622-022: Epoch: 0, Step: 234, Rank: 47, loss = 2.5920599000528455e-11
c613-102: Epoch: 0, Step: 234, Rank: 1, loss = 5.0961971282958984e-06
c622-072: Epoch: 0, Step: 234, Rank: 57, loss = 1.6079866327345371e-09
c621-141: Epoch: 0, Step: 234, Rank: 38, loss = 3.441691376337985e-14
c621-092: Epoch: 0, Step: 234, Rank: 29, loss = 1.0058283805847168e-06
c621-142: Epoch: 0, Step: 234, Rank: 39, loss = 3.774403012357652e-11
c613-112: Epoch: 0, Step: 234, Rank: 3, loss = 6.635317295611287e-17
c622-071: Epoch: 0, Step: 234, Rank: 56, loss = 1.7848833522293717e-11
c622-021: Epoch: 0, Step: 234, Rank: 46, loss = 1.318767317570746e-10
c621-102: Epoch: 0, Step: 234, Rank: 31, loss = 4.405564747785802e-33
c622-091: Epoch: 0, Step: 234, Rank: 60, loss = 0.5859375
c619-032: Epoch: 0, Step: 234, Rank: 19, loss = 7.188646122813225e-09
c621-062: Epoch: 0, Step: 234, Rank: 23, loss = 1.9806378759312793e-13
c621-071: Epoch: 0, Step: 234, Rank: 24, loss = 3.8163916471489756e-16
c622-092: Epoch: 0, Step: 234, Rank: 61, loss = 7.386127300057499e-19
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 235, Rank: 16, loss = 1.0089706847793423e-12
c622-002: Epoch: 0, Step: 235, Rank: 43, loss = 5.893525667488575e-10
c613-101: Epoch: 0, Step: 235, Rank: 0, loss = 0.0
c622-092: Epoch: 0, Step: 235, Rank: 61, loss = 5.893525667488575e-10
c619-002: Epoch: 0, Step: 235, Rank: 13, loss = 4.926614671774132e-16
c619-011: Epoch: 0, Step: 235, Rank: 14, loss = 2.9331204132176936e-11
c622-012: Epoch: 0, Step: 235, Rank: 45, loss = 0.0
c622-081: Epoch: 0, Step: 235, Rank: 58, loss = 1.2656542480726785e-14
c613-132: Epoch: 0, Step: 235, Rank: 7, loss = 4.602043190971017e-10
c622-101: Epoch: 0, Step: 235, Rank: 62, loss = 8.754432201385498e-08
c621-151: Epoch: 0, Step: 235, Rank: 40, loss = 6.927791673660977e-13
c621-081: Epoch: 0, Step: 235, Rank: 26, loss = 4.05634636990726e-10
c619-001: Epoch: 0, Step: 235, Rank: 12, loss = 1.1059455573558807e-09
c622-052: Epoch: 0, Step: 235, Rank: 53, loss = 2.7830537874251604e-10
c622-102: Epoch: 0, Step: 235, Rank: 63, loss = 0.0
c621-152: Epoch: 0, Step: 235, Rank: 41, loss = 5.893525667488575e-10
c621-072: Epoch: 0, Step: 235, Rank: 25, loss = 1.0089706847793423e-12
c619-012: Epoch: 0, Step: 235, Rank: 15, loss = 9.049472282640636e-11
c622-001: Epoch: 0, Step: 235, Rank: 42, loss = 1.0800249583553523e-11
c621-111: Epoch: 0, Step: 235, Rank: 32, loss = 7.496324301261813e-24
c622-062: Epoch: 0, Step: 235, Rank: 55, loss = 3.635980405647388e-15
c613-131: Epoch: 0, Step: 235, Rank: 6, loss = 4.7222086809427244e-20
c613-121: Epoch: 0, Step: 235, Rank: 4, loss = 0.69140625
c621-131: Epoch: 0, Step: 235, Rank: 36, loss = 1.0408340855860843e-15
c613-142: Epoch: 0, Step: 235, Rank: 9, loss = 0.0
c622-061: Epoch: 0, Step: 235, Rank: 54, loss = 7.44648787076585e-12
c622-041: Epoch: 0, Step: 235, Rank: 50, loss = 4.418687638008123e-14
c621-082: Epoch: 0, Step: 235, Rank: 27, loss = 0.69140625
c622-031: Epoch: 0, Step: 235, Rank: 48, loss = 6.439293542825908e-14
c619-032: Epoch: 0, Step: 235, Rank: 19, loss = 1.2514647096395493e-09
c621-101: Epoch: 0, Step: 235, Rank: 30, loss = 8.754432201385498e-08
c619-031: Epoch: 0, Step: 235, Rank: 18, loss = 2.648448571562767e-09
c621-122: Epoch: 0, Step: 235, Rank: 35, loss = 1.0800249583553523e-11
c613-141: Epoch: 0, Step: 235, Rank: 8, loss = 5.759824041329242e-19
c613-102: Epoch: 0, Step: 235, Rank: 1, loss = 2.066371962428093e-09
c622-072: Epoch: 0, Step: 235, Rank: 57, loss = 4.274625098332763e-11
c613-111: Epoch: 0, Step: 235, Rank: 2, loss = 7.566995918750763e-10
c622-022: Epoch: 0, Step: 235, Rank: 47, loss = 1.3869794202037156e-11
c619-022: Epoch: 0, Step: 235, Rank: 17, loss = 1.55717134475708e-06
c621-132: Epoch: 0, Step: 235, Rank: 37, loss = 2.7830537874251604e-10
c622-011: Epoch: 0, Step: 235, Rank: 44, loss = 0.00116729736328125
c613-152: Epoch: 0, Step: 235, Rank: 11, loss = 1.8367099231598242e-40
c622-051: Epoch: 0, Step: 235, Rank: 52, loss = 6.927791673660977e-13
c613-122: Epoch: 0, Step: 235, Rank: 5, loss = 2.5331974029541016e-07
c613-151: Epoch: 0, Step: 235, Rank: 10, loss = 6.927791673660977e-13
c622-071: Epoch: 0, Step: 235, Rank: 56, loss = 6.007030606269836e-08
c622-032: Epoch: 0, Step: 235, Rank: 49, loss = 2.7830537874251604e-10
c619-041: Epoch: 0, Step: 235, Rank: 20, loss = 4.231929779052734e-06
c622-021: Epoch: 0, Step: 235, Rank: 46, loss = 1.0132789611816406e-05
c621-091: Epoch: 0, Step: 235, Rank: 28, loss = 6.635317295611287e-17
c621-121: Epoch: 0, Step: 235, Rank: 34, loss = 1.9563750449481093e-27
c621-052: Epoch: 0, Step: 235, Rank: 21, loss = 0.0
c621-061: Epoch: 0, Step: 235, Rank: 22, loss = 8.543513119185775e-17
c621-141: Epoch: 0, Step: 235, Rank: 38, loss = 1.014588720084573e-24
c621-071: Epoch: 0, Step: 235, Rank: 24, loss = 2.648448571562767e-09
c613-112: Epoch: 0, Step: 235, Rank: 3, loss = 3.441691376337985e-14
c622-082: Epoch: 0, Step: 235, Rank: 59, loss = 0.0
c621-102: Epoch: 0, Step: 235, Rank: 31, loss = 1.0277290130034089e-10
c621-112: Epoch: 0, Step: 235, Rank: 33, loss = 0.69140625
c621-092: Epoch: 0, Step: 235, Rank: 29, loss = 0.0
c622-091: Epoch: 0, Step: 235, Rank: 60, loss = 3.084540367126465e-06
c621-062: Epoch: 0, Step: 235, Rank: 23, loss = 8.881784197001252e-13
c622-042: Epoch: 0, Step: 235, Rank: 51, loss = 1.0089706847793423e-12
c621-142: Epoch: 0, Step: 235, Rank: 39, loss = 1.8758328224066645e-12
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 236, Rank: 16, loss = 5.115907697472721e-12
c613-101: Epoch: 0, Step: 236, Rank: 0, loss = 3.510081114654895e-12
c622-052: Epoch: 0, Step: 236, Rank: 53, loss = 2.9331204132176936e-11
c622-002: Epoch: 0, Step: 236, Rank: 43, loss = 2.4158453015843406e-12
c622-081: Epoch: 0, Step: 236, Rank: 58, loss = 3.510081114654895e-12
c619-002: Epoch: 0, Step: 236, Rank: 13, loss = 2.4158453015843406e-12
c622-061: Epoch: 0, Step: 236, Rank: 54, loss = 1.1874362826347351e-08
c619-001: Epoch: 0, Step: 236, Rank: 12, loss = 1.6209256159527285e-14
c622-092: Epoch: 0, Step: 236, Rank: 61, loss = 1.3869794202037156e-11
c619-031: Epoch: 0, Step: 236, Rank: 18, loss = 1.4637180356658064e-12
c621-081: Epoch: 0, Step: 236, Rank: 26, loss = 6.565414878423326e-12
c622-012: Epoch: 0, Step: 236, Rank: 45, loss = 1.6540288925170898e-06
c621-152: Epoch: 0, Step: 236, Rank: 41, loss = 2.2118911147117615e-08
c621-151: Epoch: 0, Step: 236, Rank: 40, loss = 0.0017547607421875
c622-032: Epoch: 0, Step: 236, Rank: 49, loss = 2.1736923372372985e-10
c621-132: Epoch: 0, Step: 236, Rank: 37, loss = 2.1736923372372985e-10
c613-121: Epoch: 0, Step: 236, Rank: 4, loss = 3.4051481634378433e-09
c621-131: Epoch: 0, Step: 236, Rank: 36, loss = 1.6079866327345371e-09
c621-111: Epoch: 0, Step: 236, Rank: 32, loss = 1.8758328224066645e-12
c622-041: Epoch: 0, Step: 236, Rank: 50, loss = 1.3589129821411916e-13
c621-142: Epoch: 0, Step: 236, Rank: 39, loss = 2.5920599000528455e-11
c613-152: Epoch: 0, Step: 236, Rank: 11, loss = 3.268496584496461e-13
c621-082: Epoch: 0, Step: 236, Rank: 27, loss = 2.276897430419922e-05
c622-101: Epoch: 0, Step: 236, Rank: 62, loss = 5.115907697472721e-12
c619-011: Epoch: 0, Step: 236, Rank: 14, loss = 0.0
c613-151: Epoch: 0, Step: 236, Rank: 10, loss = 2.4158453015843406e-12
c621-072: Epoch: 0, Step: 236, Rank: 25, loss = 1.6209256159527285e-14
c619-032: Epoch: 0, Step: 236, Rank: 19, loss = 8.003553375601768e-11
c622-011: Epoch: 0, Step: 236, Rank: 44, loss = 3.774403012357652e-11
c622-001: Epoch: 0, Step: 236, Rank: 42, loss = 2.342858351767063e-09
c613-142: Epoch: 0, Step: 236, Rank: 9, loss = 2.0236257114447653e-11
c621-061: Epoch: 0, Step: 236, Rank: 22, loss = 0.0
c619-022: Epoch: 0, Step: 236, Rank: 17, loss = 2.574980159653073e-18
c613-122: Epoch: 0, Step: 236, Rank: 5, loss = 3.9637088775634766e-06
c622-102: Epoch: 0, Step: 236, Rank: 63, loss = 1.1641532182693481e-10
c622-062: Epoch: 0, Step: 236, Rank: 55, loss = 6.230038707144558e-11
c613-112: Epoch: 0, Step: 236, Rank: 3, loss = 2.540190280342358e-13
c613-132: Epoch: 0, Step: 236, Rank: 7, loss = 3.979039320256561e-12
c622-072: Epoch: 0, Step: 236, Rank: 57, loss = 1.0972125985553305e-16
c621-122: Epoch: 0, Step: 236, Rank: 35, loss = 7.048583938740194e-11
c613-102: Epoch: 0, Step: 236, Rank: 1, loss = 0.69140625
c622-071: Epoch: 0, Step: 236, Rank: 56, loss = 1.3589129821411916e-13
c622-021: Epoch: 0, Step: 236, Rank: 46, loss = 5.893525667488575e-10
c621-121: Epoch: 0, Step: 236, Rank: 34, loss = 2.648448571562767e-09
c622-091: Epoch: 0, Step: 236, Rank: 60, loss = 4.94765117764473e-09
c622-051: Epoch: 0, Step: 236, Rank: 52, loss = 1.3316000006114873e-34
c621-071: Epoch: 0, Step: 236, Rank: 24, loss = 1.4915713109076023e-10
c622-031: Epoch: 0, Step: 236, Rank: 48, loss = 0.0
c621-091: Epoch: 0, Step: 236, Rank: 28, loss = 3.694822225952521e-13
c621-112: Epoch: 0, Step: 236, Rank: 33, loss = 7.66053886991358e-15
c619-012: Epoch: 0, Step: 236, Rank: 15, loss = 5.759824041329242e-19
c621-092: Epoch: 0, Step: 236, Rank: 29, loss = 2.71833068627654e-38
c613-141: Epoch: 0, Step: 236, Rank: 8, loss = 0.69140625
c622-042: Epoch: 0, Step: 236, Rank: 51, loss = 5.400124791776761e-13
c621-102: Epoch: 0, Step: 236, Rank: 31, loss = 4.472333961502706e-19
c622-022: Epoch: 0, Step: 236, Rank: 47, loss = 3.1650415621697903e-10
c621-052: Epoch: 0, Step: 236, Rank: 21, loss = 4.843059286940843e-11
c621-062: Epoch: 0, Step: 236, Rank: 23, loss = 3.979039320256561e-12
c622-082: Epoch: 0, Step: 236, Rank: 59, loss = 1.895427703857422e-05
c619-041: Epoch: 0, Step: 236, Rank: 20, loss = 5.0961971282958984e-06
c613-111: Epoch: 0, Step: 236, Rank: 2, loss = 1.955777406692505e-08
c613-131: Epoch: 0, Step: 236, Rank: 6, loss = 2.0236257114447653e-11
c621-101: Epoch: 0, Step: 236, Rank: 30, loss = 3.583409124985337e-10
c621-141: Epoch: 0, Step: 236, Rank: 38, loss = 9.098986738083304e-23
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.45s, TFLOPs: 0.77, Samples/sec: 0.41, Time/seq 2.45s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 237, Rank: 0, loss = 2.540190280342358e-13
c622-002: Epoch: 0, Step: 237, Rank: 43, loss = 6.628036499023438e-05
c622-041: Epoch: 0, Step: 237, Rank: 50, loss = 1.5688783605583012e-11
c622-052: Epoch: 0, Step: 237, Rank: 53, loss = 1.7848833522293717e-11
c621-091: Epoch: 0, Step: 237, Rank: 28, loss = 1.6079866327345371e-09
c622-062: Epoch: 0, Step: 237, Rank: 55, loss = 1.3445969671010971e-08
c621-132: Epoch: 0, Step: 237, Rank: 37, loss = 6.927791673660977e-13
c622-092: Epoch: 0, Step: 237, Rank: 61, loss = 1.9190338207408786e-10
c622-001: Epoch: 0, Step: 237, Rank: 42, loss = 2.066371962428093e-09
c619-021: Epoch: 0, Step: 237, Rank: 16, loss = 3.510081114654895e-12
c622-012: Epoch: 0, Step: 237, Rank: 45, loss = 3.979039320256561e-12
c613-122: Epoch: 0, Step: 237, Rank: 5, loss = 0.69140625
c622-061: Epoch: 0, Step: 237, Rank: 54, loss = 0.0
c621-082: Epoch: 0, Step: 237, Rank: 27, loss = 10.8125
c613-121: Epoch: 0, Step: 237, Rank: 4, loss = 5.400124791776761e-13
c621-111: Epoch: 0, Step: 237, Rank: 32, loss = 3.2548216060144286e-32
c619-011: Epoch: 0, Step: 237, Rank: 14, loss = 7.963180541992188e-05
c622-022: Epoch: 0, Step: 237, Rank: 47, loss = 3.268496584496461e-13
c622-032: Epoch: 0, Step: 237, Rank: 49, loss = 3.774403012357652e-11
c621-121: Epoch: 0, Step: 237, Rank: 34, loss = 6.565414878423326e-12
c622-102: Epoch: 0, Step: 237, Rank: 63, loss = 1.9806378759312793e-13
c621-122: Epoch: 0, Step: 237, Rank: 35, loss = 2.7830537874251604e-10
c621-112: Epoch: 0, Step: 237, Rank: 33, loss = 7.383573891102493e-38
c619-002: Epoch: 0, Step: 237, Rank: 13, loss = 1.8041124150158794e-16
c613-142: Epoch: 0, Step: 237, Rank: 9, loss = 1.1059455573558807e-09
c622-072: Epoch: 0, Step: 237, Rank: 57, loss = 2.0236257114447653e-11
c619-001: Epoch: 0, Step: 237, Rank: 12, loss = 1.1059455573558807e-09
c613-112: Epoch: 0, Step: 237, Rank: 3, loss = 2.540190280342358e-13
c613-111: Epoch: 0, Step: 237, Rank: 2, loss = 2.2351741790771484e-07
c621-151: Epoch: 0, Step: 237, Rank: 40, loss = 1.8775463104248047e-06
c621-131: Epoch: 0, Step: 237, Rank: 36, loss = 7.566995918750763e-10
c613-132: Epoch: 0, Step: 237, Rank: 7, loss = 1.1546753136970622e-17
c621-072: Epoch: 0, Step: 237, Rank: 25, loss = 2.514570951461792e-08
c613-152: Epoch: 0, Step: 237, Rank: 11, loss = 2.7284841053187847e-12
c621-152: Epoch: 0, Step: 237, Rank: 41, loss = 0.0
c621-102: Epoch: 0, Step: 237, Rank: 31, loss = 9.549694368615746e-12
c619-031: Epoch: 0, Step: 237, Rank: 18, loss = 6.927791673660977e-13
c622-051: Epoch: 0, Step: 237, Rank: 52, loss = 0.0
c622-101: Epoch: 0, Step: 237, Rank: 62, loss = 1.7497114868092467e-13
c619-022: Epoch: 0, Step: 237, Rank: 17, loss = 4.3655745685100555e-09
c621-142: Epoch: 0, Step: 237, Rank: 39, loss = 8.404254913330078e-06
c613-131: Epoch: 0, Step: 237, Rank: 6, loss = 0.0
c621-101: Epoch: 0, Step: 237, Rank: 30, loss = 1.418811734765768e-09
c622-071: Epoch: 0, Step: 237, Rank: 56, loss = 3.8163916471489756e-16
c621-081: Epoch: 0, Step: 237, Rank: 26, loss = 2.1736923372372985e-10
c613-151: Epoch: 0, Step: 237, Rank: 10, loss = 2.7830537874251604e-10
c621-052: Epoch: 0, Step: 237, Rank: 21, loss = 9.880984919163893e-15
c619-041: Epoch: 0, Step: 237, Rank: 20, loss = 5.14984130859375e-05
c613-102: Epoch: 0, Step: 237, Rank: 1, loss = 9.049472282640636e-11
c622-082: Epoch: 0, Step: 237, Rank: 59, loss = 8.003553375601768e-11
c613-141: Epoch: 0, Step: 237, Rank: 8, loss = 2.1736923372372985e-10
c622-021: Epoch: 0, Step: 237, Rank: 46, loss = 6.230038707144558e-11
c619-012: Epoch: 0, Step: 237, Rank: 15, loss = 7.566995918750763e-10
c621-092: Epoch: 0, Step: 237, Rank: 29, loss = 4.926614671774132e-16
c621-141: Epoch: 0, Step: 237, Rank: 38, loss = 2.0236257114447653e-11
c622-091: Epoch: 0, Step: 237, Rank: 60, loss = 7.113753267956038e-23
c621-062: Epoch: 0, Step: 237, Rank: 23, loss = 5.916456789157589e-29
c622-011: Epoch: 0, Step: 237, Rank: 44, loss = 1.150369644165039e-05
c622-031: Epoch: 0, Step: 237, Rank: 48, loss = 2.204051907791789e-39
c622-042: Epoch: 0, Step: 237, Rank: 51, loss = 2.1175823681357508e-19
c621-061: Epoch: 0, Step: 237, Rank: 22, loss = 2.4158453015843406e-12
c622-081: Epoch: 0, Step: 237, Rank: 58, loss = 1.0277290130034089e-10
c621-071: Epoch: 0, Step: 237, Rank: 24, loss = 8.083811398051921e-16
c619-032: Epoch: 0, Step: 237, Rank: 19, loss = 5.301145283085199e-27
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 238, Rank: 0, loss = 8.003553375601768e-11
c619-021: Epoch: 0, Step: 238, Rank: 16, loss = 7.566995918750763e-10
c622-052: Epoch: 0, Step: 238, Rank: 53, loss = 9.880984919163893e-15
c622-002: Epoch: 0, Step: 238, Rank: 43, loss = 1.525040715932846e-08
c622-062: Epoch: 0, Step: 238, Rank: 55, loss = 9.486769009248164e-19
c622-081: Epoch: 0, Step: 238, Rank: 58, loss = 1.0277290130034089e-10
c622-101: Epoch: 0, Step: 238, Rank: 62, loss = 3.635980405647388e-15
c622-082: Epoch: 0, Step: 238, Rank: 59, loss = 2.6756374893466273e-14
c622-092: Epoch: 0, Step: 238, Rank: 61, loss = 0.0
c622-012: Epoch: 0, Step: 238, Rank: 45, loss = 4.843059286940843e-11
c621-142: Epoch: 0, Step: 238, Rank: 39, loss = 2.014636993408203e-05
c622-071: Epoch: 0, Step: 238, Rank: 56, loss = 2.439454888092385e-17
c622-072: Epoch: 0, Step: 238, Rank: 57, loss = 4.0332320816460765e-17
c621-132: Epoch: 0, Step: 238, Rank: 37, loss = 0.0
c622-011: Epoch: 0, Step: 238, Rank: 44, loss = 0.00012302398681640625
c622-061: Epoch: 0, Step: 238, Rank: 54, loss = 0.0
c622-001: Epoch: 0, Step: 238, Rank: 42, loss = 2.4158453015843406e-12
c621-111: Epoch: 0, Step: 238, Rank: 32, loss = 1.0408340855860843e-15
c622-031: Epoch: 0, Step: 238, Rank: 48, loss = 1.3589129821411916e-13
c621-131: Epoch: 0, Step: 238, Rank: 36, loss = 1.9806378759312793e-13
c621-151: Epoch: 0, Step: 238, Rank: 40, loss = 4.843059286940843e-11
c613-121: Epoch: 0, Step: 238, Rank: 4, loss = 5.893525667488575e-10
c619-041: Epoch: 0, Step: 238, Rank: 20, loss = 6.927791673660977e-13
c613-152: Epoch: 0, Step: 238, Rank: 11, loss = 1.7848833522293717e-11
c622-051: Epoch: 0, Step: 238, Rank: 52, loss = 1.2069940567016602e-06
c622-022: Epoch: 0, Step: 238, Rank: 47, loss = 1.9806378759312793e-13
c619-001: Epoch: 0, Step: 238, Rank: 12, loss = 1.5819829215076654e-23
c621-091: Epoch: 0, Step: 238, Rank: 28, loss = 9.549694368615746e-12
c613-122: Epoch: 0, Step: 238, Rank: 5, loss = 5.3085386753082275e-08
c619-032: Epoch: 0, Step: 238, Rank: 19, loss = 2.8405338525772095e-08
c621-121: Epoch: 0, Step: 238, Rank: 34, loss = 2.4158453015843406e-12
c621-152: Epoch: 0, Step: 238, Rank: 41, loss = 7.66053886991358e-15
c613-112: Epoch: 0, Step: 238, Rank: 3, loss = 0.0
c613-141: Epoch: 0, Step: 238, Rank: 8, loss = 8.754432201385498e-08
c621-061: Epoch: 0, Step: 238, Rank: 22, loss = 1.84297022087776e-14
c613-142: Epoch: 0, Step: 238, Rank: 9, loss = 2.648448571562767e-09
c619-011: Epoch: 0, Step: 238, Rank: 14, loss = 3.979039320256561e-12
c619-031: Epoch: 0, Step: 238, Rank: 18, loss = 7.566995918750763e-10
c621-052: Epoch: 0, Step: 238, Rank: 21, loss = 1.0800249583553523e-11
c613-151: Epoch: 0, Step: 238, Rank: 10, loss = 3.635980405647388e-15
c622-091: Epoch: 0, Step: 238, Rank: 60, loss = 0.0
c613-132: Epoch: 0, Step: 238, Rank: 7, loss = 9.370282327836321e-14
c621-122: Epoch: 0, Step: 238, Rank: 35, loss = 0.0
c621-081: Epoch: 0, Step: 238, Rank: 26, loss = 2.1736923372372985e-10
c619-012: Epoch: 0, Step: 238, Rank: 15, loss = 3.694822225952521e-13
c621-141: Epoch: 0, Step: 238, Rank: 38, loss = 8.881784197001252e-13
c619-022: Epoch: 0, Step: 238, Rank: 17, loss = 1.0800249583553523e-11
c621-082: Epoch: 0, Step: 238, Rank: 27, loss = 5.0182080713057076e-14
c622-021: Epoch: 0, Step: 238, Rank: 46, loss = 0.0
c621-102: Epoch: 0, Step: 238, Rank: 31, loss = 9.255018085241318e-09
c622-042: Epoch: 0, Step: 238, Rank: 51, loss = 0.0
c619-002: Epoch: 0, Step: 238, Rank: 13, loss = 2.831068712794149e-15
c613-131: Epoch: 0, Step: 238, Rank: 6, loss = 9.880984919163893e-15
c621-072: Epoch: 0, Step: 238, Rank: 25, loss = 0.0
c613-111: Epoch: 0, Step: 238, Rank: 2, loss = 2.2351741790771484e-07
c613-102: Epoch: 0, Step: 238, Rank: 1, loss = 1.2278178473934531e-11
c621-112: Epoch: 0, Step: 238, Rank: 33, loss = 6.565414878423326e-12
c621-062: Epoch: 0, Step: 238, Rank: 23, loss = 1.2514647096395493e-09
c622-041: Epoch: 0, Step: 238, Rank: 50, loss = 9.370282327836321e-14
c622-032: Epoch: 0, Step: 238, Rank: 49, loss = 2.3245294578089215e-16
c621-071: Epoch: 0, Step: 238, Rank: 24, loss = 7.66053886991358e-15
c622-102: Epoch: 0, Step: 238, Rank: 63, loss = 1.5688783605583012e-11
c621-101: Epoch: 0, Step: 238, Rank: 30, loss = 2.1736923372372985e-10
c621-092: Epoch: 0, Step: 238, Rank: 29, loss = 6.693881005048752e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 239, Rank: 0, loss = 4.020190679812223e-36
c619-021: Epoch: 0, Step: 239, Rank: 16, loss = 6.927791673660977e-13
c622-101: Epoch: 0, Step: 239, Rank: 62, loss = 4.1443854570388794e-08
c619-001: Epoch: 0, Step: 239, Rank: 12, loss = 2.648448571562767e-09
c622-092: Epoch: 0, Step: 239, Rank: 61, loss = 2.6056189295420372e-23
c619-002: Epoch: 0, Step: 239, Rank: 13, loss = 2.439454888092385e-17
c622-102: Epoch: 0, Step: 239, Rank: 63, loss = 3.268496584496461e-13
c619-022: Epoch: 0, Step: 239, Rank: 17, loss = 1.1059455573558807e-09
c613-142: Epoch: 0, Step: 239, Rank: 9, loss = 1.4915713109076023e-10
c622-081: Epoch: 0, Step: 239, Rank: 58, loss = 2.5920599000528455e-11
c613-111: Epoch: 0, Step: 239, Rank: 2, loss = 5.699694156646729e-07
c619-012: Epoch: 0, Step: 239, Rank: 15, loss = 1.1641532182693481e-10
c613-131: Epoch: 0, Step: 239, Rank: 6, loss = 8.003553375601768e-11
c613-152: Epoch: 0, Step: 239, Rank: 11, loss = 3.441691376337985e-14
c613-151: Epoch: 0, Step: 239, Rank: 10, loss = 3.441691376337985e-14
c613-122: Epoch: 0, Step: 239, Rank: 5, loss = 3.213062882423401e-08
c619-011: Epoch: 0, Step: 239, Rank: 14, loss = 2.1736923372372985e-10
c613-141: Epoch: 0, Step: 239, Rank: 8, loss = 1.3589129821411916e-13
c622-062: Epoch: 0, Step: 239, Rank: 55, loss = 1.4637180356658064e-12
c613-112: Epoch: 0, Step: 239, Rank: 3, loss = 0.69140625
c613-102: Epoch: 0, Step: 239, Rank: 1, loss = 2.0236257114447653e-11
c622-051: Epoch: 0, Step: 239, Rank: 52, loss = 9.74978320300579e-10
c622-052: Epoch: 0, Step: 239, Rank: 53, loss = 3.4051481634378433e-09
c613-132: Epoch: 0, Step: 239, Rank: 7, loss = 9.74978320300579e-10
c619-031: Epoch: 0, Step: 239, Rank: 18, loss = 3.694822225952521e-13
c622-071: Epoch: 0, Step: 239, Rank: 56, loss = 3.774403012357652e-11
c613-121: Epoch: 0, Step: 239, Rank: 4, loss = 7.44648787076585e-12
c622-072: Epoch: 0, Step: 239, Rank: 57, loss = 3.635980405647388e-15
c622-012: Epoch: 0, Step: 239, Rank: 45, loss = 0.69140625
c622-082: Epoch: 0, Step: 239, Rank: 59, loss = 2.066371962428093e-09
c622-002: Epoch: 0, Step: 239, Rank: 43, loss = 2.8405338525772095e-08
c621-132: Epoch: 0, Step: 239, Rank: 37, loss = 3.635980405647388e-15
c622-061: Epoch: 0, Step: 239, Rank: 54, loss = 4.3655745685100555e-09
c622-001: Epoch: 0, Step: 239, Rank: 42, loss = 8.083811398051921e-16
c622-032: Epoch: 0, Step: 239, Rank: 49, loss = 2.648448571562767e-09
c622-022: Epoch: 0, Step: 239, Rank: 47, loss = 0.00016880035400390625
c621-122: Epoch: 0, Step: 239, Rank: 35, loss = 2.6756374893466273e-14
c622-042: Epoch: 0, Step: 239, Rank: 51, loss = 0.0
c621-142: Epoch: 0, Step: 239, Rank: 39, loss = 2.648448571562767e-09
c621-082: Epoch: 0, Step: 239, Rank: 27, loss = 3.583409124985337e-10
c621-081: Epoch: 0, Step: 239, Rank: 26, loss = 0.0
c621-152: Epoch: 0, Step: 239, Rank: 41, loss = 3.8163916471489756e-16
c622-031: Epoch: 0, Step: 239, Rank: 48, loss = 2.342858351767063e-09
c622-091: Epoch: 0, Step: 239, Rank: 60, loss = 1.418811734765768e-09
c621-151: Epoch: 0, Step: 239, Rank: 40, loss = 5.115907697472721e-12
c622-011: Epoch: 0, Step: 239, Rank: 44, loss = 2.7830537874251604e-10
c621-111: Epoch: 0, Step: 239, Rank: 32, loss = 0.0
c621-131: Epoch: 0, Step: 239, Rank: 36, loss = 0.0
c621-091: Epoch: 0, Step: 239, Rank: 28, loss = 7.338821887969971e-07
c619-032: Epoch: 0, Step: 239, Rank: 19, loss = 1.0408340855860843e-15
c621-121: Epoch: 0, Step: 239, Rank: 34, loss = 2.342858351767063e-09
c621-072: Epoch: 0, Step: 239, Rank: 25, loss = 0.0
c621-102: Epoch: 0, Step: 239, Rank: 31, loss = 5.182486384480711e-17
c621-112: Epoch: 0, Step: 239, Rank: 33, loss = 0.0
c622-021: Epoch: 0, Step: 239, Rank: 46, loss = 7.048583938740194e-11
c621-092: Epoch: 0, Step: 239, Rank: 29, loss = 5.182486384480711e-17
c622-041: Epoch: 0, Step: 239, Rank: 50, loss = 6.314393452555578e-16
c621-141: Epoch: 0, Step: 239, Rank: 38, loss = 3.441691376337985e-14
c621-101: Epoch: 0, Step: 239, Rank: 30, loss = 4.6629367034256575e-15
c621-071: Epoch: 0, Step: 239, Rank: 24, loss = 2.8405338525772095e-08
c619-041: Epoch: 0, Step: 239, Rank: 20, loss = 2.2118911147117615e-08
c621-061: Epoch: 0, Step: 239, Rank: 22, loss = 7.566995918750763e-10
c621-052: Epoch: 0, Step: 239, Rank: 21, loss = 3.53125
c621-062: Epoch: 0, Step: 239, Rank: 23, loss = 1.4915713109076023e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.12s, TFLOPs: 0.89, Samples/sec: 0.47, Time/seq 2.12s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 240, Rank: 43, loss = 9.880984919163893e-15
c622-052: Epoch: 0, Step: 240, Rank: 53, loss = 8.881784197001252e-13
c613-101: Epoch: 0, Step: 240, Rank: 0, loss = 3.841705620288849e-09
c619-021: Epoch: 0, Step: 240, Rank: 16, loss = 0.0
c619-002: Epoch: 0, Step: 240, Rank: 13, loss = 1.44393100091654e-26
c622-081: Epoch: 0, Step: 240, Rank: 58, loss = 2.6756374893466273e-14
c621-111: Epoch: 0, Step: 240, Rank: 32, loss = 0.0
c613-142: Epoch: 0, Step: 240, Rank: 9, loss = 6.5635692504717e-31
c619-001: Epoch: 0, Step: 240, Rank: 12, loss = 3.510081114654895e-12
c613-132: Epoch: 0, Step: 240, Rank: 7, loss = 1.199040866595169e-13
c621-142: Epoch: 0, Step: 240, Rank: 39, loss = 0.0
c622-092: Epoch: 0, Step: 240, Rank: 61, loss = 3.510081114654895e-12
c621-061: Epoch: 0, Step: 240, Rank: 22, loss = 2.1047890186309814e-07
c622-051: Epoch: 0, Step: 240, Rank: 52, loss = 3.694822225952521e-13
c619-022: Epoch: 0, Step: 240, Rank: 17, loss = 1.3589129821411916e-13
c621-091: Epoch: 0, Step: 240, Rank: 28, loss = 5.4836273193359375e-05
c622-041: Epoch: 0, Step: 240, Rank: 50, loss = 1.2197274440461925e-18
c613-151: Epoch: 0, Step: 240, Rank: 10, loss = 1.6540288925170898e-06
c613-131: Epoch: 0, Step: 240, Rank: 6, loss = 6.3792168840089494e-21
c621-052: Epoch: 0, Step: 240, Rank: 21, loss = 2.2851054382044822e-11
c621-121: Epoch: 0, Step: 240, Rank: 34, loss = 0.0067138671875
c621-151: Epoch: 0, Step: 240, Rank: 40, loss = 2.6756374893466273e-14
c613-141: Epoch: 0, Step: 240, Rank: 8, loss = 1.6079866327345371e-09
c619-031: Epoch: 0, Step: 240, Rank: 18, loss = 0.0
c622-012: Epoch: 0, Step: 240, Rank: 45, loss = 2.983724378680108e-16
c613-152: Epoch: 0, Step: 240, Rank: 11, loss = 5.502442945726216e-11
c622-032: Epoch: 0, Step: 240, Rank: 49, loss = 1.0972125985553305e-16
c622-082: Epoch: 0, Step: 240, Rank: 59, loss = 3.774403012357652e-11
c621-112: Epoch: 0, Step: 240, Rank: 33, loss = 1.983707842718853e-32
c622-101: Epoch: 0, Step: 240, Rank: 62, loss = 7.66053886991358e-15
c621-131: Epoch: 0, Step: 240, Rank: 36, loss = 3.4897757426877174e-19
c621-152: Epoch: 0, Step: 240, Rank: 41, loss = 4.418687638008123e-14
c622-022: Epoch: 0, Step: 240, Rank: 47, loss = 6.314393452555578e-16
c622-061: Epoch: 0, Step: 240, Rank: 54, loss = 1.3869794202037156e-11
c613-112: Epoch: 0, Step: 240, Rank: 3, loss = 0.0
c613-122: Epoch: 0, Step: 240, Rank: 5, loss = 8.149072527885437e-09
c613-102: Epoch: 0, Step: 240, Rank: 1, loss = 1.2304311611726287e-23
c622-011: Epoch: 0, Step: 240, Rank: 44, loss = 3.268496584496461e-13
c621-081: Epoch: 0, Step: 240, Rank: 26, loss = 4.274625098332763e-11
c613-121: Epoch: 0, Step: 240, Rank: 4, loss = 9.486769009248164e-19
c622-071: Epoch: 0, Step: 240, Rank: 56, loss = 3.8163916471489756e-16
c622-042: Epoch: 0, Step: 240, Rank: 51, loss = 8.881784197001252e-13
c622-001: Epoch: 0, Step: 240, Rank: 42, loss = 3.8163916471489756e-16
c621-122: Epoch: 0, Step: 240, Rank: 35, loss = 1.6209256159527285e-14
c619-011: Epoch: 0, Step: 240, Rank: 14, loss = 2.983724378680108e-16
c621-101: Epoch: 0, Step: 240, Rank: 30, loss = 1.0477378964424133e-08
c622-072: Epoch: 0, Step: 240, Rank: 57, loss = 1.0089706847793423e-12
c621-071: Epoch: 0, Step: 240, Rank: 24, loss = 5.916456789157589e-29
c619-032: Epoch: 0, Step: 240, Rank: 19, loss = 1.3597309589385986e-07
c622-031: Epoch: 0, Step: 240, Rank: 48, loss = 1.2931877790833823e-12
c621-062: Epoch: 0, Step: 240, Rank: 23, loss = 0.0
c621-102: Epoch: 0, Step: 240, Rank: 31, loss = 3.1763735522036263e-22
c622-091: Epoch: 0, Step: 240, Rank: 60, loss = 1.84297022087776e-14
c619-041: Epoch: 0, Step: 240, Rank: 20, loss = 5.893525667488575e-10
c622-102: Epoch: 0, Step: 240, Rank: 63, loss = 3.1650415621697903e-10
c622-021: Epoch: 0, Step: 240, Rank: 46, loss = 2.3647750424515834e-14
c621-132: Epoch: 0, Step: 240, Rank: 37, loss = 1.4051260155412137e-16
c621-092: Epoch: 0, Step: 240, Rank: 29, loss = 1.4051260155412137e-16
c621-141: Epoch: 0, Step: 240, Rank: 38, loss = 2.0236257114447653e-11
c621-082: Epoch: 0, Step: 240, Rank: 27, loss = 7.729977369308472e-08
c619-012: Epoch: 0, Step: 240, Rank: 15, loss = 4.843059286940843e-11
c613-111: Epoch: 0, Step: 240, Rank: 2, loss = 0.69140625
c622-062: Epoch: 0, Step: 240, Rank: 55, loss = 2.648448571562767e-09
c621-072: Epoch: 0, Step: 240, Rank: 25, loss = 0.69140625
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 241, Rank: 32, loss = 1.2759119272232056e-07
c622-002: Epoch: 0, Step: 241, Rank: 43, loss = 0.0
c613-101: Epoch: 0, Step: 241, Rank: 0, loss = 9.370282327836321e-14
c622-052: Epoch: 0, Step: 241, Rank: 53, loss = 0.0751953125
c619-002: Epoch: 0, Step: 241, Rank: 13, loss = 5.0182080713057076e-14
c621-081: Epoch: 0, Step: 241, Rank: 26, loss = 1.3322676295501878e-15
c619-021: Epoch: 0, Step: 241, Rank: 16, loss = 8.881784197001252e-13
c621-072: Epoch: 0, Step: 241, Rank: 25, loss = 0.0
c621-132: Epoch: 0, Step: 241, Rank: 37, loss = 2.710505431213761e-19
c613-112: Epoch: 0, Step: 241, Rank: 3, loss = 9.549694368615746e-12
c621-131: Epoch: 0, Step: 241, Rank: 36, loss = 0.69140625
c622-001: Epoch: 0, Step: 241, Rank: 42, loss = 7.66053886991358e-15
c613-111: Epoch: 0, Step: 241, Rank: 2, loss = 2.831068712794149e-15
c622-041: Epoch: 0, Step: 241, Rank: 50, loss = 1.6432439176733427e-19
c621-151: Epoch: 0, Step: 241, Rank: 40, loss = 0.0
c619-001: Epoch: 0, Step: 241, Rank: 12, loss = 2.6756374893466273e-14
c613-122: Epoch: 0, Step: 241, Rank: 5, loss = 1.318767317570746e-10
c622-062: Epoch: 0, Step: 241, Rank: 55, loss = 2.7284841053187847e-12
c622-012: Epoch: 0, Step: 241, Rank: 45, loss = 1.0972125985553305e-16
c621-091: Epoch: 0, Step: 241, Rank: 28, loss = 0.00010251998901367188
c621-152: Epoch: 0, Step: 241, Rank: 41, loss = 1.0662875083691372e-25
c622-081: Epoch: 0, Step: 241, Rank: 58, loss = 5.115907697472721e-12
c613-152: Epoch: 0, Step: 241, Rank: 11, loss = 2.066371962428093e-09
c619-031: Epoch: 0, Step: 241, Rank: 18, loss = 3.694822225952521e-13
c622-011: Epoch: 0, Step: 241, Rank: 44, loss = 2.2118911147117615e-08
c622-101: Epoch: 0, Step: 241, Rank: 62, loss = 4.760636329592671e-13
c621-142: Epoch: 0, Step: 241, Rank: 39, loss = 1.0089706847793423e-12
c621-121: Epoch: 0, Step: 241, Rank: 34, loss = 0.0
c613-131: Epoch: 0, Step: 241, Rank: 6, loss = 1.4722347259521484e-05
c621-061: Epoch: 0, Step: 241, Rank: 22, loss = 5.502442945726216e-11
c621-052: Epoch: 0, Step: 241, Rank: 21, loss = 6.230038707144558e-11
c619-022: Epoch: 0, Step: 241, Rank: 17, loss = 1.2931877790833823e-12
c622-102: Epoch: 0, Step: 241, Rank: 63, loss = 5.502442945726216e-11
c613-102: Epoch: 0, Step: 241, Rank: 1, loss = 5.182486384480711e-17
c621-082: Epoch: 0, Step: 241, Rank: 27, loss = 1.2278178473934531e-11
c622-061: Epoch: 0, Step: 241, Rank: 54, loss = 6.565414878423326e-12
c613-142: Epoch: 0, Step: 241, Rank: 9, loss = 1.9806378759312793e-13
c619-011: Epoch: 0, Step: 241, Rank: 14, loss = 5.816113682013476e-24
c621-122: Epoch: 0, Step: 241, Rank: 35, loss = 1.0277290130034089e-10
c622-032: Epoch: 0, Step: 241, Rank: 49, loss = 2.4035605705952703e-31
c622-072: Epoch: 0, Step: 241, Rank: 57, loss = 5.44811591673966e-18
c613-151: Epoch: 0, Step: 241, Rank: 10, loss = 2.540190280342358e-13
c621-101: Epoch: 0, Step: 241, Rank: 30, loss = 1.1015625
c622-092: Epoch: 0, Step: 241, Rank: 61, loss = 0.0
c622-031: Epoch: 0, Step: 241, Rank: 48, loss = 1.6432439176733427e-19
c622-051: Epoch: 0, Step: 241, Rank: 52, loss = 1.9081958235744878e-17
c613-141: Epoch: 0, Step: 241, Rank: 8, loss = 3.268496584496461e-13
c622-071: Epoch: 0, Step: 241, Rank: 56, loss = 1.4637180356658064e-12
c622-082: Epoch: 0, Step: 241, Rank: 59, loss = 3.510081114654895e-12
c622-042: Epoch: 0, Step: 241, Rank: 51, loss = 6.230038707144558e-11
c619-041: Epoch: 0, Step: 241, Rank: 20, loss = 6.927791673660977e-13
c613-121: Epoch: 0, Step: 241, Rank: 4, loss = 4.760636329592671e-13
c621-102: Epoch: 0, Step: 241, Rank: 31, loss = 1.4637180356658064e-12
c621-141: Epoch: 0, Step: 241, Rank: 38, loss = 5.115907697472721e-12
c613-132: Epoch: 0, Step: 241, Rank: 7, loss = 1.6916601452976465e-10
c621-092: Epoch: 0, Step: 241, Rank: 29, loss = 1.4051260155412137e-16
c621-062: Epoch: 0, Step: 241, Rank: 23, loss = 2.831068712794149e-15
c622-022: Epoch: 0, Step: 241, Rank: 47, loss = 7.792703114739563e-20
c622-021: Epoch: 0, Step: 241, Rank: 46, loss = 3.635980405647388e-15
c619-012: Epoch: 0, Step: 241, Rank: 15, loss = 2.5920599000528455e-11
c622-091: Epoch: 0, Step: 241, Rank: 60, loss = 0.7265625
c619-032: Epoch: 0, Step: 241, Rank: 19, loss = 7.987216665362745e-30
c621-112: Epoch: 0, Step: 241, Rank: 33, loss = 4.926614671774132e-16
c621-071: Epoch: 0, Step: 241, Rank: 24, loss = 2.831068712794149e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 1.99s, TFLOPs: 0.95, Samples/sec: 0.50, Time/seq 1.99s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 242, Rank: 28, loss = 2.2118911147117615e-08
c622-051: Epoch: 0, Step: 242, Rank: 52, loss = 7.188646122813225e-09
c622-002: Epoch: 0, Step: 242, Rank: 43, loss = 5.893525667488575e-10
c622-052: Epoch: 0, Step: 242, Rank: 53, loss = 0.69140625
c621-081: Epoch: 0, Step: 242, Rank: 26, loss = 0.0
c621-131: Epoch: 0, Step: 242, Rank: 36, loss = 1.0788440704345703e-05
c619-001: Epoch: 0, Step: 242, Rank: 12, loss = 8.083811398051921e-16
c619-021: Epoch: 0, Step: 242, Rank: 16, loss = 4.6629367034256575e-15
c622-101: Epoch: 0, Step: 242, Rank: 62, loss = 9.880984919163893e-15
c622-012: Epoch: 0, Step: 242, Rank: 45, loss = 7.486343383789062e-05
c621-132: Epoch: 0, Step: 242, Rank: 37, loss = 8.881784197001252e-13
c622-081: Epoch: 0, Step: 242, Rank: 58, loss = 0.0
c621-111: Epoch: 0, Step: 242, Rank: 32, loss = 7.44648787076585e-12
c622-061: Epoch: 0, Step: 242, Rank: 54, loss = 4.6798959374427795e-08
c622-001: Epoch: 0, Step: 242, Rank: 42, loss = 4.3655745685100555e-09
c622-032: Epoch: 0, Step: 242, Rank: 49, loss = 4.7222086809427244e-20
c621-142: Epoch: 0, Step: 242, Rank: 39, loss = 1.6079866327345371e-09
c613-101: Epoch: 0, Step: 242, Rank: 0, loss = 2.831068712794149e-15
c621-082: Epoch: 0, Step: 242, Rank: 27, loss = 6.927791673660977e-13
c621-151: Epoch: 0, Step: 242, Rank: 40, loss = 1.525040715932846e-08
c613-122: Epoch: 0, Step: 242, Rank: 5, loss = 1.84297022087776e-14
c621-072: Epoch: 0, Step: 242, Rank: 25, loss = 4.274625098332763e-11
c621-122: Epoch: 0, Step: 242, Rank: 35, loss = 6.993104012531504e-18
c621-121: Epoch: 0, Step: 242, Rank: 34, loss = 1.1874362826347351e-08
c622-041: Epoch: 0, Step: 242, Rank: 50, loss = 0.0
c622-102: Epoch: 0, Step: 242, Rank: 63, loss = 6.007030606269836e-08
c619-022: Epoch: 0, Step: 242, Rank: 17, loss = 4.760636329592671e-13
c613-152: Epoch: 0, Step: 242, Rank: 11, loss = 1.2069940567016602e-06
c613-151: Epoch: 0, Step: 242, Rank: 10, loss = 2.726912498474121e-06
c619-002: Epoch: 0, Step: 242, Rank: 13, loss = 2.648448571562767e-09
c619-011: Epoch: 0, Step: 242, Rank: 14, loss = 0.0
c622-011: Epoch: 0, Step: 242, Rank: 44, loss = 0.00070953369140625
c622-071: Epoch: 0, Step: 242, Rank: 56, loss = 8.404254913330078e-06
c619-041: Epoch: 0, Step: 242, Rank: 20, loss = 0.0
c619-032: Epoch: 0, Step: 242, Rank: 19, loss = 1.3499587596865412e-20
c619-031: Epoch: 0, Step: 242, Rank: 18, loss = 0.0
c621-092: Epoch: 0, Step: 242, Rank: 29, loss = 1.0408340855860843e-15
c613-132: Epoch: 0, Step: 242, Rank: 7, loss = 7.188646122813225e-09
c613-131: Epoch: 0, Step: 242, Rank: 6, loss = 0.0
c622-042: Epoch: 0, Step: 242, Rank: 51, loss = 1.8758328224066645e-12
c622-092: Epoch: 0, Step: 242, Rank: 61, loss = 4.274625098332763e-11
c622-072: Epoch: 0, Step: 242, Rank: 57, loss = 1.1874362826347351e-08
c621-152: Epoch: 0, Step: 242, Rank: 41, loss = 3.694822225952521e-13
c621-062: Epoch: 0, Step: 242, Rank: 23, loss = 7.048583938740194e-11
c619-012: Epoch: 0, Step: 242, Rank: 15, loss = 2.2649765014648438e-06
c621-101: Epoch: 0, Step: 242, Rank: 30, loss = 7.566995918750763e-10
c621-112: Epoch: 0, Step: 242, Rank: 33, loss = 6.314393452555578e-16
c622-022: Epoch: 0, Step: 242, Rank: 47, loss = 1.2656542480726785e-14
c621-061: Epoch: 0, Step: 242, Rank: 22, loss = 8.543513119185775e-17
c613-141: Epoch: 0, Step: 242, Rank: 8, loss = 4.274625098332763e-11
c621-141: Epoch: 0, Step: 242, Rank: 38, loss = 1.318767317570746e-10
c622-031: Epoch: 0, Step: 242, Rank: 48, loss = 1.318767317570746e-10
c622-021: Epoch: 0, Step: 242, Rank: 46, loss = 2.3245294578089215e-16
c622-062: Epoch: 0, Step: 242, Rank: 55, loss = 6.439293542825908e-14
c613-121: Epoch: 0, Step: 242, Rank: 4, loss = 2.1047890186309814e-07
c621-052: Epoch: 0, Step: 242, Rank: 21, loss = 5.227781471335135e-22
c613-112: Epoch: 0, Step: 242, Rank: 3, loss = 1.2304311611726287e-23
c622-082: Epoch: 0, Step: 242, Rank: 59, loss = 6.927791673660977e-13
c621-071: Epoch: 0, Step: 242, Rank: 24, loss = 0.0
c622-091: Epoch: 0, Step: 242, Rank: 60, loss = 9.74978320300579e-10
c613-142: Epoch: 0, Step: 242, Rank: 9, loss = 1.4637180356658064e-12
c621-102: Epoch: 0, Step: 242, Rank: 31, loss = 1.418811734765768e-09
c613-111: Epoch: 0, Step: 242, Rank: 2, loss = 0.0
c613-102: Epoch: 0, Step: 242, Rank: 1, loss = 4.926614671774132e-16
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.00s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.00s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 243, Rank: 43, loss = 6.314393452555578e-16
c613-101: Epoch: 0, Step: 243, Rank: 0, loss = 1.983707842718853e-32
c622-081: Epoch: 0, Step: 243, Rank: 58, loss = 0.0
c619-022: Epoch: 0, Step: 243, Rank: 17, loss = 3.5695955961250784e-29
c619-021: Epoch: 0, Step: 243, Rank: 16, loss = 1.7848833522293717e-11
c619-002: Epoch: 0, Step: 243, Rank: 13, loss = 1.318767317570746e-10
c622-092: Epoch: 0, Step: 243, Rank: 61, loss = 6.927791673660977e-13
c622-071: Epoch: 0, Step: 243, Rank: 56, loss = 2.68426485998971e-33
c621-151: Epoch: 0, Step: 243, Rank: 40, loss = 5.14984130859375e-05
c622-001: Epoch: 0, Step: 243, Rank: 42, loss = 0.0
c622-082: Epoch: 0, Step: 243, Rank: 59, loss = 6.635317295611287e-17
c622-012: Epoch: 0, Step: 243, Rank: 45, loss = 3.4051481634378433e-09
c619-011: Epoch: 0, Step: 243, Rank: 14, loss = 8.307397365570068e-07
c621-132: Epoch: 0, Step: 243, Rank: 37, loss = 2.4158453015843406e-12
c613-111: Epoch: 0, Step: 243, Rank: 2, loss = 3.694822225952521e-13
c619-001: Epoch: 0, Step: 243, Rank: 12, loss = 6.139278411865234e-06
c613-132: Epoch: 0, Step: 243, Rank: 7, loss = 2.2065682614424986e-15
c619-031: Epoch: 0, Step: 243, Rank: 18, loss = 3.8163916471489756e-16
c622-022: Epoch: 0, Step: 243, Rank: 47, loss = 6.439293542825908e-14
c622-072: Epoch: 0, Step: 243, Rank: 57, loss = 2.971649718878743e-35
c613-122: Epoch: 0, Step: 243, Rank: 5, loss = 1.0788440704345703e-05
c621-111: Epoch: 0, Step: 243, Rank: 32, loss = 2.2851054382044822e-11
c613-142: Epoch: 0, Step: 243, Rank: 9, loss = 9.370282327836321e-14
c622-041: Epoch: 0, Step: 243, Rank: 50, loss = 4.0862722260119567e-22
c621-141: Epoch: 0, Step: 243, Rank: 38, loss = 1.3597309589385986e-07
c622-101: Epoch: 0, Step: 243, Rank: 62, loss = 1.5366822481155396e-07
c613-131: Epoch: 0, Step: 243, Rank: 6, loss = 8.307397365570068e-07
c621-072: Epoch: 0, Step: 243, Rank: 25, loss = 1.7848833522293717e-11
c613-112: Epoch: 0, Step: 243, Rank: 3, loss = 1.3445969671010971e-08
c619-032: Epoch: 0, Step: 243, Rank: 19, loss = 0.0
c622-031: Epoch: 0, Step: 243, Rank: 48, loss = 1.2514647096395493e-09
c613-152: Epoch: 0, Step: 243, Rank: 11, loss = 1.84297022087776e-14
c622-062: Epoch: 0, Step: 243, Rank: 55, loss = 3.314018249511719e-05
c622-032: Epoch: 0, Step: 243, Rank: 49, loss = 4.760636329592671e-13
c622-061: Epoch: 0, Step: 243, Rank: 54, loss = 3.144186300207963e-17
c621-142: Epoch: 0, Step: 243, Rank: 39, loss = 2.3245294578089215e-16
c613-102: Epoch: 0, Step: 243, Rank: 1, loss = 1.4637180356658064e-12
c621-081: Epoch: 0, Step: 243, Rank: 26, loss = 3.510081114654895e-12
c622-011: Epoch: 0, Step: 243, Rank: 44, loss = 6.891787052154541e-07
c622-051: Epoch: 0, Step: 243, Rank: 52, loss = 2.2065682614424986e-15
c613-141: Epoch: 0, Step: 243, Rank: 8, loss = 8.003553375601768e-11
c622-042: Epoch: 0, Step: 243, Rank: 51, loss = 2.540190280342358e-13
c622-091: Epoch: 0, Step: 243, Rank: 60, loss = 3.441691376337985e-14
c622-021: Epoch: 0, Step: 243, Rank: 46, loss = 1.3589129821411916e-13
c613-151: Epoch: 0, Step: 243, Rank: 10, loss = 0.0
c621-091: Epoch: 0, Step: 243, Rank: 28, loss = 3.8163916471489756e-16
c622-102: Epoch: 0, Step: 243, Rank: 63, loss = 9.370282327836321e-14
c621-131: Epoch: 0, Step: 243, Rank: 36, loss = 1.8758328224066645e-12
c619-012: Epoch: 0, Step: 243, Rank: 15, loss = 2.648448571562767e-09
c621-152: Epoch: 0, Step: 243, Rank: 41, loss = 3.8163916471489756e-16
c621-121: Epoch: 0, Step: 243, Rank: 34, loss = 7.386127300057499e-19
c613-121: Epoch: 0, Step: 243, Rank: 4, loss = 1.2790197503539935e-19
c621-071: Epoch: 0, Step: 243, Rank: 24, loss = 9.370282327836321e-14
c621-112: Epoch: 0, Step: 243, Rank: 33, loss = 2.648448571562767e-09
c621-082: Epoch: 0, Step: 243, Rank: 27, loss = 9.880984919163893e-15
c621-101: Epoch: 0, Step: 243, Rank: 30, loss = 1.4637180356658064e-12
c621-092: Epoch: 0, Step: 243, Rank: 29, loss = 4.760636329592671e-13
c621-122: Epoch: 0, Step: 243, Rank: 35, loss = 2.7550648847397363e-40
c621-102: Epoch: 0, Step: 243, Rank: 31, loss = 1.9081958235744878e-17
c622-052: Epoch: 0, Step: 243, Rank: 53, loss = 1.1546753136970622e-17
c621-062: Epoch: 0, Step: 243, Rank: 23, loss = 9.049472282640636e-11
c621-061: Epoch: 0, Step: 243, Rank: 22, loss = 4.9763185651190145e-21
c621-052: Epoch: 0, Step: 243, Rank: 21, loss = 3.979039320256561e-12
c619-041: Epoch: 0, Step: 243, Rank: 20, loss = 1.6209256159527285e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.7568359375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-121: Epoch: 0, Step: 244, Rank: 4, loss = 3.268496584496461e-13
c622-092: Epoch: 0, Step: 244, Rank: 61, loss = 6.439293542825908e-14
c622-002: Epoch: 0, Step: 244, Rank: 43, loss = 1.318767317570746e-10
c613-101: Epoch: 0, Step: 244, Rank: 0, loss = 0.0
c619-002: Epoch: 0, Step: 244, Rank: 13, loss = 3.4897757426877174e-19
c622-062: Epoch: 0, Step: 244, Rank: 55, loss = 5.115907697472721e-12
c622-081: Epoch: 0, Step: 244, Rank: 58, loss = 1.8189894035458565e-09
c619-001: Epoch: 0, Step: 244, Rank: 12, loss = 3.3068166260807885e-18
c622-032: Epoch: 0, Step: 244, Rank: 49, loss = 2.4158453015843406e-12
c621-111: Epoch: 0, Step: 244, Rank: 32, loss = 3.510081114654895e-12
c613-131: Epoch: 0, Step: 244, Rank: 6, loss = 6.635317295611287e-17
c621-131: Epoch: 0, Step: 244, Rank: 36, loss = 1.318767317570746e-10
c621-121: Epoch: 0, Step: 244, Rank: 34, loss = 2.710505431213761e-19
c613-111: Epoch: 0, Step: 244, Rank: 2, loss = 1.9273308272485545e-22
c622-001: Epoch: 0, Step: 244, Rank: 42, loss = 5.115907697472721e-12
c613-122: Epoch: 0, Step: 244, Rank: 5, loss = 1.2656542480726785e-14
c619-021: Epoch: 0, Step: 244, Rank: 16, loss = 1.150369644165039e-05
c621-151: Epoch: 0, Step: 244, Rank: 40, loss = 4.05634636990726e-10
c622-012: Epoch: 0, Step: 244, Rank: 45, loss = 0.0
c613-102: Epoch: 0, Step: 244, Rank: 1, loss = 3.8163916471489756e-16
c621-081: Epoch: 0, Step: 244, Rank: 26, loss = 1.955777406692505e-08
c613-132: Epoch: 0, Step: 244, Rank: 7, loss = 9.370282327836321e-14
c613-142: Epoch: 0, Step: 244, Rank: 9, loss = 6.927791673660977e-13
c622-102: Epoch: 0, Step: 244, Rank: 63, loss = 3.694822225952521e-13
c613-112: Epoch: 0, Step: 244, Rank: 3, loss = 2.9331204132176936e-11
c622-082: Epoch: 0, Step: 244, Rank: 59, loss = 1.3515625
c622-051: Epoch: 0, Step: 244, Rank: 52, loss = 3.4897757426877174e-19
c621-061: Epoch: 0, Step: 244, Rank: 22, loss = 7.66053886991358e-15
c613-151: Epoch: 0, Step: 244, Rank: 10, loss = 1.199040866595169e-13
c613-152: Epoch: 0, Step: 244, Rank: 11, loss = 1.4637180356658064e-12
c622-091: Epoch: 0, Step: 244, Rank: 60, loss = 4.274625098332763e-11
c621-142: Epoch: 0, Step: 244, Rank: 39, loss = 9.74978320300579e-10
c622-022: Epoch: 0, Step: 244, Rank: 47, loss = 4.760636329592671e-13
c621-082: Epoch: 0, Step: 244, Rank: 27, loss = 4.843059286940843e-11
c619-012: Epoch: 0, Step: 244, Rank: 15, loss = 1.6209256159527285e-14
c619-022: Epoch: 0, Step: 244, Rank: 17, loss = 0.69140625
c622-041: Epoch: 0, Step: 244, Rank: 50, loss = 7.283063041541027e-14
c619-032: Epoch: 0, Step: 244, Rank: 19, loss = 0.004486083984375
c621-072: Epoch: 0, Step: 244, Rank: 25, loss = 1.8189894035458565e-09
c613-141: Epoch: 0, Step: 244, Rank: 8, loss = 1.0089706847793423e-12
c622-061: Epoch: 0, Step: 244, Rank: 54, loss = 3.8163916471489756e-16
c622-011: Epoch: 0, Step: 244, Rank: 44, loss = 2.4158453015843406e-12
c621-101: Epoch: 0, Step: 244, Rank: 30, loss = 1.318767317570746e-10
c622-072: Epoch: 0, Step: 244, Rank: 57, loss = 1.2069940567016602e-06
c621-091: Epoch: 0, Step: 244, Rank: 28, loss = 7.44648787076585e-12
c621-092: Epoch: 0, Step: 244, Rank: 29, loss = 1.199040866595169e-13
c619-011: Epoch: 0, Step: 244, Rank: 14, loss = 5.39260384428426e-32
c622-101: Epoch: 0, Step: 244, Rank: 62, loss = 1.4915713109076023e-10
c622-071: Epoch: 0, Step: 244, Rank: 56, loss = 1.8758328224066645e-12
c621-132: Epoch: 0, Step: 244, Rank: 37, loss = 1.014588720084573e-24
c622-031: Epoch: 0, Step: 244, Rank: 48, loss = 3.144186300207963e-17
c621-141: Epoch: 0, Step: 244, Rank: 38, loss = 2.4158453015843406e-12
c621-052: Epoch: 0, Step: 244, Rank: 21, loss = 9.486769009248164e-19
c621-112: Epoch: 0, Step: 244, Rank: 33, loss = 2.574980159653073e-18
c621-152: Epoch: 0, Step: 244, Rank: 41, loss = 5.400124791776761e-13
c619-031: Epoch: 0, Step: 244, Rank: 18, loss = 1.199040866595169e-13
c621-102: Epoch: 0, Step: 244, Rank: 31, loss = 3.8163916471489756e-16
c622-042: Epoch: 0, Step: 244, Rank: 51, loss = 9.994988777600744e-20
c621-062: Epoch: 0, Step: 244, Rank: 23, loss = 3.293156623840332e-06
c621-071: Epoch: 0, Step: 244, Rank: 24, loss = 1.3589129821411916e-13
c619-041: Epoch: 0, Step: 244, Rank: 20, loss = 2.1457672119140625e-05
c622-021: Epoch: 0, Step: 244, Rank: 46, loss = 2.3245294578089215e-16
c622-052: Epoch: 0, Step: 244, Rank: 53, loss = 3.841705620288849e-09
c621-122: Epoch: 0, Step: 244, Rank: 35, loss = 5.182486384480711e-17
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 245, Rank: 43, loss = 5.995204332975845e-15
c613-101: Epoch: 0, Step: 245, Rank: 0, loss = 3.583409124985337e-10
c619-021: Epoch: 0, Step: 245, Rank: 16, loss = 8.083811398051921e-16
c621-111: Epoch: 0, Step: 245, Rank: 32, loss = 1.0408340855860843e-15
c622-081: Epoch: 0, Step: 245, Rank: 58, loss = 4.274625098332763e-11
c622-052: Epoch: 0, Step: 245, Rank: 53, loss = 0.0
c621-151: Epoch: 0, Step: 245, Rank: 40, loss = 5.4836273193359375e-05
c619-001: Epoch: 0, Step: 245, Rank: 12, loss = 0.000392913818359375
c622-101: Epoch: 0, Step: 245, Rank: 62, loss = 0.0
c622-062: Epoch: 0, Step: 245, Rank: 55, loss = 0.0
c622-001: Epoch: 0, Step: 245, Rank: 42, loss = 1.1546753136970622e-17
c613-132: Epoch: 0, Step: 245, Rank: 7, loss = 3.917798799689633e-26
c622-012: Epoch: 0, Step: 245, Rank: 45, loss = 3.583409124985337e-10
c619-041: Epoch: 0, Step: 245, Rank: 20, loss = 3.8163916471489756e-16
c621-132: Epoch: 0, Step: 245, Rank: 37, loss = 3.441691376337985e-14
c622-082: Epoch: 0, Step: 245, Rank: 59, loss = 5.0182080713057076e-14
c622-071: Epoch: 0, Step: 245, Rank: 56, loss = 1.5366822481155396e-07
c621-081: Epoch: 0, Step: 245, Rank: 26, loss = 4.255493527005605e-18
c621-061: Epoch: 0, Step: 245, Rank: 22, loss = 5.617039278149605e-09
c621-052: Epoch: 0, Step: 245, Rank: 21, loss = 1.2790197503539935e-19
c621-121: Epoch: 0, Step: 245, Rank: 34, loss = 0.69140625
c613-111: Epoch: 0, Step: 245, Rank: 2, loss = 1.84297022087776e-14
c619-002: Epoch: 0, Step: 245, Rank: 13, loss = 3.213062882423401e-08
c621-152: Epoch: 0, Step: 245, Rank: 41, loss = 3.979039320256561e-12
c619-022: Epoch: 0, Step: 245, Rank: 17, loss = 1.84297022087776e-14
c622-092: Epoch: 0, Step: 245, Rank: 61, loss = 8.754432201385498e-08
c622-051: Epoch: 0, Step: 245, Rank: 52, loss = 8.003553375601768e-11
c619-011: Epoch: 0, Step: 245, Rank: 14, loss = 1.053497228147536e-20
c613-151: Epoch: 0, Step: 245, Rank: 10, loss = 2.514570951461792e-08
c621-131: Epoch: 0, Step: 245, Rank: 36, loss = 9.370282327836321e-14
c619-012: Epoch: 0, Step: 245, Rank: 15, loss = 0.0
c622-061: Epoch: 0, Step: 245, Rank: 54, loss = 1.3322676295501878e-15
c622-011: Epoch: 0, Step: 245, Rank: 44, loss = 6.635317295611287e-17
c613-142: Epoch: 0, Step: 245, Rank: 9, loss = 2.831068712794149e-15
c622-041: Epoch: 0, Step: 245, Rank: 50, loss = 1.84297022087776e-14
c622-031: Epoch: 0, Step: 245, Rank: 48, loss = 5.699694156646729e-07
c621-122: Epoch: 0, Step: 245, Rank: 35, loss = 2.7550648847397363e-40
c621-082: Epoch: 0, Step: 245, Rank: 27, loss = 1.166324663699769e-22
c613-152: Epoch: 0, Step: 245, Rank: 11, loss = 2.1736923372372985e-10
c621-141: Epoch: 0, Step: 245, Rank: 38, loss = 6.635317295611287e-17
c619-031: Epoch: 0, Step: 245, Rank: 18, loss = 2.831068712794149e-15
c621-101: Epoch: 0, Step: 245, Rank: 30, loss = 0.0
c621-072: Epoch: 0, Step: 245, Rank: 25, loss = 5.893525667488575e-10
c613-122: Epoch: 0, Step: 245, Rank: 5, loss = 7.048583938740194e-11
c622-042: Epoch: 0, Step: 245, Rank: 51, loss = 4.418687638008123e-14
c622-032: Epoch: 0, Step: 245, Rank: 49, loss = 7.66053886991358e-15
c622-102: Epoch: 0, Step: 245, Rank: 63, loss = 8.003553375601768e-11
c622-091: Epoch: 0, Step: 245, Rank: 60, loss = 1.6079866327345371e-09
c622-022: Epoch: 0, Step: 245, Rank: 47, loss = 5.759824041329242e-19
c621-112: Epoch: 0, Step: 245, Rank: 33, loss = 4.418687638008123e-14
c621-091: Epoch: 0, Step: 245, Rank: 28, loss = 1.7848833522293717e-11
c613-131: Epoch: 0, Step: 245, Rank: 6, loss = 0.69140625
c613-121: Epoch: 0, Step: 245, Rank: 4, loss = 0.0
c621-062: Epoch: 0, Step: 245, Rank: 23, loss = 5.542110104105285e-23
c622-072: Epoch: 0, Step: 245, Rank: 57, loss = 5.0182080713057076e-14
c622-021: Epoch: 0, Step: 245, Rank: 46, loss = 3.441691376337985e-14
c613-102: Epoch: 0, Step: 245, Rank: 1, loss = 1.0408340855860843e-15
c613-141: Epoch: 0, Step: 245, Rank: 8, loss = 4.926614671774132e-16
c613-112: Epoch: 0, Step: 245, Rank: 3, loss = 1.84297022087776e-14
c619-032: Epoch: 0, Step: 245, Rank: 19, loss = 0.69140625
c621-092: Epoch: 0, Step: 245, Rank: 29, loss = 3.979039320256561e-12
c621-071: Epoch: 0, Step: 245, Rank: 24, loss = 1.0800249583553523e-11
c621-102: Epoch: 0, Step: 245, Rank: 31, loss = 7.66053886991358e-15
c621-142: Epoch: 0, Step: 245, Rank: 39, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 246, Rank: 43, loss = 2.6756374893466273e-14
c622-052: Epoch: 0, Step: 246, Rank: 53, loss = 0.0
c622-051: Epoch: 0, Step: 246, Rank: 52, loss = 4.9763185651190145e-21
c619-021: Epoch: 0, Step: 246, Rank: 16, loss = 3.694822225952521e-13
c622-061: Epoch: 0, Step: 246, Rank: 54, loss = 3.4897757426877174e-19
c613-121: Epoch: 0, Step: 246, Rank: 4, loss = 1.485356976305141e-17
c619-001: Epoch: 0, Step: 246, Rank: 12, loss = 2.4158453015843406e-12
c621-111: Epoch: 0, Step: 246, Rank: 32, loss = 1.84297022087776e-14
c613-101: Epoch: 0, Step: 246, Rank: 0, loss = 5.0182080713057076e-14
c621-132: Epoch: 0, Step: 246, Rank: 37, loss = 1.2931877790833823e-12
c613-132: Epoch: 0, Step: 246, Rank: 7, loss = 4.760636329592671e-13
c613-131: Epoch: 0, Step: 246, Rank: 6, loss = 8.585629984736443e-10
c613-152: Epoch: 0, Step: 246, Rank: 11, loss = 0.0002307891845703125
c619-002: Epoch: 0, Step: 246, Rank: 13, loss = 2.831068712794149e-15
c622-101: Epoch: 0, Step: 246, Rank: 62, loss = 1.2278178473934531e-11
c621-081: Epoch: 0, Step: 246, Rank: 26, loss = 1.7364175418713157e-20
c622-001: Epoch: 0, Step: 246, Rank: 42, loss = 1.7364175418713157e-20
c622-012: Epoch: 0, Step: 246, Rank: 45, loss = 6.565414878423326e-12
c619-031: Epoch: 0, Step: 246, Rank: 18, loss = 6.927791673660977e-13
c621-151: Epoch: 0, Step: 246, Rank: 40, loss = 5.995204332975845e-15
c622-041: Epoch: 0, Step: 246, Rank: 50, loss = 2.831068712794149e-15
c621-082: Epoch: 0, Step: 246, Rank: 27, loss = 0.0
c619-022: Epoch: 0, Step: 246, Rank: 17, loss = 1.84297022087776e-14
c613-151: Epoch: 0, Step: 246, Rank: 10, loss = 4.418687638008123e-14
c622-081: Epoch: 0, Step: 246, Rank: 58, loss = 0.69140625
c613-142: Epoch: 0, Step: 246, Rank: 9, loss = 2.5920599000528455e-11
c622-092: Epoch: 0, Step: 246, Rank: 61, loss = 6.314393452555578e-16
c613-122: Epoch: 0, Step: 246, Rank: 5, loss = 4.926614671774132e-16
c621-091: Epoch: 0, Step: 246, Rank: 28, loss = 0.69140625
c613-112: Epoch: 0, Step: 246, Rank: 3, loss = 1.3869794202037156e-11
c621-072: Epoch: 0, Step: 246, Rank: 25, loss = 5.400124791776761e-13
c621-152: Epoch: 0, Step: 246, Rank: 41, loss = 5.115907697472721e-12
c621-112: Epoch: 0, Step: 246, Rank: 33, loss = 2.562999725341797e-06
c621-131: Epoch: 0, Step: 246, Rank: 36, loss = 3.774403012357652e-11
c622-082: Epoch: 0, Step: 246, Rank: 59, loss = 0.0
c622-032: Epoch: 0, Step: 246, Rank: 49, loss = 0.0
c621-121: Epoch: 0, Step: 246, Rank: 34, loss = 2.9331204132176936e-11
c622-011: Epoch: 0, Step: 246, Rank: 44, loss = 1.199040866595169e-13
c621-061: Epoch: 0, Step: 246, Rank: 22, loss = 0.0
c622-021: Epoch: 0, Step: 246, Rank: 46, loss = 0.0001316070556640625
c613-111: Epoch: 0, Step: 246, Rank: 2, loss = 8.734267942607043e-27
c619-041: Epoch: 0, Step: 246, Rank: 20, loss = 1.2931877790833823e-12
c619-011: Epoch: 0, Step: 246, Rank: 14, loss = 7.66053886991358e-15
c621-052: Epoch: 0, Step: 246, Rank: 21, loss = 9.74978320300579e-10
c622-022: Epoch: 0, Step: 246, Rank: 47, loss = 2.710505431213761e-19
c622-062: Epoch: 0, Step: 246, Rank: 55, loss = 1.7848833522293717e-11
c613-102: Epoch: 0, Step: 246, Rank: 1, loss = 3.8163916471489756e-16
c619-012: Epoch: 0, Step: 246, Rank: 15, loss = 6.439293542825908e-14
c622-031: Epoch: 0, Step: 246, Rank: 48, loss = 1.565316886525947e-18
c622-042: Epoch: 0, Step: 246, Rank: 51, loss = 4.843059286940843e-11
c622-102: Epoch: 0, Step: 246, Rank: 63, loss = 3.268496584496461e-13
c613-141: Epoch: 0, Step: 246, Rank: 8, loss = 3.8163916471489756e-16
c621-101: Epoch: 0, Step: 246, Rank: 30, loss = 5.029141902923584e-07
c621-122: Epoch: 0, Step: 246, Rank: 35, loss = 3.268496584496461e-13
c619-032: Epoch: 0, Step: 246, Rank: 19, loss = 6.439293542825908e-14
c621-102: Epoch: 0, Step: 246, Rank: 31, loss = 1.0800249583553523e-11
c621-092: Epoch: 0, Step: 246, Rank: 29, loss = 3.688037395477295e-07
c621-071: Epoch: 0, Step: 246, Rank: 24, loss = 2.234049398383217e-20
c622-091: Epoch: 0, Step: 246, Rank: 60, loss = 7.66053886991358e-15
c621-062: Epoch: 0, Step: 246, Rank: 23, loss = 5.115907697472721e-12
c622-071: Epoch: 0, Step: 246, Rank: 56, loss = 9.012222290039062e-05
c621-141: Epoch: 0, Step: 246, Rank: 38, loss = 7.729977369308472e-08
c621-142: Epoch: 0, Step: 246, Rank: 39, loss = 2.905726432800293e-06
c622-072: Epoch: 0, Step: 246, Rank: 57, loss = 8.881784197001252e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 247, Rank: 43, loss = 9.880984919163893e-15
c619-002: Epoch: 0, Step: 247, Rank: 13, loss = 6.635317295611287e-17
c619-021: Epoch: 0, Step: 247, Rank: 16, loss = 1.6916601452976465e-10
c621-111: Epoch: 0, Step: 247, Rank: 32, loss = 2.7830537874251604e-10
c622-052: Epoch: 0, Step: 247, Rank: 53, loss = 1.2278178473934531e-11
c621-132: Epoch: 0, Step: 247, Rank: 37, loss = 8.543513119185775e-17
c622-012: Epoch: 0, Step: 247, Rank: 45, loss = 4.760636329592671e-13
c613-132: Epoch: 0, Step: 247, Rank: 7, loss = 3.694822225952521e-13
c619-001: Epoch: 0, Step: 247, Rank: 12, loss = 2.831068712794149e-15
c613-101: Epoch: 0, Step: 247, Rank: 0, loss = 5.424022674560547e-06
c619-022: Epoch: 0, Step: 247, Rank: 17, loss = 8.412825991399586e-12
c622-011: Epoch: 0, Step: 247, Rank: 44, loss = 3.583409124985337e-10
c621-082: Epoch: 0, Step: 247, Rank: 27, loss = 8.881784197001252e-13
c619-011: Epoch: 0, Step: 247, Rank: 14, loss = 7.486343383789062e-05
c621-091: Epoch: 0, Step: 247, Rank: 28, loss = 4.05634636990726e-10
c621-151: Epoch: 0, Step: 247, Rank: 40, loss = 4.3655745685100555e-09
c621-052: Epoch: 0, Step: 247, Rank: 21, loss = 1.4637180356658064e-12
c621-101: Epoch: 0, Step: 247, Rank: 30, loss = 5.0182080713057076e-14
c621-072: Epoch: 0, Step: 247, Rank: 25, loss = 1.2931877790833823e-12
c619-032: Epoch: 0, Step: 247, Rank: 19, loss = 6.314393452555578e-16
c621-142: Epoch: 0, Step: 247, Rank: 39, loss = 1.955777406692505e-08
c619-031: Epoch: 0, Step: 247, Rank: 18, loss = 1.7139067942650854e-15
c619-012: Epoch: 0, Step: 247, Rank: 15, loss = 5.502442945726216e-11
c622-051: Epoch: 0, Step: 247, Rank: 52, loss = 1.1546753136970622e-17
c621-152: Epoch: 0, Step: 247, Rank: 41, loss = 0.0
c613-152: Epoch: 0, Step: 247, Rank: 11, loss = 1.485356976305141e-17
c613-122: Epoch: 0, Step: 247, Rank: 5, loss = 0.0
c622-001: Epoch: 0, Step: 247, Rank: 42, loss = 1.5819829215076654e-23
c613-142: Epoch: 0, Step: 247, Rank: 9, loss = 1.8758328224066645e-12
c619-041: Epoch: 0, Step: 247, Rank: 20, loss = 2.066371962428093e-09
c621-131: Epoch: 0, Step: 247, Rank: 36, loss = 1.3589129821411916e-13
c621-061: Epoch: 0, Step: 247, Rank: 22, loss = 0.69140625
c622-101: Epoch: 0, Step: 247, Rank: 62, loss = 0.69140625
c622-041: Epoch: 0, Step: 247, Rank: 50, loss = 2.831068712794149e-15
c622-062: Epoch: 0, Step: 247, Rank: 55, loss = 2.0057740190981832e-18
c613-151: Epoch: 0, Step: 247, Rank: 10, loss = 1.0089706847793423e-12
c621-112: Epoch: 0, Step: 247, Rank: 33, loss = 5.182486384480711e-17
c622-061: Epoch: 0, Step: 247, Rank: 54, loss = 4.255493527005605e-18
c613-131: Epoch: 0, Step: 247, Rank: 6, loss = 3.510081114654895e-12
c621-102: Epoch: 0, Step: 247, Rank: 31, loss = 5.893525667488575e-10
c622-031: Epoch: 0, Step: 247, Rank: 48, loss = 0.0
c613-112: Epoch: 0, Step: 247, Rank: 3, loss = 7.792703114739563e-20
c622-081: Epoch: 0, Step: 247, Rank: 58, loss = 1.2278178473934531e-11
c621-081: Epoch: 0, Step: 247, Rank: 26, loss = 0.69140625
c622-102: Epoch: 0, Step: 247, Rank: 63, loss = 6.993104012531504e-18
c622-022: Epoch: 0, Step: 247, Rank: 47, loss = 2.831068712794149e-15
c621-141: Epoch: 0, Step: 247, Rank: 38, loss = 4.839897155761719e-05
c622-071: Epoch: 0, Step: 247, Rank: 56, loss = 2.6756374893466273e-14
c613-111: Epoch: 0, Step: 247, Rank: 2, loss = 9.549694368615746e-12
c622-092: Epoch: 0, Step: 247, Rank: 61, loss = 4.843059286940843e-11
c613-141: Epoch: 0, Step: 247, Rank: 8, loss = 1.3589129821411916e-13
c621-122: Epoch: 0, Step: 247, Rank: 35, loss = 3.441691376337985e-14
c613-102: Epoch: 0, Step: 247, Rank: 1, loss = 6.629788138637702e-36
c621-092: Epoch: 0, Step: 247, Rank: 29, loss = 2.2065682614424986e-15
c622-021: Epoch: 0, Step: 247, Rank: 46, loss = 1.7139067942650854e-15
c621-062: Epoch: 0, Step: 247, Rank: 23, loss = 1.4637180356658064e-12
c622-032: Epoch: 0, Step: 247, Rank: 49, loss = 1.84297022087776e-14
c613-121: Epoch: 0, Step: 247, Rank: 4, loss = 1.6672859221771964e-24
c622-042: Epoch: 0, Step: 247, Rank: 51, loss = 6.007030606269836e-08
c621-071: Epoch: 0, Step: 247, Rank: 24, loss = 9.880984919163893e-15
c622-091: Epoch: 0, Step: 247, Rank: 60, loss = 1.5688783605583012e-11
c622-072: Epoch: 0, Step: 247, Rank: 57, loss = 0.0
c622-082: Epoch: 0, Step: 247, Rank: 59, loss = 2.540190280342358e-13
c621-121: Epoch: 0, Step: 247, Rank: 34, loss = 1.8758328224066645e-12
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 248, Rank: 0, loss = 0.0
c622-081: Epoch: 0, Step: 248, Rank: 58, loss = 6.439293542825908e-14
c622-002: Epoch: 0, Step: 248, Rank: 43, loss = 8.083811398051921e-16
c619-002: Epoch: 0, Step: 248, Rank: 13, loss = 6.565414878423326e-12
c622-101: Epoch: 0, Step: 248, Rank: 62, loss = 1.6209256159527285e-14
c622-102: Epoch: 0, Step: 248, Rank: 63, loss = 1.3445969671010971e-08
c621-111: Epoch: 0, Step: 248, Rank: 32, loss = 0.0
c619-001: Epoch: 0, Step: 248, Rank: 12, loss = 3.979039320256561e-12
c621-112: Epoch: 0, Step: 248, Rank: 33, loss = 2.6756374893466273e-14
c613-132: Epoch: 0, Step: 248, Rank: 7, loss = 1.199040866595169e-13
c613-142: Epoch: 0, Step: 248, Rank: 9, loss = 2.5331974029541016e-07
c613-131: Epoch: 0, Step: 248, Rank: 6, loss = 1.3322676295501878e-15
c622-012: Epoch: 0, Step: 248, Rank: 45, loss = 9.370282327836321e-14
c619-021: Epoch: 0, Step: 248, Rank: 16, loss = 1.6079866327345371e-09
c613-112: Epoch: 0, Step: 248, Rank: 3, loss = 1.1059455573558807e-09
c619-011: Epoch: 0, Step: 248, Rank: 14, loss = 7.386127300057499e-19
c622-071: Epoch: 0, Step: 248, Rank: 56, loss = 0.0
c621-151: Epoch: 0, Step: 248, Rank: 40, loss = 0.0
c622-052: Epoch: 0, Step: 248, Rank: 53, loss = 1.418811734765768e-09
c613-121: Epoch: 0, Step: 248, Rank: 4, loss = 7.66053886991358e-15
c622-072: Epoch: 0, Step: 248, Rank: 57, loss = 1.0800249583553523e-11
c622-051: Epoch: 0, Step: 248, Rank: 52, loss = 7.496324301261813e-24
c622-082: Epoch: 0, Step: 248, Rank: 59, loss = 8.881784197001252e-13
c621-142: Epoch: 0, Step: 248, Rank: 39, loss = 3.342393029015511e-11
c619-022: Epoch: 0, Step: 248, Rank: 17, loss = 0.0
c613-102: Epoch: 0, Step: 248, Rank: 1, loss = 5.115907697472721e-12
c613-141: Epoch: 0, Step: 248, Rank: 8, loss = 4.0332320816460765e-17
c621-152: Epoch: 0, Step: 248, Rank: 41, loss = 1.7497114868092467e-13
c622-041: Epoch: 0, Step: 248, Rank: 50, loss = 2.2065682614424986e-15
c613-122: Epoch: 0, Step: 248, Rank: 5, loss = 1.2218952178955078e-05
c613-152: Epoch: 0, Step: 248, Rank: 11, loss = 1.3589129821411916e-13
c613-111: Epoch: 0, Step: 248, Rank: 2, loss = 0.0
c622-062: Epoch: 0, Step: 248, Rank: 55, loss = 0.69140625
c621-122: Epoch: 0, Step: 248, Rank: 35, loss = 8.881784197001252e-13
c621-061: Epoch: 0, Step: 248, Rank: 22, loss = 1.4637180356658064e-12
c613-151: Epoch: 0, Step: 248, Rank: 10, loss = 8.543513119185775e-17
c621-131: Epoch: 0, Step: 248, Rank: 36, loss = 2.066371962428093e-09
c622-061: Epoch: 0, Step: 248, Rank: 54, loss = 3.583409124985337e-10
c621-101: Epoch: 0, Step: 248, Rank: 30, loss = 1.2656542480726785e-14
c622-042: Epoch: 0, Step: 248, Rank: 51, loss = 4.6629367034256575e-15
c621-132: Epoch: 0, Step: 248, Rank: 37, loss = 3.144186300207963e-17
c622-011: Epoch: 0, Step: 248, Rank: 44, loss = 0.00019073486328125
c622-091: Epoch: 0, Step: 248, Rank: 60, loss = 1.0972125985553305e-16
c621-141: Epoch: 0, Step: 248, Rank: 38, loss = 4.760636329592671e-13
c621-091: Epoch: 0, Step: 248, Rank: 28, loss = 6.5267086029052734e-06
c621-081: Epoch: 0, Step: 248, Rank: 26, loss = 1.0408340855860843e-15
c622-032: Epoch: 0, Step: 248, Rank: 49, loss = 6.314393452555578e-16
c621-052: Epoch: 0, Step: 248, Rank: 21, loss = 0.0
c622-001: Epoch: 0, Step: 248, Rank: 42, loss = 8.543513119185775e-17
c619-032: Epoch: 0, Step: 248, Rank: 19, loss = 1.2656542480726785e-14
c619-012: Epoch: 0, Step: 248, Rank: 15, loss = 5.400124791776761e-13
c621-121: Epoch: 0, Step: 248, Rank: 34, loss = 2.0236257114447653e-11
c621-072: Epoch: 0, Step: 248, Rank: 25, loss = 1.0277290130034089e-10
c622-022: Epoch: 0, Step: 248, Rank: 47, loss = 7.566995918750763e-10
c619-031: Epoch: 0, Step: 248, Rank: 18, loss = 7.383573891102493e-38
c622-031: Epoch: 0, Step: 248, Rank: 48, loss = 2.2851054382044822e-11
c621-102: Epoch: 0, Step: 248, Rank: 31, loss = 5.182486384480711e-17
c619-041: Epoch: 0, Step: 248, Rank: 20, loss = 2.7830537874251604e-10
c621-082: Epoch: 0, Step: 248, Rank: 27, loss = 1.7848833522293717e-11
c621-062: Epoch: 0, Step: 248, Rank: 23, loss = 1.4637180356658064e-12
c621-092: Epoch: 0, Step: 248, Rank: 29, loss = 2.9331204132176936e-11
c621-071: Epoch: 0, Step: 248, Rank: 24, loss = 1.2197274440461925e-18
c622-092: Epoch: 0, Step: 248, Rank: 61, loss = 4.7222086809427244e-20
c622-021: Epoch: 0, Step: 248, Rank: 46, loss = 1.7497114868092467e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.90s, TFLOPs: 0.65, Samples/sec: 0.34, Time/seq 2.90s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 249, Rank: 0, loss = 7.44648787076585e-12
c619-021: Epoch: 0, Step: 249, Rank: 16, loss = 5.115907697472721e-12
c621-111: Epoch: 0, Step: 249, Rank: 32, loss = 2.9331204132176936e-11
c619-012: Epoch: 0, Step: 249, Rank: 15, loss = 0.0
c622-052: Epoch: 0, Step: 249, Rank: 53, loss = 5.4836273193359375e-05
c619-002: Epoch: 0, Step: 249, Rank: 13, loss = 0.69140625
c622-082: Epoch: 0, Step: 249, Rank: 59, loss = 5.400124791776761e-13
c613-132: Epoch: 0, Step: 249, Rank: 7, loss = 1.84297022087776e-14
c622-062: Epoch: 0, Step: 249, Rank: 55, loss = 1.7139067942650854e-15
c622-101: Epoch: 0, Step: 249, Rank: 62, loss = 3.979039320256561e-12
c622-081: Epoch: 0, Step: 249, Rank: 58, loss = 5.995204332975845e-15
c622-071: Epoch: 0, Step: 249, Rank: 56, loss = 2.1736923372372985e-10
c622-002: Epoch: 0, Step: 249, Rank: 43, loss = 3.979039320256561e-12
c613-111: Epoch: 0, Step: 249, Rank: 2, loss = 1.2304311611726287e-23
c622-012: Epoch: 0, Step: 249, Rank: 45, loss = 0.0
c621-081: Epoch: 0, Step: 249, Rank: 26, loss = 4.7222086809427244e-20
c613-121: Epoch: 0, Step: 249, Rank: 4, loss = 9.370282327836321e-14
c621-091: Epoch: 0, Step: 249, Rank: 28, loss = 7.386127300057499e-19
c619-011: Epoch: 0, Step: 249, Rank: 14, loss = 1.318767317570746e-10
c613-141: Epoch: 0, Step: 249, Rank: 8, loss = 5.817413330078125e-05
c622-092: Epoch: 0, Step: 249, Rank: 61, loss = 0.0002307891845703125
c619-001: Epoch: 0, Step: 249, Rank: 12, loss = 0.0
c622-041: Epoch: 0, Step: 249, Rank: 50, loss = 1.0972125985553305e-16
c621-131: Epoch: 0, Step: 249, Rank: 36, loss = 0.00012302398681640625
c613-131: Epoch: 0, Step: 249, Rank: 6, loss = 5.182486384480711e-17
c613-122: Epoch: 0, Step: 249, Rank: 5, loss = 4.843059286940843e-11
c621-132: Epoch: 0, Step: 249, Rank: 37, loss = 1.3322676295501878e-15
c613-142: Epoch: 0, Step: 249, Rank: 9, loss = 1.4051260155412137e-16
c622-001: Epoch: 0, Step: 249, Rank: 42, loss = 8.585629984736443e-10
c621-072: Epoch: 0, Step: 249, Rank: 25, loss = 2.0236257114447653e-11
c621-151: Epoch: 0, Step: 249, Rank: 40, loss = 0.0
c622-061: Epoch: 0, Step: 249, Rank: 54, loss = 3.441691376337985e-14
c621-082: Epoch: 0, Step: 249, Rank: 27, loss = 5.115907697472721e-12
c621-121: Epoch: 0, Step: 249, Rank: 34, loss = 3.510081114654895e-12
c619-041: Epoch: 0, Step: 249, Rank: 20, loss = 3.293156623840332e-06
c621-061: Epoch: 0, Step: 249, Rank: 22, loss = 2.3245294578089215e-16
c621-142: Epoch: 0, Step: 249, Rank: 39, loss = 6.993104012531504e-18
c621-052: Epoch: 0, Step: 249, Rank: 21, loss = 1.0408340855860843e-15
c621-122: Epoch: 0, Step: 249, Rank: 35, loss = 1.3709068298339844e-06
c621-112: Epoch: 0, Step: 249, Rank: 33, loss = 8.083811398051921e-16
c622-011: Epoch: 0, Step: 249, Rank: 44, loss = 2.7284841053187847e-12
c622-102: Epoch: 0, Step: 249, Rank: 63, loss = 6.439293542825908e-14
c621-152: Epoch: 0, Step: 249, Rank: 41, loss = 2.648448571562767e-09
c621-141: Epoch: 0, Step: 249, Rank: 38, loss = 0.0
c613-112: Epoch: 0, Step: 249, Rank: 3, loss = 0.00010251998901367188
c622-051: Epoch: 0, Step: 249, Rank: 52, loss = 1.4051260155412137e-16
c622-032: Epoch: 0, Step: 249, Rank: 49, loss = 3.213062882423401e-08
c619-022: Epoch: 0, Step: 249, Rank: 17, loss = 3.510081114654895e-12
c621-101: Epoch: 0, Step: 249, Rank: 30, loss = 1.166324663699769e-22
c622-031: Epoch: 0, Step: 249, Rank: 48, loss = 2.831068712794149e-15
c613-102: Epoch: 0, Step: 249, Rank: 1, loss = 1.2197274440461925e-18
c613-152: Epoch: 0, Step: 249, Rank: 11, loss = 1.2790197503539935e-19
c622-042: Epoch: 0, Step: 249, Rank: 51, loss = 2.4158453015843406e-12
c622-091: Epoch: 0, Step: 249, Rank: 60, loss = 3.0547380447387695e-07
c619-032: Epoch: 0, Step: 249, Rank: 19, loss = 1.6209256159527285e-14
c622-072: Epoch: 0, Step: 249, Rank: 57, loss = 5.3085386753082275e-08
c619-031: Epoch: 0, Step: 249, Rank: 18, loss = 6.314393452555578e-16
c621-102: Epoch: 0, Step: 249, Rank: 31, loss = 3.4897757426877174e-19
c622-021: Epoch: 0, Step: 249, Rank: 46, loss = 8.859277744181285e-32
c621-092: Epoch: 0, Step: 249, Rank: 29, loss = 6.007030606269836e-08
c621-071: Epoch: 0, Step: 249, Rank: 24, loss = 1.4051260155412137e-16
c621-062: Epoch: 0, Step: 249, Rank: 23, loss = 2.7830537874251604e-10
c622-022: Epoch: 0, Step: 249, Rank: 47, loss = 0.69140625
c613-151: Epoch: 0, Step: 249, Rank: 10, loss = 1.2931877790833823e-12
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.43s, TFLOPs: 0.78, Samples/sec: 0.41, Time/seq 2.43s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 250, Rank: 58, loss = 2.831068712794149e-15
c622-002: Epoch: 0, Step: 250, Rank: 43, loss = 0.69140625
c613-101: Epoch: 0, Step: 250, Rank: 0, loss = 5.995204332975845e-15
c621-111: Epoch: 0, Step: 250, Rank: 32, loss = 2.2649765014648438e-06
c619-002: Epoch: 0, Step: 250, Rank: 13, loss = 3.4897757426877174e-19
c619-001: Epoch: 0, Step: 250, Rank: 12, loss = 1.7229467630386353e-08
c613-122: Epoch: 0, Step: 250, Rank: 5, loss = 3.8163916471489756e-16
c622-012: Epoch: 0, Step: 250, Rank: 45, loss = 4.3655745685100555e-09
c613-132: Epoch: 0, Step: 250, Rank: 7, loss = 8.412825991399586e-12
c613-131: Epoch: 0, Step: 250, Rank: 6, loss = 0.0
c622-061: Epoch: 0, Step: 250, Rank: 54, loss = 2.540190280342358e-13
c621-132: Epoch: 0, Step: 250, Rank: 37, loss = 6.565414878423326e-12
c621-121: Epoch: 0, Step: 250, Rank: 34, loss = 3.268496584496461e-13
c613-152: Epoch: 0, Step: 250, Rank: 11, loss = 5.14984130859375e-05
c619-031: Epoch: 0, Step: 250, Rank: 18, loss = 1.1546753136970622e-17
c613-141: Epoch: 0, Step: 250, Rank: 8, loss = 2.586841583251953e-05
c622-101: Epoch: 0, Step: 250, Rank: 62, loss = 7.44648787076585e-12
c622-102: Epoch: 0, Step: 250, Rank: 63, loss = 0.2470703125
c613-151: Epoch: 0, Step: 250, Rank: 10, loss = 1.2656542480726785e-14
c619-022: Epoch: 0, Step: 250, Rank: 17, loss = 5.995204332975845e-15
c613-142: Epoch: 0, Step: 250, Rank: 9, loss = 3.4051481634378433e-09
c613-111: Epoch: 0, Step: 250, Rank: 2, loss = 2.7284841053187847e-12
c622-041: Epoch: 0, Step: 250, Rank: 50, loss = 7.66053886991358e-15
c622-011: Epoch: 0, Step: 250, Rank: 44, loss = 6.565414878423326e-12
c613-121: Epoch: 0, Step: 250, Rank: 4, loss = 6.635317295611287e-17
c621-072: Epoch: 0, Step: 250, Rank: 25, loss = 4.418687638008123e-14
c619-011: Epoch: 0, Step: 250, Rank: 14, loss = 1.0800249583553523e-11
c622-031: Epoch: 0, Step: 250, Rank: 48, loss = 0.69140625
c621-142: Epoch: 0, Step: 250, Rank: 39, loss = 1.6209256159527285e-14
c622-071: Epoch: 0, Step: 250, Rank: 56, loss = 5.893525667488575e-10
c622-032: Epoch: 0, Step: 250, Rank: 49, loss = 2.6756374893466273e-14
c622-052: Epoch: 0, Step: 250, Rank: 53, loss = 1.4637180356658064e-12
c621-052: Epoch: 0, Step: 250, Rank: 21, loss = 5.115907697472721e-12
c621-102: Epoch: 0, Step: 250, Rank: 31, loss = 1.983707842718853e-32
c619-032: Epoch: 0, Step: 250, Rank: 19, loss = 0.0
c621-061: Epoch: 0, Step: 250, Rank: 22, loss = 0.69140625
c621-112: Epoch: 0, Step: 250, Rank: 33, loss = 0.69140625
c621-131: Epoch: 0, Step: 250, Rank: 36, loss = 1.7848833522293717e-11
c622-001: Epoch: 0, Step: 250, Rank: 42, loss = 2.4158453015843406e-12
c621-151: Epoch: 0, Step: 250, Rank: 40, loss = 1.4637180356658064e-12
c622-092: Epoch: 0, Step: 250, Rank: 61, loss = 2.5920599000528455e-11
c621-152: Epoch: 0, Step: 250, Rank: 41, loss = 4.843059286940843e-11
c621-101: Epoch: 0, Step: 250, Rank: 30, loss = 2.6756374893466273e-14
c613-112: Epoch: 0, Step: 250, Rank: 3, loss = 0.69140625
c622-062: Epoch: 0, Step: 250, Rank: 55, loss = 7.44648787076585e-12
c622-022: Epoch: 0, Step: 250, Rank: 47, loss = 3.694822225952521e-13
c622-021: Epoch: 0, Step: 250, Rank: 46, loss = 7.44648787076585e-12
c622-072: Epoch: 0, Step: 250, Rank: 57, loss = 3.144186300207963e-17
c621-122: Epoch: 0, Step: 250, Rank: 35, loss = 0.69140625
c622-051: Epoch: 0, Step: 250, Rank: 52, loss = 2.6056189295420372e-23
c621-082: Epoch: 0, Step: 250, Rank: 27, loss = 1.9806378759312793e-13
c621-081: Epoch: 0, Step: 250, Rank: 26, loss = 2.574980159653073e-18
c621-071: Epoch: 0, Step: 250, Rank: 24, loss = 4.301339185275744e-23
c619-041: Epoch: 0, Step: 250, Rank: 20, loss = 1.2656542480726785e-14
c621-062: Epoch: 0, Step: 250, Rank: 23, loss = 3.8163916471489756e-16
c621-092: Epoch: 0, Step: 250, Rank: 29, loss = 1.7139067942650854e-15
c622-091: Epoch: 0, Step: 250, Rank: 60, loss = 9.870390964984584e-34
c621-091: Epoch: 0, Step: 250, Rank: 28, loss = 8.940696716308594e-06
c613-102: Epoch: 0, Step: 250, Rank: 1, loss = 0.0
c619-012: Epoch: 0, Step: 250, Rank: 15, loss = 4.843059286940843e-11
c622-042: Epoch: 0, Step: 250, Rank: 51, loss = 1.0408340855860843e-15
c622-082: Epoch: 0, Step: 250, Rank: 59, loss = 1.8758328224066645e-12
c621-141: Epoch: 0, Step: 250, Rank: 38, loss = 2.6756374893466273e-14
c619-021: Epoch: 0, Step: 250, Rank: 16, loss = 1.9806378759312793e-13
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 251, Rank: 32, loss = 1.0408340855860843e-15
c622-052: Epoch: 0, Step: 251, Rank: 53, loss = 1.0972125985553305e-16
c613-101: Epoch: 0, Step: 251, Rank: 0, loss = 3.441691376337985e-14
c622-002: Epoch: 0, Step: 251, Rank: 43, loss = 5.115907697472721e-12
c621-091: Epoch: 0, Step: 251, Rank: 28, loss = 0.0
c619-041: Epoch: 0, Step: 251, Rank: 20, loss = 1.3589129821411916e-13
c619-002: Epoch: 0, Step: 251, Rank: 13, loss = 3.8163916471489756e-16
c621-081: Epoch: 0, Step: 251, Rank: 26, loss = 9.370282327836321e-14
c622-051: Epoch: 0, Step: 251, Rank: 52, loss = 2.540190280342358e-13
c621-151: Epoch: 0, Step: 251, Rank: 40, loss = 8.265194654219209e-40
c619-031: Epoch: 0, Step: 251, Rank: 18, loss = 0.0
c621-101: Epoch: 0, Step: 251, Rank: 30, loss = 3.510081114654895e-12
c619-001: Epoch: 0, Step: 251, Rank: 12, loss = 1.6209256159527285e-14
c621-082: Epoch: 0, Step: 251, Rank: 27, loss = 3.583409124985337e-10
c622-001: Epoch: 0, Step: 251, Rank: 42, loss = 6.693881005048752e-10
c621-152: Epoch: 0, Step: 251, Rank: 41, loss = 1.4915713109076023e-10
c622-101: Epoch: 0, Step: 251, Rank: 62, loss = 1.8758328224066645e-12
c619-021: Epoch: 0, Step: 251, Rank: 16, loss = 1.4722347259521484e-05
c613-132: Epoch: 0, Step: 251, Rank: 7, loss = 1.3322676295501878e-15
c621-121: Epoch: 0, Step: 251, Rank: 34, loss = 9.918585419654846e-08
c622-041: Epoch: 0, Step: 251, Rank: 50, loss = 1.3589129821411916e-13
c621-142: Epoch: 0, Step: 251, Rank: 39, loss = 7.386127300057499e-19
c619-011: Epoch: 0, Step: 251, Rank: 14, loss = 7.66053886991358e-15
c613-131: Epoch: 0, Step: 251, Rank: 6, loss = 1.3589129821411916e-13
c622-081: Epoch: 0, Step: 251, Rank: 58, loss = 0.0002956390380859375
c622-012: Epoch: 0, Step: 251, Rank: 45, loss = 1.9081958235744878e-17
c621-122: Epoch: 0, Step: 251, Rank: 35, loss = 2.710505431213761e-19
c621-102: Epoch: 0, Step: 251, Rank: 31, loss = 2.3245294578089215e-16
c613-151: Epoch: 0, Step: 251, Rank: 10, loss = 4.843059286940843e-11
c613-142: Epoch: 0, Step: 251, Rank: 9, loss = 7.270385540061815e-33
c619-022: Epoch: 0, Step: 251, Rank: 17, loss = 4.831773044478697e-30
c619-032: Epoch: 0, Step: 251, Rank: 19, loss = 2.1736923372372985e-10
c622-022: Epoch: 0, Step: 251, Rank: 47, loss = 1.3322676295501878e-15
c621-052: Epoch: 0, Step: 251, Rank: 21, loss = 0.00020313262939453125
c621-071: Epoch: 0, Step: 251, Rank: 24, loss = 8.998878031629687e-18
c621-132: Epoch: 0, Step: 251, Rank: 37, loss = 3.4897757426877174e-19
c613-121: Epoch: 0, Step: 251, Rank: 4, loss = 2.540190280342358e-13
c613-111: Epoch: 0, Step: 251, Rank: 2, loss = 1.199040866595169e-13
c622-061: Epoch: 0, Step: 251, Rank: 54, loss = 9.370282327836321e-14
c613-152: Epoch: 0, Step: 251, Rank: 11, loss = 2.4158453015843406e-12
c621-061: Epoch: 0, Step: 251, Rank: 22, loss = 6.5267086029052734e-06
c622-011: Epoch: 0, Step: 251, Rank: 44, loss = 9.880984919163893e-15
c613-112: Epoch: 0, Step: 251, Rank: 3, loss = 3.8163916471489756e-16
c622-031: Epoch: 0, Step: 251, Rank: 48, loss = 0.0
c621-062: Epoch: 0, Step: 251, Rank: 23, loss = 0.69140625
c622-082: Epoch: 0, Step: 251, Rank: 59, loss = 2.9976945370435715e-09
c621-092: Epoch: 0, Step: 251, Rank: 29, loss = 1.9806378759312793e-13
c622-042: Epoch: 0, Step: 251, Rank: 51, loss = 3.268496584496461e-13
c622-032: Epoch: 0, Step: 251, Rank: 49, loss = 6.993104012531504e-18
c621-072: Epoch: 0, Step: 251, Rank: 25, loss = 0.0
c621-112: Epoch: 0, Step: 251, Rank: 33, loss = 1.199040866595169e-13
c622-021: Epoch: 0, Step: 251, Rank: 46, loss = 1.7229467630386353e-08
c613-122: Epoch: 0, Step: 251, Rank: 5, loss = 3.774403012357652e-11
c622-102: Epoch: 0, Step: 251, Rank: 63, loss = 4.602043190971017e-10
c613-141: Epoch: 0, Step: 251, Rank: 8, loss = 2.204051907791789e-39
c621-141: Epoch: 0, Step: 251, Rank: 38, loss = 8.412825991399586e-12
c622-092: Epoch: 0, Step: 251, Rank: 61, loss = 1.84297022087776e-14
c622-091: Epoch: 0, Step: 251, Rank: 60, loss = 0.04638671875
c613-102: Epoch: 0, Step: 251, Rank: 1, loss = 4.274625098332763e-11
c619-012: Epoch: 0, Step: 251, Rank: 15, loss = 1.4637180356658064e-12
c622-072: Epoch: 0, Step: 251, Rank: 57, loss = 1.8758328224066645e-12
c622-071: Epoch: 0, Step: 251, Rank: 56, loss = 2.4318695068359375e-05
c621-131: Epoch: 0, Step: 251, Rank: 36, loss = 2.261821987449685e-25
c622-062: Epoch: 0, Step: 251, Rank: 55, loss = 1.485356976305141e-17
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 252, Rank: 43, loss = 2.2065682614424986e-15
c622-001: Epoch: 0, Step: 252, Rank: 42, loss = 0.0
c622-052: Epoch: 0, Step: 252, Rank: 53, loss = 3.441691376337985e-14
c621-142: Epoch: 0, Step: 252, Rank: 39, loss = 5.0182080713057076e-14
c621-151: Epoch: 0, Step: 252, Rank: 40, loss = 1.84297022087776e-14
c622-012: Epoch: 0, Step: 252, Rank: 45, loss = 0.0
c613-101: Epoch: 0, Step: 252, Rank: 0, loss = 6.891787052154541e-07
c622-031: Epoch: 0, Step: 252, Rank: 48, loss = 6.635317295611287e-17
c622-062: Epoch: 0, Step: 252, Rank: 55, loss = 2.3647750424515834e-14
c621-152: Epoch: 0, Step: 252, Rank: 41, loss = 1.2197274440461925e-18
c621-111: Epoch: 0, Step: 252, Rank: 32, loss = 5.44811591673966e-18
c622-021: Epoch: 0, Step: 252, Rank: 46, loss = 3.268496584496461e-13
c622-022: Epoch: 0, Step: 252, Rank: 47, loss = 2.540190280342358e-13
c622-051: Epoch: 0, Step: 252, Rank: 52, loss = 4.602043190971017e-10
c621-132: Epoch: 0, Step: 252, Rank: 37, loss = 4.843059286940843e-11
c621-122: Epoch: 0, Step: 252, Rank: 35, loss = 2.574980159653073e-18
c622-041: Epoch: 0, Step: 252, Rank: 50, loss = 5.0182080713057076e-14
c621-091: Epoch: 0, Step: 252, Rank: 28, loss = 1.6672859221771964e-24
c622-011: Epoch: 0, Step: 252, Rank: 44, loss = 0.0
c622-032: Epoch: 0, Step: 252, Rank: 49, loss = 6.439293542825908e-14
c621-082: Epoch: 0, Step: 252, Rank: 27, loss = 6.629788138637702e-36
c621-141: Epoch: 0, Step: 252, Rank: 38, loss = 7.194411455615628e-28
c621-101: Epoch: 0, Step: 252, Rank: 30, loss = 2.574980159653073e-18
c622-061: Epoch: 0, Step: 252, Rank: 54, loss = 1.6209256159527285e-14
c622-081: Epoch: 0, Step: 252, Rank: 58, loss = 0.69140625
c621-121: Epoch: 0, Step: 252, Rank: 34, loss = 9.994988777600744e-20
c622-102: Epoch: 0, Step: 252, Rank: 63, loss = 3.268496584496461e-13
c613-152: Epoch: 0, Step: 252, Rank: 11, loss = 1.8758328224066645e-12
c613-132: Epoch: 0, Step: 252, Rank: 7, loss = 1.9806378759312793e-13
c621-102: Epoch: 0, Step: 252, Rank: 31, loss = 9.370282327836321e-14
c621-092: Epoch: 0, Step: 252, Rank: 29, loss = 0.0
c619-001: Epoch: 0, Step: 252, Rank: 12, loss = 2.3647750424515834e-14
c622-101: Epoch: 0, Step: 252, Rank: 62, loss = 1.7497114868092467e-13
c613-151: Epoch: 0, Step: 252, Rank: 10, loss = 1.0089706847793423e-12
c622-042: Epoch: 0, Step: 252, Rank: 51, loss = 1.4051260155412137e-16
c613-111: Epoch: 0, Step: 252, Rank: 2, loss = 1.6432439176733427e-19
c613-122: Epoch: 0, Step: 252, Rank: 5, loss = 3.342393029015511e-11
c613-131: Epoch: 0, Step: 252, Rank: 6, loss = 7.283063041541027e-14
c621-081: Epoch: 0, Step: 252, Rank: 26, loss = 0.0
c613-112: Epoch: 0, Step: 252, Rank: 3, loss = 1.1864468014524018e-27
c622-071: Epoch: 0, Step: 252, Rank: 56, loss = 1.4915713109076023e-10
c613-141: Epoch: 0, Step: 252, Rank: 8, loss = 1.955777406692505e-08
c613-102: Epoch: 0, Step: 252, Rank: 1, loss = 0.69140625
c621-112: Epoch: 0, Step: 252, Rank: 33, loss = 1.2790197503539935e-19
c613-142: Epoch: 0, Step: 252, Rank: 9, loss = 1.9806378759312793e-13
c613-121: Epoch: 0, Step: 252, Rank: 4, loss = 0.0
c621-072: Epoch: 0, Step: 252, Rank: 25, loss = 0.69140625
c622-072: Epoch: 0, Step: 252, Rank: 57, loss = 0.69140625
c621-131: Epoch: 0, Step: 252, Rank: 36, loss = 1.2197274440461925e-18
c622-091: Epoch: 0, Step: 252, Rank: 60, loss = 4.6629367034256575e-15
c622-082: Epoch: 0, Step: 252, Rank: 59, loss = 2.1047890186309814e-07
c622-092: Epoch: 0, Step: 252, Rank: 61, loss = 0.69140625
c621-071: Epoch: 0, Step: 252, Rank: 24, loss = 1.7229467630386353e-08
c619-002: Epoch: 0, Step: 252, Rank: 13, loss = 0.03466796875
c621-052: Epoch: 0, Step: 252, Rank: 21, loss = 1.7139067942650854e-15
c621-062: Epoch: 0, Step: 252, Rank: 23, loss = 0.0
c621-061: Epoch: 0, Step: 252, Rank: 22, loss = 2.710505431213761e-19
c619-041: Epoch: 0, Step: 252, Rank: 20, loss = 6.439293542825908e-14
c619-021: Epoch: 0, Step: 252, Rank: 16, loss = 7.566995918750763e-10
c619-011: Epoch: 0, Step: 252, Rank: 14, loss = 3.8163916471489756e-16
c619-032: Epoch: 0, Step: 252, Rank: 19, loss = 2.3647750424515834e-14
c619-031: Epoch: 0, Step: 252, Rank: 18, loss = 0.0
c619-022: Epoch: 0, Step: 252, Rank: 17, loss = 8.998878031629687e-18
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c619-012: Epoch: 0, Step: 252, Rank: 15, loss = 0.69140625
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c619-021: Epoch: 0, Step: 253, Rank: 16, loss = 1.6079866327345371e-09
c619-022: Epoch: 0, Step: 253, Rank: 17, loss = 4.0862722260119567e-22
c621-072: Epoch: 0, Step: 253, Rank: 25, loss = 7.66053886991358e-15
c619-012: Epoch: 0, Step: 253, Rank: 15, loss = 0.69140625
c621-081: Epoch: 0, Step: 253, Rank: 26, loss = 1.2656542480726785e-14
c619-001: Epoch: 0, Step: 253, Rank: 12, loss = 1.0408340855860843e-15
c619-002: Epoch: 0, Step: 253, Rank: 13, loss = 3.510081114654895e-12
c619-032: Epoch: 0, Step: 253, Rank: 19, loss = 3.635980405647388e-15
c613-121: Epoch: 0, Step: 253, Rank: 4, loss = 8.404254913330078e-06
c619-031: Epoch: 0, Step: 253, Rank: 18, loss = 2.355810384551023e-21
c621-062: Epoch: 0, Step: 253, Rank: 23, loss = 1.84297022087776e-14
c613-101: Epoch: 0, Step: 253, Rank: 0, loss = 0.0
c619-011: Epoch: 0, Step: 253, Rank: 14, loss = 7.188646122813225e-09
c621-071: Epoch: 0, Step: 253, Rank: 24, loss = 1.0408340855860843e-15
c613-152: Epoch: 0, Step: 253, Rank: 11, loss = 0.0
c621-052: Epoch: 0, Step: 253, Rank: 21, loss = 5.893525667488575e-10
c613-151: Epoch: 0, Step: 253, Rank: 10, loss = 5.115907697472721e-12
c613-122: Epoch: 0, Step: 253, Rank: 5, loss = 4.231929779052734e-06
c613-111: Epoch: 0, Step: 253, Rank: 2, loss = 8.307397365570068e-07
c613-132: Epoch: 0, Step: 253, Rank: 7, loss = 2.9335764912906377e-30
c621-091: Epoch: 0, Step: 253, Rank: 28, loss = 8.881784197001252e-13
c613-131: Epoch: 0, Step: 253, Rank: 6, loss = 9.370282327836321e-14
c622-002: Epoch: 0, Step: 253, Rank: 43, loss = 8.412825991399586e-12
c621-061: Epoch: 0, Step: 253, Rank: 22, loss = 5.995204332975845e-15
c622-081: Epoch: 0, Step: 253, Rank: 58, loss = 3.4051481634378433e-09
c613-142: Epoch: 0, Step: 253, Rank: 9, loss = 8.998878031629687e-18
c619-041: Epoch: 0, Step: 253, Rank: 20, loss = 1.1874362826347351e-08
c622-001: Epoch: 0, Step: 253, Rank: 42, loss = 2.9976945370435715e-09
c622-101: Epoch: 0, Step: 253, Rank: 62, loss = 1.053497228147536e-20
c622-012: Epoch: 0, Step: 253, Rank: 45, loss = 4.843059286940843e-11
c621-111: Epoch: 0, Step: 253, Rank: 32, loss = 1.0477378964424133e-08
c621-151: Epoch: 0, Step: 253, Rank: 40, loss = 0.0
c621-121: Epoch: 0, Step: 253, Rank: 34, loss = 5.182486384480711e-17
c613-102: Epoch: 0, Step: 253, Rank: 1, loss = 1.601387637598654e-28
c613-112: Epoch: 0, Step: 253, Rank: 3, loss = 0.69140625
c621-132: Epoch: 0, Step: 253, Rank: 37, loss = 0.0986328125
c621-101: Epoch: 0, Step: 253, Rank: 30, loss = 1.4051260155412137e-16
c613-141: Epoch: 0, Step: 253, Rank: 8, loss = 4.418687638008123e-14
c621-142: Epoch: 0, Step: 253, Rank: 39, loss = 0.0
c621-082: Epoch: 0, Step: 253, Rank: 27, loss = 0.0
c621-112: Epoch: 0, Step: 253, Rank: 33, loss = 3.5695955961250784e-29
c621-131: Epoch: 0, Step: 253, Rank: 36, loss = 2.5920599000528455e-11
c622-092: Epoch: 0, Step: 253, Rank: 61, loss = 1.4637180356658064e-12
c622-022: Epoch: 0, Step: 253, Rank: 47, loss = 3.441691376337985e-14
c621-102: Epoch: 0, Step: 253, Rank: 31, loss = 1.2931877790833823e-12
c621-152: Epoch: 0, Step: 253, Rank: 41, loss = 0.0
c621-122: Epoch: 0, Step: 253, Rank: 35, loss = 0.69140625
c622-091: Epoch: 0, Step: 253, Rank: 60, loss = 0.0
c622-011: Epoch: 0, Step: 253, Rank: 44, loss = 4.6798959374427795e-08
c622-061: Epoch: 0, Step: 253, Rank: 54, loss = 0.0
c622-072: Epoch: 0, Step: 253, Rank: 57, loss = 6.993104012531504e-18
c622-071: Epoch: 0, Step: 253, Rank: 56, loss = 3.774403012357652e-11
c622-062: Epoch: 0, Step: 253, Rank: 55, loss = 1.2278178473934531e-11
c622-021: Epoch: 0, Step: 253, Rank: 46, loss = 3.510081114654895e-12
c622-102: Epoch: 0, Step: 253, Rank: 63, loss = 1.126900315284729e-07
c622-082: Epoch: 0, Step: 253, Rank: 59, loss = 0.0
c621-141: Epoch: 0, Step: 253, Rank: 38, loss = 1.0800249583553523e-11
c622-052: Epoch: 0, Step: 253, Rank: 53, loss = 1.7497114868092467e-13
c622-031: Epoch: 0, Step: 253, Rank: 48, loss = 3.54136699749265e-24
c622-032: Epoch: 0, Step: 253, Rank: 49, loss = 4.926614671774132e-16
c622-051: Epoch: 0, Step: 253, Rank: 52, loss = 1.0800249583553523e-11
c621-092: Epoch: 0, Step: 253, Rank: 29, loss = 2.2065682614424986e-15
c622-041: Epoch: 0, Step: 253, Rank: 50, loss = 8.307397365570068e-07
c622-042: Epoch: 0, Step: 253, Rank: 51, loss = 1.0089706847793423e-12
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24853515625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.46s, TFLOPs: 0.77, Samples/sec: 0.41, Time/seq 2.46s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 254, Rank: 28, loss = 9.880984919163893e-15
c621-092: Epoch: 0, Step: 254, Rank: 29, loss = 2.648448571562767e-09
c621-082: Epoch: 0, Step: 254, Rank: 27, loss = 0.69140625
c621-072: Epoch: 0, Step: 254, Rank: 25, loss = 0.0
c621-081: Epoch: 0, Step: 254, Rank: 26, loss = 1.2993812561035156e-05
c621-101: Epoch: 0, Step: 254, Rank: 30, loss = 2.7284841053187847e-12
c621-071: Epoch: 0, Step: 254, Rank: 24, loss = 2.4158453015843406e-12
c621-062: Epoch: 0, Step: 254, Rank: 23, loss = 1.3869794202037156e-11
c621-052: Epoch: 0, Step: 254, Rank: 21, loss = 2.710505431213761e-19
c621-061: Epoch: 0, Step: 254, Rank: 22, loss = 4.4517219066619873e-07
c621-102: Epoch: 0, Step: 254, Rank: 31, loss = 1.0089706847793423e-12
c619-041: Epoch: 0, Step: 254, Rank: 20, loss = 1.4051260155412137e-16
c621-111: Epoch: 0, Step: 254, Rank: 32, loss = 0.0
c619-032: Epoch: 0, Step: 254, Rank: 19, loss = 0.0002307891845703125
c619-021: Epoch: 0, Step: 254, Rank: 16, loss = 1.0408340855860843e-15
c619-031: Epoch: 0, Step: 254, Rank: 18, loss = 1.8758328224066645e-12
c619-022: Epoch: 0, Step: 254, Rank: 17, loss = 2.2065682614424986e-15
c619-012: Epoch: 0, Step: 254, Rank: 15, loss = 5.0182080713057076e-14
c619-002: Epoch: 0, Step: 254, Rank: 13, loss = 9.742432179479496e-29
c619-011: Epoch: 0, Step: 254, Rank: 14, loss = 0.0
c621-112: Epoch: 0, Step: 254, Rank: 33, loss = 0.0
c619-001: Epoch: 0, Step: 254, Rank: 12, loss = 4.418687638008123e-14
c613-152: Epoch: 0, Step: 254, Rank: 11, loss = 2.831068712794149e-15
c613-151: Epoch: 0, Step: 254, Rank: 10, loss = 0.0
c621-121: Epoch: 0, Step: 254, Rank: 34, loss = 2.831068712794149e-15
c613-142: Epoch: 0, Step: 254, Rank: 9, loss = 4.00543212890625e-05
c613-132: Epoch: 0, Step: 254, Rank: 7, loss = 2.5920599000528455e-11
c613-141: Epoch: 0, Step: 254, Rank: 8, loss = 2.983724378680108e-16
c613-131: Epoch: 0, Step: 254, Rank: 6, loss = 2.0057740190981832e-18
c613-122: Epoch: 0, Step: 254, Rank: 5, loss = 4.418687638008123e-14
c621-122: Epoch: 0, Step: 254, Rank: 35, loss = 4.602043190971017e-10
c613-121: Epoch: 0, Step: 254, Rank: 4, loss = 7.338821887969971e-07
c613-112: Epoch: 0, Step: 254, Rank: 3, loss = 5.400124791776761e-13
c613-111: Epoch: 0, Step: 254, Rank: 2, loss = 1.4915713109076023e-10
c613-101: Epoch: 0, Step: 254, Rank: 0, loss = 5.400124791776761e-13
c622-102: Epoch: 0, Step: 254, Rank: 63, loss = 0.69140625
c613-102: Epoch: 0, Step: 254, Rank: 1, loss = 6.993104012531504e-18
c622-101: Epoch: 0, Step: 254, Rank: 62, loss = 0.000179290771484375
c621-131: Epoch: 0, Step: 254, Rank: 36, loss = 1.6916601452976465e-10
c622-092: Epoch: 0, Step: 254, Rank: 61, loss = 4.0332320816460765e-17
c622-091: Epoch: 0, Step: 254, Rank: 60, loss = 3.441691376337985e-14
c622-081: Epoch: 0, Step: 254, Rank: 58, loss = 1.053497228147536e-20
c622-082: Epoch: 0, Step: 254, Rank: 59, loss = 5.182486384480711e-17
c622-072: Epoch: 0, Step: 254, Rank: 57, loss = 1.7497114868092467e-13
c622-071: Epoch: 0, Step: 254, Rank: 56, loss = 1.2931877790833823e-12
c622-062: Epoch: 0, Step: 254, Rank: 55, loss = 8.881784197001252e-13
c622-052: Epoch: 0, Step: 254, Rank: 53, loss = 2.831068712794149e-15
c621-132: Epoch: 0, Step: 254, Rank: 37, loss = 1.0972125985553305e-16
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.244140625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-061: Epoch: 0, Step: 254, Rank: 54, loss = 5.0182080713057076e-14
c622-051: Epoch: 0, Step: 254, Rank: 52, loss = 2.3245294578089215e-16
c622-042: Epoch: 0, Step: 254, Rank: 51, loss = 7.987216665362745e-30
c622-032: Epoch: 0, Step: 254, Rank: 49, loss = 3.268496584496461e-13
c622-041: Epoch: 0, Step: 254, Rank: 50, loss = 9.74978320300579e-10
c621-141: Epoch: 0, Step: 254, Rank: 38, loss = 4.00543212890625e-05
c622-031: Epoch: 0, Step: 254, Rank: 48, loss = 1.7139067942650854e-15
c622-012: Epoch: 0, Step: 254, Rank: 45, loss = 1.9806378759312793e-13
c622-022: Epoch: 0, Step: 254, Rank: 47, loss = 8.543513119185775e-17
c622-021: Epoch: 0, Step: 254, Rank: 46, loss = 3.979039320256561e-12
c622-002: Epoch: 0, Step: 254, Rank: 43, loss = 2.6756374893466273e-14
c621-142: Epoch: 0, Step: 254, Rank: 39, loss = 5.759824041329242e-19
c622-011: Epoch: 0, Step: 254, Rank: 44, loss = 0.00116729736328125
c621-151: Epoch: 0, Step: 254, Rank: 40, loss = 6.927791673660977e-13
c621-152: Epoch: 0, Step: 254, Rank: 41, loss = 3.268496584496461e-13
c622-001: Epoch: 0, Step: 254, Rank: 42, loss = 1.7497114868092467e-13
c613-101: Model Parameters: 7.505 B, Latency: 2.02s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.02s, Batch Size: 1, Sequence Length: 2048
c621-111: Epoch: 0, Step: 255, Rank: 32, loss = 5.115907697472721e-12
c622-002: Epoch: 0, Step: 255, Rank: 43, loss = 2.203125
c619-021: Epoch: 0, Step: 255, Rank: 16, loss = 1.0408340855860843e-15
c619-002: Epoch: 0, Step: 255, Rank: 13, loss = 9.5367431640625e-06
c622-012: Epoch: 0, Step: 255, Rank: 45, loss = 5.699694156646729e-07
c622-052: Epoch: 0, Step: 255, Rank: 53, loss = 4.255493527005605e-18
c622-051: Epoch: 0, Step: 255, Rank: 52, loss = 2.9331204132176936e-11
c613-101: Epoch: 0, Step: 255, Rank: 0, loss = 2.9331204132176936e-11
c619-032: Epoch: 0, Step: 255, Rank: 19, loss = 1.3589129821411916e-13
c621-072: Epoch: 0, Step: 255, Rank: 25, loss = 2.831068712794149e-15
c621-052: Epoch: 0, Step: 255, Rank: 21, loss = 3.268496584496461e-13
c621-091: Epoch: 0, Step: 255, Rank: 28, loss = 0.0
c621-122: Epoch: 0, Step: 255, Rank: 35, loss = 1.4051260155412137e-16
c619-012: Epoch: 0, Step: 255, Rank: 15, loss = 1.6689300537109375e-05
c621-121: Epoch: 0, Step: 255, Rank: 34, loss = 1.0089706847793423e-12
c621-101: Epoch: 0, Step: 255, Rank: 30, loss = 3.144186300207963e-17
c622-001: Epoch: 0, Step: 255, Rank: 42, loss = 6.439293542825908e-14
c613-121: Epoch: 0, Step: 255, Rank: 4, loss = 1.8189894035458565e-09
c621-132: Epoch: 0, Step: 255, Rank: 37, loss = 8.754432201385498e-08
c622-042: Epoch: 0, Step: 255, Rank: 51, loss = 0.0
c622-022: Epoch: 0, Step: 255, Rank: 47, loss = 1.6432439176733427e-19
c619-011: Epoch: 0, Step: 255, Rank: 14, loss = 3.774403012357652e-11
c621-151: Epoch: 0, Step: 255, Rank: 40, loss = 2.4158453015843406e-12
c613-142: Epoch: 0, Step: 255, Rank: 9, loss = 1.8189894035458565e-09
c621-082: Epoch: 0, Step: 255, Rank: 27, loss = 1.4051260155412137e-16
c619-001: Epoch: 0, Step: 255, Rank: 12, loss = 0.00408935546875
c613-152: Epoch: 0, Step: 255, Rank: 11, loss = 5.14984130859375e-05
c619-041: Epoch: 0, Step: 255, Rank: 20, loss = 3.1650415621697903e-10
c621-061: Epoch: 0, Step: 255, Rank: 22, loss = 7.867813110351562e-06
c613-122: Epoch: 0, Step: 255, Rank: 5, loss = 5.893525667488575e-10
c622-061: Epoch: 0, Step: 255, Rank: 54, loss = 2.4158453015843406e-12
c622-062: Epoch: 0, Step: 255, Rank: 55, loss = 4.760636329592671e-13
c622-011: Epoch: 0, Step: 255, Rank: 44, loss = 1.4915713109076023e-10
c613-132: Epoch: 0, Step: 255, Rank: 7, loss = 1.318767317570746e-10
c622-041: Epoch: 0, Step: 255, Rank: 50, loss = 7.792703114739563e-20
c622-101: Epoch: 0, Step: 255, Rank: 62, loss = 2.648448571562767e-09
c621-112: Epoch: 0, Step: 255, Rank: 33, loss = 4.0862722260119567e-22
c613-151: Epoch: 0, Step: 255, Rank: 10, loss = 1.9806378759312793e-13
c621-071: Epoch: 0, Step: 255, Rank: 24, loss = 0.69140625
c621-081: Epoch: 0, Step: 255, Rank: 26, loss = 6.565414878423326e-12
c619-022: Epoch: 0, Step: 255, Rank: 17, loss = 7.44648787076585e-12
c621-062: Epoch: 0, Step: 255, Rank: 23, loss = 2.5331974029541016e-07
c622-031: Epoch: 0, Step: 255, Rank: 48, loss = 0.0
c622-071: Epoch: 0, Step: 255, Rank: 56, loss = 4.0332320816460765e-17
c621-092: Epoch: 0, Step: 255, Rank: 29, loss = 5.115907697472721e-12
c621-141: Epoch: 0, Step: 255, Rank: 38, loss = 9.370282327836321e-14
c613-131: Epoch: 0, Step: 255, Rank: 6, loss = 0.0
c621-131: Epoch: 0, Step: 255, Rank: 36, loss = 0.056640625
c613-111: Epoch: 0, Step: 255, Rank: 2, loss = 2.6756374893466273e-14
c619-031: Epoch: 0, Step: 255, Rank: 18, loss = 6.314393452555578e-16
c622-032: Epoch: 0, Step: 255, Rank: 49, loss = 1.0089706847793423e-12
c613-141: Epoch: 0, Step: 255, Rank: 8, loss = 1.3589129821411916e-13
c622-102: Epoch: 0, Step: 255, Rank: 63, loss = 1.7497114868092467e-13
c622-021: Epoch: 0, Step: 255, Rank: 46, loss = 3.510081114654895e-12
c613-112: Epoch: 0, Step: 255, Rank: 3, loss = 6.439293542825908e-14
c621-152: Epoch: 0, Step: 255, Rank: 41, loss = 4.255493527005605e-18
c622-081: Epoch: 0, Step: 255, Rank: 58, loss = 0.0
c622-092: Epoch: 0, Step: 255, Rank: 61, loss = 2.726912498474121e-06
c621-142: Epoch: 0, Step: 255, Rank: 39, loss = 2.7830537874251604e-10
c613-102: Epoch: 0, Step: 255, Rank: 1, loss = 1.3589129821411916e-13
c622-072: Epoch: 0, Step: 255, Rank: 57, loss = 1.996755599975586e-06
c621-102: Epoch: 0, Step: 255, Rank: 31, loss = 1.1874362826347351e-08
c622-091: Epoch: 0, Step: 255, Rank: 60, loss = 2.7284841053187847e-12
c622-082: Epoch: 0, Step: 255, Rank: 59, loss = 6.314393452555578e-16
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.13s, TFLOPs: 0.88, Samples/sec: 0.47, Time/seq 2.13s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 256, Rank: 0, loss = 6.927791673660977e-13
c619-002: Epoch: 0, Step: 256, Rank: 13, loss = 7.566995918750763e-10
c622-081: Epoch: 0, Step: 256, Rank: 58, loss = 3.583409124985337e-10
c613-112: Epoch: 0, Step: 256, Rank: 3, loss = 1.0477378964424133e-08
c622-101: Epoch: 0, Step: 256, Rank: 62, loss = 2.342858351767063e-09
c619-001: Epoch: 0, Step: 256, Rank: 12, loss = 2.276897430419922e-05
c613-111: Epoch: 0, Step: 256, Rank: 2, loss = 1.318767317570746e-10
c619-021: Epoch: 0, Step: 256, Rank: 16, loss = 1.0972125985553305e-16
c622-002: Epoch: 0, Step: 256, Rank: 43, loss = 7.188646122813225e-09
c613-152: Epoch: 0, Step: 256, Rank: 11, loss = 1.2931877790833823e-12
c622-052: Epoch: 0, Step: 256, Rank: 53, loss = 0.0
c613-121: Epoch: 0, Step: 256, Rank: 4, loss = 2.831068712794149e-15
c622-032: Epoch: 0, Step: 256, Rank: 49, loss = 2.7550648847397363e-40
c622-022: Epoch: 0, Step: 256, Rank: 47, loss = 5.115907697472721e-12
c619-031: Epoch: 0, Step: 256, Rank: 18, loss = 0.0
c622-041: Epoch: 0, Step: 256, Rank: 50, loss = 2.2065682614424986e-15
c613-132: Epoch: 0, Step: 256, Rank: 7, loss = 1.0089706847793423e-12
c622-062: Epoch: 0, Step: 256, Rank: 55, loss = 3.841705620288849e-09
c613-142: Epoch: 0, Step: 256, Rank: 9, loss = 6.565414878423326e-12
c619-011: Epoch: 0, Step: 256, Rank: 14, loss = 2.4158453015843406e-12
c622-031: Epoch: 0, Step: 256, Rank: 48, loss = 4.760636329592671e-13
c613-102: Epoch: 0, Step: 256, Rank: 1, loss = 1.1988913903810543e-32
c622-071: Epoch: 0, Step: 256, Rank: 56, loss = 6.927791673660977e-13
c622-012: Epoch: 0, Step: 256, Rank: 45, loss = 6.927791673660977e-13
c622-082: Epoch: 0, Step: 256, Rank: 59, loss = 9.255018085241318e-09
c622-061: Epoch: 0, Step: 256, Rank: 54, loss = 1.318767317570746e-10
c619-022: Epoch: 0, Step: 256, Rank: 17, loss = 1.0089706847793423e-12
c622-092: Epoch: 0, Step: 256, Rank: 61, loss = 1.3869794202037156e-11
c619-012: Epoch: 0, Step: 256, Rank: 15, loss = 1.3589129821411916e-13
c621-111: Epoch: 0, Step: 256, Rank: 32, loss = 1.787262988391355e-30
c613-131: Epoch: 0, Step: 256, Rank: 6, loss = 1.84297022087776e-14
c622-091: Epoch: 0, Step: 256, Rank: 60, loss = 0.69140625
c621-091: Epoch: 0, Step: 256, Rank: 28, loss = 2.6756374893466273e-14
c622-021: Epoch: 0, Step: 256, Rank: 46, loss = 3.655441105365753e-08
c613-141: Epoch: 0, Step: 256, Rank: 8, loss = 5.3085386753082275e-08
c622-102: Epoch: 0, Step: 256, Rank: 63, loss = 7.867813110351562e-06
c621-072: Epoch: 0, Step: 256, Rank: 25, loss = 1.199040866595169e-13
c613-122: Epoch: 0, Step: 256, Rank: 5, loss = 2.0057740190981832e-18
c622-072: Epoch: 0, Step: 256, Rank: 57, loss = 2.831068712794149e-15
c621-082: Epoch: 0, Step: 256, Rank: 27, loss = 4.301339185275744e-23
c621-151: Epoch: 0, Step: 256, Rank: 40, loss = 1.0800249583553523e-11
c622-051: Epoch: 0, Step: 256, Rank: 52, loss = 1.4915713109076023e-10
c621-102: Epoch: 0, Step: 256, Rank: 31, loss = 3.084540367126465e-06
c621-081: Epoch: 0, Step: 256, Rank: 26, loss = 9.74978320300579e-10
c622-001: Epoch: 0, Step: 256, Rank: 42, loss = 2.5920599000528455e-11
c621-152: Epoch: 0, Step: 256, Rank: 41, loss = 1.4051260155412137e-16
c621-142: Epoch: 0, Step: 256, Rank: 39, loss = 3.3060778616876836e-37
c621-112: Epoch: 0, Step: 256, Rank: 33, loss = 1.55717134475708e-06
c622-011: Epoch: 0, Step: 256, Rank: 44, loss = 3.6845933205562065e-20
c621-132: Epoch: 0, Step: 256, Rank: 37, loss = 3.441691376337985e-14
c621-092: Epoch: 0, Step: 256, Rank: 29, loss = 2.9331204132176936e-11
c621-101: Epoch: 0, Step: 256, Rank: 30, loss = 2.831068712794149e-15
c619-041: Epoch: 0, Step: 256, Rank: 20, loss = 4.3655745685100555e-09
c621-121: Epoch: 0, Step: 256, Rank: 34, loss = 8.66885281955573e-22
c622-042: Epoch: 0, Step: 256, Rank: 51, loss = 6.927791673660977e-13
c621-141: Epoch: 0, Step: 256, Rank: 38, loss = 3.268496584496461e-13
c621-122: Epoch: 0, Step: 256, Rank: 35, loss = 1.0089706847793423e-12
c621-052: Epoch: 0, Step: 256, Rank: 21, loss = 2.574980159653073e-18
c613-151: Epoch: 0, Step: 256, Rank: 10, loss = 1.0089706847793423e-12
c621-071: Epoch: 0, Step: 256, Rank: 24, loss = 1.0972125985553305e-16
c621-062: Epoch: 0, Step: 256, Rank: 23, loss = 6.891787052154541e-07
c619-032: Epoch: 0, Step: 256, Rank: 19, loss = 3.8163916471489756e-16
c621-131: Epoch: 0, Step: 256, Rank: 36, loss = 2.4158453015843406e-12
c621-061: Epoch: 0, Step: 256, Rank: 22, loss = 1.171875
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24267578125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c621-132: Epoch: 0, Step: 257, Rank: 37, loss = 2.2351741790771484e-07
c619-021: Epoch: 0, Step: 257, Rank: 16, loss = 1.3869794202037156e-11
c622-002: Epoch: 0, Step: 257, Rank: 43, loss = 1.1874362826347351e-08
c621-111: Epoch: 0, Step: 257, Rank: 32, loss = 1.857925203976527e-26
c613-101: Epoch: 0, Step: 257, Rank: 0, loss = 3.46451997756958e-07
c621-121: Epoch: 0, Step: 257, Rank: 34, loss = 3.583409124985337e-10
c621-131: Epoch: 0, Step: 257, Rank: 36, loss = 6.565414878423326e-12
c619-041: Epoch: 0, Step: 257, Rank: 20, loss = 2.234049398383217e-20
c619-002: Epoch: 0, Step: 257, Rank: 13, loss = 4.926614671774132e-16
c619-001: Epoch: 0, Step: 257, Rank: 12, loss = 8.083811398051921e-16
c622-101: Epoch: 0, Step: 257, Rank: 62, loss = 3.975119405215255e-31
c621-052: Epoch: 0, Step: 257, Rank: 21, loss = 1.0800249583553523e-11
c622-062: Epoch: 0, Step: 257, Rank: 55, loss = 7.283063041541027e-14
c622-081: Epoch: 0, Step: 257, Rank: 58, loss = 9.370282327836321e-14
c622-052: Epoch: 0, Step: 257, Rank: 53, loss = 1.7497114868092467e-13
c622-012: Epoch: 0, Step: 257, Rank: 45, loss = 1.955777406692505e-08
c621-151: Epoch: 0, Step: 257, Rank: 40, loss = 7.44648787076585e-12
c621-122: Epoch: 0, Step: 257, Rank: 35, loss = 7.188646122813225e-09
c619-031: Epoch: 0, Step: 257, Rank: 18, loss = 1.7139067942650854e-15
c619-011: Epoch: 0, Step: 257, Rank: 14, loss = 0.69140625
c621-142: Epoch: 0, Step: 257, Rank: 39, loss = 2.9331204132176936e-11
c613-111: Epoch: 0, Step: 257, Rank: 2, loss = 0.0
c622-042: Epoch: 0, Step: 257, Rank: 51, loss = 1.84297022087776e-14
c613-152: Epoch: 0, Step: 257, Rank: 11, loss = 1.9806378759312793e-13
c622-071: Epoch: 0, Step: 257, Rank: 56, loss = 7.66053886991358e-15
c622-051: Epoch: 0, Step: 257, Rank: 52, loss = 1.2790197503539935e-19
c619-032: Epoch: 0, Step: 257, Rank: 19, loss = 0.03271484375
c613-121: Epoch: 0, Step: 257, Rank: 4, loss = 1.199040866595169e-13
c613-112: Epoch: 0, Step: 257, Rank: 3, loss = 1.6432439176733427e-19
c621-141: Epoch: 0, Step: 257, Rank: 38, loss = 6.139278411865234e-06
c613-132: Epoch: 0, Step: 257, Rank: 7, loss = 1.2656542480726785e-14
c622-011: Epoch: 0, Step: 257, Rank: 44, loss = 3.8163916471489756e-16
c622-102: Epoch: 0, Step: 257, Rank: 63, loss = 2.1736923372372985e-10
c613-151: Epoch: 0, Step: 257, Rank: 10, loss = 1.1874362826347351e-08
c619-022: Epoch: 0, Step: 257, Rank: 17, loss = 5.995204332975845e-15
c622-022: Epoch: 0, Step: 257, Rank: 47, loss = 8.881784197001252e-13
c621-061: Epoch: 0, Step: 257, Rank: 22, loss = 0.69140625
c622-032: Epoch: 0, Step: 257, Rank: 49, loss = 0.0
c622-021: Epoch: 0, Step: 257, Rank: 46, loss = 1.7497114868092467e-13
c622-001: Epoch: 0, Step: 257, Rank: 42, loss = 9.486769009248164e-19
c621-152: Epoch: 0, Step: 257, Rank: 41, loss = 7.44648787076585e-12
c622-031: Epoch: 0, Step: 257, Rank: 48, loss = 3.694822225952521e-13
c621-112: Epoch: 0, Step: 257, Rank: 33, loss = 2.9331204132176936e-11
c619-012: Epoch: 0, Step: 257, Rank: 15, loss = 1.9081958235744878e-17
c621-081: Epoch: 0, Step: 257, Rank: 26, loss = 1.4051260155412137e-16
c621-082: Epoch: 0, Step: 257, Rank: 27, loss = 7.44648787076585e-12
c621-072: Epoch: 0, Step: 257, Rank: 25, loss = 7.66053886991358e-15
c622-092: Epoch: 0, Step: 257, Rank: 61, loss = 5.20230969414115e-10
c613-131: Epoch: 0, Step: 257, Rank: 6, loss = 5.893525667488575e-10
c621-101: Epoch: 0, Step: 257, Rank: 30, loss = 6.565414878423326e-12
c622-082: Epoch: 0, Step: 257, Rank: 59, loss = 1.6432439176733427e-19
c621-091: Epoch: 0, Step: 257, Rank: 28, loss = 1.7139067942650854e-15
c613-102: Epoch: 0, Step: 257, Rank: 1, loss = 4.926614671774132e-16
c613-122: Epoch: 0, Step: 257, Rank: 5, loss = 2.7284841053187847e-12
c622-072: Epoch: 0, Step: 257, Rank: 57, loss = 3.342393029015511e-11
c622-061: Epoch: 0, Step: 257, Rank: 54, loss = 2.1454997138094155e-24
c621-071: Epoch: 0, Step: 257, Rank: 24, loss = 1.0800249583553523e-11
c613-142: Epoch: 0, Step: 257, Rank: 9, loss = 2.9331204132176936e-11
c622-091: Epoch: 0, Step: 257, Rank: 60, loss = 2.4158453015843406e-12
c621-102: Epoch: 0, Step: 257, Rank: 31, loss = 4.760636329592671e-13
c621-092: Epoch: 0, Step: 257, Rank: 29, loss = 0.0
c613-141: Epoch: 0, Step: 257, Rank: 8, loss = 2.540190280342358e-13
c622-041: Epoch: 0, Step: 257, Rank: 50, loss = 1.9806378759312793e-13
c621-062: Epoch: 0, Step: 257, Rank: 23, loss = 2.2065682614424986e-15
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c621-091: Epoch: 0, Step: 258, Rank: 28, loss = 7.66053886991358e-15
c613-101: Epoch: 0, Step: 258, Rank: 0, loss = 3.54136699749265e-24
c622-052: Epoch: 0, Step: 258, Rank: 53, loss = 0.0
c622-081: Epoch: 0, Step: 258, Rank: 58, loss = 2.9976945370435715e-09
c621-081: Epoch: 0, Step: 258, Rank: 26, loss = 4.926614671774132e-16
c621-111: Epoch: 0, Step: 258, Rank: 32, loss = 0.0
c621-082: Epoch: 0, Step: 258, Rank: 27, loss = 3.268496584496461e-13
c622-012: Epoch: 0, Step: 258, Rank: 45, loss = 3.144186300207963e-17
c619-001: Epoch: 0, Step: 258, Rank: 12, loss = 1.4227506535912076e-21
c622-002: Epoch: 0, Step: 258, Rank: 43, loss = 7.048583938740194e-11
c621-072: Epoch: 0, Step: 258, Rank: 25, loss = 1.955777406692505e-08
c622-051: Epoch: 0, Step: 258, Rank: 52, loss = 3.268496584496461e-13
c622-101: Epoch: 0, Step: 258, Rank: 62, loss = 1.9806378759312793e-13
c622-072: Epoch: 0, Step: 258, Rank: 57, loss = 1.4051260155412137e-16
c619-011: Epoch: 0, Step: 258, Rank: 14, loss = 1.6438553812280427e-38
c621-151: Epoch: 0, Step: 258, Rank: 40, loss = 0.69140625
c622-062: Epoch: 0, Step: 258, Rank: 55, loss = 5.182486384480711e-17
c622-001: Epoch: 0, Step: 258, Rank: 42, loss = 1.84297022087776e-14
c621-121: Epoch: 0, Step: 258, Rank: 34, loss = 1.318767317570746e-10
c613-132: Epoch: 0, Step: 258, Rank: 7, loss = 2.3245294578089215e-16
c613-122: Epoch: 0, Step: 258, Rank: 5, loss = 1.2790197503539935e-19
c622-061: Epoch: 0, Step: 258, Rank: 54, loss = 3.268496584496461e-13
c613-111: Epoch: 0, Step: 258, Rank: 2, loss = 0.0
c621-152: Epoch: 0, Step: 258, Rank: 41, loss = 3.635980405647388e-15
c613-131: Epoch: 0, Step: 258, Rank: 6, loss = 7.66053886991358e-15
c621-132: Epoch: 0, Step: 258, Rank: 37, loss = 1.4637180356658064e-12
c613-152: Epoch: 0, Step: 258, Rank: 11, loss = 4.172325134277344e-07
c622-011: Epoch: 0, Step: 258, Rank: 44, loss = 3.688037395477295e-07
c622-041: Epoch: 0, Step: 258, Rank: 50, loss = 2.8919009696678116e-25
c619-021: Epoch: 0, Step: 258, Rank: 16, loss = 5.115907697472721e-12
c619-002: Epoch: 0, Step: 258, Rank: 13, loss = 1.84297022087776e-14
c619-031: Epoch: 0, Step: 258, Rank: 18, loss = 2.831068712794149e-15
c621-142: Epoch: 0, Step: 258, Rank: 39, loss = 1.4915713109076023e-10
c621-052: Epoch: 0, Step: 258, Rank: 21, loss = 2.540190280342358e-13
c621-061: Epoch: 0, Step: 258, Rank: 22, loss = 1.7497114868092467e-13
c621-101: Epoch: 0, Step: 258, Rank: 30, loss = 2.9331204132176936e-11
c622-082: Epoch: 0, Step: 258, Rank: 59, loss = 2.9331204132176936e-11
c622-102: Epoch: 0, Step: 258, Rank: 63, loss = 5.115907697472721e-12
c622-092: Epoch: 0, Step: 258, Rank: 61, loss = 4.782137916322191e-25
c619-041: Epoch: 0, Step: 258, Rank: 20, loss = 0.000148773193359375
c622-042: Epoch: 0, Step: 258, Rank: 51, loss = 1.7848833522293717e-11
c613-151: Epoch: 0, Step: 258, Rank: 10, loss = 1.3322676295501878e-15
c613-142: Epoch: 0, Step: 258, Rank: 9, loss = 1.1874362826347351e-08
c619-022: Epoch: 0, Step: 258, Rank: 17, loss = 2.7284841053187847e-12
c622-071: Epoch: 0, Step: 258, Rank: 56, loss = 1.4051260155412137e-16
c613-102: Epoch: 0, Step: 258, Rank: 1, loss = 1.996755599975586e-06
c622-021: Epoch: 0, Step: 258, Rank: 46, loss = 1.996755599975586e-06
c621-141: Epoch: 0, Step: 258, Rank: 38, loss = 0.0
c621-062: Epoch: 0, Step: 258, Rank: 23, loss = 3.774403012357652e-11
c621-131: Epoch: 0, Step: 258, Rank: 36, loss = 2.831068712794149e-15
c622-022: Epoch: 0, Step: 258, Rank: 47, loss = 2.3245294578089215e-16
c619-012: Epoch: 0, Step: 258, Rank: 15, loss = 6.439293542825908e-14
c622-032: Epoch: 0, Step: 258, Rank: 49, loss = 0.002899169921875
c622-031: Epoch: 0, Step: 258, Rank: 48, loss = 1.7139067942650854e-15
c613-121: Epoch: 0, Step: 258, Rank: 4, loss = 1.7848833522293717e-11
c613-141: Epoch: 0, Step: 258, Rank: 8, loss = 2.831068712794149e-15
c621-112: Epoch: 0, Step: 258, Rank: 33, loss = 3.268496584496461e-13
c621-071: Epoch: 0, Step: 258, Rank: 24, loss = 5.115907697472721e-12
c621-102: Epoch: 0, Step: 258, Rank: 31, loss = 7.188646122813225e-09
c621-092: Epoch: 0, Step: 258, Rank: 29, loss = 2.574980159653073e-18
c619-032: Epoch: 0, Step: 258, Rank: 19, loss = 8.881784197001252e-13
c622-091: Epoch: 0, Step: 258, Rank: 60, loss = 1.9806378759312793e-13
c621-122: Epoch: 0, Step: 258, Rank: 35, loss = 0.0
c613-112: Epoch: 0, Step: 258, Rank: 3, loss = 1.955777406692505e-08
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.48, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c622-081: Epoch: 0, Step: 259, Rank: 58, loss = 4.760636329592671e-13
c619-021: Epoch: 0, Step: 259, Rank: 16, loss = 0.69140625
c622-052: Epoch: 0, Step: 259, Rank: 53, loss = 2.3647750424515834e-14
c619-002: Epoch: 0, Step: 259, Rank: 13, loss = 0.0
c613-101: Epoch: 0, Step: 259, Rank: 0, loss = 3.864587821847745e-21
c622-002: Epoch: 0, Step: 259, Rank: 43, loss = 1.418811734765768e-09
c621-111: Epoch: 0, Step: 259, Rank: 32, loss = 2.4158453015843406e-12
c613-151: Epoch: 0, Step: 259, Rank: 10, loss = 1.6209256159527285e-14
c613-121: Epoch: 0, Step: 259, Rank: 4, loss = 2.455635694786906e-10
c621-091: Epoch: 0, Step: 259, Rank: 28, loss = 0.69140625
c622-061: Epoch: 0, Step: 259, Rank: 54, loss = 6.230038707144558e-11
c619-011: Epoch: 0, Step: 259, Rank: 14, loss = 4.760636329592671e-13
c619-001: Epoch: 0, Step: 259, Rank: 12, loss = 4.3655745685100555e-09
c619-031: Epoch: 0, Step: 259, Rank: 18, loss = 0.69140625
c613-152: Epoch: 0, Step: 259, Rank: 11, loss = 3.268496584496461e-13
c622-101: Epoch: 0, Step: 259, Rank: 62, loss = 9.74978320300579e-10
c621-151: Epoch: 0, Step: 259, Rank: 40, loss = 4.602043190971017e-10
c622-041: Epoch: 0, Step: 259, Rank: 50, loss = 1.565316886525947e-18
c613-132: Epoch: 0, Step: 259, Rank: 7, loss = 3.694822225952521e-13
c622-012: Epoch: 0, Step: 259, Rank: 45, loss = 6.927791673660977e-13
c622-102: Epoch: 0, Step: 259, Rank: 63, loss = 2.574980159653073e-18
c621-142: Epoch: 0, Step: 259, Rank: 39, loss = 4.418687638008123e-14
c621-082: Epoch: 0, Step: 259, Rank: 27, loss = 1.3589129821411916e-13
c613-111: Epoch: 0, Step: 259, Rank: 2, loss = 7.66053886991358e-15
c622-071: Epoch: 0, Step: 259, Rank: 56, loss = 0.0
c621-081: Epoch: 0, Step: 259, Rank: 26, loss = 4.760636329592671e-13
c619-012: Epoch: 0, Step: 259, Rank: 15, loss = 0.0
c613-141: Epoch: 0, Step: 259, Rank: 8, loss = 1.84297022087776e-14
c622-091: Epoch: 0, Step: 259, Rank: 60, loss = 6.314393452555578e-16
c619-022: Epoch: 0, Step: 259, Rank: 17, loss = 5.542110104105285e-23
c622-051: Epoch: 0, Step: 259, Rank: 52, loss = 4.9763185651190145e-21
c622-092: Epoch: 0, Step: 259, Rank: 61, loss = 4.760636329592671e-13
c621-072: Epoch: 0, Step: 259, Rank: 25, loss = 2.7830537874251604e-10
c621-101: Epoch: 0, Step: 259, Rank: 30, loss = 5.182486384480711e-17
c622-082: Epoch: 0, Step: 259, Rank: 59, loss = 2.3245294578089215e-16
c613-122: Epoch: 0, Step: 259, Rank: 5, loss = 6.927791673660977e-13
c622-032: Epoch: 0, Step: 259, Rank: 49, loss = 3.8163916471489756e-16
c621-152: Epoch: 0, Step: 259, Rank: 41, loss = 1.0089706847793423e-12
c622-001: Epoch: 0, Step: 259, Rank: 42, loss = 1.6438553812280427e-38
c622-062: Epoch: 0, Step: 259, Rank: 55, loss = 5.400124791776761e-13
c622-031: Epoch: 0, Step: 259, Rank: 48, loss = 3.213062882423401e-08
c622-042: Epoch: 0, Step: 259, Rank: 51, loss = 7.283063041541027e-14
c621-132: Epoch: 0, Step: 259, Rank: 37, loss = 2.1175823681357508e-19
c613-131: Epoch: 0, Step: 259, Rank: 6, loss = 4.1443854570388794e-08
c622-072: Epoch: 0, Step: 259, Rank: 57, loss = 9.370282327836321e-14
c621-131: Epoch: 0, Step: 259, Rank: 36, loss = 1.5819829215076654e-23
c613-102: Epoch: 0, Step: 259, Rank: 1, loss = 1.857925203976527e-26
c621-112: Epoch: 0, Step: 259, Rank: 33, loss = 1.9806378759312793e-13
c613-142: Epoch: 0, Step: 259, Rank: 9, loss = 7.496324301261813e-24
c621-092: Epoch: 0, Step: 259, Rank: 29, loss = 1.4915713109076023e-10
c613-112: Epoch: 0, Step: 259, Rank: 3, loss = 1.84297022087776e-14
c621-102: Epoch: 0, Step: 259, Rank: 31, loss = 7.44648787076585e-12
c621-141: Epoch: 0, Step: 259, Rank: 38, loss = 3.5017728805541992e-06
c622-011: Epoch: 0, Step: 259, Rank: 44, loss = 5.502442945726216e-11
c621-122: Epoch: 0, Step: 259, Rank: 35, loss = 0.0
c622-022: Epoch: 0, Step: 259, Rank: 47, loss = 6.927791673660977e-13
c621-071: Epoch: 0, Step: 259, Rank: 24, loss = 8.881784197001252e-13
c619-032: Epoch: 0, Step: 259, Rank: 19, loss = 0.0
c622-021: Epoch: 0, Step: 259, Rank: 46, loss = 1.5366822481155396e-07
c621-062: Epoch: 0, Step: 259, Rank: 23, loss = 0.0
c621-052: Epoch: 0, Step: 259, Rank: 21, loss = 1.0408340855860843e-15
c621-061: Epoch: 0, Step: 259, Rank: 22, loss = 1.4051260155412137e-16
c621-121: Epoch: 0, Step: 259, Rank: 34, loss = 6.635317295611287e-17
c619-041: Epoch: 0, Step: 259, Rank: 20, loss = 2.455635694786906e-10
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24609375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.04s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.04s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 260, Rank: 43, loss = 0.000148773193359375
c622-051: Epoch: 0, Step: 260, Rank: 52, loss = 1.318767317570746e-10
c621-132: Epoch: 0, Step: 260, Rank: 37, loss = 1.2656542480726785e-14
c621-111: Epoch: 0, Step: 260, Rank: 32, loss = 4.926614671774132e-16
c613-101: Epoch: 0, Step: 260, Rank: 0, loss = 5.115907697472721e-12
c621-131: Epoch: 0, Step: 260, Rank: 36, loss = 5.3085386753082275e-08
c622-081: Epoch: 0, Step: 260, Rank: 58, loss = 8.66885281955573e-22
c622-052: Epoch: 0, Step: 260, Rank: 53, loss = 2.0057740190981832e-18
c619-021: Epoch: 0, Step: 260, Rank: 16, loss = 5.182486384480711e-17
c622-012: Epoch: 0, Step: 260, Rank: 45, loss = 5.115907697472721e-12
c621-121: Epoch: 0, Step: 260, Rank: 34, loss = 0.0
c621-081: Epoch: 0, Step: 260, Rank: 26, loss = 5.182486384480711e-17
c622-041: Epoch: 0, Step: 260, Rank: 50, loss = 0.69140625
c621-082: Epoch: 0, Step: 260, Rank: 27, loss = 9.549694368615746e-12
c622-001: Epoch: 0, Step: 260, Rank: 42, loss = 2.540190280342358e-13
c621-091: Epoch: 0, Step: 260, Rank: 28, loss = 4.843059286940843e-11
c621-122: Epoch: 0, Step: 260, Rank: 35, loss = 3.54136699749265e-24
c621-072: Epoch: 0, Step: 260, Rank: 25, loss = 1.857925203976527e-26
c621-101: Epoch: 0, Step: 260, Rank: 30, loss = 1.1546753136970622e-17
c622-101: Epoch: 0, Step: 260, Rank: 62, loss = 4.839897155761719e-05
c619-002: Epoch: 0, Step: 260, Rank: 13, loss = 2.204051907791789e-39
c619-031: Epoch: 0, Step: 260, Rank: 18, loss = 2.983724378680108e-16
c622-061: Epoch: 0, Step: 260, Rank: 54, loss = 4.0332320816460765e-17
c619-011: Epoch: 0, Step: 260, Rank: 14, loss = 2.6756374893466273e-14
c621-092: Epoch: 0, Step: 260, Rank: 29, loss = 1.4915713109076023e-10
c622-022: Epoch: 0, Step: 260, Rank: 47, loss = 0.0
c622-042: Epoch: 0, Step: 260, Rank: 51, loss = 2.2851054382044822e-11
c619-001: Epoch: 0, Step: 260, Rank: 12, loss = 3.694822225952521e-13
c621-102: Epoch: 0, Step: 260, Rank: 31, loss = 5.182486384480711e-17
c621-152: Epoch: 0, Step: 260, Rank: 41, loss = 2.7284841053187847e-12
c613-132: Epoch: 0, Step: 260, Rank: 7, loss = 1.5054687148465104e-22
c621-141: Epoch: 0, Step: 260, Rank: 38, loss = 1.895427703857422e-05
c621-142: Epoch: 0, Step: 260, Rank: 39, loss = 1.6209256159527285e-14
c613-152: Epoch: 0, Step: 260, Rank: 11, loss = 0.0
c622-032: Epoch: 0, Step: 260, Rank: 49, loss = 0.0
c622-071: Epoch: 0, Step: 260, Rank: 56, loss = 2.6756374893466273e-14
c613-151: Epoch: 0, Step: 260, Rank: 10, loss = 3.6845933205562065e-20
c622-062: Epoch: 0, Step: 260, Rank: 55, loss = 6.927791673660977e-13
c621-052: Epoch: 0, Step: 260, Rank: 21, loss = 1.1546753136970622e-17
c621-061: Epoch: 0, Step: 260, Rank: 22, loss = 3.441691376337985e-14
c619-032: Epoch: 0, Step: 260, Rank: 19, loss = 5.115907697472721e-12
c621-151: Epoch: 0, Step: 260, Rank: 40, loss = 9.549694368615746e-12
c622-011: Epoch: 0, Step: 260, Rank: 44, loss = 8.083811398051921e-16
c622-031: Epoch: 0, Step: 260, Rank: 48, loss = 6.993104012531504e-18
c622-021: Epoch: 0, Step: 260, Rank: 46, loss = 3.510081114654895e-12
c622-072: Epoch: 0, Step: 260, Rank: 57, loss = 1.6540288925170898e-06
c619-012: Epoch: 0, Step: 260, Rank: 15, loss = 1.3322676295501878e-15
c619-022: Epoch: 0, Step: 260, Rank: 17, loss = 1.3322676295501878e-15
c622-082: Epoch: 0, Step: 260, Rank: 59, loss = 2.439454888092385e-17
c622-092: Epoch: 0, Step: 260, Rank: 61, loss = 1.150369644165039e-05
c621-071: Epoch: 0, Step: 260, Rank: 24, loss = 5.182486384480711e-17
c621-112: Epoch: 0, Step: 260, Rank: 33, loss = 0.69140625
c613-111: Epoch: 0, Step: 260, Rank: 2, loss = 6.693881005048752e-10
c613-102: Epoch: 0, Step: 260, Rank: 1, loss = 6.565414878423326e-12
c619-041: Epoch: 0, Step: 260, Rank: 20, loss = 3.583409124985337e-10
c613-141: Epoch: 0, Step: 260, Rank: 8, loss = 6.927791673660977e-13
c613-121: Epoch: 0, Step: 260, Rank: 4, loss = 2.1736923372372985e-10
c621-062: Epoch: 0, Step: 260, Rank: 23, loss = 0.0
c613-131: Epoch: 0, Step: 260, Rank: 6, loss = 6.439293542825908e-14
c622-091: Epoch: 0, Step: 260, Rank: 60, loss = 2.831068712794149e-15
c613-142: Epoch: 0, Step: 260, Rank: 9, loss = 2.6756374893466273e-14
c613-122: Epoch: 0, Step: 260, Rank: 5, loss = 0.69140625
c613-112: Epoch: 0, Step: 260, Rank: 3, loss = 0.0
c622-102: Epoch: 0, Step: 260, Rank: 63, loss = 9.370282327836321e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6946.75439453125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.06s, TFLOPs: 0.92, Samples/sec: 0.49, Time/seq 2.06s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 261, Rank: 0, loss = 8.881784197001252e-13
c621-132: Epoch: 0, Step: 261, Rank: 37, loss = 1.3589129821411916e-13
c621-151: Epoch: 0, Step: 261, Rank: 40, loss = 7.66053886991358e-15
c621-111: Epoch: 0, Step: 261, Rank: 32, loss = 3.8163916471489756e-16
c621-141: Epoch: 0, Step: 261, Rank: 38, loss = 0.69140625
c621-142: Epoch: 0, Step: 261, Rank: 39, loss = 3.6845933205562065e-20
c621-131: Epoch: 0, Step: 261, Rank: 36, loss = 6.973743438720703e-06
c621-091: Epoch: 0, Step: 261, Rank: 28, loss = 3.841705620288849e-09
c622-101: Epoch: 0, Step: 261, Rank: 62, loss = 3.8163916471489756e-16
c619-001: Epoch: 0, Step: 261, Rank: 12, loss = 2.4158453015843406e-12
c621-101: Epoch: 0, Step: 261, Rank: 30, loss = 0.0
c621-081: Epoch: 0, Step: 261, Rank: 26, loss = 2.831068712794149e-15
c622-092: Epoch: 0, Step: 261, Rank: 61, loss = 3.46451997756958e-07
c622-002: Epoch: 0, Step: 261, Rank: 43, loss = 4.843059286940843e-11
c622-102: Epoch: 0, Step: 261, Rank: 63, loss = 0.69140625
c621-122: Epoch: 0, Step: 261, Rank: 35, loss = 1.2656542480726785e-14
c613-132: Epoch: 0, Step: 261, Rank: 7, loss = 1.6079866327345371e-09
c613-152: Epoch: 0, Step: 261, Rank: 11, loss = 7.792703114739563e-20
c622-052: Epoch: 0, Step: 261, Rank: 53, loss = 1.3322676295501878e-15
c622-012: Epoch: 0, Step: 261, Rank: 45, loss = 0.69140625
c613-122: Epoch: 0, Step: 261, Rank: 5, loss = 1.9081958235744878e-17
c613-102: Epoch: 0, Step: 261, Rank: 1, loss = 2.1047890186309814e-07
c613-151: Epoch: 0, Step: 261, Rank: 10, loss = 7.188646122813225e-09
c613-111: Epoch: 0, Step: 261, Rank: 2, loss = 2.2065682614424986e-15
c621-112: Epoch: 0, Step: 261, Rank: 33, loss = 3.213062882423401e-08
c619-021: Epoch: 0, Step: 261, Rank: 16, loss = 0.039794921875
c619-002: Epoch: 0, Step: 261, Rank: 13, loss = 4.760636329592671e-13
c621-102: Epoch: 0, Step: 261, Rank: 31, loss = 1.8041124150158794e-16
c622-001: Epoch: 0, Step: 261, Rank: 42, loss = 2.0236257114447653e-11
c622-082: Epoch: 0, Step: 261, Rank: 59, loss = 3.635980405647388e-15
c619-022: Epoch: 0, Step: 261, Rank: 17, loss = 2.8919009696678116e-25
c619-011: Epoch: 0, Step: 261, Rank: 14, loss = 2.831068712794149e-15
c621-092: Epoch: 0, Step: 261, Rank: 29, loss = 1.7139067942650854e-15
c621-082: Epoch: 0, Step: 261, Rank: 27, loss = 3.510081114654895e-12
c621-072: Epoch: 0, Step: 261, Rank: 25, loss = 7.66053886991358e-15
c613-112: Epoch: 0, Step: 261, Rank: 3, loss = 0.0
c613-121: Epoch: 0, Step: 261, Rank: 4, loss = 2.6756374893466273e-14
c622-081: Epoch: 0, Step: 261, Rank: 58, loss = 8.083811398051921e-16
c621-152: Epoch: 0, Step: 261, Rank: 41, loss = 6.565414878423326e-12
c622-051: Epoch: 0, Step: 261, Rank: 52, loss = 4.418687638008123e-14
c619-031: Epoch: 0, Step: 261, Rank: 18, loss = 1.0277290130034089e-10
c621-061: Epoch: 0, Step: 261, Rank: 22, loss = 2.3245294578089215e-16
c622-032: Epoch: 0, Step: 261, Rank: 49, loss = 1.7497114868092467e-13
c613-142: Epoch: 0, Step: 261, Rank: 9, loss = 6.993104012531504e-18
c622-041: Epoch: 0, Step: 261, Rank: 50, loss = 6.314393452555578e-16
c622-022: Epoch: 0, Step: 261, Rank: 47, loss = 1.9806378759312793e-13
c622-031: Epoch: 0, Step: 261, Rank: 48, loss = 0.69140625
c613-131: Epoch: 0, Step: 261, Rank: 6, loss = 5.0182080713057076e-14
c622-061: Epoch: 0, Step: 261, Rank: 54, loss = 2.3647750424515834e-14
c613-141: Epoch: 0, Step: 261, Rank: 8, loss = 7.338821887969971e-07
c622-091: Epoch: 0, Step: 261, Rank: 60, loss = 1.0089706847793423e-12
c622-021: Epoch: 0, Step: 261, Rank: 46, loss = 1.8758328224066645e-12
c621-071: Epoch: 0, Step: 261, Rank: 24, loss = 4.926614671774132e-16
c619-032: Epoch: 0, Step: 261, Rank: 19, loss = 0.0
c621-121: Epoch: 0, Step: 261, Rank: 34, loss = 1.318767317570746e-10
c619-012: Epoch: 0, Step: 261, Rank: 15, loss = 1.6250033905767303e-33
c622-071: Epoch: 0, Step: 261, Rank: 56, loss = 5.227781471335135e-22
c622-042: Epoch: 0, Step: 261, Rank: 51, loss = 2.9331204132176936e-11
c622-072: Epoch: 0, Step: 261, Rank: 57, loss = 3.694822225952521e-13
c621-062: Epoch: 0, Step: 261, Rank: 23, loss = 0.000335693359375
c619-041: Epoch: 0, Step: 261, Rank: 20, loss = 0.0
c622-011: Epoch: 0, Step: 261, Rank: 44, loss = 3.268496584496461e-13
c621-052: Epoch: 0, Step: 261, Rank: 21, loss = 0.0
c622-062: Epoch: 0, Step: 261, Rank: 55, loss = 0.0
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.2431640625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.01s, TFLOPs: 0.94, Samples/sec: 0.50, Time/seq 2.01s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 262, Rank: 0, loss = 3.144186300207963e-17
c622-102: Epoch: 0, Step: 262, Rank: 63, loss = 1.4637180356658064e-12
c613-102: Epoch: 0, Step: 262, Rank: 1, loss = 6.716706573930585e-22
c622-101: Epoch: 0, Step: 262, Rank: 62, loss = 0.0888671875
c622-092: Epoch: 0, Step: 262, Rank: 61, loss = 1.1546753136970622e-17
c622-082: Epoch: 0, Step: 262, Rank: 59, loss = 4.418687638008123e-14
c622-081: Epoch: 0, Step: 262, Rank: 58, loss = 2.3647750424515834e-14
c622-091: Epoch: 0, Step: 262, Rank: 60, loss = 1.9081958235744878e-17
c622-072: Epoch: 0, Step: 262, Rank: 57, loss = 1.0089706847793423e-12
c613-111: Epoch: 0, Step: 262, Rank: 2, loss = 2.831068712794149e-15
c622-062: Epoch: 0, Step: 262, Rank: 55, loss = 1.3322676295501878e-15
c622-071: Epoch: 0, Step: 262, Rank: 56, loss = 1.053497228147536e-20
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c622-052: Epoch: 0, Step: 262, Rank: 53, loss = 2.6756374893466273e-14
c622-061: Epoch: 0, Step: 262, Rank: 54, loss = 3.8163916471489756e-16
c613-112: Epoch: 0, Step: 262, Rank: 3, loss = 2.0057740190981832e-18
c622-051: Epoch: 0, Step: 262, Rank: 52, loss = 9.74978320300579e-10
c622-042: Epoch: 0, Step: 262, Rank: 51, loss = 8.205631676526035e-21
c622-041: Epoch: 0, Step: 262, Rank: 50, loss = 3.144186300207963e-17
c622-032: Epoch: 0, Step: 262, Rank: 49, loss = 3.864587821847745e-21
c622-022: Epoch: 0, Step: 262, Rank: 47, loss = 1.1399388313293457e-06
c622-031: Epoch: 0, Step: 262, Rank: 48, loss = 7.66053886991358e-15
c613-121: Epoch: 0, Step: 262, Rank: 4, loss = 8.083811398051921e-16
c622-002: Epoch: 0, Step: 262, Rank: 43, loss = 1.485356976305141e-17
c622-012: Epoch: 0, Step: 262, Rank: 45, loss = 1.9806378759312793e-13
c622-021: Epoch: 0, Step: 262, Rank: 46, loss = 8.66885281955573e-22
c622-011: Epoch: 0, Step: 262, Rank: 44, loss = 1.0089706847793423e-12
c622-001: Epoch: 0, Step: 262, Rank: 42, loss = 8.003553375601768e-11
c621-142: Epoch: 0, Step: 262, Rank: 39, loss = 0.0
c621-152: Epoch: 0, Step: 262, Rank: 41, loss = 0.69140625
c613-122: Epoch: 0, Step: 262, Rank: 5, loss = 2.540190280342358e-13
c621-151: Epoch: 0, Step: 262, Rank: 40, loss = 6.5635692504717e-31
c621-132: Epoch: 0, Step: 262, Rank: 37, loss = 4.7222086809427244e-20
c621-141: Epoch: 0, Step: 262, Rank: 38, loss = 6.993104012531504e-18
c621-131: Epoch: 0, Step: 262, Rank: 36, loss = 3.510081114654895e-12
c621-122: Epoch: 0, Step: 262, Rank: 35, loss = 1.9190338207408786e-10
c613-131: Epoch: 0, Step: 262, Rank: 6, loss = 1.0800249583553523e-11
c621-111: Epoch: 0, Step: 262, Rank: 32, loss = 0.10498046875
c621-121: Epoch: 0, Step: 262, Rank: 34, loss = 2.4318695068359375e-05
c621-112: Epoch: 0, Step: 262, Rank: 33, loss = 3.441691376337985e-14
c621-102: Epoch: 0, Step: 262, Rank: 31, loss = 7.566995918750763e-10
c621-101: Epoch: 0, Step: 262, Rank: 30, loss = 1.0477378964424133e-08
c613-132: Epoch: 0, Step: 262, Rank: 7, loss = 0.0
c621-092: Epoch: 0, Step: 262, Rank: 29, loss = 3.510081114654895e-12
c621-091: Epoch: 0, Step: 262, Rank: 28, loss = 1.955777406692505e-08
c621-082: Epoch: 0, Step: 262, Rank: 27, loss = 1.9081958235744878e-17
c621-072: Epoch: 0, Step: 262, Rank: 25, loss = 4.274625098332763e-11
c621-081: Epoch: 0, Step: 262, Rank: 26, loss = 1.0408340855860843e-15
c621-071: Epoch: 0, Step: 262, Rank: 24, loss = 0.0
c613-141: Epoch: 0, Step: 262, Rank: 8, loss = 0.0
c621-062: Epoch: 0, Step: 262, Rank: 23, loss = 1.7139067942650854e-15
c619-041: Epoch: 0, Step: 262, Rank: 20, loss = 8.66885281955573e-22
c621-061: Epoch: 0, Step: 262, Rank: 22, loss = 1.2931877790833823e-12
c621-052: Epoch: 0, Step: 262, Rank: 21, loss = 0.0
c619-031: Epoch: 0, Step: 262, Rank: 18, loss = 3.774403012357652e-11
c619-032: Epoch: 0, Step: 262, Rank: 19, loss = 7.048583938740194e-11
c619-011: Epoch: 0, Step: 262, Rank: 14, loss = 4.418687638008123e-14
c619-021: Epoch: 0, Step: 262, Rank: 16, loss = 8.881784197001252e-13
c619-001: Epoch: 0, Step: 262, Rank: 12, loss = 4.418687638008123e-14
c619-022: Epoch: 0, Step: 262, Rank: 17, loss = 0.0
c619-002: Epoch: 0, Step: 262, Rank: 13, loss = 2.8587361969832636e-20
c613-151: Epoch: 0, Step: 262, Rank: 10, loss = 0.0
c613-142: Epoch: 0, Step: 262, Rank: 9, loss = 4.926614671774132e-16
c613-152: Epoch: 0, Step: 262, Rank: 11, loss = 3.694822225952521e-13
c619-012: Epoch: 0, Step: 262, Rank: 15, loss = 0.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
c622-002: Epoch: 0, Step: 263, Rank: 43, loss = 6.635317295611287e-17
c622-012: Epoch: 0, Step: 263, Rank: 45, loss = 3.635980405647388e-15
c621-111: Epoch: 0, Step: 263, Rank: 32, loss = 2.9331204132176936e-11
c621-132: Epoch: 0, Step: 263, Rank: 37, loss = 0.69140625
c621-081: Epoch: 0, Step: 263, Rank: 26, loss = 1.485356976305141e-17
c622-052: Epoch: 0, Step: 263, Rank: 53, loss = 3.510081114654895e-12
c621-091: Epoch: 0, Step: 263, Rank: 28, loss = 9.098986738083304e-23
c621-112: Epoch: 0, Step: 263, Rank: 33, loss = 8.66885281955573e-22
c622-001: Epoch: 0, Step: 263, Rank: 42, loss = 2.066371962428093e-09
c621-151: Epoch: 0, Step: 263, Rank: 40, loss = 4.760636329592671e-13
c621-152: Epoch: 0, Step: 263, Rank: 41, loss = 7.283063041541027e-14
c622-031: Epoch: 0, Step: 263, Rank: 48, loss = 1.7848833522293717e-11
c621-082: Epoch: 0, Step: 263, Rank: 27, loss = 3.017554874593445e-21
c622-011: Epoch: 0, Step: 263, Rank: 44, loss = 1.6438553812280427e-38
c621-072: Epoch: 0, Step: 263, Rank: 25, loss = 1.9081958235744878e-17
c621-131: Epoch: 0, Step: 263, Rank: 36, loss = 0.69140625
c622-021: Epoch: 0, Step: 263, Rank: 46, loss = 2.6756374893466273e-14
c619-001: Epoch: 0, Step: 263, Rank: 12, loss = 0.69140625
c621-121: Epoch: 0, Step: 263, Rank: 34, loss = 2.0236257114447653e-11
c622-051: Epoch: 0, Step: 263, Rank: 52, loss = 1.8041124150158794e-16
c613-101: Epoch: 0, Step: 263, Rank: 0, loss = 3.1763735522036263e-22
c622-022: Epoch: 0, Step: 263, Rank: 47, loss = 1.4637180356658064e-12
c622-092: Epoch: 0, Step: 263, Rank: 61, loss = 4.405564747785802e-33
c619-022: Epoch: 0, Step: 263, Rank: 17, loss = 8.003553375601768e-11
c619-041: Epoch: 0, Step: 263, Rank: 20, loss = 9.370282327836321e-14
c619-031: Epoch: 0, Step: 263, Rank: 18, loss = 1.9806378759312793e-13
c621-142: Epoch: 0, Step: 263, Rank: 39, loss = 2.2065682614424986e-15
c619-021: Epoch: 0, Step: 263, Rank: 16, loss = 9.549694368615746e-12
c622-041: Epoch: 0, Step: 263, Rank: 50, loss = 0.00811767578125
c622-081: Epoch: 0, Step: 263, Rank: 58, loss = 0.69140625
c621-101: Epoch: 0, Step: 263, Rank: 30, loss = 4.843059286940843e-11
c621-122: Epoch: 0, Step: 263, Rank: 35, loss = 5.400124791776761e-13
c622-042: Epoch: 0, Step: 263, Rank: 51, loss = 2.342858351767063e-09
c622-101: Epoch: 0, Step: 263, Rank: 62, loss = 4.00543212890625e-05
c621-141: Epoch: 0, Step: 263, Rank: 38, loss = 1.0089706847793423e-12
c621-061: Epoch: 0, Step: 263, Rank: 22, loss = 0.69140625
c619-002: Epoch: 0, Step: 263, Rank: 13, loss = 0.69140625
c613-121: Epoch: 0, Step: 263, Rank: 4, loss = 1.0408340855860843e-15
c621-071: Epoch: 0, Step: 263, Rank: 24, loss = 3.583409124985337e-10
c621-102: Epoch: 0, Step: 263, Rank: 31, loss = 1.4051260155412137e-16
c613-132: Epoch: 0, Step: 263, Rank: 7, loss = 1.9983403963978888e-37
c613-111: Epoch: 0, Step: 263, Rank: 2, loss = 1.7848833522293717e-11
c613-152: Epoch: 0, Step: 263, Rank: 11, loss = 7.66053886991358e-15
c622-102: Epoch: 0, Step: 263, Rank: 63, loss = 8.083811398051921e-16
c622-061: Epoch: 0, Step: 263, Rank: 54, loss = 0.69140625
c622-032: Epoch: 0, Step: 263, Rank: 49, loss = 3.6845933205562065e-20
c621-092: Epoch: 0, Step: 263, Rank: 29, loss = 4.0332320816460765e-17
c621-062: Epoch: 0, Step: 263, Rank: 23, loss = 1.7848833522293717e-11
c619-011: Epoch: 0, Step: 263, Rank: 14, loss = 1.9190338207408786e-10
c621-052: Epoch: 0, Step: 263, Rank: 21, loss = 4.760636329592671e-13
c622-071: Epoch: 0, Step: 263, Rank: 56, loss = 0.0005035400390625
c622-072: Epoch: 0, Step: 263, Rank: 57, loss = 0.0
c619-032: Epoch: 0, Step: 263, Rank: 19, loss = 1.3869794202037156e-11
c619-012: Epoch: 0, Step: 263, Rank: 15, loss = 4.760636329592671e-13
c613-141: Epoch: 0, Step: 263, Rank: 8, loss = 1.0089706847793423e-12
c613-102: Epoch: 0, Step: 263, Rank: 1, loss = 7.66053886991358e-15
c622-091: Epoch: 0, Step: 263, Rank: 60, loss = 0.0
c613-142: Epoch: 0, Step: 263, Rank: 9, loss = 1.2197274440461925e-18
c622-062: Epoch: 0, Step: 263, Rank: 55, loss = 2.7830537874251604e-10
c613-112: Epoch: 0, Step: 263, Rank: 3, loss = 5.115907697472721e-12
c613-151: Epoch: 0, Step: 263, Rank: 10, loss = 7.44648787076585e-12
c622-082: Epoch: 0, Step: 263, Rank: 59, loss = 2.831068712794149e-15
c613-131: Epoch: 0, Step: 263, Rank: 6, loss = 2.234049398383217e-20
c613-122: Epoch: 0, Step: 263, Rank: 5, loss = 4.418687638008123e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24462890625 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 264, Rank: 0, loss = 4.418687638008123e-14
c621-111: Epoch: 0, Step: 264, Rank: 32, loss = 1.199040866595169e-13
c622-002: Epoch: 0, Step: 264, Rank: 43, loss = 3.049420715222343e-26
c621-081: Epoch: 0, Step: 264, Rank: 26, loss = 1.166324663699769e-22
c622-081: Epoch: 0, Step: 264, Rank: 58, loss = 1.2790197503539935e-19
c621-112: Epoch: 0, Step: 264, Rank: 33, loss = 9.370282327836321e-14
c621-121: Epoch: 0, Step: 264, Rank: 34, loss = 1.0277290130034089e-10
c622-102: Epoch: 0, Step: 264, Rank: 63, loss = 1.199040866595169e-13
c622-101: Epoch: 0, Step: 264, Rank: 62, loss = 0.69140625
c621-091: Epoch: 0, Step: 264, Rank: 28, loss = 2.3647750424515834e-14
c621-132: Epoch: 0, Step: 264, Rank: 37, loss = 2.1175823681357508e-19
c622-012: Epoch: 0, Step: 264, Rank: 45, loss = 8.205631676526035e-21
c621-151: Epoch: 0, Step: 264, Rank: 40, loss = 1.4637180356658064e-12
c621-131: Epoch: 0, Step: 264, Rank: 36, loss = 1.7139067942650854e-15
c621-142: Epoch: 0, Step: 264, Rank: 39, loss = 3.510081114654895e-12
c622-001: Epoch: 0, Step: 264, Rank: 42, loss = 9.370282327836321e-14
c622-052: Epoch: 0, Step: 264, Rank: 53, loss = 1.4051260155412137e-16
c619-001: Epoch: 0, Step: 264, Rank: 12, loss = 2.3647750424515834e-14
c622-092: Epoch: 0, Step: 264, Rank: 61, loss = 2.540190280342358e-13
c621-122: Epoch: 0, Step: 264, Rank: 35, loss = 1.0408340855860843e-15
c621-082: Epoch: 0, Step: 264, Rank: 27, loss = 4.0332320816460765e-17
c619-041: Epoch: 0, Step: 264, Rank: 20, loss = 9.549694368615746e-12
c621-101: Epoch: 0, Step: 264, Rank: 30, loss = 1.1988913903810543e-32
c613-142: Epoch: 0, Step: 264, Rank: 9, loss = 0.0
c613-121: Epoch: 0, Step: 264, Rank: 4, loss = 0.0
c613-151: Epoch: 0, Step: 264, Rank: 10, loss = 7.66053886991358e-15
c613-141: Epoch: 0, Step: 264, Rank: 8, loss = 2.831068712794149e-15
c613-132: Epoch: 0, Step: 264, Rank: 7, loss = 7.048583938740194e-11
c621-152: Epoch: 0, Step: 264, Rank: 41, loss = 2.3647750424515834e-14
c622-071: Epoch: 0, Step: 264, Rank: 56, loss = 4.6629367034256575e-15
c619-021: Epoch: 0, Step: 264, Rank: 16, loss = 3.979039320256561e-12
c621-061: Epoch: 0, Step: 264, Rank: 22, loss = 0.0
c613-131: Epoch: 0, Step: 264, Rank: 6, loss = 2.6756374893466273e-14
c619-022: Epoch: 0, Step: 264, Rank: 17, loss = 2.439454888092385e-17
c619-002: Epoch: 0, Step: 264, Rank: 13, loss = 4.926614671774132e-16
c622-031: Epoch: 0, Step: 264, Rank: 48, loss = 0.0
c621-141: Epoch: 0, Step: 264, Rank: 38, loss = 1.857925203976527e-26
c621-071: Epoch: 0, Step: 264, Rank: 24, loss = 2.2118911147117615e-08
c622-072: Epoch: 0, Step: 264, Rank: 57, loss = 1.44393100091654e-26
c621-092: Epoch: 0, Step: 264, Rank: 29, loss = 1.9806378759312793e-13
c613-111: Epoch: 0, Step: 264, Rank: 2, loss = 2.540190280342358e-13
c622-041: Epoch: 0, Step: 264, Rank: 50, loss = 2.439454888092385e-17
c613-152: Epoch: 0, Step: 264, Rank: 11, loss = 6.565414878423326e-12
c619-031: Epoch: 0, Step: 264, Rank: 18, loss = 2.0057740190981832e-18
c621-052: Epoch: 0, Step: 264, Rank: 21, loss = 0.69140625
c621-102: Epoch: 0, Step: 264, Rank: 31, loss = 4.05634636990726e-10
c619-011: Epoch: 0, Step: 264, Rank: 14, loss = 3.8163916471489756e-16
c613-122: Epoch: 0, Step: 264, Rank: 5, loss = 5.115907697472721e-12
c622-091: Epoch: 0, Step: 264, Rank: 60, loss = 1.0408340855860843e-15
c613-102: Epoch: 0, Step: 264, Rank: 1, loss = 6.230038707144558e-11
c619-012: Epoch: 0, Step: 264, Rank: 15, loss = 1.0800249583553523e-11
c622-051: Epoch: 0, Step: 264, Rank: 52, loss = 7.66053886991358e-15
c622-032: Epoch: 0, Step: 264, Rank: 49, loss = 1.84297022087776e-14
c613-112: Epoch: 0, Step: 264, Rank: 3, loss = 1.0408340855860843e-15
c622-011: Epoch: 0, Step: 264, Rank: 44, loss = 0.69140625
c622-022: Epoch: 0, Step: 264, Rank: 47, loss = 8.881784197001252e-13
c619-032: Epoch: 0, Step: 264, Rank: 19, loss = 4.472333961502706e-19
c622-082: Epoch: 0, Step: 264, Rank: 59, loss = 4.0862722260119567e-22
c622-062: Epoch: 0, Step: 264, Rank: 55, loss = 0.0
c622-042: Epoch: 0, Step: 264, Rank: 51, loss = 5.400124791776761e-13
c622-061: Epoch: 0, Step: 264, Rank: 54, loss = 0.0
c621-062: Epoch: 0, Step: 264, Rank: 23, loss = 1.6209256159527285e-14
c622-021: Epoch: 0, Step: 264, Rank: 46, loss = 0.69140625
c621-072: Epoch: 0, Step: 264, Rank: 25, loss = 1.6209256159527285e-14
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.24365234375 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.07s, TFLOPs: 0.91, Samples/sec: 0.48, Time/seq 2.07s, Batch Size: 1, Sequence Length: 2048
c613-101: Epoch: 0, Step: 265, Rank: 0, loss = 0.0
c622-102: Epoch: 0, Step: 265, Rank: 63, loss = 2.831068712794149e-15
c622-081: Epoch: 0, Step: 265, Rank: 58, loss = 2.574980159653073e-18
c613-132: Epoch: 0, Step: 265, Rank: 7, loss = 4.255493527005605e-18
c622-092: Epoch: 0, Step: 265, Rank: 61, loss = 3.510081114654895e-12
c613-131: Epoch: 0, Step: 265, Rank: 6, loss = 8.881784197001252e-13
c622-101: Epoch: 0, Step: 265, Rank: 62, loss = 1.2514647096395493e-09
c619-021: Epoch: 0, Step: 265, Rank: 16, loss = 9.486769009248164e-19
c613-122: Epoch: 0, Step: 265, Rank: 5, loss = 0.0
c619-001: Epoch: 0, Step: 265, Rank: 12, loss = 3.4051481634378433e-09
c613-121: Epoch: 0, Step: 265, Rank: 4, loss = 0.69140625
c622-002: Epoch: 0, Step: 265, Rank: 43, loss = 2.0057740190981832e-18
c613-112: Epoch: 0, Step: 265, Rank: 3, loss = 7.44648787076585e-12
c621-111: Epoch: 0, Step: 265, Rank: 32, loss = 5.400124791776761e-13
c619-022: Epoch: 0, Step: 265, Rank: 17, loss = 1.9563750449481093e-27
c619-011: Epoch: 0, Step: 265, Rank: 14, loss = 2.1454997138094155e-24
c613-151: Epoch: 0, Step: 265, Rank: 10, loss = 1.3213420162451948e-29
c622-082: Epoch: 0, Step: 265, Rank: 59, loss = 0.0
c622-052: Epoch: 0, Step: 265, Rank: 53, loss = 1.318767317570746e-10
c613-111: Epoch: 0, Step: 265, Rank: 2, loss = 4.255493527005605e-18
c622-012: Epoch: 0, Step: 265, Rank: 45, loss = 2.983724378680108e-16
c622-062: Epoch: 0, Step: 265, Rank: 55, loss = 3.268496584496461e-13
c613-152: Epoch: 0, Step: 265, Rank: 11, loss = 1.3322676295501878e-15
c621-132: Epoch: 0, Step: 265, Rank: 37, loss = 1.485356976305141e-17
c621-151: Epoch: 0, Step: 265, Rank: 40, loss = 2.4158453015843406e-12
c621-131: Epoch: 0, Step: 265, Rank: 36, loss = 7.048583938740194e-11
c622-051: Epoch: 0, Step: 265, Rank: 52, loss = 1.1641532182693481e-10
c621-121: Epoch: 0, Step: 265, Rank: 34, loss = 3.655441105365753e-08
c622-001: Epoch: 0, Step: 265, Rank: 42, loss = 7.44648787076585e-12
c622-072: Epoch: 0, Step: 265, Rank: 57, loss = 3.8163916471489756e-16
c622-041: Epoch: 0, Step: 265, Rank: 50, loss = 1.6079866327345371e-09
c622-031: Epoch: 0, Step: 265, Rank: 48, loss = 1.150369644165039e-05
c619-012: Epoch: 0, Step: 265, Rank: 15, loss = 2.6756374893466273e-14
c622-022: Epoch: 0, Step: 265, Rank: 47, loss = 2.8919009696678116e-25
c619-002: Epoch: 0, Step: 265, Rank: 13, loss = 8.881784197001252e-13
c613-141: Epoch: 0, Step: 265, Rank: 8, loss = 3.583409124985337e-10
c621-142: Epoch: 0, Step: 265, Rank: 39, loss = 0.0
c622-091: Epoch: 0, Step: 265, Rank: 60, loss = 6.439293542825908e-14
c621-152: Epoch: 0, Step: 265, Rank: 41, loss = 5.617039278149605e-09
c621-141: Epoch: 0, Step: 265, Rank: 38, loss = 7.048583938740194e-11
c622-032: Epoch: 0, Step: 265, Rank: 49, loss = 3.1650415621697903e-10
c619-031: Epoch: 0, Step: 265, Rank: 18, loss = 2.6056189295420372e-23
c622-042: Epoch: 0, Step: 265, Rank: 51, loss = 4.05634636990726e-10
c622-021: Epoch: 0, Step: 265, Rank: 46, loss = 3.694822225952521e-13
c622-071: Epoch: 0, Step: 265, Rank: 56, loss = 6.565414878423326e-12
c621-101: Epoch: 0, Step: 265, Rank: 30, loss = 0.0
c619-032: Epoch: 0, Step: 265, Rank: 19, loss = 4.418687638008123e-14
c621-122: Epoch: 0, Step: 265, Rank: 35, loss = 4.274625098332763e-11
c613-142: Epoch: 0, Step: 265, Rank: 9, loss = 1.2790197503539935e-19
c621-082: Epoch: 0, Step: 265, Rank: 27, loss = 6.993104012531504e-18
c621-102: Epoch: 0, Step: 265, Rank: 31, loss = 4.843059286940843e-11
c622-061: Epoch: 0, Step: 265, Rank: 54, loss = 5.182486384480711e-17
c621-052: Epoch: 0, Step: 265, Rank: 21, loss = 8.003553375601768e-11
c622-011: Epoch: 0, Step: 265, Rank: 44, loss = 8.083811398051921e-16
c621-072: Epoch: 0, Step: 265, Rank: 25, loss = 2.9331204132176936e-11
c621-081: Epoch: 0, Step: 265, Rank: 26, loss = 3.841705620288849e-09
c621-112: Epoch: 0, Step: 265, Rank: 33, loss = 0.0
c621-091: Epoch: 0, Step: 265, Rank: 28, loss = 1.6250033905767303e-33
c619-041: Epoch: 0, Step: 265, Rank: 20, loss = 1.7497114868092467e-13
c613-102: Epoch: 0, Step: 265, Rank: 1, loss = 7.283063041541027e-14
c621-061: Epoch: 0, Step: 265, Rank: 22, loss = 1.0277290130034089e-10
c621-092: Epoch: 0, Step: 265, Rank: 29, loss = 7.386127300057499e-19
c621-071: Epoch: 0, Step: 265, Rank: 24, loss = 1.6079866327345371e-09
c621-062: Epoch: 0, Step: 265, Rank: 23, loss = 1.1546753136970622e-17
c613-101: [Rank 0] Training memory (MB) | allocated: 6947.25048828125 | max allocated: 11957.24169921875 | reserved: 22638.0 | max reserved: 22638.0
c613-101: Model Parameters: 7.505 B, Latency: 2.03s, TFLOPs: 0.93, Samples/sec: 0.49, Time/seq 2.03s, Batch Size: 1, Sequence Length: 2048
slurmstepd: error: *** JOB 144585 ON c613-101 CANCELLED AT 2025-03-01T18:01:36 DUE TO TIME LIMIT ***
